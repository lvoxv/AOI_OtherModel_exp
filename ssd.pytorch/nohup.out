/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
/home/xj/xu/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  self.priors = Variable(self.priorbox.forward(), volatile=True)
/home/xj/xu/ssd.pytorch/train.py:217: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  init.xavier_uniform(param)
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading base network...
Initializing weights...
Loading the dataset...
Training SSD on: MS COCO
Using the specified args:
Namespace(dataset='COCO', dataset_root='/home/xj/xu/ssd.pytorch/COCO_250423_the_thrid_optimization_datasets', basenet='vgg16_reducedfc.pth', batch_size=32, resume=None, start_iter=0, num_workers=4, cuda=True, lr=0.001, momentum=0.9, weight_decay=0.0005, gamma=0.1, visdom=False, save_folder='weights/')
Traceback (most recent call last):
  File "/home/xj/xu/ssd.pytorch/train.py", line 258, in <module>
    train()
  File "/home/xj/xu/ssd.pytorch/train.py", line 153, in train
    batch_iterator = iter(data_loader)
                     ^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 439, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 387, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1085, in __init__
    self._reset(loader, first_iter=True)
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1118, in _reset
    self._try_put_index()
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1352, in _try_put_index
    index = self._next_index()
            ^^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 621, in _next_index
    return next(self._sampler_iter)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/sampler.py", line 287, in __iter__
    for idx in self.sampler:
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/sampler.py", line 167, in __iter__
    yield from torch.randperm(n, generator=generator).tolist()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected a 'cuda' device type for generator but found 'cpu'
/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
/home/xj/xu/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  self.priors = Variable(self.priorbox.forward(), volatile=True)
/home/xj/xu/ssd.pytorch/train.py:218: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  init.xavier_uniform(param)
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading base network...
Initializing weights...
Loading the dataset...
Training SSD on: MS COCO
Using the specified args:
Namespace(dataset='COCO', dataset_root='/home/xj/xu/ssd.pytorch/COCO_250423_the_thrid_optimization_datasets', basenet='vgg16_reducedfc.pth', batch_size=32, resume=None, start_iter=0, num_workers=4, cuda=True, lr=0.001, momentum=0.9, weight_decay=0.0005, gamma=0.1, visdom=False, save_folder='weights/')
Traceback (most recent call last):
  File "/home/xj/xu/ssd.pytorch/train.py", line 259, in <module>
    train()
  File "/home/xj/xu/ssd.pytorch/train.py", line 169, in train
    images, targets = next(batch_iterator)
                      ^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
    return self._process_data(data)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
    data.reraise()
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/_utils.py", line 722, in reraise
    raise exception
ValueError: Caught ValueError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "/home/xj/xu/ssd.pytorch/data/coco.py", line 106, in __getitem__
    im, gt, h, w = self.pull_item(index)
                   ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/xu/ssd.pytorch/data/coco.py", line 133, in pull_item
    img, boxes, labels = self.transform(img, target[:, :4],
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/xu/ssd.pytorch/utils/augmentations.py", line 417, in __call__
    return self.augment(img, boxes, labels)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/xu/ssd.pytorch/utils/augmentations.py", line 52, in __call__
    img, boxes, labels = t(img, boxes, labels)
                         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/xu/ssd.pytorch/utils/augmentations.py", line 238, in __call__
    mode = random.choice(self.sample_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "numpy/random/mtrand.pyx", line 937, in numpy.random.mtrand.RandomState.choice
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6,) + inhomogeneous part.

/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
/home/xj/xu/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  self.priors = Variable(self.priorbox.forward(), volatile=True)
/home/xj/xu/ssd.pytorch/train.py:218: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  init.xavier_uniform(param)
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading base network...
Initializing weights...
Loading the dataset...
Training SSD on: MS COCO
Using the specified args:
Namespace(dataset='COCO', dataset_root='/home/xj/xu/ssd.pytorch/COCO_250423_the_thrid_optimization_datasets', basenet='vgg16_reducedfc.pth', batch_size=32, resume=None, start_iter=0, num_workers=4, cuda=True, lr=0.001, momentum=0.9, weight_decay=0.0005, gamma=0.1, visdom=False, save_folder='weights/')
/home/xj/xu/ssd.pytorch/train.py:173: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  targets = [Variable(ann.cuda(), volatile=True) for ann in targets]
/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
Traceback (most recent call last):
  File "/home/xj/xu/ssd.pytorch/train.py", line 259, in <module>
    train()
  File "/home/xj/xu/ssd.pytorch/train.py", line 182, in train
    loss_l, loss_c = criterion(out, targets)
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/xu/ssd.pytorch/layers/modules/multibox_loss.py", line 97, in forward
    loss_c[pos] = 0  # filter out pos boxes for now
    ~~~~~~^^^^^
IndexError: The shape of the mask [32, 8732] at index 0 does not match the shape of the indexed tensor [279424, 1] at index 0
/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
/home/xj/xu/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  self.priors = Variable(self.priorbox.forward(), volatile=True)
/home/xj/xu/ssd.pytorch/train.py:218: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  init.xavier_uniform(param)
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
Loading base network...
Initializing weights...
Loading the dataset...
Training SSD on: MS COCO
Using the specified args:
Namespace(dataset='COCO', dataset_root='/home/xj/xu/ssd.pytorch/COCO_250423_the_thrid_optimization_datasets', basenet='vgg16_reducedfc.pth', batch_size=32, resume=None, start_iter=0, num_workers=4, cuda=True, lr=0.001, momentum=0.9, weight_decay=0.0005, gamma=0.1, visdom=False, save_folder='weights/')
/home/xj/xu/ssd.pytorch/train.py:173: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  targets = [Variable(ann.cuda(), volatile=True) for ann in targets]
/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
Traceback (most recent call last):
  File "/home/xj/xu/ssd.pytorch/train.py", line 259, in <module>
    train()
  File "/home/xj/xu/ssd.pytorch/train.py", line 187, in train
    loc_loss += loss_l.item()
                ~~~~~~~~~~~^^^
IndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number
/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
/home/xj/xu/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  self.priors = Variable(self.priorbox.forward(), volatile=True)
/home/xj/xu/ssd.pytorch/train.py:218: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  init.xavier_uniform(param)
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading base network...
Initializing weights...
Loading the dataset...
Training SSD on: MS COCO
Using the specified args:
Namespace(dataset='COCO', dataset_root='/home/xj/xu/ssd.pytorch/COCO_250423_the_thrid_optimization_datasets', basenet='vgg16_reducedfc.pth', batch_size=32, resume=None, start_iter=0, num_workers=4, cuda=True, lr=0.001, momentum=0.9, weight_decay=0.0005, gamma=0.1, visdom=False, save_folder='weights/')
/home/xj/xu/ssd.pytorch/train.py:173: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  targets = [Variable(ann.cuda(), volatile=True) for ann in targets]
/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
timer: 8.0616 sec.
iter 0 || Loss: 32.6343 || timer: 0.0949 sec.
iter 10 || Loss: 24.1012 || timer: 0.1032 sec.
iter 20 || Loss: 23.3705 || timer: 0.0840 sec.
iter 30 || Loss: 21.2888 || Traceback (most recent call last):
  File "/home/xj/xu/ssd.pytorch/train.py", line 259, in <module>
    train()
  File "/home/xj/xu/ssd.pytorch/train.py", line 169, in train
    images, targets = next(batch_iterator)
                      ^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1319, in _next_data
    raise StopIteration
StopIteration
/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
/home/xj/xu/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  self.priors = Variable(self.priorbox.forward(), volatile=True)
/home/xj/xu/ssd.pytorch/train.py:225: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  init.xavier_uniform(param)
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading base network...
Initializing weights...
Loading the dataset...
Training SSD on: MS COCO
Using the specified args:
Namespace(dataset='COCO', dataset_root='/home/xj/xu/ssd.pytorch/COCO_250423_the_thrid_optimization_datasets', basenet='vgg16_reducedfc.pth', batch_size=32, resume=None, start_iter=0, num_workers=4, cuda=True, lr=0.001, momentum=0.9, weight_decay=0.0005, gamma=0.1, visdom=False, save_folder='weights/')
/home/xj/xu/ssd.pytorch/train.py:180: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  targets = [Variable(ann.cuda(), volatile=True) for ann in targets]
/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
timer: 8.0992 sec.
iter 0 || Loss: 31.5180 || timer: 0.0886 sec.
iter 10 || Loss: 23.9636 || timer: 0.0907 sec.
iter 20 || Loss: 23.2792 || timer: 0.0874 sec.
iter 30 || Loss: 19.5113 || timer: 0.0925 sec.
iter 40 || Loss: 21.0615 || timer: 0.1120 sec.
iter 50 || Loss: 20.1849 || timer: 0.1069 sec.
iter 60 || Loss: 12.9657 || timer: 0.0940 sec.
iter 70 || Loss: 10.3398 || timer: 0.0876 sec.
iter 80 || Loss: 10.3242 || timer: 0.0994 sec.
iter 90 || Loss: 7.5021 || timer: 0.1106 sec.
iter 100 || Loss: 8.0133 || timer: 0.0832 sec.
iter 110 || Loss: 7.8183 || timer: 0.0946 sec.
iter 120 || Loss: 6.6410 || timer: 0.0827 sec.
iter 130 || Loss: 6.0798 || timer: 0.0895 sec.
iter 140 || Loss: 7.2531 || timer: 0.0920 sec.
iter 150 || Loss: 6.7612 || timer: 0.0922 sec.
iter 160 || Loss: 6.0986 || timer: 0.0895 sec.
iter 170 || Loss: 6.8697 || timer: 0.0837 sec.
iter 180 || Loss: 7.4277 || timer: 0.0884 sec.
iter 190 || Loss: 6.4786 || timer: 0.0901 sec.
iter 200 || Loss: 5.0536 || timer: 0.1059 sec.
iter 210 || Loss: 5.7009 || timer: 0.0901 sec.
iter 220 || Loss: 5.5165 || timer: 0.0256 sec.
iter 230 || Loss: 6.1057 || timer: 0.0936 sec.
iter 240 || Loss: 7.0765 || timer: 0.0769 sec.
iter 250 || Loss: 5.1211 || timer: 0.1039 sec.
iter 260 || Loss: 5.3101 || timer: 0.1317 sec.
iter 270 || Loss: 5.8525 || timer: 0.0918 sec.
iter 280 || Loss: 5.2572 || timer: 0.0833 sec.
iter 290 || Loss: 5.7847 || timer: 0.0844 sec.
iter 300 || Loss: 5.3335 || timer: 0.0943 sec.
iter 310 || Loss: 4.8483 || timer: 0.1012 sec.
iter 320 || Loss: 5.4280 || timer: 0.1182 sec.
iter 330 || Loss: 4.9591 || timer: 0.0838 sec.
iter 340 || Loss: 6.0232 || timer: 0.0904 sec.
iter 350 || Loss: 5.1461 || timer: 0.0949 sec.
iter 360 || Loss: 5.5070 || timer: 0.0932 sec.
iter 370 || Loss: 4.8895 || timer: 0.0854 sec.
iter 380 || Loss: 5.5277 || timer: 0.0838 sec.
iter 390 || Loss: 5.0734 || timer: 0.1317 sec.
iter 400 || Loss: 5.1210 || timer: 0.1308 sec.
iter 410 || Loss: 5.0187 || timer: 0.1023 sec.
iter 420 || Loss: 4.8307 || timer: 0.0938 sec.
iter 430 || Loss: 5.7153 || timer: 0.1148 sec.
iter 440 || Loss: 4.9530 || timer: 0.0976 sec.
iter 450 || Loss: 4.5928 || timer: 0.0829 sec.
iter 460 || Loss: 5.1110 || timer: 0.0874 sec.
iter 470 || Loss: 5.6052 || timer: 0.0931 sec.
iter 480 || Loss: 4.6073 || timer: 0.0829 sec.
iter 490 || Loss: 4.4828 || timer: 0.0990 sec.
iter 500 || Loss: 4.6487 || timer: 0.1027 sec.
iter 510 || Loss: 4.7118 || timer: 0.1151 sec.
iter 520 || Loss: 4.4682 || timer: 0.0910 sec.
iter 530 || Loss: 5.1312 || timer: 0.1021 sec.
iter 540 || Loss: 5.3280 || timer: 0.0906 sec.
iter 550 || Loss: 5.0906 || timer: 0.0219 sec.
iter 560 || Loss: 5.6806 || timer: 0.0853 sec.
iter 570 || Loss: 4.7467 || timer: 0.0840 sec.
iter 580 || Loss: 4.2831 || timer: 0.1042 sec.
iter 590 || Loss: 4.4240 || timer: 0.0918 sec.
iter 600 || Loss: 5.2569 || timer: 0.0875 sec.
iter 610 || Loss: 4.8649 || timer: 0.0911 sec.
iter 620 || Loss: 4.5323 || timer: 0.0827 sec.
iter 630 || Loss: 4.8541 || timer: 0.0816 sec.
iter 640 || Loss: 3.7709 || timer: 0.0863 sec.
iter 650 || Loss: 4.5477 || timer: 0.1020 sec.
iter 660 || Loss: 4.6132 || timer: 0.0838 sec.
iter 670 || Loss: 4.3195 || timer: 0.0905 sec.
iter 680 || Loss: 4.3294 || timer: 0.0843 sec.
iter 690 || Loss: 4.2761 || timer: 0.0834 sec.
iter 700 || Loss: 5.0956 || timer: 0.1046 sec.
iter 710 || Loss: 4.0416 || timer: 0.0900 sec.
iter 720 || Loss: 4.1259 || timer: 0.0920 sec.
iter 730 || Loss: 4.6352 || timer: 0.1239 sec.
iter 740 || Loss: 4.7445 || timer: 0.1101 sec.
iter 750 || Loss: 4.1863 || timer: 0.0882 sec.
iter 760 || Loss: 4.1435 || timer: 0.0911 sec.
iter 770 || Loss: 4.1398 || timer: 0.0930 sec.
iter 780 || Loss: 4.0306 || timer: 0.0829 sec.
iter 790 || Loss: 4.6994 || timer: 0.0889 sec.
iter 800 || Loss: 4.0225 || timer: 0.0916 sec.
iter 810 || Loss: 4.2384 || timer: 0.0891 sec.
iter 820 || Loss: 3.7505 || timer: 0.0898 sec.
iter 830 || Loss: 5.0664 || timer: 0.0915 sec.
iter 840 || Loss: 4.2027 || timer: 0.1142 sec.
iter 850 || Loss: 4.0835 || timer: 0.0839 sec.
iter 860 || Loss: 3.8794 || timer: 0.0879 sec.
iter 870 || Loss: 3.7447 || timer: 0.0996 sec.
iter 880 || Loss: 4.7238 || timer: 0.0239 sec.
iter 890 || Loss: 4.0203 || timer: 0.1023 sec.
iter 900 || Loss: 4.4432 || timer: 0.0883 sec.
iter 910 || Loss: 4.3213 || timer: 0.1050 sec.
iter 920 || Loss: 3.8252 || timer: 0.0897 sec.
iter 930 || Loss: 4.4861 || timer: 0.0893 sec.
iter 940 || Loss: 3.6781 || timer: 0.0835 sec.
iter 950 || Loss: 3.5971 || timer: 0.1086 sec.
iter 960 || Loss: 4.3871 || timer: 0.0887 sec.
iter 970 || Loss: 3.8420 || timer: 0.1004 sec.
iter 980 || Loss: 4.1495 || timer: 0.0891 sec.
iter 990 || Loss: 3.7115 || timer: 0.0827 sec.
iter 1000 || Loss: 4.0910 || timer: 0.0902 sec.
iter 1010 || Loss: 4.2415 || timer: 0.0875 sec.
iter 1020 || Loss: 3.6883 || timer: 0.0844 sec.
iter 1030 || Loss: 3.9458 || timer: 0.0841 sec.
iter 1040 || Loss: 3.9844 || timer: 0.0840 sec.
iter 1050 || Loss: 3.7569 || timer: 0.1029 sec.
iter 1060 || Loss: 4.0358 || timer: 0.0907 sec.
iter 1070 || Loss: 4.0635 || timer: 0.0916 sec.
iter 1080 || Loss: 3.3603 || timer: 0.0954 sec.
iter 1090 || Loss: 4.0640 || timer: 0.1011 sec.
iter 1100 || Loss: 4.1295 || timer: 0.0967 sec.
iter 1110 || Loss: 4.1111 || timer: 0.0899 sec.
iter 1120 || Loss: 3.0402 || timer: 0.0910 sec.
iter 1130 || Loss: 4.6859 || timer: 0.0939 sec.
iter 1140 || Loss: 3.3871 || timer: 0.0952 sec.
iter 1150 || Loss: 3.2291 || timer: 0.0924 sec.
iter 1160 || Loss: 3.3783 || timer: 0.0853 sec.
iter 1170 || Loss: 3.2376 || timer: 0.1036 sec.
iter 1180 || Loss: 3.6643 || timer: 0.0845 sec.
iter 1190 || Loss: 3.2785 || timer: 0.0829 sec.
iter 1200 || Loss: 3.0774 || timer: 0.0887 sec.
iter 1210 || Loss: 3.8606 || timer: 0.0284 sec.
iter 1220 || Loss: 1.7164 || timer: 0.0838 sec.
iter 1230 || Loss: 3.9906 || timer: 0.0906 sec.
iter 1240 || Loss: 3.5306 || timer: 0.0823 sec.
iter 1250 || Loss: 3.1502 || timer: 0.0886 sec.
iter 1260 || Loss: 4.0822 || timer: 0.0927 sec.
iter 1270 || Loss: 3.4258 || timer: 0.0924 sec.
iter 1280 || Loss: 3.2911 || timer: 0.0917 sec.
iter 1290 || Loss: 4.0217 || timer: 0.0896 sec.
iter 1300 || Loss: 4.0518 || timer: 0.1043 sec.
iter 1310 || Loss: 3.3643 || timer: 0.0991 sec.
iter 1320 || Loss: 3.2500 || timer: 0.1270 sec.
iter 1330 || Loss: 3.5005 || timer: 0.0825 sec.
iter 1340 || Loss: 3.1819 || timer: 0.0872 sec.
iter 1350 || Loss: 3.2191 || timer: 0.0928 sec.
iter 1360 || Loss: 3.6110 || timer: 0.0907 sec.
iter 1370 || Loss: 3.0990 || timer: 0.1061 sec.
iter 1380 || Loss: 3.2883 || timer: 0.0926 sec.
iter 1390 || Loss: 5.0704 || timer: 0.0966 sec.
iter 1400 || Loss: 3.7478 || timer: 0.0829 sec.
iter 1410 || Loss: 3.3965 || timer: 0.0909 sec.
iter 1420 || Loss: 3.3823 || timer: 0.1189 sec.
iter 1430 || Loss: 3.1778 || timer: 0.0909 sec.
iter 1440 || Loss: 3.2269 || timer: 0.0898 sec.
iter 1450 || Loss: 3.1743 || timer: 0.0923 sec.
iter 1460 || Loss: 3.5035 || timer: 0.0896 sec.
iter 1470 || Loss: 3.7981 || timer: 0.0917 sec.
iter 1480 || Loss: 3.1169 || timer: 0.0925 sec.
iter 1490 || Loss: 3.3812 || timer: 0.0936 sec.
iter 1500 || Loss: 3.1738 || timer: 0.0863 sec.
iter 1510 || Loss: 2.8259 || timer: 0.0937 sec.
iter 1520 || Loss: 4.4884 || timer: 0.1020 sec.
iter 1530 || Loss: 3.2480 || timer: 0.0848 sec.
iter 1540 || Loss: 3.6062 || timer: 0.0204 sec.
iter 1550 || Loss: 1.2802 || timer: 0.0920 sec.
iter 1560 || Loss: 3.2111 || timer: 0.1054 sec.
iter 1570 || Loss: 3.2419 || timer: 0.0907 sec.
iter 1580 || Loss: 3.3209 || timer: 0.0841 sec.
iter 1590 || Loss: 3.3056 || timer: 0.0988 sec.
iter 1600 || Loss: 2.6233 || timer: 0.1092 sec.
iter 1610 || Loss: 2.9713 || timer: 0.0908 sec.
iter 1620 || Loss: 3.4114 || timer: 0.0862 sec.
iter 1630 || Loss: 3.2791 || timer: 0.0879 sec.
iter 1640 || Loss: 2.9953 || timer: 0.0969 sec.
iter 1650 || Loss: 3.0991 || timer: 0.0840 sec.
iter 1660 || Loss: 3.8309 || timer: 0.0921 sec.
iter 1670 || Loss: 2.9763 || timer: 0.0910 sec.
iter 1680 || Loss: 3.3951 || timer: 0.0952 sec.
iter 1690 || Loss: 5.1586 || timer: 0.1024 sec.
iter 1700 || Loss: 3.6353 || timer: 0.0800 sec.
iter 1710 || Loss: 3.4123 || timer: 0.0887 sec.
iter 1720 || Loss: 3.4186 || timer: 0.0889 sec.
iter 1730 || Loss: 3.4118 || timer: 0.0974 sec.
iter 1740 || Loss: 3.0163 || timer: 0.0955 sec.
iter 1750 || Loss: 2.8508 || timer: 0.0919 sec.
iter 1760 || Loss: 3.5240 || timer: 0.0835 sec.
iter 1770 || Loss: 3.6427 || timer: 0.0893 sec.
iter 1780 || Loss: 3.4151 || timer: 0.0920 sec.
iter 1790 || Loss: 3.4450 || timer: 0.0912 sec.
iter 1800 || Loss: 3.2897 || timer: 0.1103 sec.
iter 1810 || Loss: 2.8912 || timer: 0.0771 sec.
iter 1820 || Loss: 3.8940 || timer: 0.0876 sec.
iter 1830 || Loss: 2.9437 || timer: 0.0837 sec.
iter 1840 || Loss: 2.8459 || timer: 0.0842 sec.
iter 1850 || Loss: 3.0964 || timer: 0.0942 sec.
iter 1860 || Loss: 3.0674 || timer: 0.0840 sec.
iter 1870 || Loss: 3.1940 || timer: 0.0230 sec.
iter 1880 || Loss: 3.2892 || timer: 0.0905 sec.
iter 1890 || Loss: 3.4392 || timer: 0.0882 sec.
iter 1900 || Loss: 3.4235 || timer: 0.0876 sec.
iter 1910 || Loss: 2.8968 || timer: 0.0913 sec.
iter 1920 || Loss: 3.9773 || timer: 0.0842 sec.
iter 1930 || Loss: 2.8661 || timer: 0.0921 sec.
iter 1940 || Loss: 3.0877 || timer: 0.0847 sec.
iter 1950 || Loss: 3.5874 || timer: 0.0895 sec.
iter 1960 || Loss: 3.3716 || timer: 0.0850 sec.
iter 1970 || Loss: 2.9298 || timer: 0.1039 sec.
iter 1980 || Loss: 2.6232 || timer: 0.1001 sec.
iter 1990 || Loss: 2.7408 || timer: 0.0991 sec.
iter 2000 || Loss: 3.3410 || timer: 0.0918 sec.
iter 2010 || Loss: 2.8676 || timer: 0.0914 sec.
iter 2020 || Loss: 2.5736 || timer: 0.0943 sec.
iter 2030 || Loss: 3.0193 || timer: 0.1084 sec.
iter 2040 || Loss: 3.3595 || timer: 0.1106 sec.
iter 2050 || Loss: 3.7478 || timer: 0.0912 sec.
iter 2060 || Loss: 3.1669 || timer: 0.0912 sec.
iter 2070 || Loss: 2.9866 || timer: 0.0842 sec.
iter 2080 || Loss: 3.2102 || timer: 0.0852 sec.
iter 2090 || Loss: 2.7656 || timer: 0.0842 sec.
iter 2100 || Loss: 2.9007 || timer: 0.0922 sec.
iter 2110 || Loss: 2.6128 || timer: 0.1103 sec.
iter 2120 || Loss: 3.3648 || timer: 0.0919 sec.
iter 2130 || Loss: 3.1388 || timer: 0.1061 sec.
iter 2140 || Loss: 2.3612 || timer: 0.0968 sec.
iter 2150 || Loss: 3.6063 || timer: 0.0923 sec.
iter 2160 || Loss: 3.6218 || timer: 0.1035 sec.
iter 2170 || Loss: 3.2483 || timer: 0.0992 sec.
iter 2180 || Loss: 3.3905 || timer: 0.0834 sec.
iter 2190 || Loss: 2.9928 || timer: 0.1087 sec.
iter 2200 || Loss: 2.8351 || timer: 0.0275 sec.
iter 2210 || Loss: 1.5578 || timer: 0.0920 sec.
iter 2220 || Loss: 3.2396 || timer: 0.0845 sec.
iter 2230 || Loss: 3.1296 || timer: 0.0886 sec.
iter 2240 || Loss: 2.7852 || timer: 0.1052 sec.
iter 2250 || Loss: 2.4656 || timer: 0.0835 sec.
iter 2260 || Loss: 2.7199 || timer: 0.0825 sec.
iter 2270 || Loss: 2.8248 || timer: 0.0927 sec.
iter 2280 || Loss: 2.8624 || timer: 0.0929 sec.
iter 2290 || Loss: 3.2229 || timer: 0.0912 sec.
iter 2300 || Loss: 2.9138 || timer: 0.1301 sec.
iter 2310 || Loss: 2.8696 || timer: 0.0837 sec.
iter 2320 || Loss: 2.8887 || timer: 0.1011 sec.
iter 2330 || Loss: 2.5730 || timer: 0.0932 sec.
iter 2340 || Loss: 2.7870 || timer: 0.0840 sec.
iter 2350 || Loss: 2.8523 || timer: 0.0927 sec.
iter 2360 || Loss: 2.4627 || timer: 0.0845 sec.
iter 2370 || Loss: 2.5667 || timer: 0.0890 sec.
iter 2380 || Loss: 3.5729 || timer: 0.1072 sec.
iter 2390 || Loss: 2.8340 || timer: 0.0840 sec.
iter 2400 || Loss: 2.7921 || timer: 0.0967 sec.
iter 2410 || Loss: 2.5773 || timer: 0.1046 sec.
iter 2420 || Loss: 3.2159 || timer: 0.0908 sec.
iter 2430 || Loss: 2.4573 || timer: 0.0913 sec.
iter 2440 || Loss: 2.1454 || timer: 0.0973 sec.
iter 2450 || Loss: 3.3397 || timer: 0.0915 sec.
iter 2460 || Loss: 2.3919 || timer: 0.0832 sec.
iter 2470 || Loss: 2.6499 || timer: 0.0911 sec.
iter 2480 || Loss: 2.7613 || timer: 0.0914 sec.
iter 2490 || Loss: 2.4208 || timer: 0.0983 sec.
iter 2500 || Loss: 2.6214 || timer: 0.0836 sec.
iter 2510 || Loss: 2.5111 || timer: 0.0856 sec.
iter 2520 || Loss: 2.8435 || timer: 0.0893 sec.
iter 2530 || Loss: 2.6087 || timer: 0.0247 sec.
iter 2540 || Loss: 2.4099 || timer: 0.1258 sec.
iter 2550 || Loss: 3.4068 || timer: 0.1132 sec.
iter 2560 || Loss: 2.8147 || timer: 0.0909 sec.
iter 2570 || Loss: 2.2598 || timer: 0.0928 sec.
iter 2580 || Loss: 3.1321 || timer: 0.1074 sec.
iter 2590 || Loss: 2.4288 || timer: 0.0934 sec.
iter 2600 || Loss: 2.6181 || timer: 0.1122 sec.
iter 2610 || Loss: 2.5557 || timer: 0.0835 sec.
iter 2620 || Loss: 3.1057 || timer: 0.0835 sec.
iter 2630 || Loss: 2.5597 || timer: 0.1218 sec.
iter 2640 || Loss: 2.6861 || timer: 0.0889 sec.
iter 2650 || Loss: 2.4448 || timer: 0.0840 sec.
iter 2660 || Loss: 3.0673 || timer: 0.0848 sec.
iter 2670 || Loss: 2.5839 || timer: 0.0936 sec.
iter 2680 || Loss: 2.7798 || timer: 0.0924 sec.
iter 2690 || Loss: 2.9598 || timer: 0.0841 sec.
iter 2700 || Loss: 2.4897 || timer: 0.1085 sec.
iter 2710 || Loss: 3.2807 || timer: 0.0877 sec.
iter 2720 || Loss: 2.5631 || timer: 0.1020 sec.
iter 2730 || Loss: 2.3864 || timer: 0.0911 sec.
iter 2740 || Loss: 3.3799 || timer: 0.0913 sec.
iter 2750 || Loss: 3.2626 || timer: 0.0919 sec.
iter 2760 || Loss: 3.2606 || timer: 0.0825 sec.
iter 2770 || Loss: 3.0488 || timer: 0.0992 sec.
iter 2780 || Loss: 2.7954 || timer: 0.1091 sec.
iter 2790 || Loss: 2.5539 || timer: 0.0858 sec.
iter 2800 || Loss: 2.6775 || timer: 0.0904 sec.
iter 2810 || Loss: 3.1229 || timer: 0.1077 sec.
iter 2820 || Loss: 2.6665 || timer: 0.0914 sec.
iter 2830 || Loss: 2.8954 || timer: 0.0895 sec.
iter 2840 || Loss: 2.4180 || timer: 0.1003 sec.
iter 2850 || Loss: 2.7902 || timer: 0.0910 sec.
iter 2860 || Loss: 3.1822 || timer: 0.0269 sec.
iter 2870 || Loss: 2.5341 || timer: 0.0903 sec.
iter 2880 || Loss: 2.8396 || timer: 0.0846 sec.
iter 2890 || Loss: 2.4051 || timer: 0.0914 sec.
iter 2900 || Loss: 3.0516 || timer: 0.0922 sec.
iter 2910 || Loss: 3.0469 || timer: 0.0841 sec.
iter 2920 || Loss: 2.5353 || timer: 0.0963 sec.
iter 2930 || Loss: 2.7139 || timer: 0.0845 sec.
iter 2940 || Loss: 2.4854 || timer: 0.0918 sec.
iter 2950 || Loss: 2.6766 || timer: 0.1025 sec.
iter 2960 || Loss: 2.4107 || timer: 0.0894 sec.
iter 2970 || Loss: 3.1385 || timer: 0.0846 sec.
iter 2980 || Loss: 1.9752 || timer: 0.0990 sec.
iter 2990 || Loss: 2.9381 || timer: 0.0840 sec.
iter 3000 || Loss: 2.4906 || timer: 0.0837 sec.
iter 3010 || Loss: 2.8258 || timer: 0.0953 sec.
iter 3020 || Loss: 2.5589 || timer: 0.0895 sec.
iter 3030 || Loss: 2.2946 || timer: 0.0840 sec.
iter 3040 || Loss: 2.4120 || timer: 0.0830 sec.
iter 3050 || Loss: 3.1631 || timer: 0.1124 sec.
iter 3060 || Loss: 2.2902 || timer: 0.0833 sec.
iter 3070 || Loss: 3.6710 || timer: 0.0839 sec.
iter 3080 || Loss: 3.2019 || timer: 0.0895 sec.
iter 3090 || Loss: 2.5949 || timer: 0.0908 sec.
iter 3100 || Loss: 2.8272 || timer: 0.0921 sec.
iter 3110 || Loss: 2.5289 || timer: 0.1032 sec.
iter 3120 || Loss: 2.6816 || timer: 0.0761 sec.
iter 3130 || Loss: 3.0993 || timer: 0.0881 sec.
iter 3140 || Loss: 4.5449 || timer: 0.1055 sec.
iter 3150 || Loss: 3.1872 || timer: 0.0868 sec.
iter 3160 || Loss: 3.5521 || timer: 0.0900 sec.
iter 3170 || Loss: 3.8790 || timer: 0.0926 sec.
iter 3180 || Loss: 3.7351 || timer: 0.0935 sec.
iter 3190 || Loss: 2.7787 || timer: 0.0173 sec.
iter 3200 || Loss: 1.9568 || timer: 0.0903 sec.
iter 3210 || Loss: 2.5862 || timer: 0.0947 sec.
iter 3220 || Loss: 2.4248 || timer: 0.0827 sec.
iter 3230 || Loss: 2.6054 || timer: 0.1096 sec.
iter 3240 || Loss: 3.0968 || timer: 0.0887 sec.
iter 3250 || Loss: 2.9649 || timer: 0.0922 sec.
iter 3260 || Loss: 2.6195 || timer: 0.0925 sec.
iter 3270 || Loss: 3.4176 || timer: 0.0891 sec.
iter 3280 || Loss: 2.8386 || timer: 0.0882 sec.
iter 3290 || Loss: 2.5273 || timer: 0.1007 sec.
iter 3300 || Loss: 2.2800 || timer: 0.1096 sec.
iter 3310 || Loss: 3.4191 || timer: 0.1065 sec.
iter 3320 || Loss: 2.4849 || timer: 0.0943 sec.
iter 3330 || Loss: 2.2441 || timer: 0.0902 sec.
iter 3340 || Loss: 2.8167 || timer: 0.0928 sec.
iter 3350 || Loss: 2.9770 || timer: 0.0832 sec.
iter 3360 || Loss: 2.5545 || timer: 0.0916 sec.
iter 3370 || Loss: 3.2333 || timer: 0.1067 sec.
iter 3380 || Loss: 2.7259 || timer: 0.0803 sec.
iter 3390 || Loss: 2.3194 || timer: 0.0932 sec.
iter 3400 || Loss: 2.6861 || timer: 0.1037 sec.
iter 3410 || Loss: 2.1863 || timer: 0.0889 sec.
iter 3420 || Loss: 1.8949 || timer: 0.0968 sec.
iter 3430 || Loss: 2.6563 || timer: 0.0910 sec.
iter 3440 || Loss: 3.6399 || timer: 0.0932 sec.
iter 3450 || Loss: 2.9664 || timer: 0.0922 sec.
iter 3460 || Loss: 2.4568 || timer: 0.0935 sec.
iter 3470 || Loss: 2.7120 || timer: 0.0917 sec.
iter 3480 || Loss: 2.1873 || timer: 0.1114 sec.
iter 3490 || Loss: 2.4159 || timer: 0.0849 sec.
iter 3500 || Loss: 2.6346 || timer: 0.0924 sec.
iter 3510 || Loss: 2.1659 || timer: 0.1298 sec.
iter 3520 || Loss: 2.4666 || timer: 0.0244 sec.
iter 3530 || Loss: 1.7697 || timer: 0.0908 sec.
iter 3540 || Loss: 2.4552 || timer: 0.1096 sec.
iter 3550 || Loss: 2.3380 || timer: 0.0830 sec.
iter 3560 || Loss: 2.7044 || timer: 0.0844 sec.
iter 3570 || Loss: 2.3588 || timer: 0.0841 sec.
iter 3580 || Loss: 2.2501 || timer: 0.0832 sec.
iter 3590 || Loss: 2.5376 || timer: 0.0915 sec.
iter 3600 || Loss: 1.8699 || timer: 0.0850 sec.
iter 3610 || Loss: 2.4632 || timer: 0.0908 sec.
iter 3620 || Loss: 2.0596 || timer: 0.0962 sec.
iter 3630 || Loss: 2.7149 || timer: 0.0979 sec.
iter 3640 || Loss: 2.3723 || timer: 0.0902 sec.
iter 3650 || Loss: 2.2323 || timer: 0.0842 sec.
iter 3660 || Loss: 2.2199 || timer: 0.0938 sec.
iter 3670 || Loss: 2.8041 || timer: 0.0929 sec.
iter 3680 || Loss: 2.4366 || timer: 0.0824 sec.
iter 3690 || Loss: 2.1890 || timer: 0.0857 sec.
iter 3700 || Loss: 2.5398 || timer: 0.0908 sec.
iter 3710 || Loss: 2.7397 || timer: 0.0914 sec.
iter 3720 || Loss: 2.7963 || timer: 0.0883 sec.
iter 3730 || Loss: 2.6767 || timer: 0.1018 sec.
iter 3740 || Loss: 2.8347 || timer: 0.0910 sec.
iter 3750 || Loss: 2.4555 || timer: 0.0902 sec.
iter 3760 || Loss: 2.3741 || timer: 0.0900 sec.
iter 3770 || Loss: 2.7454 || timer: 0.0923 sec.
iter 3780 || Loss: 2.4680 || timer: 0.0898 sec.
iter 3790 || Loss: 2.3015 || timer: 0.0894 sec.
iter 3800 || Loss: 2.7011 || timer: 0.0926 sec.
iter 3810 || Loss: 2.5495 || timer: 0.0911 sec.
iter 3820 || Loss: 2.1142 || timer: 0.0914 sec.
iter 3830 || Loss: 2.1693 || timer: 0.0841 sec.
iter 3840 || Loss: 2.6990 || timer: 0.0846 sec.
iter 3850 || Loss: 2.5017 || timer: 0.0242 sec.
iter 3860 || Loss: 3.2124 || timer: 0.1043 sec.
iter 3870 || Loss: 3.5811 || timer: 0.0841 sec.
iter 3880 || Loss: 2.4087 || timer: 0.0915 sec.
iter 3890 || Loss: 2.2783 || timer: 0.1009 sec.
iter 3900 || Loss: 2.4965 || timer: 0.1068 sec.
iter 3910 || Loss: 2.6461 || timer: 0.0859 sec.
iter 3920 || Loss: 2.1663 || timer: 0.0873 sec.
iter 3930 || Loss: 2.4091 || timer: 0.0908 sec.
iter 3940 || Loss: 2.5106 || timer: 0.1039 sec.
iter 3950 || Loss: 2.1779 || timer: 0.1147 sec.
iter 3960 || Loss: 2.2889 || timer: 0.0910 sec.
iter 3970 || Loss: 2.1211 || timer: 0.0912 sec.
iter 3980 || Loss: 2.1718 || timer: 0.0886 sec.
iter 3990 || Loss: 2.0489 || timer: 0.0865 sec.
iter 4000 || Loss: 2.0527 || timer: 0.0904 sec.
iter 4010 || Loss: 2.4565 || timer: 0.0861 sec.
iter 4020 || Loss: 2.0577 || timer: 0.0901 sec.
iter 4030 || Loss: 2.0026 || timer: 0.0919 sec.
iter 4040 || Loss: 2.6348 || timer: 0.0841 sec.
iter 4050 || Loss: 2.3815 || timer: 0.0953 sec.
iter 4060 || Loss: 2.5139 || timer: 0.0928 sec.
iter 4070 || Loss: 2.5993 || timer: 0.0857 sec.
iter 4080 || Loss: 2.0785 || timer: 0.0908 sec.
iter 4090 || Loss: 2.5395 || timer: 0.0845 sec.
iter 4100 || Loss: 2.6139 || timer: 0.0842 sec.
iter 4110 || Loss: 2.5486 || timer: 0.0824 sec.
iter 4120 || Loss: 2.1138 || timer: 0.0901 sec.
iter 4130 || Loss: 2.5070 || timer: 0.0938 sec.
iter 4140 || Loss: 2.5054 || timer: 0.1086 sec.
iter 4150 || Loss: 2.2560 || timer: 0.0917 sec.
iter 4160 || Loss: 2.5423 || timer: 0.0892 sec.
iter 4170 || Loss: 2.5521 || timer: 0.0844 sec.
iter 4180 || Loss: 2.3299 || timer: 0.0151 sec.
iter 4190 || Loss: 2.9379 || timer: 0.0909 sec.
iter 4200 || Loss: 2.2194 || timer: 0.1091 sec.
iter 4210 || Loss: 2.3812 || timer: 0.0860 sec.
iter 4220 || Loss: 2.1718 || timer: 0.0838 sec.
iter 4230 || Loss: 2.2934 || timer: 0.1085 sec.
iter 4240 || Loss: 2.9353 || timer: 0.0843 sec.
iter 4250 || Loss: 1.9954 || timer: 0.0909 sec.
iter 4260 || Loss: 2.0160 || timer: 0.1055 sec.
iter 4270 || Loss: 1.9337 || timer: 0.0894 sec.
iter 4280 || Loss: 3.1284 || timer: 0.1010 sec.
iter 4290 || Loss: 2.0156 || timer: 0.0829 sec.
iter 4300 || Loss: 2.2477 || timer: 0.0900 sec.
iter 4310 || Loss: 1.9905 || timer: 0.0917 sec.
iter 4320 || Loss: 2.6395 || timer: 0.0887 sec.
iter 4330 || Loss: 2.2239 || timer: 0.0837 sec.
iter 4340 || Loss: 2.0093 || timer: 0.0922 sec.
iter 4350 || Loss: 2.4957 || timer: 0.0873 sec.
iter 4360 || Loss: 2.4042 || timer: 0.0892 sec.
iter 4370 || Loss: 2.7613 || timer: 0.0840 sec.
iter 4380 || Loss: 2.4033 || timer: 0.0847 sec.
iter 4390 || Loss: 2.4763 || timer: 0.0925 sec.
iter 4400 || Loss: 2.0974 || timer: 0.0842 sec.
iter 4410 || Loss: 2.2716 || timer: 0.0833 sec.
iter 4420 || Loss: 2.3152 || timer: 0.0977 sec.
iter 4430 || Loss: 2.2160 || timer: 0.0837 sec.
iter 4440 || Loss: 2.7417 || timer: 0.1037 sec.
iter 4450 || Loss: 1.8109 || timer: 0.0909 sec.
iter 4460 || Loss: 2.3233 || timer: 0.0920 sec.
iter 4470 || Loss: 2.4352 || timer: 0.0916 sec.
iter 4480 || Loss: 2.6046 || timer: 0.1051 sec.
iter 4490 || Loss: 2.5905 || timer: 0.1160 sec.
iter 4500 || Loss: 3.0341 || timer: 0.0839 sec.
iter 4510 || Loss: 2.6532 || timer: 0.0274 sec.
iter 4520 || Loss: 1.2133 || timer: 0.1061 sec.
iter 4530 || Loss: 2.7101 || timer: 0.0896 sec.
iter 4540 || Loss: 1.7655 || timer: 0.0845 sec.
iter 4550 || Loss: 2.3098 || timer: 0.0912 sec.
iter 4560 || Loss: 2.5602 || timer: 0.0886 sec.
iter 4570 || Loss: 3.1255 || timer: 0.0852 sec.
iter 4580 || Loss: 2.3266 || timer: 0.0774 sec.
iter 4590 || Loss: 2.2594 || timer: 0.0911 sec.
iter 4600 || Loss: 2.5561 || timer: 0.0916 sec.
iter 4610 || Loss: 2.8364 || timer: 0.0960 sec.
iter 4620 || Loss: 2.8429 || timer: 0.0890 sec.
iter 4630 || Loss: 2.5237 || timer: 0.0885 sec.
iter 4640 || Loss: 2.6159 || timer: 0.0939 sec.
iter 4650 || Loss: 2.0571 || timer: 0.1025 sec.
iter 4660 || Loss: 3.1269 || timer: 0.0870 sec.
iter 4670 || Loss: 2.5534 || timer: 0.1007 sec.
iter 4680 || Loss: 2.9836 || timer: 0.0857 sec.
iter 4690 || Loss: 2.7813 || timer: 0.1140 sec.
iter 4700 || Loss: 2.3133 || timer: 0.0926 sec.
iter 4710 || Loss: 2.3065 || timer: 0.0964 sec.
iter 4720 || Loss: 2.6913 || timer: 0.0945 sec.
iter 4730 || Loss: 3.0408 || timer: 0.0831 sec.
iter 4740 || Loss: 2.7302 || timer: 0.0887 sec.
iter 4750 || Loss: 2.8591 || timer: 0.0920 sec.
iter 4760 || Loss: 2.9913 || timer: 0.0843 sec.
iter 4770 || Loss: 2.4527 || timer: 0.0928 sec.
iter 4780 || Loss: 2.6117 || timer: 0.0845 sec.
iter 4790 || Loss: 2.7026 || timer: 0.0889 sec.
iter 4800 || Loss: 2.2443 || timer: 0.0920 sec.
iter 4810 || Loss: 2.4866 || timer: 0.0916 sec.
iter 4820 || Loss: 2.1465 || timer: 0.1122 sec.
iter 4830 || Loss: 2.4109 || timer: 0.0844 sec.
iter 4840 || Loss: 2.4136 || timer: 0.0166 sec.
iter 4850 || Loss: 0.7907 || timer: 0.0922 sec.
iter 4860 || Loss: 2.5653 || timer: 0.0911 sec.
iter 4870 || Loss: 2.0177 || timer: 0.0883 sec.
iter 4880 || Loss: 2.6220 || timer: 0.0930 sec.
iter 4890 || Loss: 2.3118 || timer: 0.0784 sec.
iter 4900 || Loss: 2.4385 || timer: 0.1047 sec.
iter 4910 || Loss: 2.0937 || timer: 0.0843 sec.
iter 4920 || Loss: 2.1023 || timer: 0.0915 sec.
iter 4930 || Loss: 2.3341 || timer: 0.1070 sec.
iter 4940 || Loss: 2.0714 || timer: 0.1004 sec.
iter 4950 || Loss: 2.0128 || timer: 0.1074 sec.
iter 4960 || Loss: 2.3611 || timer: 0.0894 sec.
iter 4970 || Loss: 2.5938 || timer: 0.0768 sec.
iter 4980 || Loss: 1.7816 || timer: 0.0928 sec.
iter 4990 || Loss: 2.2136 || timer: 0.0909 sec.
iter 5000 || Loss: 1.7927 || Saving state, iter: 5000
timer: 0.0926 sec.
iter 5010 || Loss: 2.2369 || timer: 0.0927 sec.
iter 5020 || Loss: 2.1941 || timer: 0.1033 sec.
iter 5030 || Loss: 2.2216 || timer: 0.0926 sec.
iter 5040 || Loss: 2.7755 || timer: 0.1083 sec.
iter 5050 || Loss: 1.8959 || timer: 0.0911 sec.
iter 5060 || Loss: 2.0951 || timer: 0.0837 sec.
iter 5070 || Loss: 2.0673 || timer: 0.0909 sec.
iter 5080 || Loss: 2.2607 || timer: 0.0962 sec.
iter 5090 || Loss: 1.9206 || timer: 0.0949 sec.
iter 5100 || Loss: 2.0647 || timer: 0.0831 sec.
iter 5110 || Loss: 2.3048 || timer: 0.0913 sec.
iter 5120 || Loss: 2.6240 || timer: 0.0916 sec.
iter 5130 || Loss: 2.2421 || timer: 0.0915 sec.
iter 5140 || Loss: 2.1372 || timer: 0.0891 sec.
iter 5150 || Loss: 2.7684 || timer: 0.0891 sec.
iter 5160 || Loss: 2.2463 || timer: 0.0951 sec.
iter 5170 || Loss: 2.0525 || timer: 0.0295 sec.
iter 5180 || Loss: 3.8468 || timer: 0.0932 sec.
iter 5190 || Loss: 2.2945 || timer: 0.0854 sec.
iter 5200 || Loss: 2.7565 || timer: 0.0927 sec.
iter 5210 || Loss: 2.4044 || timer: 0.0841 sec.
iter 5220 || Loss: 2.4696 || timer: 0.1036 sec.
iter 5230 || Loss: 3.0424 || timer: 0.0854 sec.
iter 5240 || Loss: 2.2221 || timer: 0.0931 sec.
iter 5250 || Loss: 2.6394 || timer: 0.0904 sec.
iter 5260 || Loss: 2.2501 || timer: 0.0829 sec.
iter 5270 || Loss: 2.2484 || timer: 0.0987 sec.
iter 5280 || Loss: 1.8684 || timer: 0.0770 sec.
iter 5290 || Loss: 1.8436 || timer: 0.1000 sec.
iter 5300 || Loss: 2.1011 || timer: 0.0869 sec.
iter 5310 || Loss: 2.1110 || timer: 0.0894 sec.
iter 5320 || Loss: 2.0023 || timer: 0.0836 sec.
iter 5330 || Loss: 1.8504 || timer: 0.1041 sec.
iter 5340 || Loss: 2.5922 || timer: 0.0919 sec.
iter 5350 || Loss: 2.7016 || timer: 0.0917 sec.
iter 5360 || Loss: 2.5398 || timer: 0.0915 sec.
iter 5370 || Loss: 2.6863 || timer: 0.0910 sec.
iter 5380 || Loss: 2.5883 || timer: 0.0807 sec.
iter 5390 || Loss: 2.4666 || timer: 0.0903 sec.
iter 5400 || Loss: 2.1160 || timer: 0.0871 sec.
iter 5410 || Loss: 2.4748 || timer: 0.1137 sec.
iter 5420 || Loss: 2.2733 || timer: 0.0862 sec.
iter 5430 || Loss: 2.4838 || timer: 0.0756 sec.
iter 5440 || Loss: 2.4187 || timer: 0.0773 sec.
iter 5450 || Loss: 2.3676 || timer: 0.0899 sec.
iter 5460 || Loss: 2.1897 || timer: 0.1147 sec.
iter 5470 || Loss: 3.1366 || timer: 0.0843 sec.
iter 5480 || Loss: 2.9411 || timer: 0.0898 sec.
iter 5490 || Loss: 2.3043 || timer: 0.0836 sec.
iter 5500 || Loss: 1.9366 || timer: 0.0179 sec.
iter 5510 || Loss: 1.8708 || timer: 0.0953 sec.
iter 5520 || Loss: 2.9147 || timer: 0.0822 sec.
iter 5530 || Loss: 2.2204 || timer: 0.0848 sec.
iter 5540 || Loss: 2.2416 || timer: 0.0850 sec.
iter 5550 || Loss: 2.0836 || timer: 0.0935 sec.
iter 5560 || Loss: 1.7661 || timer: 0.0868 sec.
iter 5570 || Loss: 2.0795 || timer: 0.0849 sec.
iter 5580 || Loss: 2.6493 || timer: 0.0902 sec.
iter 5590 || Loss: 2.5824 || timer: 0.0842 sec.
iter 5600 || Loss: 1.9317 || timer: 0.0942 sec.
iter 5610 || Loss: 1.9540 || timer: 0.0884 sec.
iter 5620 || Loss: 2.3012 || timer: 0.0882 sec.
iter 5630 || Loss: 2.6227 || timer: 0.0902 sec.
iter 5640 || Loss: 2.1636 || timer: 0.1110 sec.
iter 5650 || Loss: 2.5921 || timer: 0.0910 sec.
iter 5660 || Loss: 1.8253 || timer: 0.0916 sec.
iter 5670 || Loss: 1.6830 || timer: 0.0883 sec.
iter 5680 || Loss: 1.8501 || timer: 0.0822 sec.
iter 5690 || Loss: 2.3962 || timer: 0.0919 sec.
iter 5700 || Loss: 2.3301 || timer: 0.0771 sec.
iter 5710 || Loss: 1.9408 || timer: 0.0911 sec.
iter 5720 || Loss: 2.4731 || timer: 0.0913 sec.
iter 5730 || Loss: 2.0026 || timer: 0.0872 sec.
iter 5740 || Loss: 2.4234 || timer: 0.1038 sec.
iter 5750 || Loss: 2.3792 || timer: 0.1031 sec.
iter 5760 || Loss: 2.1967 || timer: 0.0932 sec.
iter 5770 || Loss: 2.0288 || timer: 0.0900 sec.
iter 5780 || Loss: 2.1128 || timer: 0.0886 sec.
iter 5790 || Loss: 2.3407 || timer: 0.0921 sec.
iter 5800 || Loss: 2.4085 || timer: 0.0855 sec.
iter 5810 || Loss: 2.6673 || timer: 0.0835 sec.
iter 5820 || Loss: 2.0890 || timer: 0.0906 sec.
iter 5830 || Loss: 1.7989 || timer: 0.0221 sec.
iter 5840 || Loss: 1.4385 || timer: 0.0942 sec.
iter 5850 || Loss: 2.9134 || timer: 0.0836 sec.
iter 5860 || Loss: 2.2040 || timer: 0.0856 sec.
iter 5870 || Loss: 2.3671 || timer: 0.0844 sec.
iter 5880 || Loss: 2.0306 || timer: 0.0844 sec.
iter 5890 || Loss: 2.2188 || timer: 0.0913 sec.
iter 5900 || Loss: 2.3526 || timer: 0.0905 sec.
iter 5910 || Loss: 2.5945 || timer: 0.0838 sec.
iter 5920 || Loss: 2.2708 || timer: 0.1323 sec.
iter 5930 || Loss: 2.2770 || timer: 0.0868 sec.
iter 5940 || Loss: 1.9886 || timer: 0.0780 sec.
iter 5950 || Loss: 2.2211 || timer: 0.1016 sec.
iter 5960 || Loss: 2.5481 || timer: 0.0898 sec.
iter 5970 || Loss: 2.2773 || timer: 0.0921 sec.
iter 5980 || Loss: 2.6208 || timer: 0.0908 sec.
iter 5990 || Loss: 2.2497 || timer: 0.0927 sec.
iter 6000 || Loss: 2.3468 || timer: 0.0930 sec.
iter 6010 || Loss: 2.5099 || timer: 0.0830 sec.
iter 6020 || Loss: 2.4810 || timer: 0.1019 sec.
iter 6030 || Loss: 2.2624 || timer: 0.1595 sec.
iter 6040 || Loss: 2.0851 || timer: 0.0757 sec.
iter 6050 || Loss: 2.3621 || timer: 0.0920 sec.
iter 6060 || Loss: 2.1303 || timer: 0.0946 sec.
iter 6070 || Loss: 2.1852 || timer: 0.0872 sec.
iter 6080 || Loss: 1.6883 || timer: 0.0880 sec.
iter 6090 || Loss: 2.6856 || timer: 0.0997 sec.
iter 6100 || Loss: 1.6301 || timer: 0.0972 sec.
iter 6110 || Loss: 1.9542 || timer: 0.0895 sec.
iter 6120 || Loss: 2.0933 || timer: 0.0920 sec.
iter 6130 || Loss: 1.9567 || timer: 0.0918 sec.
iter 6140 || Loss: 2.1812 || timer: 0.0896 sec.
iter 6150 || Loss: 2.5133 || timer: 0.0823 sec.
iter 6160 || Loss: 2.8391 || timer: 0.0171 sec.
iter 6170 || Loss: 1.7593 || timer: 0.0890 sec.
iter 6180 || Loss: 2.5304 || timer: 0.0923 sec.
iter 6190 || Loss: 2.6169 || timer: 0.0921 sec.
iter 6200 || Loss: 1.8877 || timer: 0.0812 sec.
iter 6210 || Loss: 1.8664 || timer: 0.0896 sec.
iter 6220 || Loss: 1.8711 || timer: 0.0882 sec.
iter 6230 || Loss: 2.1618 || timer: 0.0863 sec.
iter 6240 || Loss: 1.9902 || timer: 0.0823 sec.
iter 6250 || Loss: 2.1490 || timer: 0.1072 sec.
iter 6260 || Loss: 2.3755 || timer: 0.1089 sec.
iter 6270 || Loss: 2.0455 || timer: 0.0894 sec.
iter 6280 || Loss: 1.9340 || timer: 0.0973 sec.
iter 6290 || Loss: 2.0639 || timer: 0.0904 sec.
iter 6300 || Loss: 2.3033 || timer: 0.0861 sec.
iter 6310 || Loss: 2.1340 || timer: 0.0923 sec.
iter 6320 || Loss: 2.1671 || timer: 0.0908 sec.
iter 6330 || Loss: 2.1398 || timer: 0.1019 sec.
iter 6340 || Loss: 2.2720 || timer: 0.0825 sec.
iter 6350 || Loss: 2.1507 || timer: 0.1230 sec.
iter 6360 || Loss: 2.0908 || timer: 0.1012 sec.
iter 6370 || Loss: 1.9440 || timer: 0.0831 sec.
iter 6380 || Loss: 2.3482 || timer: 0.0835 sec.
iter 6390 || Loss: 2.0875 || timer: 0.0886 sec.
iter 6400 || Loss: 1.9837 || timer: 0.1049 sec.
iter 6410 || Loss: 1.8924 || timer: 0.0826 sec.
iter 6420 || Loss: 1.7559 || timer: 0.0914 sec.
iter 6430 || Loss: 2.3809 || timer: 0.0896 sec.
iter 6440 || Loss: 2.1135 || timer: 0.0897 sec.
iter 6450 || Loss: 2.1766 || timer: 0.0825 sec.
iter 6460 || Loss: 2.3132 || timer: 0.0788 sec.
iter 6470 || Loss: 2.2888 || timer: 0.0909 sec.
iter 6480 || Loss: 2.6106 || timer: 0.0872 sec.
iter 6490 || Loss: 1.7425 || timer: 0.0228 sec.
iter 6500 || Loss: 3.5756 || timer: 0.1092 sec.
iter 6510 || Loss: 2.8129 || timer: 0.1374 sec.
iter 6520 || Loss: 2.2369 || timer: 0.0833 sec.
iter 6530 || Loss: 1.8964 || timer: 0.0831 sec.
iter 6540 || Loss: 1.9769 || timer: 0.0767 sec.
iter 6550 || Loss: 2.3438 || timer: 0.0903 sec.
iter 6560 || Loss: 2.0905 || timer: 0.0915 sec.
iter 6570 || Loss: 1.8619 || timer: 0.0969 sec.
iter 6580 || Loss: 2.0815 || timer: 0.0856 sec.
iter 6590 || Loss: 2.4826 || timer: 0.1198 sec.
iter 6600 || Loss: 1.9782 || timer: 0.0883 sec.
iter 6610 || Loss: 1.8199 || timer: 0.0894 sec.
iter 6620 || Loss: 2.5157 || timer: 0.0902 sec.
iter 6630 || Loss: 1.5839 || timer: 0.1056 sec.
iter 6640 || Loss: 2.2122 || timer: 0.0869 sec.
iter 6650 || Loss: 2.2323 || timer: 0.0840 sec.
iter 6660 || Loss: 1.9745 || timer: 0.0855 sec.
iter 6670 || Loss: 2.4941 || timer: 0.1001 sec.
iter 6680 || Loss: 2.1514 || timer: 0.0894 sec.
iter 6690 || Loss: 2.2420 || timer: 0.1078 sec.
iter 6700 || Loss: 2.1447 || timer: 0.1023 sec.
iter 6710 || Loss: 2.5152 || timer: 0.0890 sec.
iter 6720 || Loss: 2.8304 || timer: 0.0913 sec.
iter 6730 || Loss: 2.1000 || timer: 0.0925 sec.
iter 6740 || Loss: 2.5018 || timer: 0.1061 sec.
iter 6750 || Loss: 2.0126 || timer: 0.0904 sec.
iter 6760 || Loss: 1.6609 || timer: 0.0900 sec.
iter 6770 || Loss: 2.5615 || timer: 0.0838 sec.
iter 6780 || Loss: 1.6999 || timer: 0.1067 sec.
iter 6790 || Loss: 2.2215 || timer: 0.0823 sec.
iter 6800 || Loss: 1.6137 || timer: 0.0968 sec.
iter 6810 || Loss: 1.8513 || timer: 0.0920 sec.
iter 6820 || Loss: 1.7904 || timer: 0.0178 sec.
iter 6830 || Loss: 1.9701 || timer: 0.0876 sec.
iter 6840 || Loss: 2.1827 || timer: 0.0842 sec.
iter 6850 || Loss: 2.2623 || timer: 0.0798 sec.
iter 6860 || Loss: 2.1000 || timer: 0.0879 sec.
iter 6870 || Loss: 2.5141 || timer: 0.0946 sec.
iter 6880 || Loss: 2.1710 || timer: 0.0939 sec.
iter 6890 || Loss: 1.9064 || timer: 0.1057 sec.
iter 6900 || Loss: 2.5720 || timer: 0.0909 sec.
iter 6910 || Loss: 2.6404 || timer: 0.0846 sec.
iter 6920 || Loss: 2.0266 || timer: 0.1076 sec.
iter 6930 || Loss: 1.6816 || timer: 0.0912 sec.
iter 6940 || Loss: 1.9807 || timer: 0.0829 sec.
iter 6950 || Loss: 2.2274 || timer: 0.0933 sec.
iter 6960 || Loss: 2.2998 || timer: 0.0915 sec.
iter 6970 || Loss: 1.5947 || timer: 0.0914 sec.
iter 6980 || Loss: 2.0538 || timer: 0.1029 sec.
iter 6990 || Loss: 2.1744 || timer: 0.0814 sec.
iter 7000 || Loss: 2.5055 || timer: 0.0957 sec.
iter 7010 || Loss: 2.5528 || timer: 0.0849 sec.
iter 7020 || Loss: 2.2659 || timer: 0.0847 sec.
iter 7030 || Loss: 2.4895 || timer: 0.0834 sec.
iter 7040 || Loss: 1.8889 || timer: 0.1176 sec.
iter 7050 || Loss: 2.0356 || timer: 0.0884 sec.
iter 7060 || Loss: 1.9955 || timer: 0.0923 sec.
iter 7070 || Loss: 1.9823 || timer: 0.1016 sec.
iter 7080 || Loss: 1.8776 || timer: 0.0894 sec.
iter 7090 || Loss: 1.6984 || timer: 0.0930 sec.
iter 7100 || Loss: 1.9325 || timer: 0.0917 sec.
iter 7110 || Loss: 2.3834 || timer: 0.0917 sec.
iter 7120 || Loss: 1.7643 || timer: 0.0841 sec.
iter 7130 || Loss: 2.8309 || timer: 0.1035 sec.
iter 7140 || Loss: 2.6756 || timer: 0.1008 sec.
iter 7150 || Loss: 2.2132 || timer: 0.0205 sec.
iter 7160 || Loss: 1.2727 || timer: 0.0902 sec.
iter 7170 || Loss: 2.5253 || timer: 0.0951 sec.
iter 7180 || Loss: 2.2806 || timer: 0.0856 sec.
iter 7190 || Loss: 2.1468 || timer: 0.0928 sec.
iter 7200 || Loss: 1.8204 || timer: 0.0821 sec.
iter 7210 || Loss: 2.0284 || timer: 0.0902 sec.
iter 7220 || Loss: 2.0078 || timer: 0.1031 sec.
iter 7230 || Loss: 1.7244 || timer: 0.0841 sec.
iter 7240 || Loss: 2.0591 || timer: 0.1043 sec.
iter 7250 || Loss: 2.0417 || timer: 0.0993 sec.
iter 7260 || Loss: 1.8279 || timer: 0.0915 sec.
iter 7270 || Loss: 1.9161 || timer: 0.0828 sec.
iter 7280 || Loss: 2.3705 || timer: 0.0868 sec.
iter 7290 || Loss: 2.4555 || timer: 0.0936 sec.
iter 7300 || Loss: 1.8656 || timer: 0.0841 sec.
iter 7310 || Loss: 2.0067 || timer: 0.1013 sec.
iter 7320 || Loss: 2.0452 || timer: 0.1223 sec.
iter 7330 || Loss: 2.2364 || timer: 0.0926 sec.
iter 7340 || Loss: 2.0226 || timer: 0.0924 sec.
iter 7350 || Loss: 2.0886 || timer: 0.0831 sec.
iter 7360 || Loss: 2.0633 || timer: 0.0901 sec.
iter 7370 || Loss: 2.0407 || timer: 0.0868 sec.
iter 7380 || Loss: 2.1573 || timer: 0.0827 sec.
iter 7390 || Loss: 1.9726 || timer: 0.0908 sec.
iter 7400 || Loss: 1.8089 || timer: 0.0921 sec.
iter 7410 || Loss: 1.9870 || timer: 0.0905 sec.
iter 7420 || Loss: 2.4693 || timer: 0.0841 sec.
iter 7430 || Loss: 1.8236 || timer: 0.0913 sec.
iter 7440 || Loss: 2.0422 || timer: 0.0913 sec.
iter 7450 || Loss: 2.4600 || timer: 0.0904 sec.
iter 7460 || Loss: 1.6433 || timer: 0.0993 sec.
iter 7470 || Loss: 1.9920 || timer: 0.0768 sec.
iter 7480 || Loss: 2.0919 || timer: 0.0161 sec.
iter 7490 || Loss: 1.8893 || timer: 0.0813 sec.
iter 7500 || Loss: 2.4571 || timer: 0.0870 sec.
iter 7510 || Loss: 2.5275 || timer: 0.0823 sec.
iter 7520 || Loss: 1.6406 || timer: 0.1017 sec.
iter 7530 || Loss: 2.4366 || timer: 0.0934 sec.
iter 7540 || Loss: 1.9532 || timer: 0.0772 sec.
iter 7550 || Loss: 2.2012 || timer: 0.1181 sec.
iter 7560 || Loss: 2.0544 || timer: 0.0924 sec.
iter 7570 || Loss: 1.5473 || timer: 0.0889 sec.
iter 7580 || Loss: 2.1751 || timer: 0.1013 sec.
iter 7590 || Loss: 2.3195 || timer: 0.0827 sec.
iter 7600 || Loss: 4.5392 || timer: 0.0918 sec.
iter 7610 || Loss: 4.2000 || timer: 0.0905 sec.
iter 7620 || Loss: 3.2141 || timer: 0.0780 sec.
iter 7630 || Loss: 2.6893 || timer: 0.0942 sec.
iter 7640 || Loss: 2.5781 || timer: 0.0931 sec.
iter 7650 || Loss: 1.8571 || timer: 0.0866 sec.
iter 7660 || Loss: 2.6325 || timer: 0.0946 sec.
iter 7670 || Loss: 2.7191 || timer: 0.0923 sec.
iter 7680 || Loss: 1.8938 || timer: 0.0842 sec.
iter 7690 || Loss: 2.2135 || timer: 0.0917 sec.
iter 7700 || Loss: 2.3917 || timer: 0.0780 sec.
iter 7710 || Loss: 2.0117 || timer: 0.0845 sec.
iter 7720 || Loss: 2.2006 || timer: 0.0899 sec.
iter 7730 || Loss: 2.2858 || timer: 0.0775 sec.
iter 7740 || Loss: 1.6009 || timer: 0.0841 sec.
iter 7750 || Loss: 2.1328 || timer: 0.0872 sec.
iter 7760 || Loss: 2.0346 || timer: 0.0935 sec.
iter 7770 || Loss: 2.3056 || timer: 0.0881 sec.
iter 7780 || Loss: 2.1779 || timer: 0.0934 sec.
iter 7790 || Loss: 2.2722 || timer: 0.0926 sec.
iter 7800 || Loss: 1.8887 || timer: 0.0828 sec.
iter 7810 || Loss: 1.9353 || timer: 0.0255 sec.
iter 7820 || Loss: 2.0981 || timer: 0.0840 sec.
iter 7830 || Loss: 1.9939 || timer: 0.0899 sec.
iter 7840 || Loss: 2.2629 || timer: 0.0884 sec.
iter 7850 || Loss: 1.9956 || timer: 0.0859 sec.
iter 7860 || Loss: 3.1303 || timer: 0.0941 sec.
iter 7870 || Loss: 2.4381 || timer: 0.0897 sec.
iter 7880 || Loss: 2.2447 || timer: 0.0979 sec.
iter 7890 || Loss: 2.3098 || timer: 0.1115 sec.
iter 7900 || Loss: 2.2016 || timer: 0.0840 sec.
iter 7910 || Loss: 2.0214 || timer: 0.1041 sec.
iter 7920 || Loss: 1.9246 || timer: 0.0902 sec.
iter 7930 || Loss: 1.9537 || timer: 0.0911 sec.
iter 7940 || Loss: 1.7035 || timer: 0.0869 sec.
iter 7950 || Loss: 1.9984 || timer: 0.0901 sec.
iter 7960 || Loss: 1.1908 || timer: 0.0926 sec.
iter 7970 || Loss: 1.8268 || timer: 0.0933 sec.
iter 7980 || Loss: 2.3510 || timer: 0.0958 sec.
iter 7990 || Loss: 2.1730 || timer: 0.0900 sec.
iter 8000 || Loss: 1.9867 || timer: 0.1099 sec.
iter 8010 || Loss: 2.3884 || timer: 0.0942 sec.
iter 8020 || Loss: 2.7197 || timer: 0.0840 sec.
iter 8030 || Loss: 2.3150 || timer: 0.0910 sec.
iter 8040 || Loss: 1.9120 || timer: 0.0925 sec.
iter 8050 || Loss: 2.1291 || timer: 0.0934 sec.
iter 8060 || Loss: 2.1203 || timer: 0.1146 sec.
iter 8070 || Loss: 1.9789 || timer: 0.0863 sec.
iter 8080 || Loss: 2.0379 || timer: 0.0951 sec.
iter 8090 || Loss: 1.9158 || timer: 0.0900 sec.
iter 8100 || Loss: 1.9587 || timer: 0.0921 sec.
iter 8110 || Loss: 1.9094 || timer: 0.0895 sec.
iter 8120 || Loss: 1.4650 || timer: 0.0959 sec.
iter 8130 || Loss: 1.5573 || timer: 0.0832 sec.
iter 8140 || Loss: 1.9270 || timer: 0.0192 sec.
iter 8150 || Loss: 2.0330 || timer: 0.0911 sec.
iter 8160 || Loss: 1.9550 || timer: 0.0847 sec.
iter 8170 || Loss: 1.7037 || timer: 0.0792 sec.
iter 8180 || Loss: 1.7878 || timer: 0.0757 sec.
iter 8190 || Loss: 2.5814 || timer: 0.0934 sec.
iter 8200 || Loss: 1.6020 || timer: 0.0936 sec.
iter 8210 || Loss: 2.1332 || timer: 0.0858 sec.
iter 8220 || Loss: 2.8728 || timer: 0.0852 sec.
iter 8230 || Loss: 2.3215 || timer: 0.0918 sec.
iter 8240 || Loss: 1.9252 || timer: 0.1279 sec.
iter 8250 || Loss: 2.0306 || timer: 0.0878 sec.
iter 8260 || Loss: 1.9815 || timer: 0.0998 sec.
iter 8270 || Loss: 2.4942 || timer: 0.0896 sec.
iter 8280 || Loss: 1.7045 || timer: 0.0841 sec.
iter 8290 || Loss: 1.7303 || timer: 0.1054 sec.
iter 8300 || Loss: 2.1142 || timer: 0.0884 sec.
iter 8310 || Loss: 1.9249 || timer: 0.0834 sec.
iter 8320 || Loss: 1.6069 || timer: 0.1045 sec.
iter 8330 || Loss: 2.0136 || timer: 0.1190 sec.
iter 8340 || Loss: 1.9727 || timer: 0.0878 sec.
iter 8350 || Loss: 1.6436 || timer: 0.0825 sec.
iter 8360 || Loss: 2.1849 || timer: 0.0845 sec.
iter 8370 || Loss: 1.7579 || timer: 0.0740 sec.
iter 8380 || Loss: 1.5993 || timer: 0.0892 sec.
iter 8390 || Loss: 1.9384 || timer: 0.0879 sec.
iter 8400 || Loss: 1.9990 || timer: 0.0884 sec.
iter 8410 || Loss: 1.8461 || timer: 0.0824 sec.
iter 8420 || Loss: 1.8935 || timer: 0.0895 sec.
iter 8430 || Loss: 2.3337 || timer: 0.0922 sec.
iter 8440 || Loss: 1.6509 || timer: 0.0893 sec.
iter 8450 || Loss: 1.4674 || timer: 0.0740 sec.
iter 8460 || Loss: 1.4107 || timer: 0.0887 sec.
iter 8470 || Loss: 1.7146 || timer: 0.0174 sec.
iter 8480 || Loss: 0.7874 || timer: 0.0824 sec.
iter 8490 || Loss: 2.2061 || timer: 0.0900 sec.
iter 8500 || Loss: 1.7704 || timer: 0.0964 sec.
iter 8510 || Loss: 1.7443 || timer: 0.0828 sec.
iter 8520 || Loss: 1.8824 || timer: 0.0843 sec.
iter 8530 || Loss: 1.9103 || timer: 0.1035 sec.
iter 8540 || Loss: 1.6861 || timer: 0.0839 sec.
iter 8550 || Loss: 1.6277 || timer: 0.0873 sec.
iter 8560 || Loss: 2.1735 || timer: 0.1048 sec.
iter 8570 || Loss: 1.8425 || timer: 0.1013 sec.
iter 8580 || Loss: 1.8933 || timer: 0.1081 sec.
iter 8590 || Loss: 1.7332 || timer: 0.0825 sec.
iter 8600 || Loss: 1.9950 || timer: 0.0906 sec.
iter 8610 || Loss: 1.4763 || timer: 0.0930 sec.
iter 8620 || Loss: 1.7370 || timer: 0.0865 sec.
iter 8630 || Loss: 2.1971 || timer: 0.0889 sec.
iter 8640 || Loss: 1.4303 || timer: 0.0901 sec.
iter 8650 || Loss: 2.1484 || timer: 0.0895 sec.
iter 8660 || Loss: 1.9611 || timer: 0.0910 sec.
iter 8670 || Loss: 1.6998 || timer: 0.1086 sec.
iter 8680 || Loss: 2.2153 || timer: 0.0825 sec.
iter 8690 || Loss: 2.6134 || timer: 0.0824 sec.
iter 8700 || Loss: 1.5783 || timer: 0.0812 sec.
iter 8710 || Loss: 1.7894 || timer: 0.0896 sec.
iter 8720 || Loss: 1.9074 || timer: 0.0933 sec.
iter 8730 || Loss: 2.3477 || timer: 0.1074 sec.
iter 8740 || Loss: 2.0386 || timer: 0.0914 sec.
iter 8750 || Loss: 2.0834 || timer: 0.0927 sec.
iter 8760 || Loss: 1.6974 || timer: 0.0886 sec.
iter 8770 || Loss: 1.8837 || timer: 0.0960 sec.
iter 8780 || Loss: 1.8152 || timer: 0.0926 sec.
iter 8790 || Loss: 1.6378 || timer: 0.0960 sec.
iter 8800 || Loss: 1.6668 || timer: 0.0178 sec.
iter 8810 || Loss: 1.7589 || timer: 0.0817 sec.
iter 8820 || Loss: 1.8299 || timer: 0.0828 sec.
iter 8830 || Loss: 2.4763 || timer: 0.0803 sec.
iter 8840 || Loss: 1.9768 || timer: 0.0918 sec.
iter 8850 || Loss: 2.1601 || timer: 0.0919 sec.
iter 8860 || Loss: 2.2078 || timer: 0.1006 sec.
iter 8870 || Loss: 2.0005 || timer: 0.0919 sec.
iter 8880 || Loss: 1.4870 || timer: 0.1157 sec.
iter 8890 || Loss: 2.6270 || timer: 0.0925 sec.
iter 8900 || Loss: 1.7999 || timer: 0.1130 sec.
iter 8910 || Loss: 1.8685 || timer: 0.0908 sec.
iter 8920 || Loss: 1.6288 || timer: 0.0839 sec.
iter 8930 || Loss: 1.8713 || timer: 0.0908 sec.
iter 8940 || Loss: 1.7839 || timer: 0.1016 sec.
iter 8950 || Loss: 1.9055 || timer: 0.0828 sec.
iter 8960 || Loss: 1.8464 || timer: 0.0890 sec.
iter 8970 || Loss: 1.9790 || timer: 0.0938 sec.
iter 8980 || Loss: 1.6902 || timer: 0.0873 sec.
iter 8990 || Loss: 1.4924 || timer: 0.0917 sec.
iter 9000 || Loss: 2.2751 || timer: 0.0918 sec.
iter 9010 || Loss: 2.0180 || timer: 0.0888 sec.
iter 9020 || Loss: 2.1288 || timer: 0.0918 sec.
iter 9030 || Loss: 1.6516 || timer: 0.0887 sec.
iter 9040 || Loss: 1.7426 || timer: 0.0909 sec.
iter 9050 || Loss: 2.5019 || timer: 0.0891 sec.
iter 9060 || Loss: 1.9999 || timer: 0.0876 sec.
iter 9070 || Loss: 1.8556 || timer: 0.1199 sec.
iter 9080 || Loss: 1.3869 || timer: 0.0876 sec.
iter 9090 || Loss: 1.7085 || timer: 0.0910 sec.
iter 9100 || Loss: 2.5259 || timer: 0.0981 sec.
iter 9110 || Loss: 1.7941 || timer: 0.0920 sec.
iter 9120 || Loss: 1.6883 || timer: 0.0888 sec.
iter 9130 || Loss: 1.8213 || timer: 0.0252 sec.
iter 9140 || Loss: 5.4382 || timer: 0.0850 sec.
iter 9150 || Loss: 1.9610 || timer: 0.0823 sec.
iter 9160 || Loss: 1.5648 || timer: 0.1037 sec.
iter 9170 || Loss: 1.7160 || timer: 0.0762 sec.
iter 9180 || Loss: 1.6431 || timer: 0.0814 sec.
iter 9190 || Loss: 1.7845 || timer: 0.0887 sec.
iter 9200 || Loss: 1.3580 || timer: 0.0923 sec.
iter 9210 || Loss: 1.8470 || timer: 0.0923 sec.
iter 9220 || Loss: 2.4895 || timer: 0.0821 sec.
iter 9230 || Loss: 2.2983 || timer: 0.1381 sec.
iter 9240 || Loss: 2.1659 || timer: 0.1047 sec.
iter 9250 || Loss: 2.2518 || timer: 0.0825 sec.
iter 9260 || Loss: 1.7676 || timer: 0.0903 sec.
iter 9270 || Loss: 2.0117 || timer: 0.0780 sec.
iter 9280 || Loss: 2.1011 || timer: 0.0762 sec.
iter 9290 || Loss: 1.7359 || timer: 0.0902 sec.
iter 9300 || Loss: 2.0474 || timer: 0.0912 sec.
iter 9310 || Loss: 1.9488 || timer: 0.0830 sec.
iter 9320 || Loss: 1.8146 || timer: 0.0736 sec.
iter 9330 || Loss: 1.3675 || timer: 0.0743 sec.
iter 9340 || Loss: 1.6424 || timer: 0.0920 sec.
iter 9350 || Loss: 1.9293 || timer: 0.0915 sec.
iter 9360 || Loss: 2.1058 || timer: 0.0888 sec.
iter 9370 || Loss: 2.0360 || timer: 0.0932 sec.
iter 9380 || Loss: 2.2385 || timer: 0.0897 sec.
iter 9390 || Loss: 2.5717 || timer: 0.0820 sec.
iter 9400 || Loss: 1.5796 || timer: 0.0911 sec.
iter 9410 || Loss: 2.0968 || timer: 0.0828 sec.
iter 9420 || Loss: 2.3552 || timer: 0.0817 sec.
iter 9430 || Loss: 1.7979 || timer: 0.0871 sec.
iter 9440 || Loss: 1.7334 || timer: 0.0909 sec.
iter 9450 || Loss: 1.8525 || timer: 0.0849 sec.
iter 9460 || Loss: 1.4106 || timer: 0.0176 sec.
iter 9470 || Loss: 1.9681 || timer: 0.0905 sec.
iter 9480 || Loss: 1.9749 || timer: 0.0924 sec.
iter 9490 || Loss: 2.4505 || timer: 0.0816 sec.
iter 9500 || Loss: 1.8104 || timer: 0.0832 sec.
iter 9510 || Loss: 2.6451 || timer: 0.0822 sec.
iter 9520 || Loss: 2.1830 || timer: 0.0925 sec.
iter 9530 || Loss: 1.9859 || timer: 0.0913 sec.
iter 9540 || Loss: 2.9458 || timer: 0.0858 sec.
iter 9550 || Loss: 1.6923 || timer: 0.0873 sec.
iter 9560 || Loss: 2.0459 || timer: 0.1174 sec.
iter 9570 || Loss: 1.8246 || timer: 0.0932 sec.
iter 9580 || Loss: 1.5322 || timer: 0.1102 sec.
iter 9590 || Loss: 1.7059 || timer: 0.0888 sec.
iter 9600 || Loss: 1.9502 || timer: 0.0832 sec.
iter 9610 || Loss: 1.7355 || timer: 0.0856 sec.
iter 9620 || Loss: 1.4759 || timer: 0.0914 sec.
iter 9630 || Loss: 1.5547 || timer: 0.1127 sec.
iter 9640 || Loss: 2.6009 || timer: 0.0930 sec.
iter 9650 || Loss: 2.2173 || timer: 0.0934 sec.
iter 9660 || Loss: 2.0840 || timer: 0.0919 sec.
iter 9670 || Loss: 2.1796 || timer: 0.1062 sec.
iter 9680 || Loss: 1.7407 || timer: 0.0825 sec.
iter 9690 || Loss: 1.9346 || timer: 0.0887 sec.
iter 9700 || Loss: 1.7242 || timer: 0.0938 sec.
iter 9710 || Loss: 1.8643 || timer: 0.0922 sec.
iter 9720 || Loss: 1.7015 || timer: 0.0894 sec.
iter 9730 || Loss: 2.0559 || timer: 0.0834 sec.
iter 9740 || Loss: 2.3467 || timer: 0.0903 sec.
iter 9750 || Loss: 2.0796 || timer: 0.0754 sec.
iter 9760 || Loss: 2.0259 || timer: 0.0948 sec.
iter 9770 || Loss: 1.5758 || timer: 0.0920 sec.
iter 9780 || Loss: 1.5524 || timer: 0.1125 sec.
iter 9790 || Loss: 1.8503 || timer: 0.0256 sec.
iter 9800 || Loss: 1.6640 || timer: 0.1202 sec.
iter 9810 || Loss: 1.8783 || timer: 0.0918 sec.
iter 9820 || Loss: 1.5681 || timer: 0.0866 sec.
iter 9830 || Loss: 1.8712 || timer: 0.0915 sec.
iter 9840 || Loss: 1.6696 || timer: 0.0954 sec.
iter 9850 || Loss: 1.8162 || timer: 0.0931 sec.
iter 9860 || Loss: 2.5401 || timer: 0.0849 sec.
iter 9870 || Loss: 2.1604 || timer: 0.0920 sec.
iter 9880 || Loss: 1.6467 || timer: 0.0929 sec.
iter 9890 || Loss: 1.9512 || timer: 0.1206 sec.
iter 9900 || Loss: 1.6282 || timer: 0.0869 sec.
iter 9910 || Loss: 1.7921 || timer: 0.0853 sec.
iter 9920 || Loss: 1.8312 || timer: 0.0926 sec.
iter 9930 || Loss: 1.8506 || timer: 0.0933 sec.
iter 9940 || Loss: 3.4834 || timer: 0.0838 sec.
iter 9950 || Loss: 2.0933 || timer: 0.0867 sec.
iter 9960 || Loss: 2.2260 || timer: 0.0898 sec.
iter 9970 || Loss: 2.5196 || timer: 0.0857 sec.
iter 9980 || Loss: 2.0910 || timer: 0.0870 sec.
iter 9990 || Loss: 1.7992 || timer: 0.0974 sec.
iter 10000 || Loss: 1.5599 || Saving state, iter: 10000
timer: 0.0927 sec.
iter 10010 || Loss: 2.7135 || timer: 0.0911 sec.
iter 10020 || Loss: 2.0198 || timer: 0.0868 sec.
iter 10030 || Loss: 2.0053 || timer: 0.0892 sec.
iter 10040 || Loss: 1.5961 || timer: 0.0921 sec.
iter 10050 || Loss: 1.7700 || timer: 0.0920 sec.
iter 10060 || Loss: 1.5508 || timer: 0.0856 sec.
iter 10070 || Loss: 1.5741 || timer: 0.0976 sec.
iter 10080 || Loss: 1.8295 || timer: 0.1064 sec.
iter 10090 || Loss: 1.4028 || timer: 0.0959 sec.
iter 10100 || Loss: 3.0625 || timer: 0.0836 sec.
iter 10110 || Loss: 2.1330 || timer: 0.1027 sec.
iter 10120 || Loss: 2.1242 || timer: 0.0319 sec.
iter 10130 || Loss: 0.8405 || timer: 0.0904 sec.
iter 10140 || Loss: 2.5036 || timer: 0.1051 sec.
iter 10150 || Loss: 1.9327 || timer: 0.0849 sec.
iter 10160 || Loss: 1.5367 || timer: 0.0938 sec.
iter 10170 || Loss: 2.1464 || timer: 0.0914 sec.
iter 10180 || Loss: 1.8898 || timer: 0.0899 sec.
iter 10190 || Loss: 1.5245 || timer: 0.0895 sec.
iter 10200 || Loss: 1.6711 || timer: 0.0944 sec.
iter 10210 || Loss: 2.2548 || timer: 0.1041 sec.
iter 10220 || Loss: 1.8770 || timer: 0.0968 sec.
iter 10230 || Loss: 3.0262 || timer: 0.0778 sec.
iter 10240 || Loss: 2.3918 || timer: 0.0932 sec.
iter 10250 || Loss: 2.2049 || timer: 0.0919 sec.
iter 10260 || Loss: 2.8273 || timer: 0.0935 sec.
iter 10270 || Loss: 2.0360 || timer: 0.0841 sec.
iter 10280 || Loss: 2.1148 || timer: 0.0890 sec.
iter 10290 || Loss: 2.2989 || timer: 0.0888 sec.
iter 10300 || Loss: 1.7666 || timer: 0.0861 sec.
iter 10310 || Loss: 2.2471 || timer: 0.0995 sec.
iter 10320 || Loss: 2.3012 || timer: 0.1074 sec.
iter 10330 || Loss: 1.6706 || timer: 0.0846 sec.
iter 10340 || Loss: 2.5690 || timer: 0.0932 sec.
iter 10350 || Loss: 1.6581 || timer: 0.0759 sec.
iter 10360 || Loss: 1.8542 || timer: 0.0919 sec.
iter 10370 || Loss: 1.8553 || timer: 0.1142 sec.
iter 10380 || Loss: 1.6687 || timer: 0.0948 sec.
iter 10390 || Loss: 1.9484 || timer: 0.0963 sec.
iter 10400 || Loss: 1.4774 || timer: 0.1086 sec.
iter 10410 || Loss: 1.6845 || timer: 0.0929 sec.
iter 10420 || Loss: 1.4359 || timer: 0.0835 sec.
iter 10430 || Loss: 2.1720 || timer: 0.0817 sec.
iter 10440 || Loss: 1.4492 || timer: 0.0958 sec.
iter 10450 || Loss: 1.9809 || timer: 0.0261 sec.
iter 10460 || Loss: 2.8891 || timer: 0.0852 sec.
iter 10470 || Loss: 2.6151 || timer: 0.0923 sec.
iter 10480 || Loss: 2.0807 || timer: 0.0937 sec.
iter 10490 || Loss: 2.2844 || timer: 0.1388 sec.
iter 10500 || Loss: 2.0702 || timer: 0.0922 sec.
iter 10510 || Loss: 2.4773 || timer: 0.0921 sec.
iter 10520 || Loss: 1.8225 || timer: 0.0861 sec.
iter 10530 || Loss: 1.2590 || timer: 0.1015 sec.
iter 10540 || Loss: 1.7371 || timer: 0.1027 sec.
iter 10550 || Loss: 1.9373 || timer: 0.1600 sec.
iter 10560 || Loss: 1.8229 || timer: 0.0936 sec.
iter 10570 || Loss: 1.6376 || timer: 0.0921 sec.
iter 10580 || Loss: 1.8339 || timer: 0.0920 sec.
iter 10590 || Loss: 1.6539 || timer: 0.1026 sec.
iter 10600 || Loss: 2.5849 || timer: 0.0966 sec.
iter 10610 || Loss: 1.9882 || timer: 0.1010 sec.
iter 10620 || Loss: 2.3042 || timer: 0.1189 sec.
iter 10630 || Loss: 2.5329 || timer: 0.0883 sec.
iter 10640 || Loss: 1.2692 || timer: 0.0907 sec.
iter 10650 || Loss: 1.3698 || timer: 0.0874 sec.
iter 10660 || Loss: 1.9200 || timer: 0.0916 sec.
iter 10670 || Loss: 1.6027 || timer: 0.0912 sec.
iter 10680 || Loss: 2.0495 || timer: 0.0894 sec.
iter 10690 || Loss: 1.9055 || timer: 0.1085 sec.
iter 10700 || Loss: 1.8547 || timer: 0.0933 sec.
iter 10710 || Loss: 1.8722 || timer: 0.0860 sec.
iter 10720 || Loss: 1.8272 || timer: 0.1037 sec.
iter 10730 || Loss: 1.6793 || timer: 0.0847 sec.
iter 10740 || Loss: 2.2253 || timer: 0.1047 sec.
iter 10750 || Loss: 1.8071 || timer: 0.0835 sec.
iter 10760 || Loss: 1.5071 || timer: 0.0897 sec.
iter 10770 || Loss: 2.0631 || timer: 0.1040 sec.
iter 10780 || Loss: 1.7307 || timer: 0.0269 sec.
iter 10790 || Loss: 1.6339 || timer: 0.0814 sec.
iter 10800 || Loss: 1.9511 || timer: 0.0828 sec.
iter 10810 || Loss: 1.8286 || timer: 0.0885 sec.
iter 10820 || Loss: 1.6117 || timer: 0.0904 sec.
iter 10830 || Loss: 1.5370 || timer: 0.0816 sec.
iter 10840 || Loss: 1.6555 || timer: 0.1114 sec.
iter 10850 || Loss: 1.7219 || timer: 0.0899 sec.
iter 10860 || Loss: 1.6856 || timer: 0.0907 sec.
iter 10870 || Loss: 1.8811 || timer: 0.0870 sec.
iter 10880 || Loss: 1.6060 || timer: 0.0869 sec.
iter 10890 || Loss: 1.5591 || timer: 0.1027 sec.
iter 10900 || Loss: 1.7982 || timer: 0.1046 sec.
iter 10910 || Loss: 1.4908 || timer: 0.0799 sec.
iter 10920 || Loss: 2.3671 || timer: 0.1024 sec.
iter 10930 || Loss: 1.8981 || timer: 0.0824 sec.
iter 10940 || Loss: 1.8821 || timer: 0.0931 sec.
iter 10950 || Loss: 1.7105 || timer: 0.1017 sec.
iter 10960 || Loss: 1.9043 || timer: 0.0890 sec.
iter 10970 || Loss: 1.6891 || timer: 0.0812 sec.
iter 10980 || Loss: 2.0313 || timer: 0.1250 sec.
iter 10990 || Loss: 1.6405 || timer: 0.0832 sec.
iter 11000 || Loss: 1.8673 || timer: 0.1019 sec.
iter 11010 || Loss: 1.7056 || timer: 0.0919 sec.
iter 11020 || Loss: 1.7989 || timer: 0.0830 sec.
iter 11030 || Loss: 1.8575 || timer: 0.0895 sec.
iter 11040 || Loss: 1.6719 || timer: 0.0828 sec.
iter 11050 || Loss: 2.1122 || timer: 0.0830 sec.
iter 11060 || Loss: 1.9339 || timer: 0.1062 sec.
iter 11070 || Loss: 2.0890 || timer: 0.0901 sec.
iter 11080 || Loss: 1.7200 || timer: 0.0863 sec.
iter 11090 || Loss: 1.8244 || timer: 0.0806 sec.
iter 11100 || Loss: 2.1447 || timer: 0.1208 sec.
iter 11110 || Loss: 1.4788 || timer: 0.0165 sec.
iter 11120 || Loss: 1.2549 || timer: 0.0825 sec.
iter 11130 || Loss: 2.0919 || timer: 0.0911 sec.
iter 11140 || Loss: 1.9117 || timer: 0.0808 sec.
iter 11150 || Loss: 1.4526 || timer: 0.0832 sec.
iter 11160 || Loss: 1.9224 || timer: 0.0887 sec.
iter 11170 || Loss: 1.8102 || timer: 0.0875 sec.
iter 11180 || Loss: 1.5441 || timer: 0.0907 sec.
iter 11190 || Loss: 2.4490 || timer: 0.0830 sec.
iter 11200 || Loss: 1.8653 || timer: 0.1211 sec.
iter 11210 || Loss: 1.9752 || timer: 0.1291 sec.
iter 11220 || Loss: 1.5601 || timer: 0.0837 sec.
iter 11230 || Loss: 2.0349 || timer: 0.0908 sec.
iter 11240 || Loss: 1.7156 || timer: 0.0829 sec.
iter 11250 || Loss: 1.7698 || timer: 0.0826 sec.
iter 11260 || Loss: 1.7309 || timer: 0.0823 sec.
iter 11270 || Loss: 2.1925 || timer: 0.0824 sec.
iter 11280 || Loss: 1.7310 || timer: 0.0912 sec.
iter 11290 || Loss: 1.7544 || timer: 0.0822 sec.
iter 11300 || Loss: 1.5502 || timer: 0.0888 sec.
iter 11310 || Loss: 1.6697 || timer: 0.1026 sec.
iter 11320 || Loss: 1.4129 || timer: 0.0907 sec.
iter 11330 || Loss: 1.7821 || timer: 0.1085 sec.
iter 11340 || Loss: 1.8412 || timer: 0.0823 sec.
iter 11350 || Loss: 1.7943 || timer: 0.1055 sec.
iter 11360 || Loss: 1.7023 || timer: 0.0918 sec.
iter 11370 || Loss: 1.8875 || timer: 0.0839 sec.
iter 11380 || Loss: 1.4705 || timer: 0.0889 sec.
iter 11390 || Loss: 1.3020 || timer: 0.0826 sec.
iter 11400 || Loss: 1.9669 || timer: 0.0897 sec.
iter 11410 || Loss: 1.7597 || timer: 0.0866 sec.
iter 11420 || Loss: 1.5982 || timer: 0.0887 sec.
iter 11430 || Loss: 2.1332 || timer: 0.0899 sec.
iter 11440 || Loss: 1.6966 || timer: 0.0194 sec.
iter 11450 || Loss: 2.6842 || timer: 0.0815 sec.
iter 11460 || Loss: 1.9868 || timer: 0.0906 sec.
iter 11470 || Loss: 2.1636 || timer: 0.0929 sec.
iter 11480 || Loss: 1.7548 || timer: 0.0835 sec.
iter 11490 || Loss: 1.8650 || timer: 0.0840 sec.
iter 11500 || Loss: 1.8801 || timer: 0.1094 sec.
iter 11510 || Loss: 1.4459 || timer: 0.0828 sec.
iter 11520 || Loss: 1.6468 || timer: 0.0920 sec.
iter 11530 || Loss: 1.4852 || timer: 0.0826 sec.
iter 11540 || Loss: 1.7451 || timer: 0.1163 sec.
iter 11550 || Loss: 1.6478 || timer: 0.0918 sec.
iter 11560 || Loss: 1.4270 || timer: 0.0809 sec.
iter 11570 || Loss: 1.6374 || timer: 0.0871 sec.
iter 11580 || Loss: 1.6869 || timer: 0.0824 sec.
iter 11590 || Loss: 2.0466 || timer: 0.0898 sec.
iter 11600 || Loss: 1.5099 || timer: 0.0821 sec.
iter 11610 || Loss: 1.5930 || timer: 0.0867 sec.
iter 11620 || Loss: 1.9994 || timer: 0.0830 sec.
iter 11630 || Loss: 1.8083 || timer: 0.0837 sec.
iter 11640 || Loss: 1.7165 || timer: 0.0927 sec.
iter 11650 || Loss: 1.8004 || timer: 0.0920 sec.
iter 11660 || Loss: 1.7196 || timer: 0.1082 sec.
iter 11670 || Loss: 1.8251 || timer: 0.0912 sec.
iter 11680 || Loss: 1.6664 || timer: 0.1024 sec.
iter 11690 || Loss: 1.6985 || timer: 0.0793 sec.
iter 11700 || Loss: 1.8792 || timer: 0.0894 sec.
iter 11710 || Loss: 1.6462 || timer: 0.1010 sec.
iter 11720 || Loss: 2.1767 || timer: 0.1617 sec.
iter 11730 || Loss: 1.7371 || timer: 0.1529 sec.
iter 11740 || Loss: 1.5082 || timer: 0.1761 sec.
iter 11750 || Loss: 1.7191 || timer: 0.1647 sec.
iter 11760 || Loss: 1.7469 || timer: 0.0927 sec.
iter 11770 || Loss: 1.4960 || timer: 0.0207 sec.
iter 11780 || Loss: 1.4106 || timer: 0.0842 sec.
iter 11790 || Loss: 1.3839 || timer: 0.1101 sec.
iter 11800 || Loss: 1.5386 || timer: 0.0875 sec.
iter 11810 || Loss: 2.0931 || timer: 0.0892 sec.
iter 11820 || Loss: 1.9304 || timer: 0.0882 sec.
iter 11830 || Loss: 1.8901 || timer: 0.1005 sec.
iter 11840 || Loss: 1.5440 || timer: 0.0753 sec.
iter 11850 || Loss: 1.7425 || timer: 0.0777 sec.
iter 11860 || Loss: 2.1994 || timer: 0.0909 sec.
iter 11870 || Loss: 1.8553 || timer: 0.0991 sec.
iter 11880 || Loss: 1.8300 || timer: 0.0941 sec.
iter 11890 || Loss: 1.7974 || timer: 0.0975 sec.
iter 11900 || Loss: 2.1208 || timer: 0.0924 sec.
iter 11910 || Loss: 1.8225 || timer: 0.0853 sec.
iter 11920 || Loss: 2.1629 || timer: 0.0838 sec.
iter 11930 || Loss: 1.5724 || timer: 0.0842 sec.
iter 11940 || Loss: 1.9860 || timer: 0.0836 sec.
iter 11950 || Loss: 1.7597 || timer: 0.0841 sec.
iter 11960 || Loss: 1.5000 || timer: 0.0893 sec.
iter 11970 || Loss: 1.8532 || timer: 0.0908 sec.
iter 11980 || Loss: 1.2605 || timer: 0.0890 sec.
iter 11990 || Loss: 1.7522 || timer: 0.0903 sec.
iter 12000 || Loss: 1.6699 || timer: 0.0732 sec.
iter 12010 || Loss: 1.4955 || timer: 0.1015 sec.
iter 12020 || Loss: 1.6266 || timer: 0.0833 sec.
iter 12030 || Loss: 1.7018 || timer: 0.0828 sec.
iter 12040 || Loss: 1.7634 || timer: 0.0884 sec.
iter 12050 || Loss: 1.7821 || timer: 0.1085 sec.
iter 12060 || Loss: 1.5004 || timer: 0.0827 sec.
iter 12070 || Loss: 1.7733 || timer: 0.0829 sec.
iter 12080 || Loss: 1.7817 || timer: 0.0873 sec.
iter 12090 || Loss: 1.9321 || timer: 0.0919 sec.
iter 12100 || Loss: 1.4471 || timer: 0.0165 sec.
iter 12110 || Loss: 2.1534 || timer: 0.0811 sec.
iter 12120 || Loss: 1.3619 || timer: 0.1189 sec.
iter 12130 || Loss: 2.0723 || timer: 0.0893 sec.
iter 12140 || Loss: 1.9177 || timer: 0.0822 sec.
iter 12150 || Loss: 1.7362 || timer: 0.0881 sec.
iter 12160 || Loss: 1.4730 || timer: 0.0812 sec.
iter 12170 || Loss: 1.6900 || timer: 0.0904 sec.
iter 12180 || Loss: 1.4302 || timer: 0.1010 sec.
iter 12190 || Loss: 1.8028 || timer: 0.1657 sec.
iter 12200 || Loss: 1.4642 || timer: 0.1341 sec.
iter 12210 || Loss: 1.3778 || timer: 0.1832 sec.
iter 12220 || Loss: 2.4395 || timer: 0.1427 sec.
iter 12230 || Loss: 1.8852 || timer: 0.0832 sec.
iter 12240 || Loss: 1.5419 || timer: 0.0824 sec.
iter 12250 || Loss: 1.7212 || timer: 0.0829 sec.
iter 12260 || Loss: 1.4990 || timer: 0.0850 sec.
iter 12270 || Loss: 2.2832 || timer: 0.0928 sec.
iter 12280 || Loss: 2.2182 || timer: 0.0932 sec.
iter 12290 || Loss: 2.2584 || timer: 0.0842 sec.
iter 12300 || Loss: 1.6641 || timer: 0.1137 sec.
iter 12310 || Loss: 1.6963 || timer: 0.0923 sec.
iter 12320 || Loss: 1.9565 || timer: 0.1028 sec.
iter 12330 || Loss: 1.6632 || timer: 0.0817 sec.
iter 12340 || Loss: 1.4646 || timer: 0.0900 sec.
iter 12350 || Loss: 1.8582 || timer: 0.0960 sec.
iter 12360 || Loss: 1.9314 || timer: 0.1111 sec.
iter 12370 || Loss: 1.7701 || timer: 0.0894 sec.
iter 12380 || Loss: 1.9754 || timer: 0.0829 sec.
iter 12390 || Loss: 1.6374 || timer: 0.0818 sec.
iter 12400 || Loss: 1.2951 || timer: 0.0892 sec.
iter 12410 || Loss: 1.7742 || timer: 0.0893 sec.
iter 12420 || Loss: 1.5963 || timer: 0.0829 sec.
iter 12430 || Loss: 2.1307 || timer: 0.0194 sec.
iter 12440 || Loss: 2.9165 || timer: 0.0954 sec.
iter 12450 || Loss: 1.8320 || timer: 0.1048 sec.
iter 12460 || Loss: 1.8975 || timer: 0.1314 sec.
iter 12470 || Loss: 1.6032 || timer: 0.1072 sec.
iter 12480 || Loss: 1.8593 || timer: 0.1320 sec.
iter 12490 || Loss: 1.4197 || timer: 0.1741 sec.
iter 12500 || Loss: 1.6230 || timer: 0.1722 sec.
iter 12510 || Loss: 1.9686 || timer: 0.1424 sec.
iter 12520 || Loss: 2.2493 || timer: 0.0907 sec.
iter 12530 || Loss: 2.0829 || timer: 0.1217 sec.
iter 12540 || Loss: 2.2533 || timer: 0.0905 sec.
iter 12550 || Loss: 1.3529 || timer: 0.0897 sec.
iter 12560 || Loss: 1.8802 || timer: 0.0914 sec.
iter 12570 || Loss: 1.9378 || timer: 0.0885 sec.
iter 12580 || Loss: 1.9077 || timer: 0.0920 sec.
iter 12590 || Loss: 1.8084 || timer: 0.0957 sec.
iter 12600 || Loss: 1.8995 || timer: 0.0898 sec.
iter 12610 || Loss: 1.5703 || timer: 0.0823 sec.
iter 12620 || Loss: 1.2256 || timer: 0.1115 sec.
iter 12630 || Loss: 1.7531 || timer: 0.1300 sec.
iter 12640 || Loss: 1.1708 || timer: 0.0808 sec.
iter 12650 || Loss: 1.7558 || timer: 0.0811 sec.
iter 12660 || Loss: 1.6929 || timer: 0.0921 sec.
iter 12670 || Loss: 1.7252 || timer: 0.0942 sec.
iter 12680 || Loss: 1.8032 || timer: 0.0881 sec.
iter 12690 || Loss: 1.5695 || timer: 0.1146 sec.
iter 12700 || Loss: 1.4973 || timer: 0.0826 sec.
iter 12710 || Loss: 1.5863 || timer: 0.0914 sec.
iter 12720 || Loss: 1.9902 || timer: 0.0824 sec.
iter 12730 || Loss: 1.7547 || timer: 0.0875 sec.
iter 12740 || Loss: 1.7398 || timer: 0.1038 sec.
iter 12750 || Loss: 1.6812 || timer: 0.0862 sec.
iter 12760 || Loss: 1.4745 || timer: 0.0192 sec.
iter 12770 || Loss: 1.5551 || timer: 0.0897 sec.
iter 12780 || Loss: 2.4169 || timer: 0.0887 sec.
iter 12790 || Loss: 1.9651 || timer: 0.1069 sec.
iter 12800 || Loss: 2.0753 || timer: 0.1045 sec.
iter 12810 || Loss: 2.4776 || timer: 0.0854 sec.
iter 12820 || Loss: 1.5305 || timer: 0.0924 sec.
iter 12830 || Loss: 1.8325 || timer: 0.1732 sec.
iter 12840 || Loss: 1.8600 || timer: 0.1744 sec.
iter 12850 || Loss: 1.2995 || timer: 0.1614 sec.
iter 12860 || Loss: 1.9882 || timer: 0.1558 sec.
iter 12870 || Loss: 1.6154 || timer: 0.1423 sec.
iter 12880 || Loss: 1.9229 || timer: 0.0925 sec.
iter 12890 || Loss: 1.9402 || timer: 0.0890 sec.
iter 12900 || Loss: 1.2547 || timer: 0.0822 sec.
iter 12910 || Loss: 1.9356 || timer: 0.0908 sec.
iter 12920 || Loss: 1.7021 || timer: 0.0917 sec.
iter 12930 || Loss: 1.6015 || timer: 0.0860 sec.
iter 12940 || Loss: 2.4840 || timer: 0.0898 sec.
iter 12950 || Loss: 1.6745 || timer: 0.1040 sec.
iter 12960 || Loss: 1.4534 || timer: 0.0904 sec.
iter 12970 || Loss: 1.7603 || timer: 0.0912 sec.
iter 12980 || Loss: 2.0513 || timer: 0.0904 sec.
iter 12990 || Loss: 1.9244 || timer: 0.0879 sec.
iter 13000 || Loss: 1.7703 || timer: 0.1020 sec.
iter 13010 || Loss: 1.7139 || timer: 0.0828 sec.
iter 13020 || Loss: 1.5636 || timer: 0.0936 sec.
iter 13030 || Loss: 1.6435 || timer: 0.0834 sec.
iter 13040 || Loss: 2.5501 || timer: 0.0901 sec.
iter 13050 || Loss: 2.0764 || timer: 0.0875 sec.
iter 13060 || Loss: 1.7699 || timer: 0.0866 sec.
iter 13070 || Loss: 1.4932 || timer: 0.0839 sec.
iter 13080 || Loss: 1.7478 || timer: 0.1033 sec.
iter 13090 || Loss: 1.6005 || timer: 0.0308 sec.
iter 13100 || Loss: 1.0219 || timer: 0.0902 sec.
iter 13110 || Loss: 1.6656 || timer: 0.0841 sec.
iter 13120 || Loss: 1.5192 || timer: 0.1077 sec.
iter 13130 || Loss: 1.7891 || timer: 0.0869 sec.
iter 13140 || Loss: 1.9600 || timer: 0.0818 sec.
iter 13150 || Loss: 1.6475 || timer: 0.0888 sec.
iter 13160 || Loss: 1.9790 || timer: 0.0826 sec.
iter 13170 || Loss: 1.8620 || timer: 0.0825 sec.
iter 13180 || Loss: 2.0230 || timer: 0.0884 sec.
iter 13190 || Loss: 1.9960 || timer: 0.1382 sec.
iter 13200 || Loss: 1.4345 || timer: 0.1148 sec.
iter 13210 || Loss: 1.6197 || timer: 0.0933 sec.
iter 13220 || Loss: 1.4986 || timer: 0.0992 sec.
iter 13230 || Loss: 1.6097 || timer: 0.0843 sec.
iter 13240 || Loss: 2.2274 || timer: 0.0950 sec.
iter 13250 || Loss: 2.0445 || timer: 0.1190 sec.
iter 13260 || Loss: 2.0720 || timer: 0.0967 sec.
iter 13270 || Loss: 1.3204 || timer: 0.0856 sec.
iter 13280 || Loss: 1.6946 || timer: 0.0975 sec.
iter 13290 || Loss: 1.3682 || timer: 0.1448 sec.
iter 13300 || Loss: 1.4873 || timer: 0.1680 sec.
iter 13310 || Loss: 1.4375 || timer: 0.1433 sec.
iter 13320 || Loss: 1.5653 || timer: 0.1316 sec.
iter 13330 || Loss: 1.4456 || timer: 0.1248 sec.
iter 13340 || Loss: 1.2853 || timer: 0.1676 sec.
iter 13350 || Loss: 1.6851 || timer: 0.1464 sec.
iter 13360 || Loss: 1.5197 || timer: 0.1692 sec.
iter 13370 || Loss: 2.0357 || timer: 0.1701 sec.
iter 13380 || Loss: 1.3919 || timer: 0.1101 sec.
iter 13390 || Loss: 1.5166 || timer: 0.0953 sec.
iter 13400 || Loss: 2.1522 || timer: 0.1335 sec.
iter 13410 || Loss: 1.6738 || timer: 0.1753 sec.
iter 13420 || Loss: 1.9132 || timer: 0.0314 sec.
iter 13430 || Loss: 0.7486 || timer: 0.1399 sec.
iter 13440 || Loss: 1.7025 || timer: 0.1254 sec.
iter 13450 || Loss: 1.4834 || timer: 0.1371 sec.
iter 13460 || Loss: 1.3708 || timer: 0.1456 sec.
iter 13470 || Loss: 1.5917 || timer: 0.1497 sec.
iter 13480 || Loss: 1.3538 || timer: 0.1765 sec.
iter 13490 || Loss: 1.7412 || timer: 0.1750 sec.
iter 13500 || Loss: 1.5616 || timer: 0.1265 sec.
iter 13510 || Loss: 2.0516 || timer: 0.1117 sec.
iter 13520 || Loss: 1.7691 || timer: 0.1621 sec.
iter 13530 || Loss: 1.4858 || timer: 0.1503 sec.
iter 13540 || Loss: 1.3829 || timer: 0.1742 sec.
iter 13550 || Loss: 1.8866 || timer: 0.1428 sec.
iter 13560 || Loss: 1.5980 || timer: 0.1675 sec.
iter 13570 || Loss: 1.8472 || timer: 0.1138 sec.
iter 13580 || Loss: 1.5271 || timer: 0.1541 sec.
iter 13590 || Loss: 1.7136 || timer: 0.1351 sec.
iter 13600 || Loss: 1.6304 || timer: 0.1703 sec.
iter 13610 || Loss: 2.0710 || timer: 0.1807 sec.
iter 13620 || Loss: 1.5510 || timer: 0.1705 sec.
iter 13630 || Loss: 2.1548 || timer: 0.1891 sec.
iter 13640 || Loss: 1.8045 || timer: 0.1397 sec.
iter 13650 || Loss: 1.4783 || timer: 0.1416 sec.
iter 13660 || Loss: 1.5044 || timer: 0.1644 sec.
iter 13670 || Loss: 1.8881 || timer: 0.1656 sec.
iter 13680 || Loss: 1.6681 || timer: 0.1454 sec.
iter 13690 || Loss: 1.7253 || timer: 0.1404 sec.
iter 13700 || Loss: 1.6196 || timer: 0.1752 sec.
iter 13710 || Loss: 1.7617 || timer: 0.1491 sec.
iter 13720 || Loss: 1.3743 || timer: 0.1761 sec.
iter 13730 || Loss: 1.7204 || timer: 0.1769 sec.
iter 13740 || Loss: 1.7302 || timer: 0.1798 sec.
iter 13750 || Loss: 1.7326 || timer: 0.0465 sec.
iter 13760 || Loss: 1.3037 || timer: 0.1775 sec.
iter 13770 || Loss: 1.7624 || timer: 0.1130 sec.
iter 13780 || Loss: 1.8120 || timer: 0.1219 sec.
iter 13790 || Loss: 1.8677 || timer: 0.1464 sec.
iter 13800 || Loss: 1.9591 || timer: 0.1846 sec.
iter 13810 || Loss: 2.2767 || timer: 0.1452 sec.
iter 13820 || Loss: 1.7450 || timer: 0.1433 sec.
iter 13830 || Loss: 1.6000 || timer: 0.0958 sec.
iter 13840 || Loss: 1.2447 || timer: 0.1190 sec.
iter 13850 || Loss: 1.4376 || timer: 0.0963 sec.
iter 13860 || Loss: 1.6841 || timer: 0.1756 sec.
iter 13870 || Loss: 1.6137 || timer: 0.1605 sec.
iter 13880 || Loss: 1.9064 || timer: 0.1560 sec.
iter 13890 || Loss: 1.8465 || timer: 0.1405 sec.
iter 13900 || Loss: 1.7772 || timer: 0.1452 sec.
iter 13910 || Loss: 2.0040 || timer: 0.1110 sec.
iter 13920 || Loss: 1.6511 || timer: 0.1381 sec.
iter 13930 || Loss: 2.1902 || timer: 0.1772 sec.
iter 13940 || Loss: 1.5658 || timer: 0.1591 sec.
iter 13950 || Loss: 1.5210 || timer: 0.1348 sec.
iter 13960 || Loss: 1.4942 || timer: 0.1722 sec.
iter 13970 || Loss: 1.7196 || timer: 0.1432 sec.
iter 13980 || Loss: 1.4481 || timer: 0.1218 sec.
iter 13990 || Loss: 2.0011 || timer: 0.1425 sec.
iter 14000 || Loss: 1.5062 || timer: 0.1746 sec.
iter 14010 || Loss: 1.5433 || timer: 0.1439 sec.
iter 14020 || Loss: 1.5080 || timer: 0.1506 sec.
iter 14030 || Loss: 1.7435 || timer: 0.0833 sec.
iter 14040 || Loss: 1.6514 || timer: 0.1258 sec.
iter 14050 || Loss: 1.5385 || timer: 0.1480 sec.
iter 14060 || Loss: 1.8303 || timer: 0.1436 sec.
iter 14070 || Loss: 1.7442 || timer: 0.1453 sec.
iter 14080 || Loss: 1.4007 || timer: 0.0235 sec.
iter 14090 || Loss: 2.4665 || timer: 0.1753 sec.
iter 14100 || Loss: 2.0195 || timer: 0.1426 sec.
iter 14110 || Loss: 1.6659 || timer: 0.0907 sec.
iter 14120 || Loss: 2.1543 || timer: 0.1678 sec.
iter 14130 || Loss: 1.7048 || timer: 0.1380 sec.
iter 14140 || Loss: 1.6496 || timer: 0.1444 sec.
iter 14150 || Loss: 1.4592 || timer: 0.1728 sec.
iter 14160 || Loss: 1.3766 || timer: 0.1335 sec.
iter 14170 || Loss: 1.4598 || timer: 0.1117 sec.
iter 14180 || Loss: 1.2984 || timer: 0.1562 sec.
iter 14190 || Loss: 1.7192 || timer: 0.1321 sec.
iter 14200 || Loss: 1.7391 || timer: 0.1685 sec.
iter 14210 || Loss: 1.3919 || timer: 0.1579 sec.
iter 14220 || Loss: 1.2393 || timer: 0.1031 sec.
iter 14230 || Loss: 1.6774 || timer: 0.1249 sec.
iter 14240 || Loss: 1.8462 || timer: 0.0893 sec.
iter 14250 || Loss: 1.5899 || timer: 0.1382 sec.
iter 14260 || Loss: 1.8583 || timer: 0.1250 sec.
iter 14270 || Loss: 1.4842 || timer: 0.1432 sec.
iter 14280 || Loss: 1.6360 || timer: 0.1264 sec.
iter 14290 || Loss: 1.6364 || timer: 0.1606 sec.
iter 14300 || Loss: 2.2380 || timer: 0.0832 sec.
iter 14310 || Loss: 2.1788 || timer: 0.1416 sec.
iter 14320 || Loss: 1.9044 || timer: 0.1452 sec.
iter 14330 || Loss: 1.2395 || timer: 0.1822 sec.
iter 14340 || Loss: 1.3035 || timer: 0.1537 sec.
iter 14350 || Loss: 1.8383 || timer: 0.1408 sec.
iter 14360 || Loss: 2.7861 || timer: 0.0834 sec.
iter 14370 || Loss: 2.0323 || timer: 0.1488 sec.
iter 14380 || Loss: 1.9095 || timer: 0.1391 sec.
iter 14390 || Loss: 1.4930 || timer: 0.1676 sec.
iter 14400 || Loss: 1.8410 || timer: 0.1491 sec.
iter 14410 || Loss: 1.7246 || timer: 0.0294 sec.
iter 14420 || Loss: 1.2124 || timer: 0.1415 sec.
iter 14430 || Loss: 1.8464 || timer: 0.1453 sec.
iter 14440 || Loss: 2.1106 || timer: 0.1417 sec.
iter 14450 || Loss: 1.4555 || timer: 0.1582 sec.
iter 14460 || Loss: 1.7404 || timer: 0.0919 sec.
iter 14470 || Loss: 1.7417 || timer: 0.1333 sec.
iter 14480 || Loss: 1.3299 || timer: 0.1458 sec.
iter 14490 || Loss: 2.1232 || timer: 0.1778 sec.
iter 14500 || Loss: 2.1535 || timer: 0.0908 sec.
iter 14510 || Loss: 1.8935 || timer: 0.1336 sec.
iter 14520 || Loss: 1.7744 || timer: 0.1423 sec.
iter 14530 || Loss: 1.3942 || timer: 0.1054 sec.
iter 14540 || Loss: 1.8713 || timer: 0.1257 sec.
iter 14550 || Loss: 2.1005 || timer: 0.1712 sec.
iter 14560 || Loss: 2.5556 || timer: 0.1613 sec.
iter 14570 || Loss: 1.5722 || timer: 0.1310 sec.
iter 14580 || Loss: 1.7649 || timer: 0.1768 sec.
iter 14590 || Loss: 1.7468 || timer: 0.1628 sec.
iter 14600 || Loss: 1.6537 || timer: 0.1209 sec.
iter 14610 || Loss: 1.5607 || timer: 0.0860 sec.
iter 14620 || Loss: 1.6099 || timer: 0.1355 sec.
iter 14630 || Loss: 1.8204 || timer: 0.1680 sec.
iter 14640 || Loss: 1.7723 || timer: 0.1453 sec.
iter 14650 || Loss: 1.5859 || timer: 0.1255 sec.
iter 14660 || Loss: 1.8140 || timer: 0.1446 sec.
iter 14670 || Loss: 2.0572 || timer: 0.1219 sec.
iter 14680 || Loss: 2.0116 || timer: 0.1405 sec.
iter 14690 || Loss: 1.7303 || timer: 0.1430 sec.
iter 14700 || Loss: 2.1465 || timer: 0.1451 sec.
iter 14710 || Loss: 1.4060 || timer: 0.1761 sec.
iter 14720 || Loss: 2.1394 || timer: 0.0840 sec.
iter 14730 || Loss: 2.6645 || timer: 0.1130 sec.
iter 14740 || Loss: 2.2113 || timer: 0.0191 sec.
iter 14750 || Loss: 1.5529 || timer: 0.1364 sec.
iter 14760 || Loss: 1.6263 || timer: 0.1595 sec.
iter 14770 || Loss: 1.8218 || timer: 0.1343 sec.
iter 14780 || Loss: 1.7344 || timer: 0.1726 sec.
iter 14790 || Loss: 1.5573 || timer: 0.0989 sec.
iter 14800 || Loss: 1.6008 || timer: 0.1171 sec.
iter 14810 || Loss: 1.4964 || timer: 0.1459 sec.
iter 14820 || Loss: 1.7588 || timer: 0.1755 sec.
iter 14830 || Loss: 2.1405 || timer: 0.1468 sec.
iter 14840 || Loss: 1.8957 || timer: 0.1513 sec.
iter 14850 || Loss: 1.5138 || timer: 0.1593 sec.
iter 14860 || Loss: 1.7193 || timer: 0.1426 sec.
iter 14870 || Loss: 1.5827 || timer: 0.0994 sec.
iter 14880 || Loss: 1.9964 || timer: 0.1571 sec.
iter 14890 || Loss: 1.6983 || timer: 0.1429 sec.
iter 14900 || Loss: 1.5671 || timer: 0.1706 sec.
iter 14910 || Loss: 1.8117 || timer: 0.1731 sec.
iter 14920 || Loss: 1.5437 || timer: 0.1337 sec.
iter 14930 || Loss: 1.4490 || timer: 0.1228 sec.
iter 14940 || Loss: 1.2768 || timer: 0.1461 sec.
iter 14950 || Loss: 1.8128 || timer: 0.1796 sec.
iter 14960 || Loss: 1.7871 || timer: 0.1708 sec.
iter 14970 || Loss: 1.1926 || timer: 0.1461 sec.
iter 14980 || Loss: 1.5249 || timer: 0.1112 sec.
iter 14990 || Loss: 1.2883 || timer: 0.0874 sec.
iter 15000 || Loss: 1.6220 || Saving state, iter: 15000
timer: 0.1165 sec.
iter 15010 || Loss: 1.9756 || timer: 0.1317 sec.
iter 15020 || Loss: 1.9027 || timer: 0.1223 sec.
iter 15030 || Loss: 1.7949 || timer: 0.1746 sec.
iter 15040 || Loss: 2.0309 || timer: 0.1690 sec.
iter 15050 || Loss: 2.0831 || timer: 0.1342 sec.
iter 15060 || Loss: 2.0327 || timer: 0.1273 sec.
iter 15070 || Loss: 1.8416 || timer: 0.0276 sec.
iter 15080 || Loss: 1.7951 || timer: 0.1395 sec.
iter 15090 || Loss: 1.6040 || timer: 0.1554 sec.
iter 15100 || Loss: 1.6448 || timer: 0.1740 sec.
iter 15110 || Loss: 2.2215 || timer: 0.1275 sec.
iter 15120 || Loss: 1.6468 || timer: 0.1418 sec.
iter 15130 || Loss: 2.6867 || timer: 0.0985 sec.
iter 15140 || Loss: 2.3797 || timer: 0.1501 sec.
iter 15150 || Loss: 2.1654 || timer: 0.1564 sec.
iter 15160 || Loss: 2.1524 || timer: 0.1697 sec.
iter 15170 || Loss: 1.9008 || timer: 0.1480 sec.
iter 15180 || Loss: 1.7321 || timer: 0.1685 sec.
iter 15190 || Loss: 1.6873 || timer: 0.1271 sec.
iter 15200 || Loss: 1.7345 || timer: 0.1265 sec.
iter 15210 || Loss: 1.5828 || timer: 0.1714 sec.
iter 15220 || Loss: 1.9837 || timer: 0.1664 sec.
iter 15230 || Loss: 1.3767 || timer: 0.1507 sec.
iter 15240 || Loss: 1.7290 || timer: 0.1801 sec.
iter 15250 || Loss: 1.4016 || timer: 0.1400 sec.
iter 15260 || Loss: 1.6124 || timer: 0.1062 sec.
iter 15270 || Loss: 1.6424 || timer: 0.1374 sec.
iter 15280 || Loss: 1.5504 || timer: 0.1179 sec.
iter 15290 || Loss: 2.1483 || timer: 0.1761 sec.
iter 15300 || Loss: 1.4026 || timer: 0.1361 sec.
iter 15310 || Loss: 1.8189 || timer: 0.1374 sec.
iter 15320 || Loss: 1.7752 || timer: 0.1363 sec.
iter 15330 || Loss: 1.5166 || timer: 0.1514 sec.
iter 15340 || Loss: 1.3226 || timer: 0.1648 sec.
iter 15350 || Loss: 2.4165 || timer: 0.1431 sec.
iter 15360 || Loss: 2.0022 || timer: 0.1445 sec.
iter 15370 || Loss: 1.5184 || timer: 0.1737 sec.
iter 15380 || Loss: 1.9151 || timer: 0.1599 sec.
iter 15390 || Loss: 1.3233 || timer: 0.1098 sec.
iter 15400 || Loss: 1.7724 || timer: 0.0235 sec.
iter 15410 || Loss: 2.1485 || timer: 0.1735 sec.
iter 15420 || Loss: 1.6847 || timer: 0.1587 sec.
iter 15430 || Loss: 1.5351 || timer: 0.1589 sec.
iter 15440 || Loss: 1.7233 || timer: 0.1395 sec.
iter 15450 || Loss: 1.7602 || timer: 0.1111 sec.
iter 15460 || Loss: 1.8863 || timer: 0.1230 sec.
iter 15470 || Loss: 1.5922 || timer: 0.1677 sec.
iter 15480 || Loss: 1.6196 || timer: 0.1378 sec.
iter 15490 || Loss: 1.8597 || timer: 0.1361 sec.
iter 15500 || Loss: 1.6006 || timer: 0.1324 sec.
iter 15510 || Loss: 1.4648 || timer: 0.1374 sec.
iter 15520 || Loss: 1.5167 || timer: 0.1459 sec.
iter 15530 || Loss: 1.3514 || timer: 0.1405 sec.
iter 15540 || Loss: 1.4223 || timer: 0.1690 sec.
iter 15550 || Loss: 1.7573 || timer: 0.1396 sec.
iter 15560 || Loss: 1.9732 || timer: 0.1676 sec.
iter 15570 || Loss: 1.3123 || timer: 0.1630 sec.
iter 15580 || Loss: 1.8228 || timer: 0.0915 sec.
iter 15590 || Loss: 1.6973 || timer: 0.1200 sec.
iter 15600 || Loss: 2.3632 || timer: 0.1367 sec.
iter 15610 || Loss: 1.8938 || timer: 0.1480 sec.
iter 15620 || Loss: 1.6678 || timer: 0.1298 sec.
iter 15630 || Loss: 1.5475 || timer: 0.1456 sec.
iter 15640 || Loss: 1.5782 || timer: 0.1422 sec.
iter 15650 || Loss: 1.5980 || timer: 0.1252 sec.
iter 15660 || Loss: 1.4495 || timer: 0.1223 sec.
iter 15670 || Loss: 1.6582 || timer: 0.1756 sec.
iter 15680 || Loss: 2.3445 || timer: 0.1668 sec.
iter 15690 || Loss: 2.0521 || timer: 0.1756 sec.
iter 15700 || Loss: 1.9849 || timer: 0.1485 sec.
iter 15710 || Loss: 1.7051 || timer: 0.1668 sec.
iter 15720 || Loss: 1.9402 || timer: 0.1289 sec.
iter 15730 || Loss: 1.9461 || timer: 0.0281 sec.
iter 15740 || Loss: 1.9610 || timer: 0.1691 sec.
iter 15750 || Loss: 1.6143 || timer: 0.1403 sec.
iter 15760 || Loss: 1.3969 || timer: 0.1506 sec.
iter 15770 || Loss: 1.7585 || timer: 0.1684 sec.
iter 15780 || Loss: 1.9871 || timer: 0.1273 sec.
iter 15790 || Loss: 1.6722 || timer: 0.0906 sec.
iter 15800 || Loss: 1.8403 || timer: 0.1693 sec.
iter 15810 || Loss: 1.1046 || timer: 0.1751 sec.
iter 15820 || Loss: 1.7950 || timer: 0.1445 sec.
iter 15830 || Loss: 1.4621 || timer: 0.0972 sec.
iter 15840 || Loss: 1.8725 || timer: 0.1344 sec.
iter 15850 || Loss: 1.7813 || timer: 0.1336 sec.
iter 15860 || Loss: 1.8219 || timer: 0.1440 sec.
iter 15870 || Loss: 1.8260 || timer: 0.1656 sec.
iter 15880 || Loss: 1.5673 || timer: 0.1329 sec.
iter 15890 || Loss: 1.5359 || timer: 0.1402 sec.
iter 15900 || Loss: 1.6416 || timer: 0.1199 sec.
iter 15910 || Loss: 1.3290 || timer: 0.1192 sec.
iter 15920 || Loss: 1.8829 || timer: 0.0917 sec.
iter 15930 || Loss: 1.2815 || timer: 0.1384 sec.
iter 15940 || Loss: 1.6816 || timer: 0.1743 sec.
iter 15950 || Loss: 3.6652 || timer: 0.1486 sec.
iter 15960 || Loss: 2.2754 || timer: 0.1537 sec.
iter 15970 || Loss: 1.5037 || timer: 0.1191 sec.
iter 15980 || Loss: 1.5479 || timer: 0.1051 sec.
iter 15990 || Loss: 1.4872 || timer: 0.1487 sec.
iter 16000 || Loss: 1.5110 || timer: 0.1569 sec.
iter 16010 || Loss: 1.4367 || timer: 0.1583 sec.
iter 16020 || Loss: 1.9896 || timer: 0.1510 sec.
iter 16030 || Loss: 1.6365 || timer: 0.1252 sec.
iter 16040 || Loss: 1.2953 || timer: 0.1461 sec.
iter 16050 || Loss: 1.8956 || timer: 0.1649 sec.
iter 16060 || Loss: 1.4329 || timer: 0.0235 sec.
iter 16070 || Loss: 1.1380 || timer: 0.1558 sec.
iter 16080 || Loss: 2.0653 || timer: 0.1811 sec.
iter 16090 || Loss: 1.9021 || timer: 0.1314 sec.
iter 16100 || Loss: 1.9005 || timer: 0.1205 sec.
iter 16110 || Loss: 1.4246 || timer: 0.1423 sec.
iter 16120 || Loss: 1.4301 || timer: 0.1548 sec.
iter 16130 || Loss: 1.3540 || timer: 0.1885 sec.
iter 16140 || Loss: 2.2863 || timer: 0.1800 sec.
iter 16150 || Loss: 1.6285 || timer: 0.1635 sec.
iter 16160 || Loss: 1.5249 || timer: 0.0961 sec.
iter 16170 || Loss: 1.4879 || timer: 0.1708 sec.
iter 16180 || Loss: 1.7314 || timer: 0.1440 sec.
iter 16190 || Loss: 1.3669 || timer: 0.1498 sec.
iter 16200 || Loss: 2.0336 || timer: 0.1441 sec.
iter 16210 || Loss: 2.1121 || timer: 0.1453 sec.
iter 16220 || Loss: 1.5837 || timer: 0.1526 sec.
iter 16230 || Loss: 1.8425 || timer: 0.1770 sec.
iter 16240 || Loss: 1.4993 || timer: 0.1833 sec.
iter 16250 || Loss: 1.7890 || timer: 0.1482 sec.
iter 16260 || Loss: 1.6518 || timer: 0.1355 sec.
iter 16270 || Loss: 1.5365 || timer: 0.1783 sec.
iter 16280 || Loss: 1.9256 || timer: 0.1237 sec.
iter 16290 || Loss: 1.2982 || timer: 0.1154 sec.
iter 16300 || Loss: 1.8327 || timer: 0.1653 sec.
iter 16310 || Loss: 1.5722 || timer: 0.1046 sec.
iter 16320 || Loss: 1.4855 || timer: 0.1336 sec.
iter 16330 || Loss: 1.7895 || timer: 0.1705 sec.
iter 16340 || Loss: 1.8953 || timer: 0.1206 sec.
iter 16350 || Loss: 1.8819 || timer: 0.1371 sec.
iter 16360 || Loss: 1.5920 || timer: 0.1666 sec.
iter 16370 || Loss: 1.7081 || timer: 0.1764 sec.
iter 16380 || Loss: 1.4181 || timer: 0.1381 sec.
iter 16390 || Loss: 1.3289 || timer: 0.0283 sec.
iter 16400 || Loss: 0.9642 || timer: 0.1468 sec.
iter 16410 || Loss: 1.9836 || timer: 0.1556 sec.
iter 16420 || Loss: 1.5496 || timer: 0.1403 sec.
iter 16430 || Loss: 1.7056 || timer: 0.1518 sec.
iter 16440 || Loss: 1.5956 || timer: 0.1295 sec.
iter 16450 || Loss: 1.4063 || timer: 0.1475 sec.
iter 16460 || Loss: 1.5266 || timer: 0.1859 sec.
iter 16470 || Loss: 1.6641 || timer: 0.1158 sec.
iter 16480 || Loss: 1.2962 || timer: 0.1513 sec.
iter 16490 || Loss: 1.3394 || timer: 0.0940 sec.
iter 16500 || Loss: 1.2783 || timer: 0.0914 sec.
iter 16510 || Loss: 1.7622 || timer: 0.0890 sec.
iter 16520 || Loss: 1.6328 || timer: 0.0846 sec.
iter 16530 || Loss: 1.4043 || timer: 0.0916 sec.
iter 16540 || Loss: 1.8824 || timer: 0.0898 sec.
iter 16550 || Loss: 1.6054 || timer: 0.0860 sec.
iter 16560 || Loss: 2.1100 || timer: 0.0909 sec.
iter 16570 || Loss: 1.7150 || timer: 0.0905 sec.
iter 16580 || Loss: 1.5684 || timer: 0.0901 sec.
iter 16590 || Loss: 1.6733 || timer: 0.0834 sec.
iter 16600 || Loss: 1.6794 || timer: 0.0863 sec.
iter 16610 || Loss: 1.8879 || timer: 0.0858 sec.
iter 16620 || Loss: 1.7149 || timer: 0.0858 sec.
iter 16630 || Loss: 1.9823 || timer: 0.0845 sec.
iter 16640 || Loss: 1.9227 || timer: 0.0853 sec.
iter 16650 || Loss: 2.1810 || timer: 0.1008 sec.
iter 16660 || Loss: 1.4492 || timer: 0.0862 sec.
iter 16670 || Loss: 1.7251 || timer: 0.0850 sec.
iter 16680 || Loss: 1.5745 || timer: 0.0850 sec.
iter 16690 || Loss: 1.3992 || timer: 0.0849 sec.
iter 16700 || Loss: 1.5860 || timer: 0.1061 sec.
iter 16710 || Loss: 1.7716 || timer: 0.0905 sec.
iter 16720 || Loss: 1.5447 || timer: 0.0276 sec.
iter 16730 || Loss: 1.7655 || timer: 0.1080 sec.
iter 16740 || Loss: 1.5440 || timer: 0.0932 sec.
iter 16750 || Loss: 2.1514 || timer: 0.0859 sec.
iter 16760 || Loss: 1.6966 || timer: 0.0995 sec.
iter 16770 || Loss: 1.7599 || timer: 0.0918 sec.
iter 16780 || Loss: 2.0884 || timer: 0.0897 sec.
iter 16790 || Loss: 2.1634 || timer: 0.0914 sec.
iter 16800 || Loss: 1.4148 || timer: 0.0913 sec.
iter 16810 || Loss: 1.8179 || timer: 0.1283 sec.
iter 16820 || Loss: 1.4225 || timer: 0.0945 sec.
iter 16830 || Loss: 1.3861 || timer: 0.1125 sec.
iter 16840 || Loss: 1.3266 || timer: 0.0854 sec.
iter 16850 || Loss: 1.2920 || timer: 0.0902 sec.
iter 16860 || Loss: 1.7954 || timer: 0.0829 sec.
iter 16870 || Loss: 1.5054 || timer: 0.0851 sec.
iter 16880 || Loss: 1.5696 || timer: 0.1109 sec.
iter 16890 || Loss: 1.5226 || timer: 0.0904 sec.
iter 16900 || Loss: 1.6340 || timer: 0.0902 sec.
iter 16910 || Loss: 1.3952 || timer: 0.0995 sec.
iter 16920 || Loss: 1.1821 || timer: 0.0830 sec.
iter 16930 || Loss: 1.3215 || timer: 0.1096 sec.
iter 16940 || Loss: 1.7498 || timer: 0.0928 sec.
iter 16950 || Loss: 1.5516 || timer: 0.0825 sec.
iter 16960 || Loss: 1.5730 || timer: 0.1240 sec.
iter 16970 || Loss: 2.0322 || timer: 0.0827 sec.
iter 16980 || Loss: 1.4744 || timer: 0.0913 sec.
iter 16990 || Loss: 1.5501 || timer: 0.0940 sec.
iter 17000 || Loss: 1.8149 || timer: 0.1126 sec.
iter 17010 || Loss: 1.4318 || timer: 0.0838 sec.
iter 17020 || Loss: 1.8071 || timer: 0.0908 sec.
iter 17030 || Loss: 1.4210 || timer: 0.0901 sec.
iter 17040 || Loss: 1.4832 || timer: 0.0899 sec.
iter 17050 || Loss: 1.7446 || timer: 0.0246 sec.
iter 17060 || Loss: 0.7310 || timer: 0.0813 sec.
iter 17070 || Loss: 1.6133 || timer: 0.0899 sec.
iter 17080 || Loss: 1.7973 || timer: 0.0894 sec.
iter 17090 || Loss: 1.4713 || timer: 0.1023 sec.
iter 17100 || Loss: 1.9720 || timer: 0.1079 sec.
iter 17110 || Loss: 1.7599 || timer: 0.0866 sec.
iter 17120 || Loss: 1.2907 || timer: 0.0927 sec.
iter 17130 || Loss: 2.1062 || timer: 0.0842 sec.
iter 17140 || Loss: 1.2655 || timer: 0.1130 sec.
iter 17150 || Loss: 1.7785 || timer: 0.1005 sec.
iter 17160 || Loss: 1.6859 || timer: 0.0910 sec.
iter 17170 || Loss: 1.7096 || timer: 0.0903 sec.
iter 17180 || Loss: 1.6229 || timer: 0.0953 sec.
iter 17190 || Loss: 1.6970 || timer: 0.0830 sec.
iter 17200 || Loss: 1.6322 || timer: 0.0829 sec.
iter 17210 || Loss: 1.6831 || timer: 0.0878 sec.
iter 17220 || Loss: 1.3080 || timer: 0.0886 sec.
iter 17230 || Loss: 1.6022 || timer: 0.0830 sec.
iter 17240 || Loss: 1.4353 || timer: 0.1039 sec.
iter 17250 || Loss: 1.9613 || timer: 0.0886 sec.
iter 17260 || Loss: 2.1876 || timer: 0.1004 sec.
iter 17270 || Loss: 1.8412 || timer: 0.1018 sec.
iter 17280 || Loss: 2.0233 || timer: 0.0839 sec.
iter 17290 || Loss: 1.8364 || timer: 0.0900 sec.
iter 17300 || Loss: 1.9700 || timer: 0.0930 sec.
iter 17310 || Loss: 1.4781 || timer: 0.0909 sec.
iter 17320 || Loss: 1.8444 || timer: 0.0906 sec.
iter 17330 || Loss: 1.9957 || timer: 0.1054 sec.
iter 17340 || Loss: 1.3560 || timer: 0.0846 sec.
iter 17350 || Loss: 1.9074 || timer: 0.0856 sec.
iter 17360 || Loss: 1.9139 || timer: 0.1154 sec.
iter 17370 || Loss: 1.5275 || timer: 0.1098 sec.
iter 17380 || Loss: 1.5631 || timer: 0.0266 sec.
iter 17390 || Loss: 8.5396 || timer: 0.0837 sec.
iter 17400 || Loss: 1.7918 || timer: 0.0837 sec.
iter 17410 || Loss: 1.9621 || timer: 0.0841 sec.
iter 17420 || Loss: 1.3531 || timer: 0.1104 sec.
iter 17430 || Loss: 2.0976 || timer: 0.0890 sec.
iter 17440 || Loss: 1.8104 || timer: 0.0901 sec.
iter 17450 || Loss: 1.7281 || timer: 0.0846 sec.
iter 17460 || Loss: 1.5903 || timer: 0.0889 sec.
iter 17470 || Loss: 1.7546 || timer: 0.0991 sec.
iter 17480 || Loss: 2.1999 || timer: 0.0958 sec.
iter 17490 || Loss: 1.7730 || timer: 0.0871 sec.
iter 17500 || Loss: 1.9872 || timer: 0.1111 sec.
iter 17510 || Loss: 1.4378 || timer: 0.0909 sec.
iter 17520 || Loss: 1.7055 || timer: 0.0864 sec.
iter 17530 || Loss: 1.2378 || timer: 0.0913 sec.
iter 17540 || Loss: 1.9921 || timer: 0.0830 sec.
iter 17550 || Loss: 1.7889 || timer: 0.0919 sec.
iter 17560 || Loss: 1.8254 || timer: 0.0833 sec.
iter 17570 || Loss: 1.9454 || timer: 0.1151 sec.
iter 17580 || Loss: 1.9021 || timer: 0.1238 sec.
iter 17590 || Loss: 1.5995 || timer: 0.0854 sec.
iter 17600 || Loss: 1.7361 || timer: 0.0844 sec.
iter 17610 || Loss: 1.4452 || timer: 0.0870 sec.
iter 17620 || Loss: 1.6313 || timer: 0.0824 sec.
iter 17630 || Loss: 1.2216 || timer: 0.0918 sec.
iter 17640 || Loss: 1.9892 || timer: 0.0888 sec.
iter 17650 || Loss: 1.6475 || timer: 0.0921 sec.
iter 17660 || Loss: 1.3069 || timer: 0.1067 sec.
iter 17670 || Loss: 1.1483 || timer: 0.0874 sec.
iter 17680 || Loss: 1.3279 || timer: 0.0889 sec.
iter 17690 || Loss: 1.5741 || timer: 0.0927 sec.
iter 17700 || Loss: 1.6154 || timer: 0.0915 sec.
iter 17710 || Loss: 1.4296 || timer: 0.0260 sec.
iter 17720 || Loss: 0.2512 || timer: 0.0898 sec.
iter 17730 || Loss: 1.7033 || timer: 0.0864 sec.
iter 17740 || Loss: 1.3278 || timer: 0.0891 sec.
iter 17750 || Loss: 1.3008 || timer: 0.0905 sec.
iter 17760 || Loss: 1.2285 || timer: 0.1026 sec.
iter 17770 || Loss: 1.5777 || timer: 0.0906 sec.
iter 17780 || Loss: 1.3347 || timer: 0.0921 sec.
iter 17790 || Loss: 1.9104 || timer: 0.0931 sec.
iter 17800 || Loss: 2.2249 || timer: 0.0843 sec.
iter 17810 || Loss: 1.5674 || timer: 0.1104 sec.
iter 17820 || Loss: 1.2608 || timer: 0.0906 sec.
iter 17830 || Loss: 1.7683 || timer: 0.0888 sec.
iter 17840 || Loss: 1.7433 || timer: 0.1073 sec.
iter 17850 || Loss: 1.8615 || timer: 0.1096 sec.
iter 17860 || Loss: 1.7572 || timer: 0.0895 sec.
iter 17870 || Loss: 1.3563 || timer: 0.0884 sec.
iter 17880 || Loss: 1.5552 || timer: 0.0901 sec.
iter 17890 || Loss: 1.9704 || timer: 0.1017 sec.
iter 17900 || Loss: 1.6869 || timer: 0.0820 sec.
iter 17910 || Loss: 1.4990 || timer: 0.0899 sec.
iter 17920 || Loss: 1.8679 || timer: 0.1083 sec.
iter 17930 || Loss: 1.6764 || timer: 0.0932 sec.
iter 17940 || Loss: 1.4795 || timer: 0.0928 sec.
iter 17950 || Loss: 1.3243 || timer: 0.0913 sec.
iter 17960 || Loss: 1.6916 || timer: 0.0925 sec.
iter 17970 || Loss: 1.9004 || timer: 0.1029 sec.
iter 17980 || Loss: 1.6669 || timer: 0.0842 sec.
iter 17990 || Loss: 2.3208 || timer: 0.0886 sec.
iter 18000 || Loss: 1.6323 || timer: 0.0898 sec.
iter 18010 || Loss: 1.6826 || timer: 0.0934 sec.
iter 18020 || Loss: 2.9529 || timer: 0.1130 sec.
iter 18030 || Loss: 1.7496 || timer: 0.0839 sec.
iter 18040 || Loss: 1.4799 || timer: 0.0261 sec.
iter 18050 || Loss: 1.0527 || timer: 0.0927 sec.
iter 18060 || Loss: 1.6126 || timer: 0.0841 sec.
iter 18070 || Loss: 1.4429 || timer: 0.0934 sec.
iter 18080 || Loss: 1.5792 || timer: 0.0928 sec.
iter 18090 || Loss: 1.8461 || timer: 0.0925 sec.
iter 18100 || Loss: 2.1943 || timer: 0.0894 sec.
iter 18110 || Loss: 1.5281 || timer: 0.0919 sec.
iter 18120 || Loss: 1.6225 || timer: 0.0902 sec.
iter 18130 || Loss: 1.5369 || timer: 0.1086 sec.
iter 18140 || Loss: 1.6456 || timer: 0.0993 sec.
iter 18150 || Loss: 1.3168 || timer: 0.0917 sec.
iter 18160 || Loss: 1.6955 || timer: 0.0963 sec.
iter 18170 || Loss: 1.7228 || timer: 0.0882 sec.
iter 18180 || Loss: 1.9397 || timer: 0.0856 sec.
iter 18190 || Loss: 2.1973 || timer: 0.0905 sec.
iter 18200 || Loss: 1.4161 || timer: 0.0838 sec.
iter 18210 || Loss: 1.6638 || timer: 0.0885 sec.
iter 18220 || Loss: 1.4246 || timer: 0.0923 sec.
iter 18230 || Loss: 1.6289 || timer: 0.1308 sec.
iter 18240 || Loss: 1.2679 || timer: 0.0910 sec.
iter 18250 || Loss: 2.0798 || timer: 0.1390 sec.
iter 18260 || Loss: 1.5653 || timer: 0.0898 sec.
iter 18270 || Loss: 1.2560 || timer: 0.0841 sec.
iter 18280 || Loss: 1.2634 || timer: 0.1024 sec.
iter 18290 || Loss: 1.4241 || timer: 0.1072 sec.
iter 18300 || Loss: 1.5470 || timer: 0.0919 sec.
iter 18310 || Loss: 1.4766 || timer: 0.0879 sec.
iter 18320 || Loss: 1.8983 || timer: 0.0909 sec.
iter 18330 || Loss: 1.3660 || timer: 0.0976 sec.
iter 18340 || Loss: 1.7090 || timer: 0.0884 sec.
iter 18350 || Loss: 1.3909 || timer: 0.0924 sec.
iter 18360 || Loss: 1.7046 || timer: 0.0902 sec.
iter 18370 || Loss: 1.6003 || timer: 0.0261 sec.
iter 18380 || Loss: 2.3843 || timer: 0.0852 sec.
iter 18390 || Loss: 2.1128 || timer: 0.1059 sec.
iter 18400 || Loss: 1.9300 || timer: 0.0858 sec.
iter 18410 || Loss: 1.3026 || timer: 0.1125 sec.
iter 18420 || Loss: 1.9631 || timer: 0.0903 sec.
iter 18430 || Loss: 1.4966 || timer: 0.0958 sec.
iter 18440 || Loss: 1.4940 || timer: 0.0841 sec.
iter 18450 || Loss: 1.6917 || timer: 0.0912 sec.
iter 18460 || Loss: 1.6989 || timer: 0.0895 sec.
iter 18470 || Loss: 1.4084 || timer: 0.1134 sec.
iter 18480 || Loss: 2.0504 || timer: 0.0880 sec.
iter 18490 || Loss: 1.3754 || timer: 0.0911 sec.
iter 18500 || Loss: 1.7373 || timer: 0.0897 sec.
iter 18510 || Loss: 1.9576 || timer: 0.0873 sec.
iter 18520 || Loss: 1.7842 || timer: 0.0915 sec.
iter 18530 || Loss: 1.6108 || timer: 0.0841 sec.
iter 18540 || Loss: 1.6510 || timer: 0.0904 sec.
iter 18550 || Loss: 1.6473 || timer: 0.0910 sec.
iter 18560 || Loss: 1.3584 || timer: 0.0837 sec.
iter 18570 || Loss: 1.2956 || timer: 0.0949 sec.
iter 18580 || Loss: 1.7506 || timer: 0.0841 sec.
iter 18590 || Loss: 1.8532 || timer: 0.0921 sec.
iter 18600 || Loss: 1.4670 || timer: 0.0916 sec.
iter 18610 || Loss: 1.6116 || timer: 0.1062 sec.
iter 18620 || Loss: 1.1412 || timer: 0.0913 sec.
iter 18630 || Loss: 1.3550 || timer: 0.0949 sec.
iter 18640 || Loss: 1.3172 || timer: 0.1028 sec.
iter 18650 || Loss: 1.5745 || timer: 0.1043 sec.
iter 18660 || Loss: 1.3390 || timer: 0.0919 sec.
iter 18670 || Loss: 1.7005 || timer: 0.0874 sec.
iter 18680 || Loss: 1.7309 || timer: 0.1313 sec.
iter 18690 || Loss: 1.8009 || timer: 0.0843 sec.
iter 18700 || Loss: 1.4930 || timer: 0.0268 sec.
iter 18710 || Loss: 1.3972 || timer: 0.0840 sec.
iter 18720 || Loss: 1.5476 || timer: 0.0847 sec.
iter 18730 || Loss: 1.3966 || timer: 0.0834 sec.
iter 18740 || Loss: 1.5179 || timer: 0.0840 sec.
iter 18750 || Loss: 2.5040 || timer: 0.1022 sec.
iter 18760 || Loss: 2.4143 || timer: 0.0908 sec.
iter 18770 || Loss: 1.9794 || timer: 0.0901 sec.
iter 18780 || Loss: 2.0210 || timer: 0.0915 sec.
iter 18790 || Loss: 2.1491 || timer: 0.0822 sec.
iter 18800 || Loss: 1.6560 || timer: 0.0951 sec.
iter 18810 || Loss: 1.5015 || timer: 0.0891 sec.
iter 18820 || Loss: 1.3480 || timer: 0.0823 sec.
iter 18830 || Loss: 1.9048 || timer: 0.0845 sec.
iter 18840 || Loss: 1.4604 || timer: 0.0891 sec.
iter 18850 || Loss: 1.5342 || timer: 0.0868 sec.
iter 18860 || Loss: 1.5910 || timer: 0.0977 sec.
iter 18870 || Loss: 1.7386 || timer: 0.0887 sec.
iter 18880 || Loss: 1.4960 || timer: 0.0897 sec.
iter 18890 || Loss: 1.6718 || timer: 0.0922 sec.
iter 18900 || Loss: 1.3420 || timer: 0.0826 sec.
iter 18910 || Loss: 1.5513 || timer: 0.0923 sec.
iter 18920 || Loss: 1.8263 || timer: 0.0828 sec.
iter 18930 || Loss: 1.6873 || timer: 0.0836 sec.
iter 18940 || Loss: 1.7593 || timer: 0.1035 sec.
iter 18950 || Loss: 1.9616 || timer: 0.0932 sec.
iter 18960 || Loss: 1.4664 || timer: 0.0886 sec.
iter 18970 || Loss: 1.6468 || timer: 0.0836 sec.
iter 18980 || Loss: 1.6663 || timer: 0.0875 sec.
iter 18990 || Loss: 1.2238 || timer: 0.1061 sec.
iter 19000 || Loss: 1.3905 || timer: 0.0933 sec.
iter 19010 || Loss: 2.5885 || timer: 0.0838 sec.
iter 19020 || Loss: 2.4042 || timer: 0.0849 sec.
iter 19030 || Loss: 1.6396 || timer: 0.0202 sec.
iter 19040 || Loss: 1.6431 || timer: 0.0840 sec.
iter 19050 || Loss: 1.8418 || timer: 0.0849 sec.
iter 19060 || Loss: 2.2031 || timer: 0.0820 sec.
iter 19070 || Loss: 1.7930 || timer: 0.0903 sec.
iter 19080 || Loss: 1.2910 || timer: 0.0886 sec.
iter 19090 || Loss: 1.5825 || timer: 0.1091 sec.
iter 19100 || Loss: 1.4088 || timer: 0.0917 sec.
iter 19110 || Loss: 2.1311 || timer: 0.1039 sec.
iter 19120 || Loss: 1.7834 || timer: 0.0927 sec.
iter 19130 || Loss: 1.3798 || timer: 0.1178 sec.
iter 19140 || Loss: 1.2999 || timer: 0.1060 sec.
iter 19150 || Loss: 1.2664 || timer: 0.0899 sec.
iter 19160 || Loss: 1.2315 || timer: 0.0910 sec.
iter 19170 || Loss: 1.3054 || timer: 0.1237 sec.
iter 19180 || Loss: 1.7071 || timer: 0.0827 sec.
iter 19190 || Loss: 1.9454 || timer: 0.1075 sec.
iter 19200 || Loss: 1.7007 || timer: 0.0898 sec.
iter 19210 || Loss: 2.4205 || timer: 0.0837 sec.
iter 19220 || Loss: 1.3458 || timer: 0.1093 sec.
iter 19230 || Loss: 1.4598 || timer: 0.0899 sec.
iter 19240 || Loss: 1.4834 || timer: 0.0875 sec.
iter 19250 || Loss: 1.5044 || timer: 0.1142 sec.
iter 19260 || Loss: 1.5536 || timer: 0.0905 sec.
iter 19270 || Loss: 1.4964 || timer: 0.0913 sec.
iter 19280 || Loss: 1.9204 || timer: 0.0997 sec.
iter 19290 || Loss: 2.1409 || timer: 0.0907 sec.
iter 19300 || Loss: 1.5718 || timer: 0.1140 sec.
iter 19310 || Loss: 1.4281 || timer: 0.0894 sec.
iter 19320 || Loss: 1.6042 || timer: 0.1035 sec.
iter 19330 || Loss: 1.5584 || timer: 0.1133 sec.
iter 19340 || Loss: 1.1720 || timer: 0.0838 sec.
iter 19350 || Loss: 1.5640 || timer: 0.0840 sec.
iter 19360 || Loss: 1.2450 || timer: 0.0214 sec.
iter 19370 || Loss: 0.4200 || timer: 0.0978 sec.
iter 19380 || Loss: 1.0190 || timer: 0.1116 sec.
iter 19390 || Loss: 1.4358 || timer: 0.0922 sec.
iter 19400 || Loss: 1.5578 || timer: 0.0853 sec.
iter 19410 || Loss: 1.5288 || timer: 0.0917 sec.
iter 19420 || Loss: 1.5738 || timer: 0.0831 sec.
iter 19430 || Loss: 1.8188 || timer: 0.0838 sec.
iter 19440 || Loss: 1.6906 || timer: 0.0874 sec.
iter 19450 || Loss: 1.6643 || timer: 0.0893 sec.
iter 19460 || Loss: 1.2787 || timer: 0.1013 sec.
iter 19470 || Loss: 1.6471 || timer: 0.0858 sec.
iter 19480 || Loss: 1.3681 || timer: 0.0840 sec.
iter 19490 || Loss: 1.4186 || timer: 0.0849 sec.
iter 19500 || Loss: 1.5952 || timer: 0.0833 sec.
iter 19510 || Loss: 1.5713 || timer: 0.0832 sec.
iter 19520 || Loss: 1.4747 || timer: 0.1049 sec.
iter 19530 || Loss: 1.7504 || timer: 0.0948 sec.
iter 19540 || Loss: 1.3482 || timer: 0.0913 sec.
iter 19550 || Loss: 1.5293 || timer: 0.0906 sec.
iter 19560 || Loss: 2.1234 || timer: 0.0876 sec.
iter 19570 || Loss: 1.3847 || timer: 0.0910 sec.
iter 19580 || Loss: 1.6920 || timer: 0.0889 sec.
iter 19590 || Loss: 1.6729 || timer: 0.0854 sec.
iter 19600 || Loss: 1.4454 || timer: 0.1045 sec.
iter 19610 || Loss: 1.4123 || timer: 0.0903 sec.
iter 19620 || Loss: 1.6405 || timer: 0.0943 sec.
iter 19630 || Loss: 1.8798 || timer: 0.0904 sec.
iter 19640 || Loss: 1.3470 || timer: 0.0923 sec.
iter 19650 || Loss: 1.5331 || timer: 0.0906 sec.
iter 19660 || Loss: 1.6680 || timer: 0.1055 sec.
iter 19670 || Loss: 1.7849 || timer: 0.1399 sec.
iter 19680 || Loss: 1.7111 || timer: 0.1466 sec.
iter 19690 || Loss: 1.6305 || timer: 0.0464 sec.
iter 19700 || Loss: 1.2419 || timer: 0.1339 sec.
iter 19710 || Loss: 1.0393 || timer: 0.1403 sec.
iter 19720 || Loss: 1.0907 || timer: 0.1592 sec.
iter 19730 || Loss: 1.2665 || timer: 0.0958 sec.
iter 19740 || Loss: 1.2326 || timer: 0.1644 sec.
iter 19750 || Loss: 1.6493 || timer: 0.1442 sec.
iter 19760 || Loss: 1.2591 || timer: 0.1890 sec.
iter 19770 || Loss: 1.8024 || timer: 0.1040 sec.
iter 19780 || Loss: 1.6400 || timer: 0.1588 sec.
iter 19790 || Loss: 1.6144 || timer: 0.1584 sec.
iter 19800 || Loss: 1.4308 || timer: 0.1657 sec.
iter 19810 || Loss: 1.3978 || timer: 0.1605 sec.
iter 19820 || Loss: 1.4074 || timer: 0.1458 sec.
iter 19830 || Loss: 1.3344 || timer: 0.1725 sec.
iter 19840 || Loss: 1.5005 || timer: 0.1685 sec.
iter 19850 || Loss: 1.6697 || timer: 0.1763 sec.
iter 19860 || Loss: 1.8512 || timer: 0.1893 sec.
iter 19870 || Loss: 1.8155 || timer: 0.1549 sec.
iter 19880 || Loss: 1.8829 || timer: 0.1873 sec.
iter 19890 || Loss: 1.4451 || timer: 0.1125 sec.
iter 19900 || Loss: 1.6204 || timer: 0.1634 sec.
iter 19910 || Loss: 1.6820 || timer: 0.1504 sec.
iter 19920 || Loss: 1.9868 || timer: 0.1578 sec.
iter 19930 || Loss: 2.1941 || timer: 0.1586 sec.
iter 19940 || Loss: 2.1311 || timer: 0.1550 sec.
iter 19950 || Loss: 1.5384 || timer: 0.1400 sec.
iter 19960 || Loss: 1.8666 || timer: 0.1301 sec.
iter 19970 || Loss: 1.5278 || timer: 0.1397 sec.
iter 19980 || Loss: 1.8623 || timer: 0.1763 sec.
iter 19990 || Loss: 1.5978 || timer: 0.1544 sec.
iter 20000 || Loss: 1.1375 || Saving state, iter: 20000
timer: 0.1784 sec.
iter 20010 || Loss: 1.5396 || timer: 0.1320 sec.
iter 20020 || Loss: 1.6316 || timer: 0.0290 sec.
iter 20030 || Loss: 2.3270 || timer: 0.1429 sec.
iter 20040 || Loss: 2.0742 || timer: 0.1420 sec.
iter 20050 || Loss: 1.4214 || timer: 0.1653 sec.
iter 20060 || Loss: 1.2814 || timer: 0.1480 sec.
iter 20070 || Loss: 1.4180 || timer: 0.1406 sec.
iter 20080 || Loss: 1.5053 || timer: 0.1359 sec.
iter 20090 || Loss: 1.2727 || timer: 0.1870 sec.
iter 20100 || Loss: 1.9353 || timer: 0.1119 sec.
iter 20110 || Loss: 1.7000 || timer: 0.1441 sec.
iter 20120 || Loss: 1.4901 || timer: 0.1532 sec.
iter 20130 || Loss: 1.2907 || timer: 0.1463 sec.
iter 20140 || Loss: 2.0426 || timer: 0.1718 sec.
iter 20150 || Loss: 1.5861 || timer: 0.1423 sec.
iter 20160 || Loss: 1.6266 || timer: 0.1760 sec.
iter 20170 || Loss: 1.3526 || timer: 0.1513 sec.
iter 20180 || Loss: 1.9704 || timer: 0.1392 sec.
iter 20190 || Loss: 1.4803 || timer: 0.1355 sec.
iter 20200 || Loss: 1.5515 || timer: 0.0931 sec.
iter 20210 || Loss: 1.6433 || timer: 0.1464 sec.
iter 20220 || Loss: 1.4276 || timer: 0.1625 sec.
iter 20230 || Loss: 1.4865 || timer: 0.1414 sec.
iter 20240 || Loss: 1.6030 || timer: 0.1518 sec.
iter 20250 || Loss: 1.2830 || timer: 0.1393 sec.
iter 20260 || Loss: 1.3337 || timer: 0.1490 sec.
iter 20270 || Loss: 1.6147 || timer: 0.1443 sec.
iter 20280 || Loss: 1.2007 || timer: 0.1496 sec.
iter 20290 || Loss: 1.4606 || timer: 0.1663 sec.
iter 20300 || Loss: 1.6438 || timer: 0.1595 sec.
iter 20310 || Loss: 1.6091 || timer: 0.1815 sec.
iter 20320 || Loss: 1.7154 || timer: 0.1437 sec.
iter 20330 || Loss: 1.6727 || timer: 0.1800 sec.
iter 20340 || Loss: 1.7058 || timer: 0.1471 sec.
iter 20350 || Loss: 1.0154 || timer: 0.0279 sec.
iter 20360 || Loss: 3.0725 || timer: 0.1725 sec.
iter 20370 || Loss: 2.3305 || timer: 0.1114 sec.
iter 20380 || Loss: 1.8781 || timer: 0.1418 sec.
iter 20390 || Loss: 2.1074 || timer: 0.1323 sec.
iter 20400 || Loss: 1.6519 || timer: 0.0919 sec.
iter 20410 || Loss: 1.8373 || timer: 0.1648 sec.
iter 20420 || Loss: 1.6407 || timer: 0.1482 sec.
iter 20430 || Loss: 1.7428 || timer: 0.1128 sec.
iter 20440 || Loss: 1.8544 || timer: 0.1508 sec.
iter 20450 || Loss: 1.2332 || timer: 0.1572 sec.
iter 20460 || Loss: 1.6847 || timer: 0.1719 sec.
iter 20470 || Loss: 2.0667 || timer: 0.1809 sec.
iter 20480 || Loss: 1.6570 || timer: 0.1497 sec.
iter 20490 || Loss: 1.3330 || timer: 0.1457 sec.
iter 20500 || Loss: 1.8923 || timer: 0.1444 sec.
iter 20510 || Loss: 1.2167 || timer: 0.1509 sec.
iter 20520 || Loss: 1.3725 || timer: 0.1435 sec.
iter 20530 || Loss: 1.5821 || timer: 0.1487 sec.
iter 20540 || Loss: 1.0869 || timer: 0.1733 sec.
iter 20550 || Loss: 1.3826 || timer: 0.1483 sec.
iter 20560 || Loss: 1.2533 || timer: 0.1690 sec.
iter 20570 || Loss: 1.8556 || timer: 0.1706 sec.
iter 20580 || Loss: 1.6374 || timer: 0.1311 sec.
iter 20590 || Loss: 1.7207 || timer: 0.1373 sec.
iter 20600 || Loss: 2.0847 || timer: 0.1515 sec.
iter 20610 || Loss: 1.5156 || timer: 0.1693 sec.
iter 20620 || Loss: 1.5343 || timer: 0.1010 sec.
iter 20630 || Loss: 1.9567 || timer: 0.1517 sec.
iter 20640 || Loss: 2.0883 || timer: 0.1723 sec.
iter 20650 || Loss: 1.4616 || timer: 0.1414 sec.
iter 20660 || Loss: 1.2808 || timer: 0.1492 sec.
iter 20670 || Loss: 1.7582 || timer: 0.1554 sec.
iter 20680 || Loss: 1.4358 || timer: 0.0257 sec.
iter 20690 || Loss: 2.3750 || timer: 0.1750 sec.
iter 20700 || Loss: 1.8601 || timer: 0.0837 sec.
iter 20710 || Loss: 1.6950 || timer: 0.1656 sec.
iter 20720 || Loss: 1.4205 || timer: 0.1629 sec.
iter 20730 || Loss: 1.4663 || timer: 0.1677 sec.
iter 20740 || Loss: 1.6584 || timer: 0.1626 sec.
iter 20750 || Loss: 1.3828 || timer: 0.1850 sec.
iter 20760 || Loss: 1.7471 || timer: 0.1472 sec.
iter 20770 || Loss: 1.7849 || timer: 0.1407 sec.
iter 20780 || Loss: 1.7829 || timer: 0.1182 sec.
iter 20790 || Loss: 1.4605 || timer: 0.1616 sec.
iter 20800 || Loss: 1.3360 || timer: 0.1696 sec.
iter 20810 || Loss: 1.5936 || timer: 0.1447 sec.
iter 20820 || Loss: 1.9365 || timer: 0.1491 sec.
iter 20830 || Loss: 1.5612 || timer: 0.1596 sec.
iter 20840 || Loss: 1.2831 || timer: 0.1380 sec.
iter 20850 || Loss: 1.7489 || timer: 0.0921 sec.
iter 20860 || Loss: 1.6953 || timer: 0.1709 sec.
iter 20870 || Loss: 1.4914 || timer: 0.1691 sec.
iter 20880 || Loss: 1.4622 || timer: 0.1723 sec.
iter 20890 || Loss: 1.2293 || timer: 0.1636 sec.
iter 20900 || Loss: 1.5773 || timer: 0.1596 sec.
iter 20910 || Loss: 1.4470 || timer: 0.1351 sec.
iter 20920 || Loss: 1.7641 || timer: 0.1458 sec.
iter 20930 || Loss: 1.7758 || timer: 0.1451 sec.
iter 20940 || Loss: 1.7041 || timer: 0.1419 sec.
iter 20950 || Loss: 1.4967 || timer: 0.1467 sec.
iter 20960 || Loss: 1.3604 || timer: 0.1246 sec.
iter 20970 || Loss: 1.4164 || timer: 0.1423 sec.
iter 20980 || Loss: 1.3685 || timer: 0.1812 sec.
iter 20990 || Loss: 1.4640 || timer: 0.1472 sec.
iter 21000 || Loss: 1.6123 || timer: 0.1728 sec.
iter 21010 || Loss: 1.9627 || timer: 0.0298 sec.
iter 21020 || Loss: 1.4157 || timer: 0.1175 sec.
iter 21030 || Loss: 1.6944 || timer: 0.1729 sec.
iter 21040 || Loss: 1.4405 || timer: 0.1407 sec.
iter 21050 || Loss: 1.4101 || timer: 0.1424 sec.
iter 21060 || Loss: 1.2217 || timer: 0.1842 sec.
iter 21070 || Loss: 1.0019 || timer: 0.1808 sec.
iter 21080 || Loss: 1.6600 || timer: 0.1505 sec.
iter 21090 || Loss: 1.4171 || timer: 0.1843 sec.
iter 21100 || Loss: 1.5354 || timer: 0.1328 sec.
iter 21110 || Loss: 1.5141 || timer: 0.1132 sec.
iter 21120 || Loss: 1.5680 || timer: 0.1431 sec.
iter 21130 || Loss: 1.7756 || timer: 0.1300 sec.
iter 21140 || Loss: 1.2281 || timer: 0.1306 sec.
iter 21150 || Loss: 1.6009 || timer: 0.1230 sec.
iter 21160 || Loss: 1.4935 || timer: 0.1197 sec.
iter 21170 || Loss: 1.4598 || timer: 0.1228 sec.
iter 21180 || Loss: 1.4236 || timer: 0.1621 sec.
iter 21190 || Loss: 1.4735 || timer: 0.1493 sec.
iter 21200 || Loss: 1.0669 || timer: 0.1349 sec.
iter 21210 || Loss: 1.3948 || timer: 0.0934 sec.
iter 21220 || Loss: 1.5271 || timer: 0.1494 sec.
iter 21230 || Loss: 1.3776 || timer: 0.1585 sec.
iter 21240 || Loss: 1.8523 || timer: 0.1391 sec.
iter 21250 || Loss: 1.5788 || timer: 0.1385 sec.
iter 21260 || Loss: 1.8748 || timer: 0.1448 sec.
iter 21270 || Loss: 1.3869 || timer: 0.1446 sec.
iter 21280 || Loss: 1.5851 || timer: 0.1753 sec.
iter 21290 || Loss: 1.8055 || timer: 0.1766 sec.
iter 21300 || Loss: 1.6148 || timer: 0.1754 sec.
iter 21310 || Loss: 1.2671 || timer: 0.1444 sec.
iter 21320 || Loss: 1.5055 || timer: 0.1275 sec.
iter 21330 || Loss: 1.9279 || timer: 0.1404 sec.
iter 21340 || Loss: 1.3073 || timer: 0.0276 sec.
iter 21350 || Loss: 0.6292 || timer: 0.1745 sec.
iter 21360 || Loss: 1.4658 || timer: 0.1703 sec.
iter 21370 || Loss: 1.9615 || timer: 0.1545 sec.
iter 21380 || Loss: 1.7609 || timer: 0.1402 sec.
iter 21390 || Loss: 1.8715 || timer: 0.1724 sec.
iter 21400 || Loss: 1.5425 || timer: 0.1411 sec.
iter 21410 || Loss: 1.7167 || timer: 0.1765 sec.
iter 21420 || Loss: 1.5730 || timer: 0.1049 sec.
iter 21430 || Loss: 1.3708 || timer: 0.1683 sec.
iter 21440 || Loss: 1.4681 || timer: 0.1516 sec.
iter 21450 || Loss: 1.3337 || timer: 0.1809 sec.
iter 21460 || Loss: 1.8006 || timer: 0.1429 sec.
iter 21470 || Loss: 1.3057 || timer: 0.1383 sec.
iter 21480 || Loss: 1.5223 || timer: 0.1720 sec.
iter 21490 || Loss: 1.1157 || timer: 0.1695 sec.
iter 21500 || Loss: 1.1866 || timer: 0.1484 sec.
iter 21510 || Loss: 1.4151 || timer: 0.1388 sec.
iter 21520 || Loss: 2.3413 || timer: 0.1653 sec.
iter 21530 || Loss: 1.8745 || timer: 0.1873 sec.
iter 21540 || Loss: 1.1605 || timer: 0.1188 sec.
iter 21550 || Loss: 1.5854 || timer: 0.1354 sec.
iter 21560 || Loss: 1.4831 || timer: 0.1469 sec.
iter 21570 || Loss: 1.2909 || timer: 0.1629 sec.
iter 21580 || Loss: 1.8443 || timer: 0.1476 sec.
iter 21590 || Loss: 1.2075 || timer: 0.1433 sec.
iter 21600 || Loss: 2.0648 || timer: 0.1379 sec.
iter 21610 || Loss: 2.3719 || timer: 0.0882 sec.
iter 21620 || Loss: 2.0065 || timer: 0.1748 sec.
iter 21630 || Loss: 1.7389 || timer: 0.1527 sec.
iter 21640 || Loss: 1.8324 || timer: 0.1568 sec.
iter 21650 || Loss: 1.7488 || timer: 0.1789 sec.
iter 21660 || Loss: 1.5215 || timer: 0.1493 sec.
iter 21670 || Loss: 1.2497 || timer: 0.0237 sec.
iter 21680 || Loss: 0.8023 || timer: 0.1634 sec.
iter 21690 || Loss: 1.4074 || timer: 0.1778 sec.
iter 21700 || Loss: 1.7845 || timer: 0.1459 sec.
iter 21710 || Loss: 2.2688 || timer: 0.1843 sec.
iter 21720 || Loss: 1.7359 || timer: 0.1348 sec.
iter 21730 || Loss: 1.7685 || timer: 0.1507 sec.
iter 21740 || Loss: 1.6218 || timer: 0.1431 sec.
iter 21750 || Loss: 1.8194 || timer: 0.1498 sec.
iter 21760 || Loss: 1.3182 || timer: 0.1659 sec.
iter 21770 || Loss: 1.5623 || timer: 0.1937 sec.
iter 21780 || Loss: 1.0765 || timer: 0.1725 sec.
iter 21790 || Loss: 1.6667 || timer: 0.1474 sec.
iter 21800 || Loss: 1.6444 || timer: 0.1487 sec.
iter 21810 || Loss: 1.1765 || timer: 0.1572 sec.
iter 21820 || Loss: 1.2464 || timer: 0.0917 sec.
iter 21830 || Loss: 1.9206 || timer: 0.1739 sec.
iter 21840 || Loss: 1.4045 || timer: 0.1092 sec.
iter 21850 || Loss: 1.7460 || timer: 0.1704 sec.
iter 21860 || Loss: 1.5054 || timer: 0.1751 sec.
iter 21870 || Loss: 2.0317 || timer: 0.1289 sec.
iter 21880 || Loss: 1.6159 || timer: 0.1715 sec.
iter 21890 || Loss: 1.3944 || timer: 0.1693 sec.
iter 21900 || Loss: 1.8335 || timer: 0.1418 sec.
iter 21910 || Loss: 1.4948 || timer: 0.1426 sec.
iter 21920 || Loss: 2.0155 || timer: 0.1622 sec.
iter 21930 || Loss: 2.3173 || timer: 0.1727 sec.
iter 21940 || Loss: 1.7262 || timer: 0.0906 sec.
iter 21950 || Loss: 1.3698 || timer: 0.1659 sec.
iter 21960 || Loss: 1.1189 || timer: 0.1666 sec.
iter 21970 || Loss: 1.6764 || timer: 0.1511 sec.
iter 21980 || Loss: 1.7044 || timer: 0.1707 sec.
iter 21990 || Loss: 1.3546 || timer: 0.1789 sec.
iter 22000 || Loss: 1.4867 || timer: 0.0331 sec.
iter 22010 || Loss: 2.3727 || timer: 0.1610 sec.
iter 22020 || Loss: 1.7582 || timer: 0.1455 sec.
iter 22030 || Loss: 1.3990 || timer: 0.1240 sec.
iter 22040 || Loss: 1.5061 || timer: 0.0911 sec.
iter 22050 || Loss: 1.6910 || timer: 0.1500 sec.
iter 22060 || Loss: 1.9604 || timer: 0.1730 sec.
iter 22070 || Loss: 1.7594 || timer: 0.1744 sec.
iter 22080 || Loss: 1.8372 || timer: 0.1047 sec.
iter 22090 || Loss: 1.7442 || timer: 0.1440 sec.
iter 22100 || Loss: 1.4032 || timer: 0.1344 sec.
iter 22110 || Loss: 1.6975 || timer: 0.1644 sec.
iter 22120 || Loss: 1.2694 || timer: 0.1827 sec.
iter 22130 || Loss: 1.5668 || timer: 0.1667 sec.
iter 22140 || Loss: 1.6596 || timer: 0.1398 sec.
iter 22150 || Loss: 1.3148 || timer: 0.1793 sec.
iter 22160 || Loss: 1.7189 || timer: 0.1423 sec.
iter 22170 || Loss: 1.0749 || timer: 0.1433 sec.
iter 22180 || Loss: 1.7464 || timer: 0.1762 sec.
iter 22190 || Loss: 1.8050 || timer: 0.1595 sec.
iter 22200 || Loss: 1.2205 || timer: 0.1750 sec.
iter 22210 || Loss: 1.3297 || timer: 0.1711 sec.
iter 22220 || Loss: 1.3203 || timer: 0.1679 sec.
iter 22230 || Loss: 1.6987 || timer: 0.1658 sec.
iter 22240 || Loss: 1.4885 || timer: 0.1358 sec.
iter 22250 || Loss: 1.4608 || timer: 0.1443 sec.
iter 22260 || Loss: 1.7370 || timer: 0.1395 sec.
iter 22270 || Loss: 1.1845 || timer: 0.1354 sec.
iter 22280 || Loss: 1.6050 || timer: 0.1478 sec.
iter 22290 || Loss: 1.2887 || timer: 0.1516 sec.
iter 22300 || Loss: 1.4987 || timer: 0.1495 sec.
iter 22310 || Loss: 1.6332 || timer: 0.1220 sec.
iter 22320 || Loss: 1.4651 || timer: 0.1294 sec.
iter 22330 || Loss: 1.5429 || timer: 0.0246 sec.
iter 22340 || Loss: 3.8289 || timer: 0.1631 sec.
iter 22350 || Loss: 2.2497 || timer: 0.1849 sec.
iter 22360 || Loss: 1.7025 || timer: 0.1600 sec.
iter 22370 || Loss: 1.5707 || timer: 0.1342 sec.
iter 22380 || Loss: 1.8960 || timer: 0.1513 sec.
iter 22390 || Loss: 1.7826 || timer: 0.1794 sec.
iter 22400 || Loss: 1.5465 || timer: 0.1665 sec.
iter 22410 || Loss: 1.6587 || timer: 0.1681 sec.
iter 22420 || Loss: 1.4756 || timer: 0.1505 sec.
iter 22430 || Loss: 2.0536 || timer: 0.1720 sec.
iter 22440 || Loss: 1.4899 || timer: 0.1459 sec.
iter 22450 || Loss: 1.3407 || timer: 0.1739 sec.
iter 22460 || Loss: 1.3779 || timer: 0.1535 sec.
iter 22470 || Loss: 1.4839 || timer: 0.1719 sec.
iter 22480 || Loss: 1.4021 || timer: 0.1375 sec.
iter 22490 || Loss: 1.8480 || timer: 0.1352 sec.
iter 22500 || Loss: 1.9870 || timer: 0.1525 sec.
iter 22510 || Loss: 2.0419 || timer: 0.1753 sec.
iter 22520 || Loss: 2.2664 || timer: 0.1776 sec.
iter 22530 || Loss: 1.3517 || timer: 0.1412 sec.
iter 22540 || Loss: 1.4925 || timer: 0.1120 sec.
iter 22550 || Loss: 1.0622 || timer: 0.1875 sec.
iter 22560 || Loss: 2.5637 || timer: 0.1240 sec.
iter 22570 || Loss: 1.6748 || timer: 0.1485 sec.
iter 22580 || Loss: 1.5095 || timer: 0.1563 sec.
iter 22590 || Loss: 1.5563 || timer: 0.1513 sec.
iter 22600 || Loss: 1.2954 || timer: 0.1535 sec.
iter 22610 || Loss: 1.9955 || timer: 0.1493 sec.
iter 22620 || Loss: 1.5802 || timer: 0.1702 sec.
iter 22630 || Loss: 1.7515 || timer: 0.1292 sec.
iter 22640 || Loss: 1.3456 || timer: 0.1410 sec.
iter 22650 || Loss: 1.6778 || timer: 0.1692 sec.
iter 22660 || Loss: 1.2309 || timer: 0.0308 sec.
iter 22670 || Loss: 1.7093 || timer: 0.1564 sec.
iter 22680 || Loss: 1.7513 || timer: 0.1496 sec.
iter 22690 || Loss: 1.7135 || timer: 0.0895 sec.
iter 22700 || Loss: 1.6165 || timer: 0.1543 sec.
iter 22710 || Loss: 1.6978 || timer: 0.1540 sec.
iter 22720 || Loss: 1.4566 || timer: 0.1797 sec.
iter 22730 || Loss: 1.5874 || timer: 0.1467 sec.
iter 22740 || Loss: 2.2537 || timer: 0.1711 sec.
iter 22750 || Loss: 1.9094 || timer: 0.1419 sec.
iter 22760 || Loss: 2.3299 || timer: 0.1819 sec.
iter 22770 || Loss: 1.5049 || timer: 0.1395 sec.
iter 22780 || Loss: 1.3600 || timer: 0.0898 sec.
iter 22790 || Loss: 1.6429 || timer: 0.1658 sec.
iter 22800 || Loss: 1.8878 || timer: 0.1427 sec.
iter 22810 || Loss: 1.3064 || timer: 0.1379 sec.
iter 22820 || Loss: 1.5458 || timer: 0.1496 sec.
iter 22830 || Loss: 1.3181 || timer: 0.1769 sec.
iter 22840 || Loss: 1.2498 || timer: 0.1620 sec.
iter 22850 || Loss: 1.8497 || timer: 0.1719 sec.
iter 22860 || Loss: 1.6008 || timer: 0.0915 sec.
iter 22870 || Loss: 1.7795 || timer: 0.1516 sec.
iter 22880 || Loss: 1.6206 || timer: 0.1683 sec.
iter 22890 || Loss: 1.7509 || timer: 0.1021 sec.
iter 22900 || Loss: 1.4294 || timer: 0.1613 sec.
iter 22910 || Loss: 1.2683 || timer: 0.1469 sec.
iter 22920 || Loss: 1.8509 || timer: 0.1731 sec.
iter 22930 || Loss: 1.0160 || timer: 0.1686 sec.
iter 22940 || Loss: 1.5188 || timer: 0.1712 sec.
iter 22950 || Loss: 1.4075 || timer: 0.1762 sec.
iter 22960 || Loss: 1.4263 || timer: 0.1529 sec.
iter 22970 || Loss: 1.7989 || timer: 0.1425 sec.
iter 22980 || Loss: 1.7490 || timer: 0.1809 sec.
iter 22990 || Loss: 1.4073 || timer: 0.0318 sec.
iter 23000 || Loss: 2.3807 || timer: 0.1746 sec.
iter 23010 || Loss: 1.4731 || timer: 0.1359 sec.
iter 23020 || Loss: 1.6566 || timer: 0.1753 sec.
iter 23030 || Loss: 1.5601 || timer: 0.1362 sec.
iter 23040 || Loss: 1.3672 || timer: 0.1661 sec.
iter 23050 || Loss: 1.9586 || timer: 0.1459 sec.
iter 23060 || Loss: 1.3137 || timer: 0.1718 sec.
iter 23070 || Loss: 1.2327 || timer: 0.1165 sec.
iter 23080 || Loss: 1.9388 || timer: 0.1133 sec.
iter 23090 || Loss: 1.5522 || timer: 0.1472 sec.
iter 23100 || Loss: 1.1835 || timer: 0.1422 sec.
iter 23110 || Loss: 1.1364 || timer: 0.1337 sec.
iter 23120 || Loss: 1.2244 || timer: 0.1603 sec.
iter 23130 || Loss: 1.3897 || timer: 0.0940 sec.
iter 23140 || Loss: 1.4018 || timer: 0.1705 sec.
iter 23150 || Loss: 1.5531 || timer: 0.1483 sec.
iter 23160 || Loss: 1.6581 || timer: 0.1498 sec.
iter 23170 || Loss: 1.8332 || timer: 0.1651 sec.
iter 23180 || Loss: 1.3069 || timer: 0.1540 sec.
iter 23190 || Loss: 1.3011 || timer: 0.1413 sec.
iter 23200 || Loss: 1.3371 || timer: 0.1685 sec.
iter 23210 || Loss: 1.7274 || timer: 0.1623 sec.
iter 23220 || Loss: 1.6052 || timer: 0.1314 sec.
iter 23230 || Loss: 1.5117 || timer: 0.1499 sec.
iter 23240 || Loss: 1.3969 || timer: 0.1390 sec.
iter 23250 || Loss: 1.7740 || timer: 0.1495 sec.
iter 23260 || Loss: 1.4552 || timer: 0.1568 sec.
iter 23270 || Loss: 1.5115 || timer: 0.1502 sec.
iter 23280 || Loss: 1.5834 || timer: 0.1711 sec.
iter 23290 || Loss: 2.2005 || timer: 0.1325 sec.
iter 23300 || Loss: 1.5101 || timer: 0.1531 sec.
iter 23310 || Loss: 1.6726 || timer: 0.1452 sec.
iter 23320 || Loss: 1.4848 || timer: 0.0229 sec.
iter 23330 || Loss: 1.3992 || timer: 0.1635 sec.
iter 23340 || Loss: 1.8117 || timer: 0.1669 sec.
iter 23350 || Loss: 1.6792 || timer: 0.1415 sec.
iter 23360 || Loss: 1.9180 || timer: 0.1714 sec.
iter 23370 || Loss: 1.6587 || timer: 0.1729 sec.
iter 23380 || Loss: 1.6770 || timer: 0.1627 sec.
iter 23390 || Loss: 1.6446 || timer: 0.1627 sec.
iter 23400 || Loss: 1.8175 || timer: 0.1648 sec.
iter 23410 || Loss: 1.4709 || timer: 0.1538 sec.
iter 23420 || Loss: 1.5307 || timer: 0.1581 sec.
iter 23430 || Loss: 1.2869 || timer: 0.1730 sec.
iter 23440 || Loss: 0.9507 || timer: 0.1368 sec.
iter 23450 || Loss: 1.2564 || timer: 0.1452 sec.
iter 23460 || Loss: 1.2833 || timer: 0.1472 sec.
iter 23470 || Loss: 1.3636 || timer: 0.1531 sec.
iter 23480 || Loss: 1.8683 || timer: 0.1509 sec.
iter 23490 || Loss: 1.3565 || timer: 0.1431 sec.
iter 23500 || Loss: 1.1244 || timer: 0.1413 sec.
iter 23510 || Loss: 1.3445 || timer: 0.1651 sec.
iter 23520 || Loss: 1.1981 || timer: 0.1455 sec.
iter 23530 || Loss: 1.3380 || timer: 0.1732 sec.
iter 23540 || Loss: 1.8612 || timer: 0.1733 sec.
iter 23550 || Loss: 1.0642 || timer: 0.1549 sec.
iter 23560 || Loss: 1.0466 || timer: 0.1678 sec.
iter 23570 || Loss: 1.1828 || timer: 0.1826 sec.
iter 23580 || Loss: 1.9414 || timer: 0.1689 sec.
iter 23590 || Loss: 1.3607 || timer: 0.1509 sec.
iter 23600 || Loss: 1.3957 || timer: 0.1661 sec.
iter 23610 || Loss: 1.6496 || timer: 0.0840 sec.
iter 23620 || Loss: 1.3546 || timer: 0.1748 sec.
iter 23630 || Loss: 1.2745 || timer: 0.1534 sec.
iter 23640 || Loss: 1.6958 || timer: 0.1741 sec.
iter 23650 || Loss: 1.6929 || timer: 0.0230 sec.
iter 23660 || Loss: 2.1072 || timer: 0.1455 sec.
iter 23670 || Loss: 1.6950 || timer: 0.1489 sec.
iter 23680 || Loss: 1.5399 || timer: 0.1514 sec.
iter 23690 || Loss: 1.4070 || timer: 0.0942 sec.
iter 23700 || Loss: 1.5645 || timer: 0.1555 sec.
iter 23710 || Loss: 1.3624 || timer: 0.1582 sec.
iter 23720 || Loss: 1.0668 || timer: 0.1482 sec.
iter 23730 || Loss: 2.0165 || timer: 0.1536 sec.
iter 23740 || Loss: 1.3588 || timer: 0.1646 sec.
iter 23750 || Loss: 1.2512 || timer: 0.1813 sec.
iter 23760 || Loss: 1.6764 || timer: 0.1443 sec.
iter 23770 || Loss: 1.4541 || timer: 0.1565 sec.
iter 23780 || Loss: 1.4624 || timer: 0.1373 sec.
iter 23790 || Loss: 1.3324 || timer: 0.1623 sec.
iter 23800 || Loss: 1.4685 || timer: 0.1521 sec.
iter 23810 || Loss: 1.2473 || timer: 0.1544 sec.
iter 23820 || Loss: 2.0263 || timer: 0.1377 sec.
iter 23830 || Loss: 1.6388 || timer: 0.1427 sec.
iter 23840 || Loss: 1.9071 || timer: 0.1328 sec.
iter 23850 || Loss: 1.6451 || timer: 0.1079 sec.
iter 23860 || Loss: 1.1997 || timer: 0.1766 sec.
iter 23870 || Loss: 1.5055 || timer: 0.1733 sec.
iter 23880 || Loss: 1.3640 || timer: 0.1371 sec.
iter 23890 || Loss: 1.8425 || timer: 0.1575 sec.
iter 23900 || Loss: 1.5815 || timer: 0.1637 sec.
iter 23910 || Loss: 1.2725 || timer: 0.1591 sec.
iter 23920 || Loss: 1.0798 || timer: 0.1447 sec.
iter 23930 || Loss: 1.2110 || timer: 0.1484 sec.
iter 23940 || Loss: 1.4892 || timer: 0.1687 sec.
iter 23950 || Loss: 1.2441 || timer: 0.1669 sec.
iter 23960 || Loss: 1.0653 || timer: 0.1719 sec.
iter 23970 || Loss: 1.2994 || timer: 0.1469 sec.
iter 23980 || Loss: 1.2802 || timer: 0.0250 sec.
iter 23990 || Loss: 0.7722 || timer: 0.1313 sec.
iter 24000 || Loss: 1.6320 || timer: 0.1752 sec.
iter 24010 || Loss: 1.5064 || timer: 0.1409 sec.
iter 24020 || Loss: 1.6619 || timer: 0.1607 sec.
iter 24030 || Loss: 1.7571 || timer: 0.1700 sec.
iter 24040 || Loss: 1.4325 || timer: 0.1796 sec.
iter 24050 || Loss: 1.4880 || timer: 0.1536 sec.
iter 24060 || Loss: 2.3068 || timer: 0.1446 sec.
iter 24070 || Loss: 1.6636 || timer: 0.1511 sec.
iter 24080 || Loss: 1.3325 || timer: 0.1559 sec.
iter 24090 || Loss: 1.2310 || timer: 0.1789 sec.
iter 24100 || Loss: 1.4990 || timer: 0.1388 sec.
iter 24110 || Loss: 1.4640 || timer: 0.1678 sec.
iter 24120 || Loss: 1.6478 || timer: 0.1458 sec.
iter 24130 || Loss: 1.2211 || timer: 0.1588 sec.
iter 24140 || Loss: 1.3112 || timer: 0.1486 sec.
iter 24150 || Loss: 1.5593 || timer: 0.1600 sec.
iter 24160 || Loss: 1.2622 || timer: 0.1476 sec.
iter 24170 || Loss: 1.7127 || timer: 0.1763 sec.
iter 24180 || Loss: 1.4577 || timer: 0.1686 sec.
iter 24190 || Loss: 1.5075 || timer: 0.1648 sec.
iter 24200 || Loss: 1.8482 || timer: 0.1443 sec.
iter 24210 || Loss: 1.4896 || timer: 0.1280 sec.
iter 24220 || Loss: 1.2147 || timer: 0.1397 sec.
iter 24230 || Loss: 1.5033 || timer: 0.1023 sec.
iter 24240 || Loss: 1.3756 || timer: 0.1637 sec.
iter 24250 || Loss: 1.3866 || timer: 0.1714 sec.
iter 24260 || Loss: 1.5638 || timer: 0.1736 sec.
iter 24270 || Loss: 2.0563 || timer: 0.1386 sec.
iter 24280 || Loss: 1.3511 || timer: 0.1642 sec.
iter 24290 || Loss: 1.7742 || timer: 0.1625 sec.
iter 24300 || Loss: 1.6601 || timer: 0.1485 sec.
iter 24310 || Loss: 1.5732 || timer: 0.0267 sec.
iter 24320 || Loss: 4.0947 || timer: 0.1418 sec.
iter 24330 || Loss: 3.0544 || timer: 0.1552 sec.
iter 24340 || Loss: 1.5148 || timer: 0.1620 sec.
iter 24350 || Loss: 1.3774 || timer: 0.1359 sec.
iter 24360 || Loss: 1.9179 || timer: 0.1765 sec.
iter 24370 || Loss: 1.8106 || timer: 0.1473 sec.
iter 24380 || Loss: 1.4448 || timer: 0.1465 sec.
iter 24390 || Loss: 1.2352 || timer: 0.1415 sec.
iter 24400 || Loss: 1.3113 || timer: 0.1544 sec.
iter 24410 || Loss: 1.7432 || timer: 0.1608 sec.
iter 24420 || Loss: 1.5183 || timer: 0.1504 sec.
iter 24430 || Loss: 1.7139 || timer: 0.1440 sec.
iter 24440 || Loss: 1.8115 || timer: 0.1348 sec.
iter 24450 || Loss: 1.2915 || timer: 0.1870 sec.
iter 24460 || Loss: 1.4687 || timer: 0.1743 sec.
iter 24470 || Loss: 1.5639 || timer: 0.1389 sec.
iter 24480 || Loss: 1.5209 || timer: 0.1384 sec.
iter 24490 || Loss: 1.6739 || timer: 0.1756 sec.
iter 24500 || Loss: 1.1779 || timer: 0.1704 sec.
iter 24510 || Loss: 1.5793 || timer: 0.1468 sec.
iter 24520 || Loss: 1.1775 || timer: 0.1976 sec.
iter 24530 || Loss: 1.5246 || timer: 0.1836 sec.
iter 24540 || Loss: 1.4778 || timer: 0.1530 sec.
iter 24550 || Loss: 1.1831 || timer: 0.1387 sec.
iter 24560 || Loss: 1.4585 || timer: 0.1586 sec.
iter 24570 || Loss: 1.6199 || timer: 0.1526 sec.
iter 24580 || Loss: 2.1321 || timer: 0.1593 sec.
iter 24590 || Loss: 1.5628 || timer: 0.1059 sec.
iter 24600 || Loss: 1.5611 || timer: 0.1263 sec.
iter 24610 || Loss: 1.1624 || timer: 0.1409 sec.
iter 24620 || Loss: 1.9415 || timer: 0.1445 sec.
iter 24630 || Loss: 1.6948 || timer: 0.1509 sec.
iter 24640 || Loss: 1.6235 || timer: 0.0217 sec.
iter 24650 || Loss: 0.9263 || timer: 0.1744 sec.
iter 24660 || Loss: 1.5216 || timer: 0.1742 sec.
iter 24670 || Loss: 1.1318 || timer: 0.1711 sec.
iter 24680 || Loss: 1.3476 || timer: 0.1804 sec.
iter 24690 || Loss: 1.8640 || timer: 0.1496 sec.
iter 24700 || Loss: 1.3474 || timer: 0.1705 sec.
iter 24710 || Loss: 1.5425 || timer: 0.1715 sec.
iter 24720 || Loss: 1.2460 || timer: 0.1491 sec.
iter 24730 || Loss: 1.5996 || timer: 0.1400 sec.
iter 24740 || Loss: 1.4817 || timer: 0.1546 sec.
iter 24750 || Loss: 1.4473 || timer: 0.1467 sec.
iter 24760 || Loss: 1.5802 || timer: 0.1749 sec.
iter 24770 || Loss: 1.5676 || timer: 0.1751 sec.
iter 24780 || Loss: 1.0487 || timer: 0.1429 sec.
iter 24790 || Loss: 1.2175 || timer: 0.1750 sec.
iter 24800 || Loss: 1.3231 || timer: 0.1437 sec.
iter 24810 || Loss: 1.0773 || timer: 0.1503 sec.
iter 24820 || Loss: 1.7700 || timer: 0.1504 sec.
iter 24830 || Loss: 1.4981 || timer: 0.1479 sec.
iter 24840 || Loss: 1.3912 || timer: 0.1365 sec.
iter 24850 || Loss: 1.1942 || timer: 0.1680 sec.
iter 24860 || Loss: 2.0461 || timer: 0.1642 sec.
iter 24870 || Loss: 1.8355 || timer: 0.1409 sec.
iter 24880 || Loss: 1.4633 || timer: 0.1404 sec.
iter 24890 || Loss: 1.2562 || timer: 0.1533 sec.
iter 24900 || Loss: 1.7372 || timer: 0.0867 sec.
iter 24910 || Loss: 1.4229 || timer: 0.1355 sec.
iter 24920 || Loss: 1.5221 || timer: 0.1648 sec.
iter 24930 || Loss: 1.2120 || timer: 0.1675 sec.
iter 24940 || Loss: 1.3098 || timer: 0.1743 sec.
iter 24950 || Loss: 1.7730 || timer: 0.1625 sec.
iter 24960 || Loss: 1.1840 || timer: 0.1547 sec.
iter 24970 || Loss: 1.4478 || timer: 0.0306 sec.
iter 24980 || Loss: 1.1927 || timer: 0.1122 sec.
iter 24990 || Loss: 1.3796 || timer: 0.0927 sec.
iter 25000 || Loss: 1.9226 || Saving state, iter: 25000
timer: 0.1647 sec.
iter 25010 || Loss: 1.5518 || timer: 0.1733 sec.
iter 25020 || Loss: 1.6365 || timer: 0.1665 sec.
iter 25030 || Loss: 1.2198 || timer: 0.1618 sec.
iter 25040 || Loss: 1.6288 || timer: 0.1640 sec.
iter 25050 || Loss: 1.2100 || timer: 0.1414 sec.
iter 25060 || Loss: 1.5111 || timer: 0.1782 sec.
iter 25070 || Loss: 1.6413 || timer: 0.1107 sec.
iter 25080 || Loss: 1.5850 || timer: 0.1056 sec.
iter 25090 || Loss: 1.1869 || timer: 0.1787 sec.
iter 25100 || Loss: 1.2780 || timer: 0.1639 sec.
iter 25110 || Loss: 1.1729 || timer: 0.1788 sec.
iter 25120 || Loss: 1.3034 || timer: 0.1807 sec.
iter 25130 || Loss: 1.6727 || timer: 0.1544 sec.
iter 25140 || Loss: 1.3290 || timer: 0.1409 sec.
iter 25150 || Loss: 2.4683 || timer: 0.1620 sec.
iter 25160 || Loss: 1.8903 || timer: 0.1591 sec.
iter 25170 || Loss: 2.0610 || timer: 0.1720 sec.
iter 25180 || Loss: 1.5236 || timer: 0.1271 sec.
iter 25190 || Loss: 1.5073 || timer: 0.1703 sec.
iter 25200 || Loss: 1.7438 || timer: 0.1502 sec.
iter 25210 || Loss: 1.0932 || timer: 0.1527 sec.
iter 25220 || Loss: 1.6864 || timer: 0.1414 sec.
iter 25230 || Loss: 1.4468 || timer: 0.1676 sec.
iter 25240 || Loss: 1.2351 || timer: 0.0913 sec.
iter 25250 || Loss: 1.2998 || timer: 0.1746 sec.
iter 25260 || Loss: 1.1850 || timer: 0.1644 sec.
iter 25270 || Loss: 1.4841 || timer: 0.1463 sec.
iter 25280 || Loss: 1.5098 || timer: 0.1883 sec.
iter 25290 || Loss: 1.3588 || timer: 0.1451 sec.
iter 25300 || Loss: 1.3825 || timer: 0.0292 sec.
iter 25310 || Loss: 0.6915 || timer: 0.1484 sec.
iter 25320 || Loss: 1.4866 || timer: 0.1392 sec.
iter 25330 || Loss: 1.5541 || timer: 0.1445 sec.
iter 25340 || Loss: 1.5193 || timer: 0.1312 sec.
iter 25350 || Loss: 1.1461 || timer: 0.1406 sec.
iter 25360 || Loss: 1.1508 || timer: 0.1472 sec.
iter 25370 || Loss: 1.0473 || timer: 0.1606 sec.
iter 25380 || Loss: 1.7709 || timer: 0.1536 sec.
iter 25390 || Loss: 1.4352 || timer: 0.1530 sec.
iter 25400 || Loss: 1.6268 || timer: 0.1190 sec.
iter 25410 || Loss: 1.4350 || timer: 0.1761 sec.
iter 25420 || Loss: 1.3289 || timer: 0.1247 sec.
iter 25430 || Loss: 1.3219 || timer: 0.0852 sec.
iter 25440 || Loss: 1.3941 || timer: 0.1646 sec.
iter 25450 || Loss: 1.7113 || timer: 0.1408 sec.
iter 25460 || Loss: 1.2732 || timer: 0.1476 sec.
iter 25470 || Loss: 1.4203 || timer: 0.1488 sec.
iter 25480 || Loss: 1.8368 || timer: 0.1695 sec.
iter 25490 || Loss: 1.5669 || timer: 0.1775 sec.
iter 25500 || Loss: 1.5698 || timer: 0.1519 sec.
iter 25510 || Loss: 1.0900 || timer: 0.1558 sec.
iter 25520 || Loss: 1.7596 || timer: 0.1484 sec.
iter 25530 || Loss: 1.3411 || timer: 0.1500 sec.
iter 25540 || Loss: 1.2255 || timer: 0.1858 sec.
iter 25550 || Loss: 2.0694 || timer: 0.1767 sec.
iter 25560 || Loss: 1.5378 || timer: 0.1719 sec.
iter 25570 || Loss: 1.3149 || timer: 0.1480 sec.
iter 25580 || Loss: 1.7224 || timer: 0.1409 sec.
iter 25590 || Loss: 1.7081 || timer: 0.1655 sec.
iter 25600 || Loss: 1.9771 || timer: 0.1691 sec.
iter 25610 || Loss: 1.4484 || timer: 0.1601 sec.
iter 25620 || Loss: 1.8098 || timer: 0.1512 sec.
iter 25630 || Loss: 1.2225 || timer: 0.0296 sec.
iter 25640 || Loss: 0.5315 || timer: 0.1515 sec.
iter 25650 || Loss: 1.1920 || timer: 0.1621 sec.
iter 25660 || Loss: 1.8159 || timer: 0.1369 sec.
iter 25670 || Loss: 1.2809 || timer: 0.1602 sec.
iter 25680 || Loss: 1.8313 || timer: 0.1556 sec.
iter 25690 || Loss: 1.3101 || timer: 0.2190 sec.
iter 25700 || Loss: 2.1382 || timer: 0.1750 sec.
iter 25710 || Loss: 1.8344 || timer: 0.1372 sec.
iter 25720 || Loss: 2.2013 || timer: 0.1698 sec.
iter 25730 || Loss: 1.7700 || timer: 0.1321 sec.
iter 25740 || Loss: 1.6366 || timer: 0.1039 sec.
iter 25750 || Loss: 1.6647 || timer: 0.1635 sec.
iter 25760 || Loss: 1.5479 || timer: 0.1674 sec.
iter 25770 || Loss: 1.3099 || timer: 0.1688 sec.
iter 25780 || Loss: 1.2140 || timer: 0.1714 sec.
iter 25790 || Loss: 1.4778 || timer: 0.1611 sec.
iter 25800 || Loss: 1.5788 || timer: 0.1622 sec.
iter 25810 || Loss: 1.3377 || timer: 0.1417 sec.
iter 25820 || Loss: 1.4725 || timer: 0.1492 sec.
iter 25830 || Loss: 1.3867 || timer: 0.1358 sec.
iter 25840 || Loss: 1.8840 || timer: 0.1799 sec.
iter 25850 || Loss: 1.4929 || timer: 0.1680 sec.
iter 25860 || Loss: 1.5554 || timer: 0.0886 sec.
iter 25870 || Loss: 1.2662 || timer: 0.1637 sec.
iter 25880 || Loss: 1.7513 || timer: 0.1688 sec.
iter 25890 || Loss: 1.5195 || timer: 0.1677 sec.
iter 25900 || Loss: 1.5230 || timer: 0.1368 sec.
iter 25910 || Loss: 1.3186 || timer: 0.1402 sec.
iter 25920 || Loss: 1.3489 || timer: 0.1447 sec.
iter 25930 || Loss: 1.1120 || timer: 0.1629 sec.
iter 25940 || Loss: 1.6685 || timer: 0.1436 sec.
iter 25950 || Loss: 1.7075 || timer: 0.1823 sec.
iter 25960 || Loss: 1.4503 || timer: 0.0305 sec.
iter 25970 || Loss: 0.6375 || timer: 0.1405 sec.
iter 25980 || Loss: 1.5056 || timer: 0.1784 sec.
iter 25990 || Loss: 1.6537 || timer: 0.1473 sec.
iter 26000 || Loss: 1.0202 || timer: 0.1736 sec.
iter 26010 || Loss: 1.1842 || timer: 0.1722 sec.
iter 26020 || Loss: 1.6428 || timer: 0.1846 sec.
iter 26030 || Loss: 1.1994 || timer: 0.1800 sec.
iter 26040 || Loss: 1.3943 || timer: 0.1739 sec.
iter 26050 || Loss: 1.6188 || timer: 0.1696 sec.
iter 26060 || Loss: 1.6170 || timer: 0.1267 sec.
iter 26070 || Loss: 1.4463 || timer: 0.1735 sec.
iter 26080 || Loss: 2.3618 || timer: 0.1732 sec.
iter 26090 || Loss: 2.3940 || timer: 0.1475 sec.
iter 26100 || Loss: 1.8163 || timer: 0.1382 sec.
iter 26110 || Loss: 1.6461 || timer: 0.1072 sec.
iter 26120 || Loss: 2.1009 || timer: 0.1250 sec.
iter 26130 || Loss: 1.7402 || timer: 0.1525 sec.
iter 26140 || Loss: 1.7801 || timer: 0.1623 sec.
iter 26150 || Loss: 1.1601 || timer: 0.1748 sec.
iter 26160 || Loss: 1.6695 || timer: 0.0919 sec.
iter 26170 || Loss: 1.3123 || timer: 0.1514 sec.
iter 26180 || Loss: 1.1966 || timer: 0.1474 sec.
iter 26190 || Loss: 1.5219 || timer: 0.1437 sec.
iter 26200 || Loss: 1.4012 || timer: 0.1776 sec.
iter 26210 || Loss: 1.9402 || timer: 0.1666 sec.
iter 26220 || Loss: 1.2399 || timer: 0.1479 sec.
iter 26230 || Loss: 1.8407 || timer: 0.1388 sec.
iter 26240 || Loss: 1.5419 || timer: 0.1712 sec.
iter 26250 || Loss: 1.5304 || timer: 0.1469 sec.
iter 26260 || Loss: 1.4279 || timer: 0.1447 sec.
iter 26270 || Loss: 1.4388 || timer: 0.1751 sec.
iter 26280 || Loss: 1.7935 || timer: 0.1658 sec.
iter 26290 || Loss: 1.7132 || timer: 0.0211 sec.
iter 26300 || Loss: 1.2058 || timer: 0.1428 sec.
iter 26310 || Loss: 1.5893 || timer: 0.1512 sec.
iter 26320 || Loss: 1.4744 || timer: 0.1364 sec.
iter 26330 || Loss: 1.3466 || timer: 0.1520 sec.
iter 26340 || Loss: 1.2688 || timer: 0.1483 sec.
iter 26350 || Loss: 1.4942 || timer: 0.1635 sec.
iter 26360 || Loss: 1.8837 || timer: 0.1793 sec.
iter 26370 || Loss: 1.5426 || timer: 0.0900 sec.
iter 26380 || Loss: 1.4533 || timer: 0.1406 sec.
iter 26390 || Loss: 1.9247 || timer: 0.1504 sec.
iter 26400 || Loss: 1.5939 || timer: 0.1675 sec.
iter 26410 || Loss: 1.3530 || timer: 0.1491 sec.
iter 26420 || Loss: 1.2738 || timer: 0.1328 sec.
iter 26430 || Loss: 1.7913 || timer: 0.1560 sec.
iter 26440 || Loss: 1.6527 || timer: 0.1650 sec.
iter 26450 || Loss: 1.2385 || timer: 0.1507 sec.
iter 26460 || Loss: 1.1836 || timer: 0.1412 sec.
iter 26470 || Loss: 1.5687 || timer: 0.1392 sec.
iter 26480 || Loss: 1.3739 || timer: 0.1417 sec.
iter 26490 || Loss: 1.4388 || timer: 0.0830 sec.
iter 26500 || Loss: 1.5934 || timer: 0.0961 sec.
iter 26510 || Loss: 1.1735 || timer: 0.1658 sec.
iter 26520 || Loss: 1.4755 || timer: 0.1382 sec.
iter 26530 || Loss: 1.5117 || timer: 0.1325 sec.
iter 26540 || Loss: 1.1705 || timer: 0.1475 sec.
iter 26550 || Loss: 1.2339 || timer: 0.1465 sec.
iter 26560 || Loss: 1.6263 || timer: 0.1605 sec.
iter 26570 || Loss: 1.1894 || timer: 0.1715 sec.
iter 26580 || Loss: 1.4765 || timer: 0.1477 sec.
iter 26590 || Loss: 1.0014 || timer: 0.0870 sec.
iter 26600 || Loss: 1.5294 || timer: 0.1582 sec.
iter 26610 || Loss: 1.2965 || timer: 0.1666 sec.
iter 26620 || Loss: 1.4794 || timer: 0.0242 sec.
iter 26630 || Loss: 3.5936 || timer: 0.1430 sec.
iter 26640 || Loss: 1.4790 || timer: 0.1505 sec.
iter 26650 || Loss: 1.5278 || timer: 0.1580 sec.
iter 26660 || Loss: 1.3452 || timer: 0.1763 sec.
iter 26670 || Loss: 1.3614 || timer: 0.1429 sec.
iter 26680 || Loss: 1.0937 || timer: 0.1706 sec.
iter 26690 || Loss: 1.4734 || timer: 0.1409 sec.
iter 26700 || Loss: 3.3749 || timer: 0.1711 sec.
iter 26710 || Loss: 2.3844 || timer: 0.1563 sec.
iter 26720 || Loss: 2.7402 || timer: 0.1485 sec.
iter 26730 || Loss: 1.5181 || timer: 0.1712 sec.
iter 26740 || Loss: 1.4496 || timer: 0.1591 sec.
iter 26750 || Loss: 1.4081 || timer: 0.1252 sec.
iter 26760 || Loss: 1.4728 || timer: 0.1477 sec.
iter 26770 || Loss: 1.7413 || timer: 0.1725 sec.
iter 26780 || Loss: 1.6985 || timer: 0.1580 sec.
iter 26790 || Loss: 1.3293 || timer: 0.1363 sec.
iter 26800 || Loss: 1.3151 || timer: 0.1799 sec.
iter 26810 || Loss: 1.6255 || timer: 0.1684 sec.
iter 26820 || Loss: 1.0480 || timer: 0.1504 sec.
iter 26830 || Loss: 1.5748 || timer: 0.1223 sec.
iter 26840 || Loss: 1.4801 || timer: 0.1521 sec.
iter 26850 || Loss: 1.5630 || timer: 0.0922 sec.
iter 26860 || Loss: 1.3372 || timer: 0.1526 sec.
iter 26870 || Loss: 1.2157 || timer: 0.1588 sec.
iter 26880 || Loss: 1.3732 || timer: 0.1549 sec.
iter 26890 || Loss: 1.7335 || timer: 0.1592 sec.
iter 26900 || Loss: 1.4613 || timer: 0.1779 sec.
iter 26910 || Loss: 2.1541 || timer: 0.1788 sec.
iter 26920 || Loss: 1.6105 || timer: 0.1720 sec.
iter 26930 || Loss: 1.9621 || timer: 0.1808 sec.
iter 26940 || Loss: 1.1600 || timer: 0.1467 sec.
iter 26950 || Loss: 1.5151 || timer: 0.0263 sec.
iter 26960 || Loss: 1.5504 || timer: 0.1614 sec.
iter 26970 || Loss: 1.2533 || timer: 0.1851 sec.
iter 26980 || Loss: 1.8605 || timer: 0.1367 sec.
iter 26990 || Loss: 1.4946 || timer: 0.1888 sec.
iter 27000 || Loss: 1.5592 || timer: 0.1456 sec.
iter 27010 || Loss: 1.3505 || timer: 0.1399 sec.
iter 27020 || Loss: 1.1691 || timer: 0.1503 sec.
iter 27030 || Loss: 1.5415 || timer: 0.1647 sec.
iter 27040 || Loss: 1.9035 || timer: 0.1494 sec.
iter 27050 || Loss: 1.6493 || timer: 0.1537 sec.
iter 27060 || Loss: 1.4787 || timer: 0.1387 sec.
iter 27070 || Loss: 1.7297 || timer: 0.1614 sec.
iter 27080 || Loss: 1.6237 || timer: 0.1462 sec.
iter 27090 || Loss: 1.1723 || timer: 0.1505 sec.
iter 27100 || Loss: 1.4721 || timer: 0.1439 sec.
iter 27110 || Loss: 1.7845 || timer: 0.1735 sec.
iter 27120 || Loss: 1.3388 || timer: 0.1473 sec.
iter 27130 || Loss: 1.8309 || timer: 0.1408 sec.
iter 27140 || Loss: 1.7089 || timer: 0.1859 sec.
iter 27150 || Loss: 1.8789 || timer: 0.0853 sec.
iter 27160 || Loss: 1.4610 || timer: 0.1366 sec.
iter 27170 || Loss: 1.3235 || timer: 0.1500 sec.
iter 27180 || Loss: 1.5657 || timer: 0.1694 sec.
iter 27190 || Loss: 1.1632 || timer: 0.0891 sec.
iter 27200 || Loss: 1.5959 || timer: 0.1729 sec.
iter 27210 || Loss: 1.5461 || timer: 0.1444 sec.
iter 27220 || Loss: 0.9885 || timer: 0.1380 sec.
iter 27230 || Loss: 1.4637 || timer: 0.1737 sec.
iter 27240 || Loss: 1.3748 || timer: 0.1738 sec.
iter 27250 || Loss: 1.4185 || timer: 0.0861 sec.
iter 27260 || Loss: 1.1697 || timer: 0.1518 sec.
iter 27270 || Loss: 1.4898 || timer: 0.0845 sec.
iter 27280 || Loss: 1.3790 || timer: 0.0572 sec.
iter 27290 || Loss: 0.4538 || timer: 0.1769 sec.
iter 27300 || Loss: 1.5567 || timer: 0.1546 sec.
iter 27310 || Loss: 1.2460 || timer: 0.1467 sec.
iter 27320 || Loss: 1.5105 || timer: 0.1444 sec.
iter 27330 || Loss: 1.4801 || timer: 0.1421 sec.
iter 27340 || Loss: 1.1399 || timer: 0.1448 sec.
iter 27350 || Loss: 1.7290 || timer: 0.1471 sec.
iter 27360 || Loss: 1.1429 || timer: 0.1730 sec.
iter 27370 || Loss: 1.2962 || timer: 0.1619 sec.
iter 27380 || Loss: 1.6161 || timer: 0.1813 sec.
iter 27390 || Loss: 1.0796 || timer: 0.1634 sec.
iter 27400 || Loss: 1.5444 || timer: 0.1489 sec.
iter 27410 || Loss: 1.1341 || timer: 0.1714 sec.
iter 27420 || Loss: 1.4531 || timer: 0.1729 sec.
iter 27430 || Loss: 1.7317 || timer: 0.1520 sec.
iter 27440 || Loss: 1.3744 || timer: 0.1356 sec.
iter 27450 || Loss: 1.7707 || timer: 0.1434 sec.
iter 27460 || Loss: 1.9535 || timer: 0.0979 sec.
iter 27470 || Loss: 1.5639 || timer: 0.1741 sec.
iter 27480 || Loss: 1.5620 || timer: 0.1472 sec.
iter 27490 || Loss: 0.9860 || timer: 0.1314 sec.
iter 27500 || Loss: 1.3004 || timer: 0.1393 sec.
iter 27510 || Loss: 1.4069 || timer: 0.0898 sec.
iter 27520 || Loss: 1.4705 || timer: 0.1646 sec.
iter 27530 || Loss: 1.2552 || timer: 0.1399 sec.
iter 27540 || Loss: 1.5580 || timer: 0.1644 sec.
iter 27550 || Loss: 1.2468 || timer: 0.1635 sec.
iter 27560 || Loss: 1.6568 || timer: 0.1418 sec.
iter 27570 || Loss: 1.6843 || timer: 0.1739 sec.
iter 27580 || Loss: 1.3380 || timer: 0.0917 sec.
iter 27590 || Loss: 1.5200 || timer: 0.1424 sec.
iter 27600 || Loss: 1.5465 || timer: 0.1791 sec.
iter 27610 || Loss: 1.6261 || timer: 0.0231 sec.
iter 27620 || Loss: 2.9297 || timer: 0.2055 sec.
iter 27630 || Loss: 1.6948 || timer: 0.1404 sec.
iter 27640 || Loss: 1.4493 || timer: 0.1195 sec.
iter 27650 || Loss: 1.4795 || timer: 0.1343 sec.
iter 27660 || Loss: 1.5498 || timer: 0.1736 sec.
iter 27670 || Loss: 1.4767 || timer: 0.1615 sec.
iter 27680 || Loss: 1.4963 || timer: 0.0868 sec.
iter 27690 || Loss: 1.4032 || timer: 0.1358 sec.
iter 27700 || Loss: 1.3991 || timer: 0.1560 sec.
iter 27710 || Loss: 1.9607 || timer: 0.1720 sec.
iter 27720 || Loss: 1.5015 || timer: 0.1452 sec.
iter 27730 || Loss: 1.4551 || timer: 0.1694 sec.
iter 27740 || Loss: 1.4040 || timer: 0.1545 sec.
iter 27750 || Loss: 1.6006 || timer: 0.1612 sec.
iter 27760 || Loss: 1.2640 || timer: 0.1543 sec.
iter 27770 || Loss: 1.7696 || timer: 0.1403 sec.
iter 27780 || Loss: 1.2287 || timer: 0.1684 sec.
iter 27790 || Loss: 1.2723 || timer: 0.1309 sec.
iter 27800 || Loss: 1.0502 || timer: 0.1430 sec.
iter 27810 || Loss: 1.0080 || timer: 0.1418 sec.
iter 27820 || Loss: 1.4478 || timer: 0.1722 sec.
iter 27830 || Loss: 1.5989 || timer: 0.1480 sec.
iter 27840 || Loss: 1.4210 || timer: 0.1550 sec.
iter 27850 || Loss: 1.4723 || timer: 0.1419 sec.
iter 27860 || Loss: 1.5205 || timer: 0.1354 sec.
iter 27870 || Loss: 1.0442 || timer: 0.1627 sec.
iter 27880 || Loss: 1.3426 || timer: 0.1532 sec.
iter 27890 || Loss: 2.2202 || timer: 0.1790 sec.
iter 27900 || Loss: 1.5776 || timer: 0.1649 sec.
iter 27910 || Loss: 1.8122 || timer: 0.1460 sec.
iter 27920 || Loss: 1.2919 || timer: 0.1844 sec.
iter 27930 || Loss: 1.2166 || timer: 0.1658 sec.
iter 27940 || Loss: 1.4026 || timer: 0.0318 sec.
iter 27950 || Loss: 2.2012 || timer: 0.1389 sec.
iter 27960 || Loss: 1.2731 || timer: 0.1488 sec.
iter 27970 || Loss: 1.2440 || timer: 0.1423 sec.
iter 27980 || Loss: 1.5102 || timer: 0.1563 sec.
iter 27990 || Loss: 1.1618 || timer: 0.1685 sec.
iter 28000 || Loss: 1.4634 || timer: 0.1484 sec.
iter 28010 || Loss: 1.8369 || timer: 0.1180 sec.
iter 28020 || Loss: 1.1847 || timer: 0.0834 sec.
iter 28030 || Loss: 1.3384 || timer: 0.1660 sec.
iter 28040 || Loss: 1.2427 || timer: 0.1430 sec.
iter 28050 || Loss: 1.4095 || timer: 0.1709 sec.
iter 28060 || Loss: 1.5272 || timer: 0.1741 sec.
iter 28070 || Loss: 1.9357 || timer: 0.1052 sec.
iter 28080 || Loss: 1.4273 || timer: 0.1500 sec.
iter 28090 || Loss: 1.8567 || timer: 0.1414 sec.
iter 28100 || Loss: 1.8985 || timer: 0.1756 sec.
iter 28110 || Loss: 1.3799 || timer: 0.0907 sec.
iter 28120 || Loss: 1.5484 || timer: 0.1803 sec.
iter 28130 || Loss: 1.2822 || timer: 0.1649 sec.
iter 28140 || Loss: 1.2891 || timer: 0.1302 sec.
iter 28150 || Loss: 1.8769 || timer: 0.1308 sec.
iter 28160 || Loss: 1.3158 || timer: 0.1761 sec.
iter 28170 || Loss: 1.5925 || timer: 0.1399 sec.
iter 28180 || Loss: 1.2631 || timer: 0.1451 sec.
iter 28190 || Loss: 1.5278 || timer: 0.1625 sec.
iter 28200 || Loss: 1.1744 || timer: 0.1733 sec.
iter 28210 || Loss: 1.6553 || timer: 0.1503 sec.
iter 28220 || Loss: 1.5846 || timer: 0.1278 sec.
iter 28230 || Loss: 1.2239 || timer: 0.1115 sec.
iter 28240 || Loss: 1.0716 || timer: 0.1378 sec.
iter 28250 || Loss: 1.0604 || timer: 0.1486 sec.
iter 28260 || Loss: 1.4931 || timer: 0.1730 sec.
iter 28270 || Loss: 1.5080 || timer: 0.0235 sec.
iter 28280 || Loss: 1.9814 || timer: 0.1486 sec.
iter 28290 || Loss: 1.3859 || timer: 0.1491 sec.
iter 28300 || Loss: 1.6070 || timer: 0.1257 sec.
iter 28310 || Loss: 1.5586 || timer: 0.1329 sec.
iter 28320 || Loss: 1.3286 || timer: 0.1456 sec.
iter 28330 || Loss: 1.5468 || timer: 0.1804 sec.
iter 28340 || Loss: 1.0987 || timer: 0.1426 sec.
iter 28350 || Loss: 1.0843 || timer: 0.1539 sec.
iter 28360 || Loss: 1.4711 || timer: 0.1485 sec.
iter 28370 || Loss: 1.8310 || timer: 0.1387 sec.
iter 28380 || Loss: 1.4838 || timer: 0.1695 sec.
iter 28390 || Loss: 1.2092 || timer: 0.1436 sec.
iter 28400 || Loss: 1.5606 || timer: 0.1192 sec.
iter 28410 || Loss: 1.4245 || timer: 0.1657 sec.
iter 28420 || Loss: 1.5128 || timer: 0.1018 sec.
iter 28430 || Loss: 1.3025 || timer: 0.1508 sec.
iter 28440 || Loss: 1.4500 || timer: 0.1688 sec.
iter 28450 || Loss: 1.1846 || timer: 0.1538 sec.
iter 28460 || Loss: 1.7848 || timer: 0.1582 sec.
iter 28470 || Loss: 1.2960 || timer: 0.1109 sec.
iter 28480 || Loss: 1.3994 || timer: 0.1696 sec.
iter 28490 || Loss: 1.8049 || timer: 0.1547 sec.
iter 28500 || Loss: 1.5331 || timer: 0.1181 sec.
iter 28510 || Loss: 1.1852 || timer: 0.1709 sec.
iter 28520 || Loss: 1.3987 || timer: 0.1530 sec.
iter 28530 || Loss: 1.3212 || timer: 0.1467 sec.
iter 28540 || Loss: 1.0827 || timer: 0.0844 sec.
iter 28550 || Loss: 1.2912 || timer: 0.1735 sec.
iter 28560 || Loss: 1.7218 || timer: 0.1723 sec.
iter 28570 || Loss: 1.4343 || timer: 0.1808 sec.
iter 28580 || Loss: 1.4940 || timer: 0.1469 sec.
iter 28590 || Loss: 1.0432 || timer: 0.1485 sec.
iter 28600 || Loss: 1.1124 || timer: 0.0392 sec.
iter 28610 || Loss: 2.1253 || timer: 0.1486 sec.
iter 28620 || Loss: 1.2695 || timer: 0.1624 sec.
iter 28630 || Loss: 1.4053 || timer: 0.1260 sec.
iter 28640 || Loss: 1.2032 || timer: 0.1811 sec.
iter 28650 || Loss: 1.9349 || timer: 0.1427 sec.
iter 28660 || Loss: 1.4976 || timer: 0.1505 sec.
iter 28670 || Loss: 1.2977 || timer: 0.1833 sec.
iter 28680 || Loss: 1.4969 || timer: 0.1527 sec.
iter 28690 || Loss: 1.1598 || timer: 0.1925 sec.
iter 28700 || Loss: 1.6943 || timer: 0.1676 sec.
iter 28710 || Loss: 1.1579 || timer: 0.1650 sec.
iter 28720 || Loss: 1.4678 || timer: 0.1607 sec.
iter 28730 || Loss: 1.9694 || timer: 0.1616 sec.
iter 28740 || Loss: 1.6694 || timer: 0.1744 sec.
iter 28750 || Loss: 1.2768 || timer: 0.1785 sec.
iter 28760 || Loss: 0.7678 || timer: 0.1472 sec.
iter 28770 || Loss: 1.8055 || timer: 0.1693 sec.
iter 28780 || Loss: 1.2560 || timer: 0.1097 sec.
iter 28790 || Loss: 1.2749 || timer: 0.1026 sec.
iter 28800 || Loss: 1.2645 || timer: 0.0764 sec.
iter 28810 || Loss: 1.2608 || timer: 0.0844 sec.
iter 28820 || Loss: 1.7265 || timer: 0.0845 sec.
iter 28830 || Loss: 1.6395 || timer: 0.0851 sec.
iter 28840 || Loss: 1.1283 || timer: 0.0936 sec.
iter 28850 || Loss: 1.3726 || timer: 0.0915 sec.
iter 28860 || Loss: 1.8785 || timer: 0.0823 sec.
iter 28870 || Loss: 1.1260 || timer: 0.0940 sec.
iter 28880 || Loss: 1.6145 || timer: 0.0922 sec.
iter 28890 || Loss: 1.4057 || timer: 0.0910 sec.
iter 28900 || Loss: 1.4280 || timer: 0.0929 sec.
iter 28910 || Loss: 1.7977 || timer: 0.0891 sec.
iter 28920 || Loss: 1.1381 || timer: 0.0906 sec.
iter 28930 || Loss: 1.2168 || timer: 0.0204 sec.
iter 28940 || Loss: 0.3009 || timer: 0.0933 sec.
iter 28950 || Loss: 1.1596 || timer: 0.0768 sec.
iter 28960 || Loss: 1.5254 || timer: 0.0915 sec.
iter 28970 || Loss: 1.5238 || timer: 0.0906 sec.
iter 28980 || Loss: 1.5773 || timer: 0.0904 sec.
iter 28990 || Loss: 1.7818 || timer: 0.0923 sec.
iter 29000 || Loss: 1.3595 || timer: 0.0853 sec.
iter 29010 || Loss: 1.0369 || timer: 0.0840 sec.
iter 29020 || Loss: 1.5546 || timer: 0.1099 sec.
iter 29030 || Loss: 1.6624 || timer: 0.1225 sec.
iter 29040 || Loss: 1.2988 || timer: 0.0788 sec.
iter 29050 || Loss: 1.7128 || timer: 0.0993 sec.
iter 29060 || Loss: 1.6062 || timer: 0.0894 sec.
iter 29070 || Loss: 1.2084 || timer: 0.0912 sec.
iter 29080 || Loss: 1.7431 || timer: 0.0970 sec.
iter 29090 || Loss: 1.4614 || timer: 0.1071 sec.
iter 29100 || Loss: 1.2656 || timer: 0.0962 sec.
iter 29110 || Loss: 1.0903 || timer: 0.0854 sec.
iter 29120 || Loss: 1.1547 || timer: 0.0835 sec.
iter 29130 || Loss: 1.4323 || timer: 0.0769 sec.
iter 29140 || Loss: 1.1509 || timer: 0.0919 sec.
iter 29150 || Loss: 1.3874 || timer: 0.0854 sec.
iter 29160 || Loss: 1.5117 || timer: 0.0922 sec.
iter 29170 || Loss: 0.9791 || timer: 0.0911 sec.
iter 29180 || Loss: 1.9159 || timer: 0.0907 sec.
iter 29190 || Loss: 1.3826 || timer: 0.0921 sec.
iter 29200 || Loss: 1.3375 || timer: 0.0935 sec.
iter 29210 || Loss: 1.2502 || timer: 0.0913 sec.
iter 29220 || Loss: 1.8108 || timer: 0.0875 sec.
iter 29230 || Loss: 1.1841 || timer: 0.0835 sec.
iter 29240 || Loss: 1.1714 || timer: 0.0910 sec.
iter 29250 || Loss: 1.4662 || timer: 0.0837 sec.
iter 29260 || Loss: 1.4224 || timer: 0.0216 sec.
iter 29270 || Loss: 0.2968 || timer: 0.0843 sec.
iter 29280 || Loss: 1.1522 || timer: 0.0843 sec.
iter 29290 || Loss: 1.3919 || timer: 0.0931 sec.
iter 29300 || Loss: 1.7081 || timer: 0.0892 sec.
iter 29310 || Loss: 1.5463 || timer: 0.0901 sec.
iter 29320 || Loss: 1.4358 || timer: 0.0892 sec.
iter 29330 || Loss: 1.4686 || timer: 0.0891 sec.
iter 29340 || Loss: 2.1361 || timer: 0.0893 sec.
iter 29350 || Loss: 1.9232 || timer: 0.0904 sec.
iter 29360 || Loss: 1.3710 || timer: 0.0897 sec.
iter 29370 || Loss: 1.1866 || timer: 0.0809 sec.
iter 29380 || Loss: 2.1625 || timer: 0.0903 sec.
iter 29390 || Loss: 1.6300 || timer: 0.0830 sec.
iter 29400 || Loss: 1.2741 || timer: 0.1049 sec.
iter 29410 || Loss: 1.5684 || timer: 0.0912 sec.
iter 29420 || Loss: 1.7736 || timer: 0.0878 sec.
iter 29430 || Loss: 0.9021 || timer: 0.0871 sec.
iter 29440 || Loss: 1.4552 || timer: 0.0899 sec.
iter 29450 || Loss: 1.2195 || timer: 0.0835 sec.
iter 29460 || Loss: 1.5285 || timer: 0.1012 sec.
iter 29470 || Loss: 1.5275 || timer: 0.0865 sec.
iter 29480 || Loss: 1.3768 || timer: 0.0909 sec.
iter 29490 || Loss: 1.5577 || timer: 0.0914 sec.
iter 29500 || Loss: 1.4713 || timer: 0.1024 sec.
iter 29510 || Loss: 1.4817 || timer: 0.0842 sec.
iter 29520 || Loss: 1.6856 || timer: 0.0905 sec.
iter 29530 || Loss: 1.3203 || timer: 0.0913 sec.
iter 29540 || Loss: 1.4680 || timer: 0.0922 sec.
iter 29550 || Loss: 1.2684 || timer: 0.0826 sec.
iter 29560 || Loss: 1.8995 || timer: 0.0929 sec.
iter 29570 || Loss: 1.9535 || timer: 0.0911 sec.
iter 29580 || Loss: 1.5898 || timer: 0.0838 sec.
iter 29590 || Loss: 1.2724 || timer: 0.0150 sec.
iter 29600 || Loss: 0.5947 || timer: 0.0925 sec.
iter 29610 || Loss: 1.3586 || timer: 0.1050 sec.
iter 29620 || Loss: 1.2642 || timer: 0.0874 sec.
iter 29630 || Loss: 1.4965 || timer: 0.0908 sec.
iter 29640 || Loss: 1.3424 || timer: 0.1084 sec.
iter 29650 || Loss: 1.7606 || timer: 0.1013 sec.
iter 29660 || Loss: 1.1990 || timer: 0.0913 sec.
iter 29670 || Loss: 1.5006 || timer: 0.1045 sec.
iter 29680 || Loss: 1.0762 || timer: 0.0832 sec.
iter 29690 || Loss: 1.2005 || timer: 0.0923 sec.
iter 29700 || Loss: 1.0411 || timer: 0.0874 sec.
iter 29710 || Loss: 1.1916 || timer: 0.0832 sec.
iter 29720 || Loss: 1.6382 || timer: 0.0911 sec.
iter 29730 || Loss: 1.0910 || timer: 0.0816 sec.
iter 29740 || Loss: 1.3778 || timer: 0.0757 sec.
iter 29750 || Loss: 1.2419 || timer: 0.0933 sec.
iter 29760 || Loss: 1.1956 || timer: 0.0925 sec.
iter 29770 || Loss: 1.2760 || timer: 0.0864 sec.
iter 29780 || Loss: 1.4057 || timer: 0.0908 sec.
iter 29790 || Loss: 1.5753 || timer: 0.0902 sec.
iter 29800 || Loss: 1.1286 || timer: 0.0892 sec.
iter 29810 || Loss: 1.5650 || timer: 0.0790 sec.
iter 29820 || Loss: 1.3427 || timer: 0.0840 sec.
iter 29830 || Loss: 1.5372 || timer: 0.0905 sec.
iter 29840 || Loss: 1.6087 || timer: 0.0906 sec.
iter 29850 || Loss: 2.1520 || timer: 0.0906 sec.
iter 29860 || Loss: 1.1666 || timer: 0.0997 sec.
iter 29870 || Loss: 1.8684 || timer: 0.0924 sec.
iter 29880 || Loss: 1.6311 || timer: 0.0904 sec.
iter 29890 || Loss: 1.1413 || timer: 0.0893 sec.
iter 29900 || Loss: 1.3327 || timer: 0.0837 sec.
iter 29910 || Loss: 1.6931 || timer: 0.0842 sec.
iter 29920 || Loss: 1.2054 || timer: 0.0215 sec.
iter 29930 || Loss: 0.8297 || timer: 0.0915 sec.
iter 29940 || Loss: 1.3388 || timer: 0.0839 sec.
iter 29950 || Loss: 1.3946 || timer: 0.0842 sec.
iter 29960 || Loss: 1.4431 || timer: 0.0900 sec.
iter 29970 || Loss: 1.1439 || timer: 0.1099 sec.
iter 29980 || Loss: 1.3667 || timer: 0.0842 sec.
iter 29990 || Loss: 1.4820 || timer: 0.0923 sec.
iter 30000 || Loss: 1.7566 || Saving state, iter: 30000
timer: 0.0916 sec.
iter 30010 || Loss: 1.4798 || timer: 0.0898 sec.
iter 30020 || Loss: 1.2290 || timer: 0.1111 sec.
iter 30030 || Loss: 1.3481 || timer: 0.1073 sec.
iter 30040 || Loss: 1.6227 || timer: 0.0930 sec.
iter 30050 || Loss: 1.0133 || timer: 0.0873 sec.
iter 30060 || Loss: 1.1586 || timer: 0.0974 sec.
iter 30070 || Loss: 1.8357 || timer: 0.0888 sec.
iter 30080 || Loss: 1.9180 || timer: 0.0882 sec.
iter 30090 || Loss: 1.9114 || timer: 0.1019 sec.
iter 30100 || Loss: 1.8395 || timer: 0.0829 sec.
iter 30110 || Loss: 1.5850 || timer: 0.0820 sec.
iter 30120 || Loss: 1.9548 || timer: 0.0839 sec.
iter 30130 || Loss: 1.7351 || timer: 0.1021 sec.
iter 30140 || Loss: 1.3195 || timer: 0.0842 sec.
iter 30150 || Loss: 1.1599 || timer: 0.0904 sec.
iter 30160 || Loss: 0.9659 || timer: 0.0851 sec.
iter 30170 || Loss: 1.6198 || timer: 0.0820 sec.
iter 30180 || Loss: 1.7575 || timer: 0.0930 sec.
iter 30190 || Loss: 1.7856 || timer: 0.0839 sec.
iter 30200 || Loss: 1.8914 || timer: 0.0916 sec.
iter 30210 || Loss: 1.4060 || timer: 0.0902 sec.
iter 30220 || Loss: 1.0730 || timer: 0.1055 sec.
iter 30230 || Loss: 1.1847 || timer: 0.0835 sec.
iter 30240 || Loss: 1.8938 || timer: 0.0923 sec.
iter 30250 || Loss: 1.3943 || timer: 0.0264 sec.
iter 30260 || Loss: 3.3661 || timer: 0.0842 sec.
iter 30270 || Loss: 1.4991 || timer: 0.0770 sec.
iter 30280 || Loss: 1.7227 || timer: 0.0820 sec.
iter 30290 || Loss: 1.3944 || timer: 0.0810 sec.
iter 30300 || Loss: 1.7640 || timer: 0.0769 sec.
iter 30310 || Loss: 1.4001 || timer: 0.1074 sec.
iter 30320 || Loss: 1.5450 || timer: 0.0842 sec.
iter 30330 || Loss: 1.5972 || timer: 0.1039 sec.
iter 30340 || Loss: 1.5947 || timer: 0.0836 sec.
iter 30350 || Loss: 1.2715 || timer: 0.0983 sec.
iter 30360 || Loss: 1.3100 || timer: 0.0921 sec.
iter 30370 || Loss: 1.1459 || timer: 0.0762 sec.
iter 30380 || Loss: 1.3952 || timer: 0.0842 sec.
iter 30390 || Loss: 1.2165 || timer: 0.1184 sec.
iter 30400 || Loss: 1.4798 || timer: 0.0894 sec.
iter 30410 || Loss: 1.4123 || timer: 0.0904 sec.
iter 30420 || Loss: 1.1451 || timer: 0.1059 sec.
iter 30430 || Loss: 1.1591 || timer: 0.1089 sec.
iter 30440 || Loss: 1.4431 || timer: 0.0906 sec.
iter 30450 || Loss: 1.4509 || timer: 0.0848 sec.
iter 30460 || Loss: 1.3366 || timer: 0.0977 sec.
iter 30470 || Loss: 1.6276 || timer: 0.0918 sec.
iter 30480 || Loss: 1.4878 || timer: 0.0843 sec.
iter 30490 || Loss: 1.2835 || timer: 0.0895 sec.
iter 30500 || Loss: 1.5937 || timer: 0.0929 sec.
iter 30510 || Loss: 2.0556 || timer: 0.1017 sec.
iter 30520 || Loss: 1.2708 || timer: 0.0915 sec.
iter 30530 || Loss: 1.6874 || timer: 0.0908 sec.
iter 30540 || Loss: 1.6700 || timer: 0.0945 sec.
iter 30550 || Loss: 1.0512 || timer: 0.0906 sec.
iter 30560 || Loss: 1.1503 || timer: 0.0891 sec.
iter 30570 || Loss: 1.2382 || timer: 0.0836 sec.
iter 30580 || Loss: 1.5096 || timer: 0.0275 sec.
iter 30590 || Loss: 1.5553 || timer: 0.0839 sec.
iter 30600 || Loss: 1.5209 || timer: 0.0923 sec.
iter 30610 || Loss: 1.4945 || timer: 0.0851 sec.
iter 30620 || Loss: 1.2413 || timer: 0.0916 sec.
iter 30630 || Loss: 1.3034 || timer: 0.0895 sec.
iter 30640 || Loss: 1.9283 || timer: 0.1033 sec.
iter 30650 || Loss: 1.3543 || timer: 0.0894 sec.
iter 30660 || Loss: 1.7075 || timer: 0.0952 sec.
iter 30670 || Loss: 1.4342 || timer: 0.0871 sec.
iter 30680 || Loss: 1.4521 || timer: 0.0987 sec.
iter 30690 || Loss: 1.3879 || timer: 0.0954 sec.
iter 30700 || Loss: 1.7820 || timer: 0.1035 sec.
iter 30710 || Loss: 1.5938 || timer: 0.0917 sec.
iter 30720 || Loss: 1.3892 || timer: 0.0938 sec.
iter 30730 || Loss: 1.6111 || timer: 0.0906 sec.
iter 30740 || Loss: 1.5676 || timer: 0.1062 sec.
iter 30750 || Loss: 1.2034 || timer: 0.0897 sec.
iter 30760 || Loss: 1.3123 || timer: 0.0929 sec.
iter 30770 || Loss: 1.1078 || timer: 0.0845 sec.
iter 30780 || Loss: 1.1500 || timer: 0.0847 sec.
iter 30790 || Loss: 1.1516 || timer: 0.0827 sec.
iter 30800 || Loss: 1.6857 || timer: 0.0892 sec.
iter 30810 || Loss: 1.5500 || timer: 0.0921 sec.
iter 30820 || Loss: 1.0710 || timer: 0.0923 sec.
iter 30830 || Loss: 2.2890 || timer: 0.0773 sec.
iter 30840 || Loss: 1.6120 || timer: 0.0802 sec.
iter 30850 || Loss: 1.2661 || timer: 0.0846 sec.
iter 30860 || Loss: 1.2584 || timer: 0.0821 sec.
iter 30870 || Loss: 1.1286 || timer: 0.0891 sec.
iter 30880 || Loss: 1.0754 || timer: 0.0839 sec.
iter 30890 || Loss: 1.4870 || timer: 0.0904 sec.
iter 30900 || Loss: 1.6072 || timer: 0.0940 sec.
iter 30910 || Loss: 1.1922 || timer: 0.0267 sec.
iter 30920 || Loss: 1.3171 || timer: 0.0916 sec.
iter 30930 || Loss: 1.6396 || timer: 0.0902 sec.
iter 30940 || Loss: 1.2052 || timer: 0.0921 sec.
iter 30950 || Loss: 1.1588 || timer: 0.0924 sec.
iter 30960 || Loss: 1.4355 || timer: 0.0863 sec.
iter 30970 || Loss: 1.2288 || timer: 0.0841 sec.
iter 30980 || Loss: 1.4417 || timer: 0.0845 sec.
iter 30990 || Loss: 1.5648 || timer: 0.0888 sec.
iter 31000 || Loss: 1.5274 || timer: 0.0933 sec.
iter 31010 || Loss: 1.2592 || timer: 0.0976 sec.
iter 31020 || Loss: 1.1676 || timer: 0.0846 sec.
iter 31030 || Loss: 1.1659 || timer: 0.0922 sec.
iter 31040 || Loss: 0.7711 || timer: 0.0983 sec.
iter 31050 || Loss: 1.7499 || timer: 0.0935 sec.
iter 31060 || Loss: 1.3988 || timer: 0.0826 sec.
iter 31070 || Loss: 1.2852 || timer: 0.0841 sec.
iter 31080 || Loss: 1.1779 || timer: 0.1076 sec.
iter 31090 || Loss: 1.1723 || timer: 0.0838 sec.
iter 31100 || Loss: 1.3444 || timer: 0.0916 sec.
iter 31110 || Loss: 1.6561 || timer: 0.0929 sec.
iter 31120 || Loss: 1.0481 || timer: 0.1016 sec.
iter 31130 || Loss: 1.2868 || timer: 0.0960 sec.
iter 31140 || Loss: 1.1628 || timer: 0.0906 sec.
iter 31150 || Loss: 1.0700 || timer: 0.0907 sec.
iter 31160 || Loss: 1.5824 || timer: 0.0917 sec.
iter 31170 || Loss: 1.8710 || timer: 0.0930 sec.
iter 31180 || Loss: 1.3572 || timer: 0.0907 sec.
iter 31190 || Loss: 1.6469 || timer: 0.0950 sec.
iter 31200 || Loss: 1.5765 || timer: 0.0754 sec.
iter 31210 || Loss: 1.7893 || timer: 0.0881 sec.
iter 31220 || Loss: 1.2702 || timer: 0.0850 sec.
iter 31230 || Loss: 1.4683 || timer: 0.1076 sec.
iter 31240 || Loss: 1.3355 || timer: 0.0152 sec.
iter 31250 || Loss: 1.1808 || timer: 0.1095 sec.
iter 31260 || Loss: 1.3988 || timer: 0.1077 sec.
iter 31270 || Loss: 1.3347 || timer: 0.1046 sec.
iter 31280 || Loss: 1.0014 || timer: 0.0840 sec.
iter 31290 || Loss: 1.6057 || timer: 0.0916 sec.
iter 31300 || Loss: 1.3340 || timer: 0.0835 sec.
iter 31310 || Loss: 1.5723 || timer: 0.0858 sec.
iter 31320 || Loss: 1.7297 || timer: 0.0837 sec.
iter 31330 || Loss: 1.4256 || timer: 0.1061 sec.
iter 31340 || Loss: 1.3884 || timer: 0.1133 sec.
iter 31350 || Loss: 1.6079 || timer: 0.0899 sec.
iter 31360 || Loss: 1.5738 || timer: 0.0914 sec.
iter 31370 || Loss: 1.8218 || timer: 0.0920 sec.
iter 31380 || Loss: 1.3853 || timer: 0.1057 sec.
iter 31390 || Loss: 1.3769 || timer: 0.0917 sec.
iter 31400 || Loss: 1.6260 || timer: 0.0850 sec.
iter 31410 || Loss: 1.5989 || timer: 0.0909 sec.
iter 31420 || Loss: 1.7924 || timer: 0.0927 sec.
iter 31430 || Loss: 1.2415 || timer: 0.0903 sec.
iter 31440 || Loss: 1.2279 || timer: 0.0857 sec.
iter 31450 || Loss: 1.1905 || timer: 0.0844 sec.
iter 31460 || Loss: 1.0745 || timer: 0.0915 sec.
iter 31470 || Loss: 1.3784 || timer: 0.0919 sec.
iter 31480 || Loss: 1.1892 || timer: 0.0829 sec.
iter 31490 || Loss: 1.9174 || timer: 0.1002 sec.
iter 31500 || Loss: 1.6190 || timer: 0.0916 sec.
iter 31510 || Loss: 1.5537 || timer: 0.0829 sec.
iter 31520 || Loss: 1.4423 || timer: 0.0988 sec.
iter 31530 || Loss: 1.4733 || timer: 0.0844 sec.
iter 31540 || Loss: 1.5202 || timer: 0.0915 sec.
iter 31550 || Loss: 1.0386 || timer: 0.0989 sec.
iter 31560 || Loss: 1.8598 || timer: 0.1077 sec.
iter 31570 || Loss: 1.7853 || timer: 0.0163 sec.
iter 31580 || Loss: 3.1756 || timer: 0.0837 sec.
iter 31590 || Loss: 1.5267 || timer: 0.0842 sec.
iter 31600 || Loss: 1.5813 || timer: 0.1071 sec.
iter 31610 || Loss: 1.1767 || timer: 0.0989 sec.
iter 31620 || Loss: 2.1161 || timer: 0.0834 sec.
iter 31630 || Loss: 1.2383 || timer: 0.0922 sec.
iter 31640 || Loss: 1.4805 || timer: 0.1108 sec.
iter 31650 || Loss: 1.7048 || timer: 0.0828 sec.
iter 31660 || Loss: 1.7383 || timer: 0.0841 sec.
iter 31670 || Loss: 1.8477 || timer: 0.1008 sec.
iter 31680 || Loss: 1.1891 || timer: 0.0895 sec.
iter 31690 || Loss: 1.3423 || timer: 0.0944 sec.
iter 31700 || Loss: 1.6481 || timer: 0.0894 sec.
iter 31710 || Loss: 1.9995 || timer: 0.0779 sec.
iter 31720 || Loss: 1.6548 || timer: 0.0882 sec.
iter 31730 || Loss: 2.2935 || timer: 0.0903 sec.
iter 31740 || Loss: 1.0895 || timer: 0.0854 sec.
iter 31750 || Loss: 1.3372 || timer: 0.0838 sec.
iter 31760 || Loss: 1.2778 || timer: 0.0911 sec.
iter 31770 || Loss: 1.1397 || timer: 0.0834 sec.
iter 31780 || Loss: 1.5671 || timer: 0.1025 sec.
iter 31790 || Loss: 1.6940 || timer: 0.0938 sec.
iter 31800 || Loss: 1.4249 || timer: 0.0858 sec.
iter 31810 || Loss: 1.4682 || timer: 0.1067 sec.
iter 31820 || Loss: 1.2025 || timer: 0.0924 sec.
iter 31830 || Loss: 1.9156 || timer: 0.0831 sec.
iter 31840 || Loss: 1.3542 || timer: 0.0911 sec.
iter 31850 || Loss: 1.9331 || timer: 0.0837 sec.
iter 31860 || Loss: 1.6660 || timer: 0.0918 sec.
iter 31870 || Loss: 1.4237 || timer: 0.0906 sec.
iter 31880 || Loss: 1.4874 || timer: 0.0903 sec.
iter 31890 || Loss: 1.3264 || timer: 0.0839 sec.
iter 31900 || Loss: 1.5185 || timer: 0.0277 sec.
iter 31910 || Loss: 1.2168 || timer: 0.0777 sec.
iter 31920 || Loss: 1.2511 || timer: 0.1046 sec.
iter 31930 || Loss: 0.9733 || timer: 0.0772 sec.
iter 31940 || Loss: 1.2736 || timer: 0.0851 sec.
iter 31950 || Loss: 1.4545 || timer: 0.0916 sec.
iter 31960 || Loss: 1.8090 || timer: 0.0924 sec.
iter 31970 || Loss: 1.3643 || timer: 0.0846 sec.
iter 31980 || Loss: 1.4684 || timer: 0.1001 sec.
iter 31990 || Loss: 1.3418 || timer: 0.0762 sec.
iter 32000 || Loss: 1.3889 || timer: 0.0970 sec.
iter 32010 || Loss: 1.2626 || timer: 0.0898 sec.
iter 32020 || Loss: 1.3591 || timer: 0.0931 sec.
iter 32030 || Loss: 1.4014 || timer: 0.1120 sec.
iter 32040 || Loss: 1.0321 || timer: 0.0907 sec.
iter 32050 || Loss: 1.3814 || timer: 0.0911 sec.
iter 32060 || Loss: 1.2949 || timer: 0.0835 sec.
iter 32070 || Loss: 1.4645 || timer: 0.0845 sec.
iter 32080 || Loss: 1.1865 || timer: 0.0984 sec.
iter 32090 || Loss: 1.3601 || timer: 0.0902 sec.
iter 32100 || Loss: 1.1861 || timer: 0.0767 sec.
iter 32110 || Loss: 1.6620 || timer: 0.0906 sec.
iter 32120 || Loss: 1.4734 || timer: 0.0963 sec.
iter 32130 || Loss: 1.2847 || timer: 0.0928 sec.
iter 32140 || Loss: 1.2192 || timer: 0.0821 sec.
iter 32150 || Loss: 1.6595 || timer: 0.1172 sec.
iter 32160 || Loss: 1.0404 || timer: 0.0913 sec.
iter 32170 || Loss: 1.2783 || timer: 0.0895 sec.
iter 32180 || Loss: 0.9027 || timer: 0.0836 sec.
iter 32190 || Loss: 1.2608 || timer: 0.0979 sec.
iter 32200 || Loss: 1.3394 || timer: 0.0808 sec.
iter 32210 || Loss: 1.4632 || timer: 0.0879 sec.
iter 32220 || Loss: 1.6113 || timer: 0.0890 sec.
iter 32230 || Loss: 1.5296 || timer: 0.0209 sec.
iter 32240 || Loss: 0.8385 || timer: 0.0832 sec.
iter 32250 || Loss: 1.4534 || timer: 0.0917 sec.
iter 32260 || Loss: 1.3987 || timer: 0.0946 sec.
iter 32270 || Loss: 0.9900 || timer: 0.0914 sec.
iter 32280 || Loss: 1.6309 || timer: 0.0924 sec.
iter 32290 || Loss: 1.1116 || timer: 0.1124 sec.
iter 32300 || Loss: 1.4224 || timer: 0.0930 sec.
iter 32310 || Loss: 1.3829 || timer: 0.1035 sec.
iter 32320 || Loss: 1.1426 || timer: 0.0878 sec.
iter 32330 || Loss: 1.3262 || timer: 0.1319 sec.
iter 32340 || Loss: 1.3212 || timer: 0.1082 sec.
iter 32350 || Loss: 1.8474 || timer: 0.1070 sec.
iter 32360 || Loss: 1.7610 || timer: 0.0923 sec.
iter 32370 || Loss: 1.1169 || timer: 0.0852 sec.
iter 32380 || Loss: 1.5798 || timer: 0.0766 sec.
iter 32390 || Loss: 1.8011 || timer: 0.0866 sec.
iter 32400 || Loss: 1.6249 || timer: 0.0824 sec.
iter 32410 || Loss: 1.5309 || timer: 0.0901 sec.
iter 32420 || Loss: 1.2194 || timer: 0.0917 sec.
iter 32430 || Loss: 1.4084 || timer: 0.0917 sec.
iter 32440 || Loss: 1.4468 || timer: 0.0864 sec.
iter 32450 || Loss: 1.1998 || timer: 0.0838 sec.
iter 32460 || Loss: 1.2872 || timer: 0.0796 sec.
iter 32470 || Loss: 1.4045 || timer: 0.0862 sec.
iter 32480 || Loss: 1.1496 || timer: 0.0831 sec.
iter 32490 || Loss: 1.4446 || timer: 0.0887 sec.
iter 32500 || Loss: 1.5568 || timer: 0.0905 sec.
iter 32510 || Loss: 1.2380 || timer: 0.0859 sec.
iter 32520 || Loss: 1.3724 || timer: 0.0902 sec.
iter 32530 || Loss: 1.5443 || timer: 0.1036 sec.
iter 32540 || Loss: 1.6151 || timer: 0.0882 sec.
iter 32550 || Loss: 1.5603 || timer: 0.0912 sec.
iter 32560 || Loss: 1.1919 || timer: 0.0254 sec.
iter 32570 || Loss: 1.0194 || timer: 0.0885 sec.
iter 32580 || Loss: 1.2617 || timer: 0.1022 sec.
iter 32590 || Loss: 1.1889 || timer: 0.0916 sec.
iter 32600 || Loss: 1.0078 || timer: 0.0838 sec.
iter 32610 || Loss: 1.0694 || timer: 0.0970 sec.
iter 32620 || Loss: 1.3185 || timer: 0.0787 sec.
iter 32630 || Loss: 1.1157 || timer: 0.0837 sec.
iter 32640 || Loss: 1.4267 || timer: 0.0925 sec.
iter 32650 || Loss: 1.3663 || timer: 0.1062 sec.
iter 32660 || Loss: 1.2918 || timer: 0.1051 sec.
iter 32670 || Loss: 1.7525 || timer: 0.0892 sec.
iter 32680 || Loss: 1.3191 || timer: 0.0980 sec.
iter 32690 || Loss: 1.3597 || timer: 0.0912 sec.
iter 32700 || Loss: 1.7077 || timer: 0.0939 sec.
iter 32710 || Loss: 1.4031 || timer: 0.0908 sec.
iter 32720 || Loss: 1.5075 || timer: 0.0927 sec.
iter 32730 || Loss: 1.2681 || timer: 0.0917 sec.
iter 32740 || Loss: 1.2976 || timer: 0.0886 sec.
iter 32750 || Loss: 1.4306 || timer: 0.0867 sec.
iter 32760 || Loss: 0.8700 || timer: 0.0880 sec.
iter 32770 || Loss: 1.2396 || timer: 0.0906 sec.
iter 32780 || Loss: 1.7888 || timer: 0.0947 sec.
iter 32790 || Loss: 1.5157 || timer: 0.0926 sec.
iter 32800 || Loss: 1.1982 || timer: 0.0901 sec.
iter 32810 || Loss: 1.4134 || timer: 0.0882 sec.
iter 32820 || Loss: 1.6455 || timer: 0.0946 sec.
iter 32830 || Loss: 1.4353 || timer: 0.0889 sec.
iter 32840 || Loss: 1.0753 || timer: 0.0891 sec.
iter 32850 || Loss: 1.2126 || timer: 0.0899 sec.
iter 32860 || Loss: 1.1818 || timer: 0.0908 sec.
iter 32870 || Loss: 1.4468 || timer: 0.0901 sec.
iter 32880 || Loss: 1.2701 || timer: 0.0831 sec.
iter 32890 || Loss: 1.0668 || timer: 0.0272 sec.
iter 32900 || Loss: 1.8155 || timer: 0.0836 sec.
iter 32910 || Loss: 1.1101 || timer: 0.0893 sec.
iter 32920 || Loss: 1.4970 || timer: 0.0909 sec.
iter 32930 || Loss: 1.2416 || timer: 0.0840 sec.
iter 32940 || Loss: 1.4515 || timer: 0.0881 sec.
iter 32950 || Loss: 1.0684 || timer: 0.0822 sec.
iter 32960 || Loss: 1.4240 || timer: 0.0829 sec.
iter 32970 || Loss: 1.1596 || timer: 0.1080 sec.
iter 32980 || Loss: 1.3155 || timer: 0.0895 sec.
iter 32990 || Loss: 1.1935 || timer: 0.1124 sec.
iter 33000 || Loss: 1.5777 || timer: 0.0958 sec.
iter 33010 || Loss: 2.3863 || timer: 0.0884 sec.
iter 33020 || Loss: 1.4450 || timer: 0.1073 sec.
iter 33030 || Loss: 1.4308 || timer: 0.1025 sec.
iter 33040 || Loss: 2.6691 || timer: 0.0852 sec.
iter 33050 || Loss: 2.2949 || timer: 0.0851 sec.
iter 33060 || Loss: 1.2703 || timer: 0.0913 sec.
iter 33070 || Loss: 1.5412 || timer: 0.1090 sec.
iter 33080 || Loss: 1.6945 || timer: 0.0844 sec.
iter 33090 || Loss: 1.7992 || timer: 0.0830 sec.
iter 33100 || Loss: 3.0841 || timer: 0.0891 sec.
iter 33110 || Loss: 1.3791 || timer: 0.0877 sec.
iter 33120 || Loss: 1.7242 || timer: 0.0899 sec.
iter 33130 || Loss: 1.9079 || timer: 0.0888 sec.
iter 33140 || Loss: 1.6903 || timer: 0.1342 sec.
iter 33150 || Loss: 1.6505 || timer: 0.0843 sec.
iter 33160 || Loss: 1.8540 || timer: 0.0825 sec.
iter 33170 || Loss: 1.6101 || timer: 0.0834 sec.
iter 33180 || Loss: 1.4985 || timer: 0.0808 sec.
iter 33190 || Loss: 1.3043 || timer: 0.1111 sec.
iter 33200 || Loss: 1.7080 || timer: 0.0875 sec.
iter 33210 || Loss: 1.3418 || timer: 0.1054 sec.
iter 33220 || Loss: 1.5587 || timer: 0.0251 sec.
iter 33230 || Loss: 0.9247 || timer: 0.0849 sec.
iter 33240 || Loss: 1.5813 || timer: 0.0883 sec.
iter 33250 || Loss: 1.4645 || timer: 0.0814 sec.
iter 33260 || Loss: 1.1475 || timer: 0.0806 sec.
iter 33270 || Loss: 1.1561 || timer: 0.0894 sec.
iter 33280 || Loss: 1.4899 || timer: 0.0898 sec.
iter 33290 || Loss: 1.2321 || timer: 0.0876 sec.
iter 33300 || Loss: 1.6145 || timer: 0.0900 sec.
iter 33310 || Loss: 1.4257 || timer: 0.1051 sec.
iter 33320 || Loss: 1.3823 || timer: 0.0967 sec.
iter 33330 || Loss: 1.9118 || timer: 0.0931 sec.
iter 33340 || Loss: 1.3334 || timer: 0.1055 sec.
iter 33350 || Loss: 1.6698 || timer: 0.0880 sec.
iter 33360 || Loss: 1.1410 || timer: 0.0887 sec.
iter 33370 || Loss: 1.6082 || timer: 0.0906 sec.
iter 33380 || Loss: 1.3519 || timer: 0.0808 sec.
iter 33390 || Loss: 1.3846 || timer: 0.0912 sec.
iter 33400 || Loss: 1.6547 || timer: 0.1335 sec.
iter 33410 || Loss: 1.7695 || timer: 0.0883 sec.
iter 33420 || Loss: 1.4735 || timer: 0.0833 sec.
iter 33430 || Loss: 1.3232 || timer: 0.1097 sec.
iter 33440 || Loss: 1.5604 || timer: 0.0889 sec.
iter 33450 || Loss: 1.4453 || timer: 0.0816 sec.
iter 33460 || Loss: 1.0120 || timer: 0.0917 sec.
iter 33470 || Loss: 1.1872 || timer: 0.0803 sec.
iter 33480 || Loss: 0.9862 || timer: 0.0881 sec.
iter 33490 || Loss: 1.2341 || timer: 0.0828 sec.
iter 33500 || Loss: 1.6174 || timer: 0.0896 sec.
iter 33510 || Loss: 1.4931 || timer: 0.0823 sec.
iter 33520 || Loss: 1.1334 || timer: 0.0843 sec.
iter 33530 || Loss: 1.6426 || timer: 0.1001 sec.
iter 33540 || Loss: 1.4396 || timer: 0.0928 sec.
iter 33550 || Loss: 1.6390 || timer: 0.0231 sec.
iter 33560 || Loss: 1.4765 || timer: 0.0890 sec.
iter 33570 || Loss: 2.2438 || timer: 0.0899 sec.
iter 33580 || Loss: 1.3341 || timer: 0.0824 sec.
iter 33590 || Loss: 1.5064 || timer: 0.0936 sec.
iter 33600 || Loss: 1.6946 || timer: 0.0923 sec.
iter 33610 || Loss: 1.4359 || timer: 0.0810 sec.
iter 33620 || Loss: 1.7973 || timer: 0.1060 sec.
iter 33630 || Loss: 1.5703 || timer: 0.0827 sec.
iter 33640 || Loss: 1.3936 || timer: 0.0829 sec.
iter 33650 || Loss: 1.3915 || timer: 0.1136 sec.
iter 33660 || Loss: 1.4269 || timer: 0.0964 sec.
iter 33670 || Loss: 1.6708 || timer: 0.0900 sec.
iter 33680 || Loss: 1.6970 || timer: 0.0931 sec.
iter 33690 || Loss: 1.5207 || timer: 0.0903 sec.
iter 33700 || Loss: 1.4008 || timer: 0.0844 sec.
iter 33710 || Loss: 1.8643 || timer: 0.1095 sec.
iter 33720 || Loss: 1.6179 || timer: 0.0901 sec.
iter 33730 || Loss: 1.1303 || timer: 0.0903 sec.
iter 33740 || Loss: 1.4550 || timer: 0.0869 sec.
iter 33750 || Loss: 1.7566 || timer: 0.0833 sec.
iter 33760 || Loss: 1.2371 || timer: 0.0925 sec.
iter 33770 || Loss: 1.7500 || timer: 0.0928 sec.
iter 33780 || Loss: 1.2133 || timer: 0.0920 sec.
iter 33790 || Loss: 1.3450 || timer: 0.1030 sec.
iter 33800 || Loss: 1.2663 || timer: 0.0910 sec.
iter 33810 || Loss: 1.5086 || timer: 0.0808 sec.
iter 33820 || Loss: 2.0095 || timer: 0.0830 sec.
iter 33830 || Loss: 1.1311 || timer: 0.1056 sec.
iter 33840 || Loss: 1.2685 || timer: 0.1144 sec.
iter 33850 || Loss: 0.9484 || timer: 0.1066 sec.
iter 33860 || Loss: 1.3492 || timer: 0.0928 sec.
iter 33870 || Loss: 1.4676 || timer: 0.0983 sec.
iter 33880 || Loss: 1.3799 || timer: 0.0161 sec.
iter 33890 || Loss: 2.9175 || timer: 0.0980 sec.
iter 33900 || Loss: 1.8954 || timer: 0.0821 sec.
iter 33910 || Loss: 1.4310 || timer: 0.0920 sec.
iter 33920 || Loss: 2.1667 || timer: 0.0812 sec.
iter 33930 || Loss: 1.3810 || timer: 0.0904 sec.
iter 33940 || Loss: 1.7499 || timer: 0.0874 sec.
iter 33950 || Loss: 1.5720 || timer: 0.0816 sec.
iter 33960 || Loss: 1.1014 || timer: 0.0901 sec.
iter 33970 || Loss: 0.9673 || timer: 0.0892 sec.
iter 33980 || Loss: 1.3855 || timer: 0.0940 sec.
iter 33990 || Loss: 1.3354 || timer: 0.0892 sec.
iter 34000 || Loss: 1.3502 || timer: 0.0895 sec.
iter 34010 || Loss: 1.4556 || timer: 0.0827 sec.
iter 34020 || Loss: 0.9597 || timer: 0.0975 sec.
iter 34030 || Loss: 1.5601 || timer: 0.0916 sec.
iter 34040 || Loss: 1.3304 || timer: 0.0860 sec.
iter 34050 || Loss: 1.3436 || timer: 0.0942 sec.
iter 34060 || Loss: 1.4715 || timer: 0.0874 sec.
iter 34070 || Loss: 1.4339 || timer: 0.0888 sec.
iter 34080 || Loss: 1.5769 || timer: 0.0824 sec.
iter 34090 || Loss: 1.5512 || timer: 0.0822 sec.
iter 34100 || Loss: 1.6105 || timer: 0.0828 sec.
iter 34110 || Loss: 1.8190 || timer: 0.0888 sec.
iter 34120 || Loss: 1.4220 || timer: 0.0812 sec.
iter 34130 || Loss: 1.4504 || timer: 0.0915 sec.
iter 34140 || Loss: 1.1389 || timer: 0.0906 sec.
iter 34150 || Loss: 1.0175 || timer: 0.0980 sec.
iter 34160 || Loss: 1.4809 || timer: 0.0804 sec.
iter 34170 || Loss: 1.1365 || timer: 0.0818 sec.
iter 34180 || Loss: 0.9633 || timer: 0.0903 sec.
iter 34190 || Loss: 1.5821 || timer: 0.0962 sec.
iter 34200 || Loss: 1.1273 || timer: 0.0819 sec.
iter 34210 || Loss: 1.0208 || timer: 0.0231 sec.
iter 34220 || Loss: 1.0263 || timer: 0.0877 sec.
iter 34230 || Loss: 1.5051 || timer: 0.0992 sec.
iter 34240 || Loss: 1.4812 || timer: 0.0919 sec.
iter 34250 || Loss: 1.2499 || timer: 0.0827 sec.
iter 34260 || Loss: 1.2959 || timer: 0.0749 sec.
iter 34270 || Loss: 1.1059 || timer: 0.0752 sec.
iter 34280 || Loss: 1.1162 || timer: 0.0825 sec.
iter 34290 || Loss: 1.4760 || timer: 0.1048 sec.
iter 34300 || Loss: 1.5447 || timer: 0.0918 sec.
iter 34310 || Loss: 1.7069 || timer: 0.0950 sec.
iter 34320 || Loss: 1.2522 || timer: 0.0899 sec.
iter 34330 || Loss: 1.1923 || timer: 0.0902 sec.
iter 34340 || Loss: 1.2022 || timer: 0.0789 sec.
iter 34350 || Loss: 1.1749 || timer: 0.0838 sec.
iter 34360 || Loss: 1.9099 || timer: 0.0826 sec.
iter 34370 || Loss: 1.4945 || timer: 0.0751 sec.
iter 34380 || Loss: 1.3867 || timer: 0.0984 sec.
iter 34390 || Loss: 1.2770 || timer: 0.0849 sec.
iter 34400 || Loss: 1.3037 || timer: 0.0903 sec.
iter 34410 || Loss: 1.3461 || timer: 0.0841 sec.
iter 34420 || Loss: 1.9268 || timer: 0.1037 sec.
iter 34430 || Loss: 1.8739 || timer: 0.0888 sec.
iter 34440 || Loss: 1.2552 || timer: 0.0902 sec.
iter 34450 || Loss: 1.3735 || timer: 0.1172 sec.
iter 34460 || Loss: 1.3866 || timer: 0.0826 sec.
iter 34470 || Loss: 1.4515 || timer: 0.0816 sec.
iter 34480 || Loss: 1.1677 || timer: 0.0844 sec.
iter 34490 || Loss: 1.3239 || timer: 0.0900 sec.
iter 34500 || Loss: 1.3553 || timer: 0.1063 sec.
iter 34510 || Loss: 1.5919 || timer: 0.0825 sec.
iter 34520 || Loss: 1.7184 || timer: 0.0817 sec.
iter 34530 || Loss: 1.2262 || timer: 0.0836 sec.
iter 34540 || Loss: 1.3106 || timer: 0.0247 sec.
iter 34550 || Loss: 0.4566 || timer: 0.0873 sec.
iter 34560 || Loss: 1.1993 || timer: 0.0792 sec.
iter 34570 || Loss: 0.9674 || timer: 0.0866 sec.
iter 34580 || Loss: 1.5411 || timer: 0.0849 sec.
iter 34590 || Loss: 1.3159 || timer: 0.0813 sec.
iter 34600 || Loss: 1.3324 || timer: 0.0887 sec.
iter 34610 || Loss: 1.3698 || timer: 0.0886 sec.
iter 34620 || Loss: 1.0666 || timer: 0.0898 sec.
iter 34630 || Loss: 1.4598 || timer: 0.0899 sec.
iter 34640 || Loss: 1.4390 || timer: 0.0992 sec.
iter 34650 || Loss: 1.4549 || timer: 0.0822 sec.
iter 34660 || Loss: 1.3399 || timer: 0.0964 sec.
iter 34670 || Loss: 1.5788 || timer: 0.0746 sec.
iter 34680 || Loss: 1.4000 || timer: 0.0832 sec.
iter 34690 || Loss: 1.2083 || timer: 0.0876 sec.
iter 34700 || Loss: 1.3147 || timer: 0.1056 sec.
iter 34710 || Loss: 1.3786 || timer: 0.0812 sec.
iter 34720 || Loss: 1.3860 || timer: 0.0868 sec.
iter 34730 || Loss: 1.1571 || timer: 0.0885 sec.
iter 34740 || Loss: 1.6100 || timer: 0.0895 sec.
iter 34750 || Loss: 1.4644 || timer: 0.0744 sec.
iter 34760 || Loss: 1.1156 || timer: 0.0747 sec.
iter 34770 || Loss: 1.4100 || timer: 0.0870 sec.
iter 34780 || Loss: 1.2955 || timer: 0.1315 sec.
iter 34790 || Loss: 1.9306 || timer: 0.1031 sec.
iter 34800 || Loss: 1.5829 || timer: 0.0815 sec.
iter 34810 || Loss: 1.2113 || timer: 0.0824 sec.
iter 34820 || Loss: 1.3305 || timer: 0.0824 sec.
iter 34830 || Loss: 1.3774 || timer: 0.0958 sec.
iter 34840 || Loss: 1.0400 || timer: 0.0890 sec.
iter 34850 || Loss: 1.5541 || timer: 0.0893 sec.
iter 34860 || Loss: 1.5051 || timer: 0.0897 sec.
iter 34870 || Loss: 1.1872 || timer: 0.0220 sec.
iter 34880 || Loss: 1.2267 || timer: 0.0867 sec.
iter 34890 || Loss: 1.5561 || timer: 0.0765 sec.
iter 34900 || Loss: 1.3510 || timer: 0.0813 sec.
iter 34910 || Loss: 1.2649 || timer: 0.0893 sec.
iter 34920 || Loss: 1.4451 || timer: 0.0812 sec.
iter 34930 || Loss: 1.3087 || timer: 0.0824 sec.
iter 34940 || Loss: 1.1114 || timer: 0.0821 sec.
iter 34950 || Loss: 1.1492 || timer: 0.0820 sec.
iter 34960 || Loss: 1.9265 || timer: 0.1316 sec.
iter 34970 || Loss: 1.7334 || timer: 0.0933 sec.
iter 34980 || Loss: 1.2801 || timer: 0.0758 sec.
iter 34990 || Loss: 1.5228 || timer: 0.0836 sec.
iter 35000 || Loss: 1.1938 || Saving state, iter: 35000
timer: 0.0881 sec.
iter 35010 || Loss: 0.9974 || timer: 0.0850 sec.
iter 35020 || Loss: 1.4925 || timer: 0.0964 sec.
iter 35030 || Loss: 1.5814 || timer: 0.0876 sec.
iter 35040 || Loss: 1.4284 || timer: 0.0817 sec.
iter 35050 || Loss: 1.1160 || timer: 0.0805 sec.
iter 35060 || Loss: 1.4035 || timer: 0.0815 sec.
iter 35070 || Loss: 1.3292 || timer: 0.0955 sec.
iter 35080 || Loss: 1.5293 || timer: 0.0896 sec.
iter 35090 || Loss: 1.0265 || timer: 0.0898 sec.
iter 35100 || Loss: 1.5769 || timer: 0.0803 sec.
iter 35110 || Loss: 1.4574 || timer: 0.0929 sec.
iter 35120 || Loss: 1.3280 || timer: 0.0862 sec.
iter 35130 || Loss: 1.9011 || timer: 0.0919 sec.
iter 35140 || Loss: 1.3606 || timer: 0.0822 sec.
iter 35150 || Loss: 1.2091 || timer: 0.1007 sec.
iter 35160 || Loss: 1.2357 || timer: 0.0924 sec.
iter 35170 || Loss: 1.2671 || timer: 0.0825 sec.
iter 35180 || Loss: 1.4751 || timer: 0.0914 sec.
iter 35190 || Loss: 1.6472 || timer: 0.0850 sec.
iter 35200 || Loss: 1.0455 || timer: 0.0158 sec.
iter 35210 || Loss: 0.8384 || timer: 0.0823 sec.
iter 35220 || Loss: 1.6134 || timer: 0.0897 sec.
iter 35230 || Loss: 1.2641 || timer: 0.0905 sec.
iter 35240 || Loss: 1.4486 || timer: 0.0889 sec.
iter 35250 || Loss: 1.4075 || timer: 0.1110 sec.
iter 35260 || Loss: 1.2905 || timer: 0.1019 sec.
iter 35270 || Loss: 1.3211 || timer: 0.0883 sec.
iter 35280 || Loss: 1.5120 || timer: 0.1141 sec.
iter 35290 || Loss: 1.3371 || timer: 0.0950 sec.
iter 35300 || Loss: 1.1612 || timer: 0.0948 sec.
iter 35310 || Loss: 1.3279 || timer: 0.0843 sec.
iter 35320 || Loss: 1.5927 || timer: 0.0882 sec.
iter 35330 || Loss: 1.2108 || timer: 0.0845 sec.
iter 35340 || Loss: 1.8739 || timer: 0.0829 sec.
iter 35350 || Loss: 1.9472 || timer: 0.0840 sec.
iter 35360 || Loss: 1.5079 || timer: 0.0751 sec.
iter 35370 || Loss: 1.5183 || timer: 0.0838 sec.
iter 35380 || Loss: 1.2275 || timer: 0.1094 sec.
iter 35390 || Loss: 1.5420 || timer: 0.0881 sec.
iter 35400 || Loss: 1.3470 || timer: 0.0850 sec.
iter 35410 || Loss: 1.5128 || timer: 0.0825 sec.
iter 35420 || Loss: 1.9757 || timer: 0.0874 sec.
iter 35430 || Loss: 1.3566 || timer: 0.0816 sec.
iter 35440 || Loss: 1.2782 || timer: 0.0825 sec.
iter 35450 || Loss: 1.3959 || timer: 0.0895 sec.
iter 35460 || Loss: 1.5455 || timer: 0.0804 sec.
iter 35470 || Loss: 1.3015 || timer: 0.0832 sec.
iter 35480 || Loss: 1.5332 || timer: 0.1158 sec.
iter 35490 || Loss: 1.9305 || timer: 0.0824 sec.
iter 35500 || Loss: 1.8959 || timer: 0.0838 sec.
iter 35510 || Loss: 1.1724 || timer: 0.0999 sec.
iter 35520 || Loss: 1.1599 || timer: 0.0822 sec.
iter 35530 || Loss: 1.3369 || timer: 0.0251 sec.
iter 35540 || Loss: 0.9774 || timer: 0.0866 sec.
iter 35550 || Loss: 1.4525 || timer: 0.0825 sec.
iter 35560 || Loss: 1.7908 || timer: 0.0840 sec.
iter 35570 || Loss: 1.1305 || timer: 0.0822 sec.
iter 35580 || Loss: 1.0781 || timer: 0.1003 sec.
iter 35590 || Loss: 1.4045 || timer: 0.0944 sec.
iter 35600 || Loss: 1.4182 || timer: 0.0844 sec.
iter 35610 || Loss: 1.1743 || timer: 0.0836 sec.
iter 35620 || Loss: 1.6615 || timer: 0.0959 sec.
iter 35630 || Loss: 1.3571 || timer: 0.0849 sec.
iter 35640 || Loss: 1.0907 || timer: 0.0890 sec.
iter 35650 || Loss: 1.1159 || timer: 0.0896 sec.
iter 35660 || Loss: 1.4683 || timer: 0.0898 sec.
iter 35670 || Loss: 1.2092 || timer: 0.0902 sec.
iter 35680 || Loss: 1.4687 || timer: 0.0876 sec.
iter 35690 || Loss: 1.1869 || timer: 0.0917 sec.
iter 35700 || Loss: 1.1355 || timer: 0.0873 sec.
iter 35710 || Loss: 1.4805 || timer: 0.0823 sec.
iter 35720 || Loss: 1.3965 || timer: 0.1118 sec.
iter 35730 || Loss: 1.7783 || timer: 0.1112 sec.
iter 35740 || Loss: 1.6200 || timer: 0.0828 sec.
iter 35750 || Loss: 1.5517 || timer: 0.0823 sec.
iter 35760 || Loss: 1.5905 || timer: 0.0916 sec.
iter 35770 || Loss: 1.2157 || timer: 0.0754 sec.
iter 35780 || Loss: 1.2251 || timer: 0.0846 sec.
iter 35790 || Loss: 1.1848 || timer: 0.0824 sec.
iter 35800 || Loss: 1.0759 || timer: 0.0863 sec.
iter 35810 || Loss: 1.3188 || timer: 0.0891 sec.
iter 35820 || Loss: 1.2985 || timer: 0.0852 sec.
iter 35830 || Loss: 1.0695 || timer: 0.0814 sec.
iter 35840 || Loss: 1.3529 || timer: 0.1014 sec.
iter 35850 || Loss: 1.7679 || timer: 0.0889 sec.
iter 35860 || Loss: 1.5267 || timer: 0.0303 sec.
iter 35870 || Loss: 2.3954 || timer: 0.0889 sec.
iter 35880 || Loss: 2.1138 || timer: 0.0913 sec.
iter 35890 || Loss: 1.3977 || timer: 0.0906 sec.
iter 35900 || Loss: 1.4738 || timer: 0.0924 sec.
iter 35910 || Loss: 1.8728 || timer: 0.0886 sec.
iter 35920 || Loss: 1.2624 || timer: 0.0890 sec.
iter 35930 || Loss: 1.0916 || timer: 0.0885 sec.
iter 35940 || Loss: 1.5223 || timer: 0.0903 sec.
iter 35950 || Loss: 1.2287 || timer: 0.0825 sec.
iter 35960 || Loss: 1.3253 || timer: 0.1054 sec.
iter 35970 || Loss: 1.2965 || timer: 0.0897 sec.
iter 35980 || Loss: 1.4654 || timer: 0.0820 sec.
iter 35990 || Loss: 1.1183 || timer: 0.0883 sec.
iter 36000 || Loss: 1.4457 || timer: 0.0908 sec.
iter 36010 || Loss: 2.0042 || timer: 0.0763 sec.
iter 36020 || Loss: 1.2569 || timer: 0.0910 sec.
iter 36030 || Loss: 1.2438 || timer: 0.0892 sec.
iter 36040 || Loss: 1.9330 || timer: 0.0900 sec.
iter 36050 || Loss: 1.6323 || timer: 0.0890 sec.
iter 36060 || Loss: 1.1774 || timer: 0.0829 sec.
iter 36070 || Loss: 1.5795 || timer: 0.0973 sec.
iter 36080 || Loss: 1.2678 || timer: 0.0835 sec.
iter 36090 || Loss: 1.5869 || timer: 0.0820 sec.
iter 36100 || Loss: 1.5325 || timer: 0.0961 sec.
iter 36110 || Loss: 1.9478 || timer: 0.0822 sec.
iter 36120 || Loss: 1.2132 || timer: 0.0814 sec.
iter 36130 || Loss: 1.2979 || timer: 0.0893 sec.
iter 36140 || Loss: 1.7728 || timer: 0.0830 sec.
iter 36150 || Loss: 1.2369 || timer: 0.0968 sec.
iter 36160 || Loss: 1.4029 || timer: 0.0897 sec.
iter 36170 || Loss: 1.4474 || timer: 0.0892 sec.
iter 36180 || Loss: 1.2763 || timer: 0.0902 sec.
iter 36190 || Loss: 1.3436 || timer: 0.0158 sec.
iter 36200 || Loss: 0.4368 || timer: 0.0899 sec.
iter 36210 || Loss: 1.4769 || timer: 0.1053 sec.
iter 36220 || Loss: 1.4889 || timer: 0.0815 sec.
iter 36230 || Loss: 1.3683 || timer: 0.0926 sec.
iter 36240 || Loss: 1.4112 || timer: 0.0819 sec.
iter 36250 || Loss: 2.0197 || timer: 0.0827 sec.
iter 36260 || Loss: 1.6527 || timer: 0.0962 sec.
iter 36270 || Loss: 1.4816 || timer: 0.0906 sec.
iter 36280 || Loss: 1.5398 || timer: 0.0823 sec.
iter 36290 || Loss: 1.0007 || timer: 0.0996 sec.
iter 36300 || Loss: 1.6671 || timer: 0.0825 sec.
iter 36310 || Loss: 1.6600 || timer: 0.0928 sec.
iter 36320 || Loss: 1.1497 || timer: 0.0806 sec.
iter 36330 || Loss: 1.7073 || timer: 0.0822 sec.
iter 36340 || Loss: 1.2419 || timer: 0.0745 sec.
iter 36350 || Loss: 1.2635 || timer: 0.0819 sec.
iter 36360 || Loss: 1.4190 || timer: 0.1009 sec.
iter 36370 || Loss: 1.7972 || timer: 0.0961 sec.
iter 36380 || Loss: 1.2526 || timer: 0.0891 sec.
iter 36390 || Loss: 1.6739 || timer: 0.1045 sec.
iter 36400 || Loss: 1.3629 || timer: 0.0825 sec.
iter 36410 || Loss: 1.4376 || timer: 0.0892 sec.
iter 36420 || Loss: 1.0759 || timer: 0.0835 sec.
iter 36430 || Loss: 1.3998 || timer: 0.1017 sec.
iter 36440 || Loss: 1.4233 || timer: 0.0824 sec.
iter 36450 || Loss: 0.9375 || timer: 0.0893 sec.
iter 36460 || Loss: 1.8943 || timer: 0.0825 sec.
iter 36470 || Loss: 1.4655 || timer: 0.1080 sec.
iter 36480 || Loss: 1.4813 || timer: 0.0736 sec.
iter 36490 || Loss: 1.4155 || timer: 0.0853 sec.
iter 36500 || Loss: 1.2683 || timer: 0.1000 sec.
iter 36510 || Loss: 1.6933 || timer: 0.1156 sec.
iter 36520 || Loss: 1.2235 || timer: 0.0261 sec.
iter 36530 || Loss: 1.0106 || timer: 0.1204 sec.
iter 36540 || Loss: 1.3583 || timer: 0.0833 sec.
iter 36550 || Loss: 1.3900 || timer: 0.0909 sec.
iter 36560 || Loss: 1.1630 || timer: 0.0902 sec.
iter 36570 || Loss: 1.4516 || timer: 0.0996 sec.
iter 36580 || Loss: 0.9272 || timer: 0.1277 sec.
iter 36590 || Loss: 1.8052 || timer: 0.0830 sec.
iter 36600 || Loss: 1.1098 || timer: 0.0999 sec.
iter 36610 || Loss: 1.3766 || timer: 0.1043 sec.
iter 36620 || Loss: 1.2219 || timer: 0.1230 sec.
iter 36630 || Loss: 1.2369 || timer: 0.1000 sec.
iter 36640 || Loss: 1.3264 || timer: 0.0844 sec.
iter 36650 || Loss: 1.9230 || timer: 0.0910 sec.
iter 36660 || Loss: 1.4808 || timer: 0.0911 sec.
iter 36670 || Loss: 1.4118 || timer: 0.0915 sec.
iter 36680 || Loss: 1.3470 || timer: 0.0908 sec.
iter 36690 || Loss: 1.0998 || timer: 0.0902 sec.
iter 36700 || Loss: 1.3638 || timer: 0.0829 sec.
iter 36710 || Loss: 1.4030 || timer: 0.0813 sec.
iter 36720 || Loss: 1.3483 || timer: 0.0910 sec.
iter 36730 || Loss: 1.3711 || timer: 0.0844 sec.
iter 36740 || Loss: 1.4535 || timer: 0.0899 sec.
iter 36750 || Loss: 1.3275 || timer: 0.0825 sec.
iter 36760 || Loss: 1.5736 || timer: 0.0932 sec.
iter 36770 || Loss: 1.1714 || timer: 0.0899 sec.
iter 36780 || Loss: 1.1140 || timer: 0.0861 sec.
iter 36790 || Loss: 1.5975 || timer: 0.0829 sec.
iter 36800 || Loss: 1.6907 || timer: 0.0969 sec.
iter 36810 || Loss: 1.2396 || timer: 0.1126 sec.
iter 36820 || Loss: 1.1346 || timer: 0.0914 sec.
iter 36830 || Loss: 1.5254 || timer: 0.0946 sec.
iter 36840 || Loss: 1.4991 || timer: 0.0867 sec.
iter 36850 || Loss: 1.1573 || timer: 0.0268 sec.
iter 36860 || Loss: 0.7279 || timer: 0.0843 sec.
iter 36870 || Loss: 1.2787 || timer: 0.1045 sec.
iter 36880 || Loss: 1.3118 || timer: 0.0899 sec.
iter 36890 || Loss: 1.1985 || timer: 0.0903 sec.
iter 36900 || Loss: 1.0829 || timer: 0.0908 sec.
iter 36910 || Loss: 1.1549 || timer: 0.0824 sec.
iter 36920 || Loss: 1.2407 || timer: 0.0895 sec.
iter 36930 || Loss: 1.3637 || timer: 0.0874 sec.
iter 36940 || Loss: 1.4168 || timer: 0.0882 sec.
iter 36950 || Loss: 1.1422 || timer: 0.0933 sec.
iter 36960 || Loss: 1.0812 || timer: 0.0868 sec.
iter 36970 || Loss: 1.1647 || timer: 0.0844 sec.
iter 36980 || Loss: 1.4986 || timer: 0.0913 sec.
iter 36990 || Loss: 1.4689 || timer: 0.1001 sec.
iter 37000 || Loss: 1.2291 || timer: 0.0992 sec.
iter 37010 || Loss: 1.0038 || timer: 0.0838 sec.
iter 37020 || Loss: 1.2659 || timer: 0.0903 sec.
iter 37030 || Loss: 1.1401 || timer: 0.1064 sec.
iter 37040 || Loss: 1.6878 || timer: 0.0887 sec.
iter 37050 || Loss: 1.2319 || timer: 0.0842 sec.
iter 37060 || Loss: 1.4669 || timer: 0.0908 sec.
iter 37070 || Loss: 1.7873 || timer: 0.0919 sec.
iter 37080 || Loss: 1.4285 || timer: 0.0854 sec.
iter 37090 || Loss: 1.8704 || timer: 0.1017 sec.
iter 37100 || Loss: 1.8317 || timer: 0.0844 sec.
iter 37110 || Loss: 1.1973 || timer: 0.1030 sec.
iter 37120 || Loss: 1.1217 || timer: 0.0907 sec.
iter 37130 || Loss: 1.7374 || timer: 0.0907 sec.
iter 37140 || Loss: 1.0019 || timer: 0.0914 sec.
iter 37150 || Loss: 1.0982 || timer: 0.0849 sec.
iter 37160 || Loss: 1.8085 || timer: 0.0899 sec.
iter 37170 || Loss: 1.0810 || timer: 0.0851 sec.
iter 37180 || Loss: 1.4660 || timer: 0.0175 sec.
iter 37190 || Loss: 1.3820 || timer: 0.0880 sec.
iter 37200 || Loss: 1.1280 || timer: 0.1069 sec.
iter 37210 || Loss: 1.1003 || timer: 0.0844 sec.
iter 37220 || Loss: 0.8478 || timer: 0.0903 sec.
iter 37230 || Loss: 1.7080 || timer: 0.1036 sec.
iter 37240 || Loss: 1.1753 || timer: 0.1053 sec.
iter 37250 || Loss: 0.9502 || timer: 0.0946 sec.
iter 37260 || Loss: 1.4250 || timer: 0.0830 sec.
iter 37270 || Loss: 1.1288 || timer: 0.0898 sec.
iter 37280 || Loss: 1.2151 || timer: 0.0936 sec.
iter 37290 || Loss: 0.9679 || timer: 0.1078 sec.
iter 37300 || Loss: 1.8168 || timer: 0.0902 sec.
iter 37310 || Loss: 1.2889 || timer: 0.0910 sec.
iter 37320 || Loss: 1.1024 || timer: 0.1002 sec.
iter 37330 || Loss: 1.1864 || timer: 0.0929 sec.
iter 37340 || Loss: 0.9870 || timer: 0.0844 sec.
iter 37350 || Loss: 1.6716 || timer: 0.0931 sec.
iter 37360 || Loss: 1.3492 || timer: 0.0830 sec.
iter 37370 || Loss: 1.3797 || timer: 0.0946 sec.
iter 37380 || Loss: 1.5936 || timer: 0.0920 sec.
iter 37390 || Loss: 1.8049 || timer: 0.1051 sec.
iter 37400 || Loss: 1.6489 || timer: 0.0894 sec.
iter 37410 || Loss: 1.4887 || timer: 0.0913 sec.
iter 37420 || Loss: 1.1282 || timer: 0.0901 sec.
iter 37430 || Loss: 1.1640 || timer: 0.0920 sec.
iter 37440 || Loss: 1.0443 || timer: 0.0829 sec.
iter 37450 || Loss: 1.4176 || timer: 0.0919 sec.
iter 37460 || Loss: 1.2957 || timer: 0.0909 sec.
iter 37470 || Loss: 1.4844 || timer: 0.0916 sec.
iter 37480 || Loss: 1.5499 || timer: 0.1065 sec.
iter 37490 || Loss: 1.5068 || timer: 0.1085 sec.
iter 37500 || Loss: 1.7207 || timer: 0.0857 sec.
iter 37510 || Loss: 1.3718 || timer: 0.0261 sec.
iter 37520 || Loss: 0.6571 || timer: 0.0868 sec.
iter 37530 || Loss: 1.2018 || timer: 0.0947 sec.
iter 37540 || Loss: 1.3153 || timer: 0.0895 sec.
iter 37550 || Loss: 1.2686 || timer: 0.1064 sec.
iter 37560 || Loss: 1.2322 || timer: 0.0846 sec.
iter 37570 || Loss: 0.9938 || timer: 0.0918 sec.
iter 37580 || Loss: 1.2509 || timer: 0.0856 sec.
iter 37590 || Loss: 1.2512 || timer: 0.0830 sec.
iter 37600 || Loss: 1.4364 || timer: 0.1036 sec.
iter 37610 || Loss: 1.1500 || timer: 0.1190 sec.
iter 37620 || Loss: 1.3991 || timer: 0.0883 sec.
iter 37630 || Loss: 1.2305 || timer: 0.1023 sec.
iter 37640 || Loss: 1.1628 || timer: 0.0888 sec.
iter 37650 || Loss: 1.3179 || timer: 0.0836 sec.
iter 37660 || Loss: 1.3081 || timer: 0.0841 sec.
iter 37670 || Loss: 1.4757 || timer: 0.0900 sec.
iter 37680 || Loss: 1.7126 || timer: 0.0902 sec.
iter 37690 || Loss: 1.2192 || timer: 0.0833 sec.
iter 37700 || Loss: 0.9567 || timer: 0.0842 sec.
iter 37710 || Loss: 1.1777 || timer: 0.0928 sec.
iter 37720 || Loss: 1.3443 || timer: 0.0901 sec.
iter 37730 || Loss: 1.2123 || timer: 0.1041 sec.
iter 37740 || Loss: 1.1987 || timer: 0.0909 sec.
iter 37750 || Loss: 1.2796 || timer: 0.0901 sec.
iter 37760 || Loss: 1.2429 || timer: 0.0865 sec.
iter 37770 || Loss: 1.3764 || timer: 0.0893 sec.
iter 37780 || Loss: 1.5865 || timer: 0.0961 sec.
iter 37790 || Loss: 1.0542 || timer: 0.0957 sec.
iter 37800 || Loss: 1.0426 || timer: 0.0908 sec.
iter 37810 || Loss: 1.1610 || timer: 0.0981 sec.
iter 37820 || Loss: 1.2818 || timer: 0.0934 sec.
iter 37830 || Loss: 1.1430 || timer: 0.0848 sec.
iter 37840 || Loss: 1.5318 || timer: 0.0250 sec.
iter 37850 || Loss: 2.1613 || timer: 0.0886 sec.
iter 37860 || Loss: 1.4380 || timer: 0.0897 sec.
iter 37870 || Loss: 1.1893 || timer: 0.0895 sec.
iter 37880 || Loss: 1.3354 || timer: 0.0836 sec.
iter 37890 || Loss: 1.2997 || timer: 0.1041 sec.
iter 37900 || Loss: 1.6353 || timer: 0.0837 sec.
iter 37910 || Loss: 1.5913 || timer: 0.0838 sec.
iter 37920 || Loss: 1.4043 || timer: 0.0832 sec.
iter 37930 || Loss: 1.1744 || timer: 0.0818 sec.
iter 37940 || Loss: 1.1882 || timer: 0.1147 sec.
iter 37950 || Loss: 0.9530 || timer: 0.0843 sec.
iter 37960 || Loss: 1.6683 || timer: 0.0829 sec.
iter 37970 || Loss: 1.1859 || timer: 0.0908 sec.
iter 37980 || Loss: 1.3445 || timer: 0.0849 sec.
iter 37990 || Loss: 1.4316 || timer: 0.0838 sec.
iter 38000 || Loss: 1.3154 || timer: 0.0912 sec.
iter 38010 || Loss: 1.7946 || timer: 0.0937 sec.
iter 38020 || Loss: 1.2460 || timer: 0.1165 sec.
iter 38030 || Loss: 0.8141 || timer: 0.1028 sec.
iter 38040 || Loss: 1.7885 || timer: 0.0876 sec.
iter 38050 || Loss: 1.4009 || timer: 0.0975 sec.
iter 38060 || Loss: 1.2824 || timer: 0.0930 sec.
iter 38070 || Loss: 1.4902 || timer: 0.0916 sec.
iter 38080 || Loss: 1.2171 || timer: 0.0889 sec.
iter 38090 || Loss: 1.5515 || timer: 0.0836 sec.
iter 38100 || Loss: 1.3874 || timer: 0.0945 sec.
iter 38110 || Loss: 1.3928 || timer: 0.0884 sec.
iter 38120 || Loss: 1.2726 || timer: 0.0823 sec.
iter 38130 || Loss: 1.3255 || timer: 0.1064 sec.
iter 38140 || Loss: 1.6112 || timer: 0.0868 sec.
iter 38150 || Loss: 1.2873 || timer: 0.0884 sec.
iter 38160 || Loss: 1.4872 || timer: 0.0896 sec.
iter 38170 || Loss: 1.2177 || timer: 0.0212 sec.
iter 38180 || Loss: 0.6689 || timer: 0.1098 sec.
iter 38190 || Loss: 1.1993 || timer: 0.1044 sec.
iter 38200 || Loss: 1.4019 || timer: 0.1021 sec.
iter 38210 || Loss: 1.2442 || timer: 0.0903 sec.
iter 38220 || Loss: 1.1065 || timer: 0.0992 sec.
iter 38230 || Loss: 1.3232 || timer: 0.0923 sec.
iter 38240 || Loss: 1.2374 || timer: 0.0845 sec.
iter 38250 || Loss: 1.0715 || timer: 0.0892 sec.
iter 38260 || Loss: 1.2788 || timer: 0.0906 sec.
iter 38270 || Loss: 1.4351 || timer: 0.1324 sec.
iter 38280 || Loss: 1.4893 || timer: 0.0908 sec.
iter 38290 || Loss: 1.1514 || timer: 0.0905 sec.
iter 38300 || Loss: 1.1571 || timer: 0.0837 sec.
iter 38310 || Loss: 1.8112 || timer: 0.0920 sec.
iter 38320 || Loss: 1.2062 || timer: 0.1028 sec.
iter 38330 || Loss: 1.8232 || timer: 0.0828 sec.
iter 38340 || Loss: 1.1585 || timer: 0.1209 sec.
iter 38350 || Loss: 1.7694 || timer: 0.1044 sec.
iter 38360 || Loss: 1.0937 || timer: 0.0827 sec.
iter 38370 || Loss: 1.1288 || timer: 0.0871 sec.
iter 38380 || Loss: 1.5888 || timer: 0.0951 sec.
iter 38390 || Loss: 1.5641 || timer: 0.0866 sec.
iter 38400 || Loss: 0.9004 || timer: 0.0917 sec.
iter 38410 || Loss: 1.4461 || timer: 0.0904 sec.
iter 38420 || Loss: 1.6469 || timer: 0.1106 sec.
iter 38430 || Loss: 1.2701 || timer: 0.1060 sec.
iter 38440 || Loss: 1.8546 || timer: 0.0982 sec.
iter 38450 || Loss: 1.3698 || timer: 0.0833 sec.
iter 38460 || Loss: 1.0617 || timer: 0.0907 sec.
iter 38470 || Loss: 1.4950 || timer: 0.0906 sec.
iter 38480 || Loss: 1.2951 || timer: 0.0911 sec.
iter 38490 || Loss: 1.2076 || timer: 0.0917 sec.
iter 38500 || Loss: 1.6142 || timer: 0.0169 sec.
iter 38510 || Loss: 1.9596 || timer: 0.1047 sec.
iter 38520 || Loss: 1.2529 || timer: 0.0822 sec.
iter 38530 || Loss: 1.2225 || timer: 0.0838 sec.
iter 38540 || Loss: 1.3990 || timer: 0.0890 sec.
iter 38550 || Loss: 1.5069 || timer: 0.0927 sec.
iter 38560 || Loss: 1.0698 || timer: 0.0911 sec.
iter 38570 || Loss: 1.3586 || timer: 0.1079 sec.
iter 38580 || Loss: 1.6178 || timer: 0.0981 sec.
iter 38590 || Loss: 1.5002 || timer: 0.1055 sec.
iter 38600 || Loss: 1.8427 || timer: 0.1118 sec.
iter 38610 || Loss: 1.1097 || timer: 0.0886 sec.
iter 38620 || Loss: 1.5580 || timer: 0.0912 sec.
iter 38630 || Loss: 1.0958 || timer: 0.0924 sec.
iter 38640 || Loss: 1.7652 || timer: 0.0878 sec.
iter 38650 || Loss: 1.4768 || timer: 0.0879 sec.
iter 38660 || Loss: 1.4018 || timer: 0.0894 sec.
iter 38670 || Loss: 1.3190 || timer: 0.0873 sec.
iter 38680 || Loss: 1.2947 || timer: 0.0926 sec.
iter 38690 || Loss: 1.0599 || timer: 0.0893 sec.
iter 38700 || Loss: 1.4615 || timer: 0.0847 sec.
iter 38710 || Loss: 1.4054 || timer: 0.0904 sec.
iter 38720 || Loss: 1.4083 || timer: 0.0907 sec.
iter 38730 || Loss: 2.1754 || timer: 0.0921 sec.
iter 38740 || Loss: 1.4741 || timer: 0.0900 sec.
iter 38750 || Loss: 1.4351 || timer: 0.0889 sec.
iter 38760 || Loss: 1.2499 || timer: 0.0871 sec.
iter 38770 || Loss: 1.6411 || timer: 0.1012 sec.
iter 38780 || Loss: 1.0325 || timer: 0.0897 sec.
iter 38790 || Loss: 1.0862 || timer: 0.1076 sec.
iter 38800 || Loss: 1.2016 || timer: 0.0918 sec.
iter 38810 || Loss: 1.3625 || timer: 0.0901 sec.
iter 38820 || Loss: 1.3377 || timer: 0.0988 sec.
iter 38830 || Loss: 1.2524 || timer: 0.0247 sec.
iter 38840 || Loss: 1.7361 || timer: 0.0822 sec.
iter 38850 || Loss: 1.8201 || timer: 0.0878 sec.
iter 38860 || Loss: 1.6559 || timer: 0.0827 sec.
iter 38870 || Loss: 1.3861 || timer: 0.0976 sec.
iter 38880 || Loss: 1.0937 || timer: 0.1039 sec.
iter 38890 || Loss: 1.1540 || timer: 0.0908 sec.
iter 38900 || Loss: 1.2596 || timer: 0.0902 sec.
iter 38910 || Loss: 1.2920 || timer: 0.0835 sec.
iter 38920 || Loss: 1.1656 || timer: 0.1028 sec.
iter 38930 || Loss: 1.0207 || timer: 0.0957 sec.
iter 38940 || Loss: 1.2548 || timer: 0.0940 sec.
iter 38950 || Loss: 1.0585 || timer: 0.1042 sec.
iter 38960 || Loss: 1.8279 || timer: 0.0894 sec.
iter 38970 || Loss: 1.2686 || timer: 0.0978 sec.
iter 38980 || Loss: 1.3009 || timer: 0.0875 sec.
iter 38990 || Loss: 1.0079 || timer: 0.0900 sec.
iter 39000 || Loss: 1.3508 || timer: 0.0929 sec.
iter 39010 || Loss: 1.3165 || timer: 0.0836 sec.
iter 39020 || Loss: 1.1201 || timer: 0.1144 sec.
iter 39030 || Loss: 1.8086 || timer: 0.0917 sec.
iter 39040 || Loss: 0.9109 || timer: 0.1169 sec.
iter 39050 || Loss: 1.1065 || timer: 0.1048 sec.
iter 39060 || Loss: 1.2523 || timer: 0.0792 sec.
iter 39070 || Loss: 1.2225 || timer: 0.0992 sec.
iter 39080 || Loss: 1.1611 || timer: 0.1239 sec.
iter 39090 || Loss: 0.9876 || timer: 0.0966 sec.
iter 39100 || Loss: 1.0209 || timer: 0.0946 sec.
iter 39110 || Loss: 1.4342 || timer: 0.0829 sec.
iter 39120 || Loss: 1.2853 || timer: 0.0897 sec.
iter 39130 || Loss: 1.5115 || timer: 0.1011 sec.
iter 39140 || Loss: 1.1619 || timer: 0.0907 sec.
iter 39150 || Loss: 1.2041 || timer: 0.1027 sec.
iter 39160 || Loss: 1.1208 || timer: 0.0267 sec.
iter 39170 || Loss: 1.4644 || timer: 0.0933 sec.
iter 39180 || Loss: 1.3557 || timer: 0.1147 sec.
iter 39190 || Loss: 1.1625 || timer: 0.0900 sec.
iter 39200 || Loss: 1.2157 || timer: 0.0887 sec.
iter 39210 || Loss: 1.2393 || timer: 0.1015 sec.
iter 39220 || Loss: 1.1380 || timer: 0.0870 sec.
iter 39230 || Loss: 1.2216 || timer: 0.0943 sec.
iter 39240 || Loss: 1.6927 || timer: 0.0916 sec.
iter 39250 || Loss: 1.5071 || timer: 0.0892 sec.
iter 39260 || Loss: 1.4263 || timer: 0.0961 sec.
iter 39270 || Loss: 1.3305 || timer: 0.0827 sec.
iter 39280 || Loss: 1.2984 || timer: 0.0894 sec.
iter 39290 || Loss: 1.2197 || timer: 0.0913 sec.
iter 39300 || Loss: 1.5420 || timer: 0.0832 sec.
iter 39310 || Loss: 1.3095 || timer: 0.1039 sec.
iter 39320 || Loss: 1.5040 || timer: 0.0949 sec.
iter 39330 || Loss: 1.2072 || timer: 0.1045 sec.
iter 39340 || Loss: 0.9949 || timer: 0.1048 sec.
iter 39350 || Loss: 1.5116 || timer: 0.1178 sec.
iter 39360 || Loss: 1.2793 || timer: 0.0932 sec.
iter 39370 || Loss: 1.6027 || timer: 0.0919 sec.
iter 39380 || Loss: 1.2456 || timer: 0.0824 sec.
iter 39390 || Loss: 1.3863 || timer: 0.0893 sec.
iter 39400 || Loss: 0.9947 || timer: 0.0982 sec.
iter 39410 || Loss: 1.0880 || timer: 0.0839 sec.
iter 39420 || Loss: 1.0568 || timer: 0.0917 sec.
iter 39430 || Loss: 1.7329 || timer: 0.0837 sec.
iter 39440 || Loss: 1.2243 || timer: 0.0935 sec.
iter 39450 || Loss: 1.7053 || timer: 0.0841 sec.
iter 39460 || Loss: 1.6552 || timer: 0.0930 sec.
iter 39470 || Loss: 1.7398 || timer: 0.0830 sec.
iter 39480 || Loss: 1.2485 || timer: 0.0848 sec.
iter 39490 || Loss: 1.4816 || timer: 0.0265 sec.
iter 39500 || Loss: 0.8270 || timer: 0.0911 sec.
iter 39510 || Loss: 1.4396 || timer: 0.0837 sec.
iter 39520 || Loss: 1.7542 || timer: 0.0912 sec.
iter 39530 || Loss: 0.9707 || timer: 0.0826 sec.
iter 39540 || Loss: 1.0776 || timer: 0.0957 sec.
iter 39550 || Loss: 1.0489 || timer: 0.1041 sec.
iter 39560 || Loss: 1.0825 || timer: 0.0823 sec.
iter 39570 || Loss: 1.3581 || timer: 0.0921 sec.
iter 39580 || Loss: 1.1228 || timer: 0.0907 sec.
iter 39590 || Loss: 1.2962 || timer: 0.1197 sec.
iter 39600 || Loss: 1.1184 || timer: 0.0894 sec.
iter 39610 || Loss: 1.2222 || timer: 0.0860 sec.
iter 39620 || Loss: 1.0625 || timer: 0.0822 sec.
iter 39630 || Loss: 1.1419 || timer: 0.0896 sec.
iter 39640 || Loss: 1.0789 || timer: 0.0773 sec.
iter 39650 || Loss: 1.5144 || timer: 0.0807 sec.
iter 39660 || Loss: 1.2698 || timer: 0.0937 sec.
iter 39670 || Loss: 1.1098 || timer: 0.1086 sec.
iter 39680 || Loss: 1.2874 || timer: 0.0870 sec.
iter 39690 || Loss: 1.1634 || timer: 0.0840 sec.
iter 39700 || Loss: 1.6304 || timer: 0.1129 sec.
iter 39710 || Loss: 1.8325 || timer: 0.0825 sec.
iter 39720 || Loss: 1.3313 || timer: 0.0839 sec.
iter 39730 || Loss: 1.1922 || timer: 0.0817 sec.
iter 39740 || Loss: 0.8586 || timer: 0.0830 sec.
iter 39750 || Loss: 1.3515 || timer: 0.1089 sec.
iter 39760 || Loss: 1.1724 || timer: 0.0924 sec.
iter 39770 || Loss: 1.5266 || timer: 0.0832 sec.
iter 39780 || Loss: 1.3783 || timer: 0.0910 sec.
iter 39790 || Loss: 1.2130 || timer: 0.0915 sec.
iter 39800 || Loss: 1.4230 || timer: 0.0827 sec.
iter 39810 || Loss: 1.8207 || timer: 0.0934 sec.
iter 39820 || Loss: 1.4352 || timer: 0.0200 sec.
iter 39830 || Loss: 0.8521 || timer: 0.0913 sec.
iter 39840 || Loss: 1.7095 || timer: 0.0837 sec.
iter 39850 || Loss: 1.1805 || timer: 0.0922 sec.
iter 39860 || Loss: 1.0231 || timer: 0.0916 sec.
iter 39870 || Loss: 1.1010 || timer: 0.0842 sec.
iter 39880 || Loss: 1.3874 || timer: 0.0847 sec.
iter 39890 || Loss: 1.1112 || timer: 0.0937 sec.
iter 39900 || Loss: 1.4901 || timer: 0.0903 sec.
iter 39910 || Loss: 1.0309 || timer: 0.0852 sec.
iter 39920 || Loss: 1.5873 || timer: 0.1002 sec.
iter 39930 || Loss: 1.2567 || timer: 0.0941 sec.
iter 39940 || Loss: 1.6828 || timer: 0.1111 sec.
iter 39950 || Loss: 1.1708 || timer: 0.0922 sec.
iter 39960 || Loss: 1.0147 || timer: 0.0904 sec.
iter 39970 || Loss: 1.6450 || timer: 0.0826 sec.
iter 39980 || Loss: 1.6195 || timer: 0.0924 sec.
iter 39990 || Loss: 1.4337 || timer: 0.0824 sec.
iter 40000 || Loss: 1.1926 || Saving state, iter: 40000
timer: 0.0892 sec.
iter 40010 || Loss: 1.4288 || timer: 0.0911 sec.
iter 40020 || Loss: 1.6422 || timer: 0.0819 sec.
iter 40030 || Loss: 1.3306 || timer: 0.0822 sec.
iter 40040 || Loss: 1.2085 || timer: 0.0875 sec.
iter 40050 || Loss: 1.5030 || timer: 0.0836 sec.
iter 40060 || Loss: 1.3700 || timer: 0.1014 sec.
iter 40070 || Loss: 0.9586 || timer: 0.0827 sec.
iter 40080 || Loss: 1.3794 || timer: 0.1050 sec.
iter 40090 || Loss: 1.5432 || timer: 0.0919 sec.
iter 40100 || Loss: 1.3060 || timer: 0.1051 sec.
iter 40110 || Loss: 1.1971 || timer: 0.0838 sec.
iter 40120 || Loss: 1.3071 || timer: 0.0918 sec.
iter 40130 || Loss: 1.1048 || timer: 0.0920 sec.
iter 40140 || Loss: 1.4334 || timer: 0.0979 sec.
iter 40150 || Loss: 1.1923 || timer: 0.0189 sec.
iter 40160 || Loss: 1.0383 || timer: 0.0914 sec.
iter 40170 || Loss: 1.2265 || timer: 0.1019 sec.
iter 40180 || Loss: 1.2042 || timer: 0.0905 sec.
iter 40190 || Loss: 1.0582 || timer: 0.0863 sec.
iter 40200 || Loss: 1.2263 || timer: 0.0963 sec.
iter 40210 || Loss: 1.2024 || timer: 0.0837 sec.
iter 40220 || Loss: 0.9712 || timer: 0.0866 sec.
iter 40230 || Loss: 1.7459 || timer: 0.0909 sec.
iter 40240 || Loss: 1.0553 || timer: 0.0900 sec.
iter 40250 || Loss: 1.7486 || timer: 0.0967 sec.
iter 40260 || Loss: 1.2923 || timer: 0.0771 sec.
iter 40270 || Loss: 1.2135 || timer: 0.0887 sec.
iter 40280 || Loss: 1.2473 || timer: 0.0822 sec.
iter 40290 || Loss: 1.4342 || timer: 0.1073 sec.
iter 40300 || Loss: 1.1362 || timer: 0.0835 sec.
iter 40310 || Loss: 1.3773 || timer: 0.0906 sec.
iter 40320 || Loss: 0.9125 || timer: 0.0829 sec.
iter 40330 || Loss: 1.5485 || timer: 0.0842 sec.
iter 40340 || Loss: 1.2744 || timer: 0.0823 sec.
iter 40350 || Loss: 1.2560 || timer: 0.0938 sec.
iter 40360 || Loss: 1.1138 || timer: 0.0825 sec.
iter 40370 || Loss: 1.1184 || timer: 0.0849 sec.
iter 40380 || Loss: 1.1050 || timer: 0.0884 sec.
iter 40390 || Loss: 1.1690 || timer: 0.0984 sec.
iter 40400 || Loss: 1.1837 || timer: 0.0910 sec.
iter 40410 || Loss: 1.3335 || timer: 0.0929 sec.
iter 40420 || Loss: 1.1780 || timer: 0.0828 sec.
iter 40430 || Loss: 1.9215 || timer: 0.1306 sec.
iter 40440 || Loss: 1.6915 || timer: 0.0899 sec.
iter 40450 || Loss: 1.2141 || timer: 0.0926 sec.
iter 40460 || Loss: 1.5104 || timer: 0.0962 sec.
iter 40470 || Loss: 1.3839 || timer: 0.0885 sec.
iter 40480 || Loss: 1.3949 || timer: 0.0167 sec.
iter 40490 || Loss: 0.9523 || timer: 0.0827 sec.
iter 40500 || Loss: 1.3667 || timer: 0.0862 sec.
iter 40510 || Loss: 1.2449 || timer: 0.0881 sec.
iter 40520 || Loss: 1.3305 || timer: 0.1072 sec.
iter 40530 || Loss: 1.3705 || timer: 0.0823 sec.
iter 40540 || Loss: 1.5295 || timer: 0.0815 sec.
iter 40550 || Loss: 1.3423 || timer: 0.0842 sec.
iter 40560 || Loss: 1.6296 || timer: 0.0909 sec.
iter 40570 || Loss: 1.4337 || timer: 0.0878 sec.
iter 40580 || Loss: 1.2593 || timer: 0.0952 sec.
iter 40590 || Loss: 1.1055 || timer: 0.1065 sec.
iter 40600 || Loss: 1.3782 || timer: 0.0904 sec.
iter 40610 || Loss: 1.2865 || timer: 0.0882 sec.
iter 40620 || Loss: 1.7425 || timer: 0.0917 sec.
iter 40630 || Loss: 1.3828 || timer: 0.0833 sec.
iter 40640 || Loss: 1.8393 || timer: 0.0931 sec.
iter 40650 || Loss: 1.2302 || timer: 0.0921 sec.
iter 40660 || Loss: 1.3297 || timer: 0.1248 sec.
iter 40670 || Loss: 0.9721 || timer: 0.0889 sec.
iter 40680 || Loss: 1.3081 || timer: 0.0903 sec.
iter 40690 || Loss: 1.2265 || timer: 0.0897 sec.
iter 40700 || Loss: 1.0295 || timer: 0.1022 sec.
iter 40710 || Loss: 1.0391 || timer: 0.0841 sec.
iter 40720 || Loss: 1.4094 || timer: 0.0913 sec.
iter 40730 || Loss: 1.6712 || timer: 0.1112 sec.
iter 40740 || Loss: 1.2056 || timer: 0.0907 sec.
iter 40750 || Loss: 1.1868 || timer: 0.0836 sec.
iter 40760 || Loss: 1.0625 || timer: 0.0968 sec.
iter 40770 || Loss: 1.3008 || timer: 0.0946 sec.
iter 40780 || Loss: 0.9350 || timer: 0.0914 sec.
iter 40790 || Loss: 1.0572 || timer: 0.0881 sec.
iter 40800 || Loss: 1.2939 || timer: 0.0831 sec.
iter 40810 || Loss: 1.2341 || timer: 0.0320 sec.
iter 40820 || Loss: 1.2087 || timer: 0.0825 sec.
iter 40830 || Loss: 1.1851 || timer: 0.0975 sec.
iter 40840 || Loss: 1.2985 || timer: 0.0887 sec.
iter 40850 || Loss: 1.2418 || timer: 0.0915 sec.
iter 40860 || Loss: 1.3099 || timer: 0.0908 sec.
iter 40870 || Loss: 1.4026 || timer: 0.1094 sec.
iter 40880 || Loss: 1.6413 || timer: 0.0910 sec.
iter 40890 || Loss: 1.2489 || timer: 0.0893 sec.
iter 40900 || Loss: 1.2873 || timer: 0.1098 sec.
iter 40910 || Loss: 1.4715 || timer: 0.0967 sec.
iter 40920 || Loss: 1.3592 || timer: 0.0893 sec.
iter 40930 || Loss: 1.7432 || timer: 0.1117 sec.
iter 40940 || Loss: 1.1433 || timer: 0.0925 sec.
iter 40950 || Loss: 1.5519 || timer: 0.0829 sec.
iter 40960 || Loss: 1.6362 || timer: 0.0875 sec.
iter 40970 || Loss: 1.7775 || timer: 0.1107 sec.
iter 40980 || Loss: 1.0276 || timer: 0.0903 sec.
iter 40990 || Loss: 1.9841 || timer: 0.0813 sec.
iter 41000 || Loss: 1.1601 || timer: 0.0841 sec.
iter 41010 || Loss: 1.5133 || timer: 0.0905 sec.
iter 41020 || Loss: 1.4736 || timer: 0.0880 sec.
iter 41030 || Loss: 1.4311 || timer: 0.1001 sec.
iter 41040 || Loss: 0.9459 || timer: 0.0902 sec.
iter 41050 || Loss: 1.5494 || timer: 0.1038 sec.
iter 41060 || Loss: 1.5830 || timer: 0.1043 sec.
iter 41070 || Loss: 1.3117 || timer: 0.0832 sec.
iter 41080 || Loss: 1.1033 || timer: 0.0853 sec.
iter 41090 || Loss: 1.0480 || timer: 0.0831 sec.
iter 41100 || Loss: 1.2017 || timer: 0.1013 sec.
iter 41110 || Loss: 1.1932 || timer: 0.0904 sec.
iter 41120 || Loss: 1.3684 || timer: 0.0923 sec.
iter 41130 || Loss: 0.9272 || timer: 0.0908 sec.
iter 41140 || Loss: 1.4240 || timer: 0.0244 sec.
iter 41150 || Loss: 1.6106 || timer: 0.0829 sec.
iter 41160 || Loss: 1.2195 || timer: 0.0916 sec.
iter 41170 || Loss: 1.7076 || timer: 0.1027 sec.
iter 41180 || Loss: 1.6691 || timer: 0.0897 sec.
iter 41190 || Loss: 1.2663 || timer: 0.0888 sec.
iter 41200 || Loss: 1.4741 || timer: 0.0836 sec.
iter 41210 || Loss: 1.8356 || timer: 0.0909 sec.
iter 41220 || Loss: 1.9355 || timer: 0.1021 sec.
iter 41230 || Loss: 1.2309 || timer: 0.0905 sec.
iter 41240 || Loss: 1.1808 || timer: 0.0944 sec.
iter 41250 || Loss: 1.4868 || timer: 0.0838 sec.
iter 41260 || Loss: 1.0432 || timer: 0.0895 sec.
iter 41270 || Loss: 1.0654 || timer: 0.0837 sec.
iter 41280 || Loss: 1.1645 || timer: 0.0874 sec.
iter 41290 || Loss: 1.0952 || timer: 0.0873 sec.
iter 41300 || Loss: 1.2242 || timer: 0.0901 sec.
iter 41310 || Loss: 1.7028 || timer: 0.0935 sec.
iter 41320 || Loss: 1.5301 || timer: 0.0862 sec.
iter 41330 || Loss: 1.6236 || timer: 0.0897 sec.
iter 41340 || Loss: 1.2829 || timer: 0.0830 sec.
iter 41350 || Loss: 1.3376 || timer: 0.0922 sec.
iter 41360 || Loss: 1.4248 || timer: 0.0901 sec.
iter 41370 || Loss: 1.2866 || timer: 0.0828 sec.
iter 41380 || Loss: 1.0528 || timer: 0.0884 sec.
iter 41390 || Loss: 1.4820 || timer: 0.0933 sec.
iter 41400 || Loss: 1.3559 || timer: 0.0910 sec.
iter 41410 || Loss: 1.4401 || timer: 0.0891 sec.
iter 41420 || Loss: 1.0818 || timer: 0.1025 sec.
iter 41430 || Loss: 1.6715 || timer: 0.0907 sec.
iter 41440 || Loss: 1.3107 || timer: 0.0902 sec.
iter 41450 || Loss: 1.6584 || timer: 0.0926 sec.
iter 41460 || Loss: 1.6946 || timer: 0.0835 sec.
iter 41470 || Loss: 1.3469 || timer: 0.0175 sec.
iter 41480 || Loss: 0.9423 || timer: 0.1052 sec.
iter 41490 || Loss: 1.3009 || timer: 0.1106 sec.
iter 41500 || Loss: 0.9668 || timer: 0.0834 sec.
iter 41510 || Loss: 1.3714 || timer: 0.0819 sec.
iter 41520 || Loss: 1.3643 || timer: 0.0875 sec.
iter 41530 || Loss: 1.2956 || timer: 0.0888 sec.
iter 41540 || Loss: 1.4399 || timer: 0.0833 sec.
iter 41550 || Loss: 1.3912 || timer: 0.0867 sec.
iter 41560 || Loss: 1.7891 || timer: 0.1003 sec.
iter 41570 || Loss: 1.2337 || timer: 0.0973 sec.
iter 41580 || Loss: 1.4277 || timer: 0.0829 sec.
iter 41590 || Loss: 1.5659 || timer: 0.0847 sec.
iter 41600 || Loss: 1.3823 || timer: 0.0896 sec.
iter 41610 || Loss: 0.9907 || timer: 0.0821 sec.
iter 41620 || Loss: 1.3441 || timer: 0.0908 sec.
iter 41630 || Loss: 1.3634 || timer: 0.0899 sec.
iter 41640 || Loss: 1.1851 || timer: 0.0892 sec.
iter 41650 || Loss: 1.0192 || timer: 0.0970 sec.
iter 41660 || Loss: 1.3972 || timer: 0.0823 sec.
iter 41670 || Loss: 2.0736 || timer: 0.0895 sec.
iter 41680 || Loss: 1.2680 || timer: 0.0923 sec.
iter 41690 || Loss: 0.7663 || timer: 0.0917 sec.
iter 41700 || Loss: 0.8451 || timer: 0.0838 sec.
iter 41710 || Loss: 1.5530 || timer: 0.0890 sec.
iter 41720 || Loss: 1.9689 || timer: 0.0911 sec.
iter 41730 || Loss: 0.9256 || timer: 0.0901 sec.
iter 41740 || Loss: 1.1958 || timer: 0.1005 sec.
iter 41750 || Loss: 1.2553 || timer: 0.0855 sec.
iter 41760 || Loss: 1.2099 || timer: 0.1140 sec.
iter 41770 || Loss: 1.2928 || timer: 0.0869 sec.
iter 41780 || Loss: 1.3076 || timer: 0.1014 sec.
iter 41790 || Loss: 1.2213 || timer: 0.0891 sec.
iter 41800 || Loss: 1.1143 || timer: 0.0173 sec.
iter 41810 || Loss: 0.5617 || timer: 0.0995 sec.
iter 41820 || Loss: 1.2925 || timer: 0.0904 sec.
iter 41830 || Loss: 1.4260 || timer: 0.0874 sec.
iter 41840 || Loss: 1.2284 || timer: 0.0915 sec.
iter 41850 || Loss: 1.0225 || timer: 0.1057 sec.
iter 41860 || Loss: 1.2235 || timer: 0.0921 sec.
iter 41870 || Loss: 0.9610 || timer: 0.0902 sec.
iter 41880 || Loss: 1.6107 || timer: 0.0907 sec.
iter 41890 || Loss: 1.1333 || timer: 0.0897 sec.
iter 41900 || Loss: 1.3167 || timer: 0.1193 sec.
iter 41910 || Loss: 1.3168 || timer: 0.0885 sec.
iter 41920 || Loss: 1.1002 || timer: 0.1103 sec.
iter 41930 || Loss: 1.0397 || timer: 0.0913 sec.
iter 41940 || Loss: 1.3261 || timer: 0.0888 sec.
iter 41950 || Loss: 1.5430 || timer: 0.0884 sec.
iter 41960 || Loss: 1.1214 || timer: 0.0839 sec.
iter 41970 || Loss: 1.0648 || timer: 0.0922 sec.
iter 41980 || Loss: 1.3711 || timer: 0.0850 sec.
iter 41990 || Loss: 1.4378 || timer: 0.0897 sec.
iter 42000 || Loss: 1.8891 || timer: 0.0885 sec.
iter 42010 || Loss: 1.2908 || timer: 0.0844 sec.
iter 42020 || Loss: 0.9976 || timer: 0.0829 sec.
iter 42030 || Loss: 1.1494 || timer: 0.0840 sec.
iter 42040 || Loss: 1.3681 || timer: 0.0845 sec.
iter 42050 || Loss: 1.1462 || timer: 0.0889 sec.
iter 42060 || Loss: 1.2740 || timer: 0.0876 sec.
iter 42070 || Loss: 1.0195 || timer: 0.0996 sec.
iter 42080 || Loss: 1.0567 || timer: 0.0907 sec.
iter 42090 || Loss: 1.3112 || timer: 0.0893 sec.
iter 42100 || Loss: 0.9556 || timer: 0.0826 sec.
iter 42110 || Loss: 2.2791 || timer: 0.0827 sec.
iter 42120 || Loss: 1.1346 || timer: 0.0911 sec.
iter 42130 || Loss: 1.5808 || timer: 0.0264 sec.
iter 42140 || Loss: 4.7864 || timer: 0.0926 sec.
iter 42150 || Loss: 1.1402 || timer: 0.0902 sec.
iter 42160 || Loss: 1.7049 || timer: 0.0894 sec.
iter 42170 || Loss: 1.4224 || timer: 0.1265 sec.
iter 42180 || Loss: 1.4567 || timer: 0.0825 sec.
iter 42190 || Loss: 1.5386 || timer: 0.0898 sec.
iter 42200 || Loss: 1.5379 || timer: 0.0873 sec.
iter 42210 || Loss: 1.3033 || timer: 0.0824 sec.
iter 42220 || Loss: 0.9319 || timer: 0.0920 sec.
iter 42230 || Loss: 1.1176 || timer: 0.1242 sec.
iter 42240 || Loss: 1.2497 || timer: 0.0862 sec.
iter 42250 || Loss: 1.2445 || timer: 0.0899 sec.
iter 42260 || Loss: 0.9733 || timer: 0.0906 sec.
iter 42270 || Loss: 1.2585 || timer: 0.1100 sec.
iter 42280 || Loss: 1.4018 || timer: 0.1026 sec.
iter 42290 || Loss: 1.1932 || timer: 0.0824 sec.
iter 42300 || Loss: 1.1715 || timer: 0.0909 sec.
iter 42310 || Loss: 0.9815 || timer: 0.0912 sec.
iter 42320 || Loss: 1.3208 || timer: 0.0897 sec.
iter 42330 || Loss: 1.2136 || timer: 0.1158 sec.
iter 42340 || Loss: 1.2069 || timer: 0.1058 sec.
iter 42350 || Loss: 1.1164 || timer: 0.0897 sec.
iter 42360 || Loss: 1.2786 || timer: 0.0836 sec.
iter 42370 || Loss: 1.6322 || timer: 0.1072 sec.
iter 42380 || Loss: 1.2964 || timer: 0.0914 sec.
iter 42390 || Loss: 1.3274 || timer: 0.1069 sec.
iter 42400 || Loss: 1.1670 || timer: 0.0970 sec.
iter 42410 || Loss: 1.5180 || timer: 0.0905 sec.
iter 42420 || Loss: 1.2864 || timer: 0.0906 sec.
iter 42430 || Loss: 1.4130 || timer: 0.0926 sec.
iter 42440 || Loss: 1.2318 || timer: 0.0869 sec.
iter 42450 || Loss: 1.2245 || timer: 0.0826 sec.
iter 42460 || Loss: 1.3162 || timer: 0.0267 sec.
iter 42470 || Loss: 3.6557 || timer: 0.0920 sec.
iter 42480 || Loss: 1.3599 || timer: 0.0918 sec.
iter 42490 || Loss: 1.4796 || timer: 0.0823 sec.
iter 42500 || Loss: 1.1934 || timer: 0.0836 sec.
iter 42510 || Loss: 1.2245 || timer: 0.0930 sec.
iter 42520 || Loss: 1.2618 || timer: 0.0843 sec.
iter 42530 || Loss: 1.3130 || timer: 0.0845 sec.
iter 42540 || Loss: 1.2872 || timer: 0.0844 sec.
iter 42550 || Loss: 1.4555 || timer: 0.0840 sec.
iter 42560 || Loss: 0.9300 || timer: 0.1259 sec.
iter 42570 || Loss: 1.5030 || timer: 0.0833 sec.
iter 42580 || Loss: 1.1480 || timer: 0.0842 sec.
iter 42590 || Loss: 1.0101 || timer: 0.0845 sec.
iter 42600 || Loss: 1.3663 || timer: 0.0918 sec.
iter 42610 || Loss: 1.7446 || timer: 0.0903 sec.
iter 42620 || Loss: 1.2788 || timer: 0.0851 sec.
iter 42630 || Loss: 1.1714 || timer: 0.0844 sec.
iter 42640 || Loss: 1.9811 || timer: 0.0911 sec.
iter 42650 || Loss: 2.1220 || timer: 0.0897 sec.
iter 42660 || Loss: 1.3353 || timer: 0.0961 sec.
iter 42670 || Loss: 1.2427 || timer: 0.0909 sec.
iter 42680 || Loss: 1.2739 || timer: 0.1091 sec.
iter 42690 || Loss: 1.8050 || timer: 0.0840 sec.
iter 42700 || Loss: 1.5773 || timer: 0.0932 sec.
iter 42710 || Loss: 1.7785 || timer: 0.0987 sec.
iter 42720 || Loss: 1.4882 || timer: 0.0824 sec.
iter 42730 || Loss: 1.9736 || timer: 0.0911 sec.
iter 42740 || Loss: 1.7315 || timer: 0.0905 sec.
iter 42750 || Loss: 1.2429 || timer: 0.0893 sec.
iter 42760 || Loss: 1.2457 || timer: 0.0924 sec.
iter 42770 || Loss: 0.9953 || timer: 0.0894 sec.
iter 42780 || Loss: 1.2433 || timer: 0.1073 sec.
iter 42790 || Loss: 1.4536 || timer: 0.0289 sec.
iter 42800 || Loss: 0.5570 || timer: 0.0863 sec.
iter 42810 || Loss: 1.5562 || timer: 0.0896 sec.
iter 42820 || Loss: 1.1208 || timer: 0.0901 sec.
iter 42830 || Loss: 1.3065 || timer: 0.0834 sec.
iter 42840 || Loss: 1.2669 || timer: 0.1155 sec.
iter 42850 || Loss: 1.2014 || timer: 0.0822 sec.
iter 42860 || Loss: 1.2929 || timer: 0.0927 sec.
iter 42870 || Loss: 1.2187 || timer: 0.0825 sec.
iter 42880 || Loss: 1.9337 || timer: 0.0922 sec.
iter 42890 || Loss: 1.5196 || timer: 0.1336 sec.
iter 42900 || Loss: 1.5029 || timer: 0.0908 sec.
iter 42910 || Loss: 0.9707 || timer: 0.0883 sec.
iter 42920 || Loss: 1.5075 || timer: 0.0903 sec.
iter 42930 || Loss: 1.3242 || timer: 0.0835 sec.
iter 42940 || Loss: 1.2790 || timer: 0.0838 sec.
iter 42950 || Loss: 1.4449 || timer: 0.0842 sec.
iter 42960 || Loss: 1.3090 || timer: 0.0933 sec.
iter 42970 || Loss: 1.0754 || timer: 0.0907 sec.
iter 42980 || Loss: 1.2345 || timer: 0.0887 sec.
iter 42990 || Loss: 1.0082 || timer: 0.0879 sec.
iter 43000 || Loss: 1.6040 || timer: 0.1048 sec.
iter 43010 || Loss: 1.7033 || timer: 0.0837 sec.
iter 43020 || Loss: 1.5199 || timer: 0.0900 sec.
iter 43030 || Loss: 0.8570 || timer: 0.0851 sec.
iter 43040 || Loss: 1.1136 || timer: 0.0904 sec.
iter 43050 || Loss: 0.9667 || timer: 0.0971 sec.
iter 43060 || Loss: 1.3417 || timer: 0.0844 sec.
iter 43070 || Loss: 1.4643 || timer: 0.0837 sec.
iter 43080 || Loss: 1.6378 || timer: 0.1143 sec.
iter 43090 || Loss: 1.3321 || timer: 0.1046 sec.
iter 43100 || Loss: 1.3116 || timer: 0.0876 sec.
iter 43110 || Loss: 1.4065 || timer: 0.0946 sec.
iter 43120 || Loss: 1.4243 || timer: 0.0246 sec.
iter 43130 || Loss: 2.0521 || timer: 0.0901 sec.
iter 43140 || Loss: 1.3243 || timer: 0.0897 sec.
iter 43150 || Loss: 1.2596 || timer: 0.1033 sec.
iter 43160 || Loss: 1.1138 || timer: 0.0918 sec.
iter 43170 || Loss: 4.8256 || timer: 0.1068 sec.
iter 43180 || Loss: 2.1005 || timer: 0.0828 sec.
iter 43190 || Loss: 1.8594 || timer: 0.0950 sec.
iter 43200 || Loss: 1.7569 || timer: 0.0873 sec.
iter 43210 || Loss: 1.5408 || timer: 0.0907 sec.
iter 43220 || Loss: 1.8416 || timer: 0.1032 sec.
iter 43230 || Loss: 1.1660 || timer: 0.0982 sec.
iter 43240 || Loss: 1.5395 || timer: 0.0825 sec.
iter 43250 || Loss: 1.2707 || timer: 0.0906 sec.
iter 43260 || Loss: 1.2545 || timer: 0.0849 sec.
iter 43270 || Loss: 1.1915 || timer: 0.0879 sec.
iter 43280 || Loss: 1.2878 || timer: 0.0904 sec.
iter 43290 || Loss: 1.4386 || timer: 0.0893 sec.
iter 43300 || Loss: 1.0988 || timer: 0.0900 sec.
iter 43310 || Loss: 1.3616 || timer: 0.0833 sec.
iter 43320 || Loss: 1.2090 || timer: 0.0921 sec.
iter 43330 || Loss: 0.9606 || timer: 0.0909 sec.
iter 43340 || Loss: 0.9795 || timer: 0.0860 sec.
iter 43350 || Loss: 1.3826 || timer: 0.0908 sec.
iter 43360 || Loss: 1.4338 || timer: 0.0904 sec.
iter 43370 || Loss: 1.4417 || timer: 0.0986 sec.
iter 43380 || Loss: 1.1415 || timer: 0.0828 sec.
iter 43390 || Loss: 1.0120 || timer: 0.1116 sec.
iter 43400 || Loss: 1.3422 || timer: 0.0928 sec.
iter 43410 || Loss: 1.3695 || timer: 0.0834 sec.
iter 43420 || Loss: 1.3853 || timer: 0.0900 sec.
iter 43430 || Loss: 1.0062 || timer: 0.0893 sec.
iter 43440 || Loss: 1.3558 || timer: 0.0850 sec.
iter 43450 || Loss: 1.4801 || timer: 0.0246 sec.
iter 43460 || Loss: 2.0183 || timer: 0.0926 sec.
iter 43470 || Loss: 1.8998 || timer: 0.0919 sec.
iter 43480 || Loss: 1.6185 || timer: 0.1148 sec.
iter 43490 || Loss: 1.7825 || timer: 0.0887 sec.
iter 43500 || Loss: 1.4753 || timer: 0.0888 sec.
iter 43510 || Loss: 1.2817 || timer: 0.1066 sec.
iter 43520 || Loss: 1.1620 || timer: 0.1008 sec.
iter 43530 || Loss: 1.4707 || timer: 0.0899 sec.
iter 43540 || Loss: 1.3189 || timer: 0.0995 sec.
iter 43550 || Loss: 1.3206 || timer: 0.0958 sec.
iter 43560 || Loss: 1.2412 || timer: 0.0832 sec.
iter 43570 || Loss: 1.7739 || timer: 0.0901 sec.
iter 43580 || Loss: 1.4505 || timer: 0.0889 sec.
iter 43590 || Loss: 1.1955 || timer: 0.0887 sec.
iter 43600 || Loss: 1.5042 || timer: 0.0908 sec.
iter 43610 || Loss: 1.3193 || timer: 0.1024 sec.
iter 43620 || Loss: 1.2576 || timer: 0.0907 sec.
iter 43630 || Loss: 1.4171 || timer: 0.0828 sec.
iter 43640 || Loss: 1.3317 || timer: 0.0895 sec.
iter 43650 || Loss: 1.1181 || timer: 0.0835 sec.
iter 43660 || Loss: 1.7209 || timer: 0.0916 sec.
iter 43670 || Loss: 1.4693 || timer: 0.0913 sec.
iter 43680 || Loss: 1.0548 || timer: 0.0875 sec.
iter 43690 || Loss: 1.5898 || timer: 0.0844 sec.
iter 43700 || Loss: 1.4475 || timer: 0.0915 sec.
iter 43710 || Loss: 1.4422 || timer: 0.0756 sec.
iter 43720 || Loss: 1.1147 || timer: 0.0831 sec.
iter 43730 || Loss: 1.9884 || timer: 0.0913 sec.
iter 43740 || Loss: 1.3990 || timer: 0.1154 sec.
iter 43750 || Loss: 1.6437 || timer: 0.0848 sec.
iter 43760 || Loss: 1.3565 || timer: 0.0973 sec.
iter 43770 || Loss: 1.1849 || timer: 0.0905 sec.
iter 43780 || Loss: 1.3592 || timer: 0.0215 sec.
iter 43790 || Loss: 0.6694 || timer: 0.0859 sec.
iter 43800 || Loss: 1.2346 || timer: 0.0870 sec.
iter 43810 || Loss: 1.0839 || timer: 0.0855 sec.
iter 43820 || Loss: 1.4112 || timer: 0.0884 sec.
iter 43830 || Loss: 1.3323 || timer: 0.0824 sec.
iter 43840 || Loss: 1.2787 || timer: 0.0903 sec.
iter 43850 || Loss: 1.0730 || timer: 0.0925 sec.
iter 43860 || Loss: 1.0529 || timer: 0.1052 sec.
iter 43870 || Loss: 1.4436 || timer: 0.0923 sec.
iter 43880 || Loss: 1.9919 || timer: 0.1270 sec.
iter 43890 || Loss: 1.4738 || timer: 0.1105 sec.
iter 43900 || Loss: 1.2482 || timer: 0.0916 sec.
iter 43910 || Loss: 1.4000 || timer: 0.0915 sec.
iter 43920 || Loss: 1.5156 || timer: 0.0897 sec.
iter 43930 || Loss: 1.2526 || timer: 0.0897 sec.
iter 43940 || Loss: 1.6572 || timer: 0.0930 sec.
iter 43950 || Loss: 1.0043 || timer: 0.0895 sec.
iter 43960 || Loss: 1.6984 || timer: 0.0837 sec.
iter 43970 || Loss: 1.4702 || timer: 0.1026 sec.
iter 43980 || Loss: 1.4067 || timer: 0.0903 sec.
iter 43990 || Loss: 1.0970 || timer: 0.0846 sec.
iter 44000 || Loss: 1.1597 || timer: 0.0883 sec.
iter 44010 || Loss: 1.3243 || timer: 0.0915 sec.
iter 44020 || Loss: 1.3694 || timer: 0.0940 sec.
iter 44030 || Loss: 1.2598 || timer: 0.0769 sec.
iter 44040 || Loss: 1.0560 || timer: 0.0905 sec.
iter 44050 || Loss: 1.0674 || timer: 0.0838 sec.
iter 44060 || Loss: 1.6831 || timer: 0.0908 sec.
iter 44070 || Loss: 1.4180 || timer: 0.0925 sec.
iter 44080 || Loss: 1.2691 || timer: 0.0843 sec.
iter 44090 || Loss: 1.7499 || timer: 0.0936 sec.
iter 44100 || Loss: 1.4549 || timer: 0.0765 sec.
iter 44110 || Loss: 1.9254 || timer: 0.0159 sec.
iter 44120 || Loss: 1.8644 || timer: 0.0929 sec.
iter 44130 || Loss: 1.2560 || timer: 0.0976 sec.
iter 44140 || Loss: 1.4123 || timer: 0.0893 sec.
iter 44150 || Loss: 1.5431 || timer: 0.0840 sec.
iter 44160 || Loss: 1.2186 || timer: 0.0936 sec.
iter 44170 || Loss: 1.7508 || timer: 0.0906 sec.
iter 44180 || Loss: 1.1614 || timer: 0.0957 sec.
iter 44190 || Loss: 1.5989 || timer: 0.0895 sec.
iter 44200 || Loss: 1.8375 || timer: 0.0861 sec.
iter 44210 || Loss: 1.0577 || timer: 0.0948 sec.
iter 44220 || Loss: 1.6065 || timer: 0.0870 sec.
iter 44230 || Loss: 1.6871 || timer: 0.0848 sec.
iter 44240 || Loss: 1.1843 || timer: 0.0913 sec.
iter 44250 || Loss: 1.3015 || timer: 0.0936 sec.
iter 44260 || Loss: 1.2805 || timer: 0.0890 sec.
iter 44270 || Loss: 1.4697 || timer: 0.0904 sec.
iter 44280 || Loss: 1.3006 || timer: 0.0978 sec.
iter 44290 || Loss: 1.3117 || timer: 0.0942 sec.
iter 44300 || Loss: 1.0789 || timer: 0.1014 sec.
iter 44310 || Loss: 1.5676 || timer: 0.0844 sec.
iter 44320 || Loss: 1.0954 || timer: 0.1022 sec.
iter 44330 || Loss: 1.2391 || timer: 0.0853 sec.
iter 44340 || Loss: 1.2640 || timer: 0.0886 sec.
iter 44350 || Loss: 1.4038 || timer: 0.0758 sec.
iter 44360 || Loss: 1.1361 || timer: 0.0926 sec.
iter 44370 || Loss: 1.1105 || timer: 0.0921 sec.
iter 44380 || Loss: 1.1406 || timer: 0.1083 sec.
iter 44390 || Loss: 1.3055 || timer: 0.0846 sec.
iter 44400 || Loss: 1.5296 || timer: 0.0829 sec.
iter 44410 || Loss: 1.4582 || timer: 0.0837 sec.
iter 44420 || Loss: 1.5061 || timer: 0.0849 sec.
iter 44430 || Loss: 1.2508 || timer: 0.0843 sec.
iter 44440 || Loss: 0.8893 || timer: 0.0220 sec.
iter 44450 || Loss: 0.5552 || timer: 0.0832 sec.
iter 44460 || Loss: 1.2380 || timer: 0.0838 sec.
iter 44470 || Loss: 1.2599 || timer: 0.0825 sec.
iter 44480 || Loss: 1.2012 || timer: 0.0819 sec.
iter 44490 || Loss: 1.2859 || timer: 0.0966 sec.
iter 44500 || Loss: 1.5783 || timer: 0.0819 sec.
iter 44510 || Loss: 1.8060 || timer: 0.1014 sec.
iter 44520 || Loss: 1.2596 || timer: 0.0927 sec.
iter 44530 || Loss: 1.2870 || timer: 0.0831 sec.
iter 44540 || Loss: 0.9726 || timer: 0.1193 sec.
iter 44550 || Loss: 1.3465 || timer: 0.0874 sec.
iter 44560 || Loss: 1.1503 || timer: 0.0847 sec.
iter 44570 || Loss: 1.3138 || timer: 0.0938 sec.
iter 44580 || Loss: 1.3662 || timer: 0.0912 sec.
iter 44590 || Loss: 1.0729 || timer: 0.0911 sec.
iter 44600 || Loss: 1.5389 || timer: 0.0837 sec.
iter 44610 || Loss: 1.0523 || timer: 0.0912 sec.
iter 44620 || Loss: 1.3488 || timer: 0.0968 sec.
iter 44630 || Loss: 1.1860 || timer: 0.0758 sec.
iter 44640 || Loss: 1.2115 || timer: 0.0847 sec.
iter 44650 || Loss: 1.3007 || timer: 0.0928 sec.
iter 44660 || Loss: 1.7204 || timer: 0.0840 sec.
iter 44670 || Loss: 1.3555 || timer: 0.0923 sec.
iter 44680 || Loss: 1.1682 || timer: 0.0841 sec.
iter 44690 || Loss: 1.3439 || timer: 0.0841 sec.
iter 44700 || Loss: 1.3269 || timer: 0.1062 sec.
iter 44710 || Loss: 1.1285 || timer: 0.0832 sec.
iter 44720 || Loss: 1.3974 || timer: 0.1040 sec.
iter 44730 || Loss: 0.8143 || timer: 0.1018 sec.
iter 44740 || Loss: 1.2890 || timer: 0.0852 sec.
iter 44750 || Loss: 1.2939 || timer: 0.0950 sec.
iter 44760 || Loss: 1.0416 || timer: 0.1018 sec.
iter 44770 || Loss: 1.0562 || timer: 0.0313 sec.
iter 44780 || Loss: 0.7586 || timer: 0.0776 sec.
iter 44790 || Loss: 1.0469 || timer: 0.0897 sec.
iter 44800 || Loss: 1.4001 || timer: 0.0863 sec.
iter 44810 || Loss: 1.6070 || timer: 0.0912 sec.
iter 44820 || Loss: 1.0698 || timer: 0.0944 sec.
iter 44830 || Loss: 1.2557 || timer: 0.0841 sec.
iter 44840 || Loss: 1.1179 || timer: 0.0838 sec.
iter 44850 || Loss: 1.2267 || timer: 0.0911 sec.
iter 44860 || Loss: 1.3840 || timer: 0.1024 sec.
iter 44870 || Loss: 1.2754 || timer: 0.0961 sec.
iter 44880 || Loss: 1.5035 || timer: 0.0840 sec.
iter 44890 || Loss: 1.3446 || timer: 0.0825 sec.
iter 44900 || Loss: 1.0362 || timer: 0.1107 sec.
iter 44910 || Loss: 1.2519 || timer: 0.0847 sec.
iter 44920 || Loss: 1.2897 || timer: 0.0916 sec.
iter 44930 || Loss: 1.2995 || timer: 0.0907 sec.
iter 44940 || Loss: 1.1428 || timer: 0.0963 sec.
iter 44950 || Loss: 2.7817 || timer: 0.1008 sec.
iter 44960 || Loss: 1.2357 || timer: 0.0850 sec.
iter 44970 || Loss: 1.2833 || timer: 0.0931 sec.
iter 44980 || Loss: 1.1543 || timer: 0.1001 sec.
iter 44990 || Loss: 1.5182 || timer: 0.0768 sec.
iter 45000 || Loss: 1.1574 || Saving state, iter: 45000
timer: 0.0931 sec.
iter 45010 || Loss: 1.2406 || timer: 0.0907 sec.
iter 45020 || Loss: 1.1066 || timer: 0.1006 sec.
iter 45030 || Loss: 1.3483 || timer: 0.0917 sec.
iter 45040 || Loss: 1.4659 || timer: 0.0915 sec.
iter 45050 || Loss: 1.2532 || timer: 0.1080 sec.
iter 45060 || Loss: 1.4889 || timer: 0.1058 sec.
iter 45070 || Loss: 1.2250 || timer: 0.0927 sec.
iter 45080 || Loss: 1.0812 || timer: 0.0920 sec.
iter 45090 || Loss: 1.3127 || timer: 0.1116 sec.
iter 45100 || Loss: 1.2463 || timer: 0.0275 sec.
iter 45110 || Loss: 1.2101 || timer: 0.0855 sec.
iter 45120 || Loss: 1.3429 || timer: 0.0790 sec.
iter 45130 || Loss: 0.9265 || timer: 0.0752 sec.
iter 45140 || Loss: 1.1809 || timer: 0.0770 sec.
iter 45150 || Loss: 1.4224 || timer: 0.0775 sec.
iter 45160 || Loss: 1.2961 || timer: 0.0977 sec.
iter 45170 || Loss: 1.6441 || timer: 0.0915 sec.
iter 45180 || Loss: 1.4201 || timer: 0.1026 sec.
iter 45190 || Loss: 1.2890 || timer: 0.1031 sec.
iter 45200 || Loss: 1.4003 || timer: 0.0976 sec.
iter 45210 || Loss: 1.6971 || timer: 0.1008 sec.
iter 45220 || Loss: 1.6611 || timer: 0.0888 sec.
iter 45230 || Loss: 1.1977 || timer: 0.0882 sec.
iter 45240 || Loss: 1.2639 || timer: 0.0942 sec.
iter 45250 || Loss: 1.1780 || timer: 0.0917 sec.
iter 45260 || Loss: 1.0558 || timer: 0.0824 sec.
iter 45270 || Loss: 1.2981 || timer: 0.0906 sec.
iter 45280 || Loss: 1.3345 || timer: 0.0847 sec.
iter 45290 || Loss: 0.9465 || timer: 0.1104 sec.
iter 45300 || Loss: 1.2167 || timer: 0.0921 sec.
iter 45310 || Loss: 1.2176 || timer: 0.0918 sec.
iter 45320 || Loss: 1.4126 || timer: 0.0849 sec.
iter 45330 || Loss: 1.2268 || timer: 0.0820 sec.
iter 45340 || Loss: 1.3007 || timer: 0.0906 sec.
iter 45350 || Loss: 1.4333 || timer: 0.0911 sec.
iter 45360 || Loss: 1.2369 || timer: 0.0918 sec.
iter 45370 || Loss: 1.0360 || timer: 0.1044 sec.
iter 45380 || Loss: 1.6505 || timer: 0.0879 sec.
iter 45390 || Loss: 1.6304 || timer: 0.0838 sec.
iter 45400 || Loss: 1.1357 || timer: 0.0917 sec.
iter 45410 || Loss: 1.4155 || timer: 0.0851 sec.
iter 45420 || Loss: 1.2080 || timer: 0.0992 sec.
iter 45430 || Loss: 1.4138 || timer: 0.0211 sec.
iter 45440 || Loss: 0.9149 || timer: 0.0932 sec.
iter 45450 || Loss: 1.2474 || timer: 0.0924 sec.
iter 45460 || Loss: 1.3589 || timer: 0.0913 sec.
iter 45470 || Loss: 1.2187 || timer: 0.0858 sec.
iter 45480 || Loss: 1.1180 || timer: 0.0878 sec.
iter 45490 || Loss: 1.3446 || timer: 0.0910 sec.
iter 45500 || Loss: 1.1802 || timer: 0.0907 sec.
iter 45510 || Loss: 1.0281 || timer: 0.0819 sec.
iter 45520 || Loss: 1.4912 || timer: 0.0854 sec.
iter 45530 || Loss: 1.5966 || timer: 0.0966 sec.
iter 45540 || Loss: 1.1733 || timer: 0.0765 sec.
iter 45550 || Loss: 1.5415 || timer: 0.0907 sec.
iter 45560 || Loss: 1.8469 || timer: 0.0917 sec.
iter 45570 || Loss: 1.0309 || timer: 0.0943 sec.
iter 45580 || Loss: 1.6149 || timer: 0.0910 sec.
iter 45590 || Loss: 1.7674 || timer: 0.0910 sec.
iter 45600 || Loss: 1.0165 || timer: 0.0867 sec.
iter 45610 || Loss: 1.7431 || timer: 0.0932 sec.
iter 45620 || Loss: 1.5088 || timer: 0.1077 sec.
iter 45630 || Loss: 1.5275 || timer: 0.0968 sec.
iter 45640 || Loss: 1.3389 || timer: 0.0894 sec.
iter 45650 || Loss: 1.1596 || timer: 0.0917 sec.
iter 45660 || Loss: 1.2812 || timer: 0.0732 sec.
iter 45670 || Loss: 0.9279 || timer: 0.0768 sec.
iter 45680 || Loss: 1.3148 || timer: 0.0904 sec.
iter 45690 || Loss: 1.1676 || timer: 0.0911 sec.
iter 45700 || Loss: 1.1953 || timer: 0.0919 sec.
iter 45710 || Loss: 1.3997 || timer: 0.0917 sec.
iter 45720 || Loss: 1.0195 || timer: 0.0909 sec.
iter 45730 || Loss: 1.3515 || timer: 0.0867 sec.
iter 45740 || Loss: 1.3939 || timer: 0.1062 sec.
iter 45750 || Loss: 1.0466 || timer: 0.0825 sec.
iter 45760 || Loss: 1.3700 || timer: 0.0189 sec.
iter 45770 || Loss: 1.9352 || timer: 0.0827 sec.
iter 45780 || Loss: 1.3106 || timer: 0.1058 sec.
iter 45790 || Loss: 1.3813 || timer: 0.0898 sec.
iter 45800 || Loss: 1.3203 || timer: 0.1011 sec.
iter 45810 || Loss: 1.2748 || timer: 0.0896 sec.
iter 45820 || Loss: 1.2786 || timer: 0.0897 sec.
iter 45830 || Loss: 1.1187 || timer: 0.0865 sec.
iter 45840 || Loss: 1.2112 || timer: 0.0840 sec.
iter 45850 || Loss: 1.3110 || timer: 0.0887 sec.
iter 45860 || Loss: 1.2912 || timer: 0.1183 sec.
iter 45870 || Loss: 1.4357 || timer: 0.0816 sec.
iter 45880 || Loss: 1.2967 || timer: 0.0845 sec.
iter 45890 || Loss: 1.1123 || timer: 0.0810 sec.
iter 45900 || Loss: 1.4006 || timer: 0.0839 sec.
iter 45910 || Loss: 1.2103 || timer: 0.0835 sec.
iter 45920 || Loss: 1.1916 || timer: 0.0999 sec.
iter 45930 || Loss: 1.3399 || timer: 0.0883 sec.
iter 45940 || Loss: 1.4328 || timer: 0.0814 sec.
iter 45950 || Loss: 1.0477 || timer: 0.0837 sec.
iter 45960 || Loss: 1.6694 || timer: 0.0855 sec.
iter 45970 || Loss: 1.0410 || timer: 0.0891 sec.
iter 45980 || Loss: 0.8020 || timer: 0.1020 sec.
iter 45990 || Loss: 1.0417 || timer: 0.0821 sec.
iter 46000 || Loss: 1.0901 || timer: 0.0910 sec.
iter 46010 || Loss: 1.2219 || timer: 0.0821 sec.
iter 46020 || Loss: 1.0403 || timer: 0.0906 sec.
iter 46030 || Loss: 1.2672 || timer: 0.0808 sec.
iter 46040 || Loss: 1.4853 || timer: 0.0822 sec.
iter 46050 || Loss: 1.1884 || timer: 0.0817 sec.
iter 46060 || Loss: 1.2987 || timer: 0.0829 sec.
iter 46070 || Loss: 1.4044 || timer: 0.0898 sec.
iter 46080 || Loss: 1.6087 || timer: 0.0874 sec.
iter 46090 || Loss: 1.1247 || timer: 0.0222 sec.
iter 46100 || Loss: 0.5400 || timer: 0.0815 sec.
iter 46110 || Loss: 1.6502 || timer: 0.0838 sec.
iter 46120 || Loss: 1.0591 || timer: 0.0827 sec.
iter 46130 || Loss: 1.1633 || timer: 0.0899 sec.
iter 46140 || Loss: 2.0769 || timer: 0.0926 sec.
iter 46150 || Loss: 1.1566 || timer: 0.0823 sec.
iter 46160 || Loss: 1.5527 || timer: 0.0828 sec.
iter 46170 || Loss: 1.0585 || timer: 0.0953 sec.
iter 46180 || Loss: 1.5156 || timer: 0.0839 sec.
iter 46190 || Loss: 0.8515 || timer: 0.0924 sec.
iter 46200 || Loss: 1.3386 || timer: 0.0919 sec.
iter 46210 || Loss: 1.2454 || timer: 0.0856 sec.
iter 46220 || Loss: 1.4348 || timer: 0.0896 sec.
iter 46230 || Loss: 1.1030 || timer: 0.1036 sec.
iter 46240 || Loss: 0.9729 || timer: 0.0903 sec.
iter 46250 || Loss: 0.9575 || timer: 0.0823 sec.
iter 46260 || Loss: 1.2607 || timer: 0.0854 sec.
iter 46270 || Loss: 1.1601 || timer: 0.0901 sec.
iter 46280 || Loss: 1.1043 || timer: 0.0913 sec.
iter 46290 || Loss: 1.1282 || timer: 0.0834 sec.
iter 46300 || Loss: 1.1330 || timer: 0.0927 sec.
iter 46310 || Loss: 0.9229 || timer: 0.0907 sec.
iter 46320 || Loss: 0.9355 || timer: 0.0834 sec.
iter 46330 || Loss: 1.2540 || timer: 0.0868 sec.
iter 46340 || Loss: 1.0435 || timer: 0.0896 sec.
iter 46350 || Loss: 1.2553 || timer: 0.0904 sec.
iter 46360 || Loss: 1.1652 || timer: 0.0985 sec.
iter 46370 || Loss: 1.0995 || timer: 0.0925 sec.
iter 46380 || Loss: 1.5554 || timer: 0.0815 sec.
iter 46390 || Loss: 1.0637 || timer: 0.0905 sec.
iter 46400 || Loss: 1.0822 || timer: 0.0812 sec.
iter 46410 || Loss: 1.1507 || timer: 0.0920 sec.
iter 46420 || Loss: 1.5525 || timer: 0.0187 sec.
iter 46430 || Loss: 2.1817 || timer: 0.0877 sec.
iter 46440 || Loss: 1.4316 || timer: 0.1059 sec.
iter 46450 || Loss: 0.9824 || timer: 0.0883 sec.
iter 46460 || Loss: 1.0107 || timer: 0.0895 sec.
iter 46470 || Loss: 1.3134 || timer: 0.0901 sec.
iter 46480 || Loss: 1.3250 || timer: 0.0840 sec.
iter 46490 || Loss: 1.3168 || timer: 0.0819 sec.
iter 46500 || Loss: 1.2193 || timer: 0.0882 sec.
iter 46510 || Loss: 1.4110 || timer: 0.0987 sec.
iter 46520 || Loss: 1.9590 || timer: 0.0941 sec.
iter 46530 || Loss: 0.9567 || timer: 0.0923 sec.
iter 46540 || Loss: 1.4163 || timer: 0.0917 sec.
iter 46550 || Loss: 1.0515 || timer: 0.0846 sec.
iter 46560 || Loss: 1.4286 || timer: 0.0829 sec.
iter 46570 || Loss: 1.5722 || timer: 0.0912 sec.
iter 46580 || Loss: 1.3611 || timer: 0.0828 sec.
iter 46590 || Loss: 1.5299 || timer: 0.0942 sec.
iter 46600 || Loss: 1.4274 || timer: 0.0910 sec.
iter 46610 || Loss: 2.0360 || timer: 0.0912 sec.
iter 46620 || Loss: 1.3777 || timer: 0.0934 sec.
iter 46630 || Loss: 1.2622 || timer: 0.1206 sec.
iter 46640 || Loss: 1.4332 || timer: 0.0818 sec.
iter 46650 || Loss: 1.5018 || timer: 0.0828 sec.
iter 46660 || Loss: 1.5119 || timer: 0.0872 sec.
iter 46670 || Loss: 1.0538 || timer: 0.1051 sec.
iter 46680 || Loss: 1.3180 || timer: 0.0899 sec.
iter 46690 || Loss: 1.3004 || timer: 0.0876 sec.
iter 46700 || Loss: 1.9875 || timer: 0.0994 sec.
iter 46710 || Loss: 1.4398 || timer: 0.0888 sec.
iter 46720 || Loss: 1.6053 || timer: 0.0895 sec.
iter 46730 || Loss: 0.8772 || timer: 0.0919 sec.
iter 46740 || Loss: 1.0894 || timer: 0.0890 sec.
iter 46750 || Loss: 1.0688 || timer: 0.0190 sec.
iter 46760 || Loss: 5.0668 || timer: 0.0829 sec.
iter 46770 || Loss: 1.8672 || timer: 0.0874 sec.
iter 46780 || Loss: 1.1516 || timer: 0.0825 sec.
iter 46790 || Loss: 1.1879 || timer: 0.0905 sec.
iter 46800 || Loss: 1.4535 || timer: 0.0882 sec.
iter 46810 || Loss: 1.3513 || timer: 0.1069 sec.
iter 46820 || Loss: 1.1594 || timer: 0.0902 sec.
iter 46830 || Loss: 1.1665 || timer: 0.0895 sec.
iter 46840 || Loss: 1.3500 || timer: 0.1568 sec.
iter 46850 || Loss: 1.0981 || timer: 0.1199 sec.
iter 46860 || Loss: 0.8802 || timer: 0.0839 sec.
iter 46870 || Loss: 1.3291 || timer: 0.0889 sec.
iter 46880 || Loss: 1.0694 || timer: 0.0911 sec.
iter 46890 || Loss: 1.2854 || timer: 0.0877 sec.
iter 46900 || Loss: 1.3780 || timer: 0.0839 sec.
iter 46910 || Loss: 1.3999 || timer: 0.0881 sec.
iter 46920 || Loss: 1.1581 || timer: 0.0888 sec.
iter 46930 || Loss: 1.7670 || timer: 0.0894 sec.
iter 46940 || Loss: 1.4947 || timer: 0.1298 sec.
iter 46950 || Loss: 1.9272 || timer: 0.0764 sec.
iter 46960 || Loss: 1.4463 || timer: 0.0903 sec.
iter 46970 || Loss: 1.5983 || timer: 0.0834 sec.
iter 46980 || Loss: 1.5050 || timer: 0.0870 sec.
iter 46990 || Loss: 1.0824 || timer: 0.0821 sec.
iter 47000 || Loss: 1.2645 || timer: 0.0824 sec.
iter 47010 || Loss: 1.1019 || timer: 0.0888 sec.
iter 47020 || Loss: 0.9794 || timer: 0.0809 sec.
iter 47030 || Loss: 1.3374 || timer: 0.0895 sec.
iter 47040 || Loss: 1.2797 || timer: 0.0878 sec.
iter 47050 || Loss: 1.2546 || timer: 0.0995 sec.
iter 47060 || Loss: 1.2028 || timer: 0.0908 sec.
iter 47070 || Loss: 1.2291 || timer: 0.1136 sec.
iter 47080 || Loss: 1.2599 || timer: 0.0247 sec.
iter 47090 || Loss: 1.0466 || timer: 0.0814 sec.
iter 47100 || Loss: 1.1201 || timer: 0.0924 sec.
iter 47110 || Loss: 1.5178 || timer: 0.0866 sec.
iter 47120 || Loss: 1.3739 || timer: 0.1035 sec.
iter 47130 || Loss: 1.3032 || timer: 0.0919 sec.
iter 47140 || Loss: 1.0673 || timer: 0.0824 sec.
iter 47150 || Loss: 1.1435 || timer: 0.0888 sec.
iter 47160 || Loss: 1.1432 || timer: 0.0914 sec.
iter 47170 || Loss: 0.9755 || timer: 0.0819 sec.
iter 47180 || Loss: 1.2599 || timer: 0.0999 sec.
iter 47190 || Loss: 1.6036 || timer: 0.0812 sec.
iter 47200 || Loss: 0.8918 || timer: 0.0982 sec.
iter 47210 || Loss: 0.8770 || timer: 0.0820 sec.
iter 47220 || Loss: 1.3363 || timer: 0.0985 sec.
iter 47230 || Loss: 1.6300 || timer: 0.0912 sec.
iter 47240 || Loss: 1.4267 || timer: 0.0912 sec.
iter 47250 || Loss: 1.1286 || timer: 0.0819 sec.
iter 47260 || Loss: 0.9978 || timer: 0.0824 sec.
iter 47270 || Loss: 1.1406 || timer: 0.0853 sec.
iter 47280 || Loss: 1.1856 || timer: 0.0815 sec.
iter 47290 || Loss: 1.0958 || timer: 0.0852 sec.
iter 47300 || Loss: 0.9996 || timer: 0.0824 sec.
iter 47310 || Loss: 1.2263 || timer: 0.0914 sec.
iter 47320 || Loss: 1.3416 || timer: 0.1015 sec.
iter 47330 || Loss: 1.0172 || timer: 0.0912 sec.
iter 47340 || Loss: 1.0254 || timer: 0.0877 sec.
iter 47350 || Loss: 1.5503 || timer: 0.0887 sec.
iter 47360 || Loss: 1.4955 || timer: 0.0888 sec.
iter 47370 || Loss: 1.1279 || timer: 0.0808 sec.
iter 47380 || Loss: 1.3166 || timer: 0.0804 sec.
iter 47390 || Loss: 1.0661 || timer: 0.1042 sec.
iter 47400 || Loss: 0.8938 || timer: 0.1127 sec.
iter 47410 || Loss: 1.4881 || timer: 0.0159 sec.
iter 47420 || Loss: 1.8205 || timer: 0.0818 sec.
iter 47430 || Loss: 1.4532 || timer: 0.1443 sec.
iter 47440 || Loss: 1.1077 || timer: 0.0879 sec.
iter 47450 || Loss: 1.1751 || timer: 0.0861 sec.
iter 47460 || Loss: 1.0241 || timer: 0.0878 sec.
iter 47470 || Loss: 1.0747 || timer: 0.0884 sec.
iter 47480 || Loss: 1.3452 || timer: 0.0905 sec.
iter 47490 || Loss: 2.2015 || timer: 0.0896 sec.
iter 47500 || Loss: 1.2272 || timer: 0.0820 sec.
iter 47510 || Loss: 1.1522 || timer: 0.1103 sec.
iter 47520 || Loss: 1.5408 || timer: 0.1200 sec.
iter 47530 || Loss: 0.9461 || timer: 0.1065 sec.
iter 47540 || Loss: 1.0409 || timer: 0.0836 sec.
iter 47550 || Loss: 1.1483 || timer: 0.0961 sec.
iter 47560 || Loss: 1.1850 || timer: 0.0878 sec.
iter 47570 || Loss: 1.3912 || timer: 0.0905 sec.
iter 47580 || Loss: 1.0148 || timer: 0.0899 sec.
iter 47590 || Loss: 1.9062 || timer: 0.0876 sec.
iter 47600 || Loss: 1.4794 || timer: 0.1280 sec.
iter 47610 || Loss: 1.4510 || timer: 0.0906 sec.
iter 47620 || Loss: 1.3298 || timer: 0.1020 sec.
iter 47630 || Loss: 1.2249 || timer: 0.1200 sec.
iter 47640 || Loss: 1.0100 || timer: 0.0817 sec.
iter 47650 || Loss: 1.1484 || timer: 0.1017 sec.
iter 47660 || Loss: 0.9628 || timer: 0.0899 sec.
iter 47670 || Loss: 0.9811 || timer: 0.1014 sec.
iter 47680 || Loss: 1.1927 || timer: 0.0807 sec.
iter 47690 || Loss: 1.7660 || timer: 0.1042 sec.
iter 47700 || Loss: 2.0970 || timer: 0.0808 sec.
iter 47710 || Loss: 1.3474 || timer: 0.0882 sec.
iter 47720 || Loss: 1.4444 || timer: 0.0823 sec.
iter 47730 || Loss: 1.1081 || timer: 0.0821 sec.
iter 47740 || Loss: 0.8618 || timer: 0.0219 sec.
iter 47750 || Loss: 0.4017 || timer: 0.0896 sec.
iter 47760 || Loss: 1.2667 || timer: 0.1046 sec.
iter 47770 || Loss: 1.0058 || timer: 0.0808 sec.
iter 47780 || Loss: 1.0024 || timer: 0.0924 sec.
iter 47790 || Loss: 1.6580 || timer: 0.0899 sec.
iter 47800 || Loss: 1.3446 || timer: 0.0834 sec.
iter 47810 || Loss: 1.3413 || timer: 0.0934 sec.
iter 47820 || Loss: 1.5117 || timer: 0.0838 sec.
iter 47830 || Loss: 1.4174 || timer: 0.0893 sec.
iter 47840 || Loss: 2.4244 || timer: 0.0990 sec.
iter 47850 || Loss: 1.5279 || timer: 0.0810 sec.
iter 47860 || Loss: 2.0144 || timer: 0.0915 sec.
iter 47870 || Loss: 1.4900 || timer: 0.0813 sec.
iter 47880 || Loss: 1.5395 || timer: 0.0903 sec.
iter 47890 || Loss: 1.6275 || timer: 0.0817 sec.
iter 47900 || Loss: 2.1759 || timer: 0.1076 sec.
iter 47910 || Loss: 1.6604 || timer: 0.0807 sec.
iter 47920 || Loss: 1.6488 || timer: 0.0940 sec.
iter 47930 || Loss: 1.4431 || timer: 0.0846 sec.
iter 47940 || Loss: 1.3510 || timer: 0.0811 sec.
iter 47950 || Loss: 1.5146 || timer: 0.1011 sec.
iter 47960 || Loss: 1.2290 || timer: 0.0900 sec.
iter 47970 || Loss: 1.0611 || timer: 0.0891 sec.
iter 47980 || Loss: 1.3489 || timer: 0.0800 sec.
iter 47990 || Loss: 1.0203 || timer: 0.0812 sec.
iter 48000 || Loss: 1.1257 || timer: 0.0888 sec.
iter 48010 || Loss: 1.1313 || timer: 0.1043 sec.
iter 48020 || Loss: 1.2594 || timer: 0.1004 sec.
iter 48030 || Loss: 1.6801 || timer: 0.0825 sec.
iter 48040 || Loss: 1.0156 || timer: 0.0857 sec.
iter 48050 || Loss: 1.1091 || timer: 0.1109 sec.
iter 48060 || Loss: 2.1399 || timer: 0.0812 sec.
iter 48070 || Loss: 1.2722 || timer: 0.0225 sec.
iter 48080 || Loss: 1.2394 || timer: 0.0918 sec.
iter 48090 || Loss: 1.4146 || timer: 0.0807 sec.
iter 48100 || Loss: 1.1430 || timer: 0.0815 sec.
iter 48110 || Loss: 1.0424 || timer: 0.0797 sec.
iter 48120 || Loss: 2.2790 || timer: 0.0816 sec.
iter 48130 || Loss: 1.7695 || timer: 0.0930 sec.
iter 48140 || Loss: 1.0121 || timer: 0.1091 sec.
iter 48150 || Loss: 1.8888 || timer: 0.0896 sec.
iter 48160 || Loss: 1.4642 || timer: 0.0824 sec.
iter 48170 || Loss: 1.6798 || timer: 0.0980 sec.
iter 48180 || Loss: 1.5105 || timer: 0.0848 sec.
iter 48190 || Loss: 1.6820 || timer: 0.0889 sec.
iter 48200 || Loss: 1.4578 || timer: 0.0815 sec.
iter 48210 || Loss: 1.1842 || timer: 0.1080 sec.
iter 48220 || Loss: 1.7540 || timer: 0.0876 sec.
iter 48230 || Loss: 1.4110 || timer: 0.0893 sec.
iter 48240 || Loss: 1.9499 || timer: 0.0827 sec.
iter 48250 || Loss: 1.7306 || timer: 0.1020 sec.
iter 48260 || Loss: 1.4736 || timer: 0.0870 sec.
iter 48270 || Loss: 1.3797 || timer: 0.0952 sec.
iter 48280 || Loss: 1.3579 || timer: 0.0883 sec.
iter 48290 || Loss: 1.2552 || timer: 0.0810 sec.
iter 48300 || Loss: 1.1994 || timer: 0.0874 sec.
iter 48310 || Loss: 0.8897 || timer: 0.0893 sec.
iter 48320 || Loss: 1.1753 || timer: 0.0901 sec.
iter 48330 || Loss: 1.3202 || timer: 0.0894 sec.
iter 48340 || Loss: 1.2539 || timer: 0.0885 sec.
iter 48350 || Loss: 1.3070 || timer: 0.0822 sec.
iter 48360 || Loss: 1.8485 || timer: 0.0893 sec.
iter 48370 || Loss: 1.4781 || timer: 0.0819 sec.
iter 48380 || Loss: 0.9346 || timer: 0.0911 sec.
iter 48390 || Loss: 1.3871 || timer: 0.1058 sec.
iter 48400 || Loss: 1.1816 || timer: 0.0274 sec.
iter 48410 || Loss: 1.0331 || timer: 0.0937 sec.
iter 48420 || Loss: 1.4103 || timer: 0.0819 sec.
iter 48430 || Loss: 1.0532 || timer: 0.1089 sec.
iter 48440 || Loss: 1.5399 || timer: 0.0865 sec.
iter 48450 || Loss: 1.6399 || timer: 0.0968 sec.
iter 48460 || Loss: 1.1762 || timer: 0.0888 sec.
iter 48470 || Loss: 1.1491 || timer: 0.0823 sec.
iter 48480 || Loss: 1.1049 || timer: 0.0886 sec.
iter 48490 || Loss: 1.3413 || timer: 0.0898 sec.
iter 48500 || Loss: 0.9805 || timer: 0.1353 sec.
iter 48510 || Loss: 1.6416 || timer: 0.0828 sec.
iter 48520 || Loss: 1.4751 || timer: 0.0909 sec.
iter 48530 || Loss: 1.2301 || timer: 0.0871 sec.
iter 48540 || Loss: 1.5582 || timer: 0.0840 sec.
iter 48550 || Loss: 1.1152 || timer: 0.0825 sec.
iter 48560 || Loss: 1.2248 || timer: 0.0831 sec.
iter 48570 || Loss: 1.7277 || timer: 0.1004 sec.
iter 48580 || Loss: 1.5451 || timer: 0.0819 sec.
iter 48590 || Loss: 1.3888 || timer: 0.1010 sec.
iter 48600 || Loss: 1.5326 || timer: 0.0966 sec.
iter 48610 || Loss: 1.6003 || timer: 0.0810 sec.
iter 48620 || Loss: 1.1370 || timer: 0.0956 sec.
iter 48630 || Loss: 1.1851 || timer: 0.0814 sec.
iter 48640 || Loss: 1.0077 || timer: 0.1032 sec.
iter 48650 || Loss: 1.1143 || timer: 0.0818 sec.
iter 48660 || Loss: 1.7817 || timer: 0.0850 sec.
iter 48670 || Loss: 1.1437 || timer: 0.0809 sec.
iter 48680 || Loss: 1.5011 || timer: 0.0817 sec.
iter 48690 || Loss: 0.9040 || timer: 0.0916 sec.
iter 48700 || Loss: 1.0805 || timer: 0.0816 sec.
iter 48710 || Loss: 1.2037 || timer: 0.1105 sec.
iter 48720 || Loss: 1.6723 || timer: 0.0931 sec.
iter 48730 || Loss: 1.2636 || timer: 0.0166 sec.
iter 48740 || Loss: 0.7547 || timer: 0.1017 sec.
iter 48750 || Loss: 0.9161 || timer: 0.0911 sec.
iter 48760 || Loss: 1.1467 || timer: 0.0817 sec.
iter 48770 || Loss: 1.0009 || timer: 0.0950 sec.
iter 48780 || Loss: 2.0043 || timer: 0.0819 sec.
iter 48790 || Loss: 1.2073 || timer: 0.0903 sec.
iter 48800 || Loss: 1.0526 || timer: 0.0812 sec.
iter 48810 || Loss: 1.2467 || timer: 0.0919 sec.
iter 48820 || Loss: 1.4781 || timer: 0.0895 sec.
iter 48830 || Loss: 1.2465 || timer: 0.1121 sec.
iter 48840 || Loss: 1.2427 || timer: 0.0980 sec.
iter 48850 || Loss: 1.2736 || timer: 0.1087 sec.
iter 48860 || Loss: 1.1853 || timer: 0.0895 sec.
iter 48870 || Loss: 1.2846 || timer: 0.0983 sec.
iter 48880 || Loss: 1.6885 || timer: 0.1088 sec.
iter 48890 || Loss: 1.2243 || timer: 0.0925 sec.
iter 48900 || Loss: 1.2795 || timer: 0.1205 sec.
iter 48910 || Loss: 1.0927 || timer: 0.1022 sec.
iter 48920 || Loss: 1.0387 || timer: 0.0804 sec.
iter 48930 || Loss: 1.0456 || timer: 0.0843 sec.
iter 48940 || Loss: 1.0648 || timer: 0.0920 sec.
iter 48950 || Loss: 1.2927 || timer: 0.0899 sec.
iter 48960 || Loss: 1.4831 || timer: 0.0822 sec.
iter 48970 || Loss: 1.3354 || timer: 0.1037 sec.
iter 48980 || Loss: 1.0640 || timer: 0.0900 sec.
iter 48990 || Loss: 1.4898 || timer: 0.0908 sec.
iter 49000 || Loss: 1.1152 || timer: 0.0816 sec.
iter 49010 || Loss: 1.7865 || timer: 0.0830 sec.
iter 49020 || Loss: 1.0508 || timer: 0.0866 sec.
iter 49030 || Loss: 1.4413 || timer: 0.1050 sec.
iter 49040 || Loss: 2.0724 || timer: 0.1298 sec.
iter 49050 || Loss: 1.7082 || timer: 0.0946 sec.
iter 49060 || Loss: 1.3281 || timer: 0.0174 sec.
iter 49070 || Loss: 3.8598 || timer: 0.0810 sec.
iter 49080 || Loss: 1.8630 || timer: 0.1068 sec.
iter 49090 || Loss: 1.6161 || timer: 0.1024 sec.
iter 49100 || Loss: 1.6239 || timer: 0.0849 sec.
iter 49110 || Loss: 1.1692 || timer: 0.0924 sec.
iter 49120 || Loss: 1.8666 || timer: 0.1240 sec.
iter 49130 || Loss: 1.4784 || timer: 0.1068 sec.
iter 49140 || Loss: 1.3277 || timer: 0.0892 sec.
iter 49150 || Loss: 1.7384 || timer: 0.1124 sec.
iter 49160 || Loss: 1.2483 || timer: 0.1054 sec.
iter 49170 || Loss: 1.2276 || timer: 0.0903 sec.
iter 49180 || Loss: 1.3333 || timer: 0.0899 sec.
iter 49190 || Loss: 1.2433 || timer: 0.0817 sec.
iter 49200 || Loss: 1.0049 || timer: 0.0909 sec.
iter 49210 || Loss: 1.6987 || timer: 0.1079 sec.
iter 49220 || Loss: 1.2638 || timer: 0.0880 sec.
iter 49230 || Loss: 1.3427 || timer: 0.0898 sec.
iter 49240 || Loss: 1.3628 || timer: 0.0889 sec.
iter 49250 || Loss: 1.5847 || timer: 0.0894 sec.
iter 49260 || Loss: 1.1010 || timer: 0.0958 sec.
iter 49270 || Loss: 1.3184 || timer: 0.0823 sec.
iter 49280 || Loss: 1.0565 || timer: 0.1193 sec.
iter 49290 || Loss: 0.9598 || timer: 0.0809 sec.
iter 49300 || Loss: 1.3785 || timer: 0.0930 sec.
iter 49310 || Loss: 2.4677 || timer: 0.0915 sec.
iter 49320 || Loss: 1.2933 || timer: 0.0806 sec.
iter 49330 || Loss: 1.2920 || timer: 0.0813 sec.
iter 49340 || Loss: 1.1764 || timer: 0.1063 sec.
iter 49350 || Loss: 1.4178 || timer: 0.0961 sec.
iter 49360 || Loss: 1.2997 || timer: 0.0869 sec.
iter 49370 || Loss: 1.7235 || timer: 0.0808 sec.
iter 49380 || Loss: 1.3568 || timer: 0.0830 sec.
iter 49390 || Loss: 1.2440 || timer: 0.0162 sec.
iter 49400 || Loss: 3.9818 || timer: 0.1181 sec.
iter 49410 || Loss: 1.7297 || timer: 0.1043 sec.
iter 49420 || Loss: 1.9334 || timer: 0.0835 sec.
iter 49430 || Loss: 1.6392 || timer: 0.0968 sec.
iter 49440 || Loss: 1.2106 || timer: 0.0921 sec.
iter 49450 || Loss: 1.3239 || timer: 0.0915 sec.
iter 49460 || Loss: 1.1445 || timer: 0.0873 sec.
iter 49470 || Loss: 1.5263 || timer: 0.0832 sec.
iter 49480 || Loss: 1.2658 || timer: 0.0914 sec.
iter 49490 || Loss: 1.5556 || timer: 0.1179 sec.
iter 49500 || Loss: 1.0891 || timer: 0.0883 sec.
iter 49510 || Loss: 1.6039 || timer: 0.0924 sec.
iter 49520 || Loss: 1.3837 || timer: 0.0889 sec.
iter 49530 || Loss: 1.5254 || timer: 0.0910 sec.
iter 49540 || Loss: 0.8777 || timer: 0.1212 sec.
iter 49550 || Loss: 1.8993 || timer: 0.0806 sec.
iter 49560 || Loss: 1.4342 || timer: 0.0868 sec.
iter 49570 || Loss: 1.5359 || timer: 0.0893 sec.
iter 49580 || Loss: 1.1060 || timer: 0.0794 sec.
iter 49590 || Loss: 1.4047 || timer: 0.0839 sec.
iter 49600 || Loss: 2.2689 || timer: 0.0819 sec.
iter 49610 || Loss: 1.2906 || timer: 0.0884 sec.
iter 49620 || Loss: 1.7124 || timer: 0.0855 sec.
iter 49630 || Loss: 0.9074 || timer: 0.0895 sec.
iter 49640 || Loss: 1.3892 || timer: 0.0903 sec.
iter 49650 || Loss: 1.0934 || timer: 0.0873 sec.
iter 49660 || Loss: 1.2528 || timer: 0.0906 sec.
iter 49670 || Loss: 1.1494 || timer: 0.0999 sec.
iter 49680 || Loss: 1.1353 || timer: 0.0917 sec.
iter 49690 || Loss: 1.3647 || timer: 0.0911 sec.
iter 49700 || Loss: 1.2184 || timer: 0.0809 sec.
iter 49710 || Loss: 1.1890 || timer: 0.0813 sec.
iter 49720 || Loss: 0.9755 || timer: 0.0160 sec.
iter 49730 || Loss: 4.0482 || timer: 0.0904 sec.
iter 49740 || Loss: 1.1298 || timer: 0.0895 sec.
iter 49750 || Loss: 1.3793 || timer: 0.0823 sec.
iter 49760 || Loss: 1.0383 || timer: 0.0906 sec.
iter 49770 || Loss: 1.0414 || timer: 0.1159 sec.
iter 49780 || Loss: 1.5984 || timer: 0.0818 sec.
iter 49790 || Loss: 1.1061 || timer: 0.0918 sec.
iter 49800 || Loss: 2.0322 || timer: 0.0808 sec.
iter 49810 || Loss: 1.2332 || timer: 0.0830 sec.
iter 49820 || Loss: 1.4687 || timer: 0.1145 sec.
iter 49830 || Loss: 1.3324 || timer: 0.0931 sec.
iter 49840 || Loss: 1.3977 || timer: 0.0904 sec.
iter 49850 || Loss: 1.3286 || timer: 0.0868 sec.
iter 49860 || Loss: 1.0643 || timer: 0.0820 sec.
iter 49870 || Loss: 2.0224 || timer: 0.0807 sec.
iter 49880 || Loss: 1.5223 || timer: 0.1010 sec.
iter 49890 || Loss: 1.5369 || timer: 0.0919 sec.
iter 49900 || Loss: 1.8699 || timer: 0.1147 sec.
iter 49910 || Loss: 1.6268 || timer: 0.0888 sec.
iter 49920 || Loss: 1.2861 || timer: 0.0910 sec.
iter 49930 || Loss: 1.3969 || timer: 0.0894 sec.
iter 49940 || Loss: 1.1065 || timer: 0.0889 sec.
iter 49950 || Loss: 1.5823 || timer: 0.0864 sec.
iter 49960 || Loss: 1.3499 || timer: 0.1377 sec.
iter 49970 || Loss: 1.1545 || timer: 0.0852 sec.
iter 49980 || Loss: 1.1692 || timer: 0.0816 sec.
iter 49990 || Loss: 1.3022 || timer: 0.0912 sec.
iter 50000 || Loss: 1.5305 || Saving state, iter: 50000
timer: 0.1085 sec.
iter 50010 || Loss: 1.1648 || timer: 0.0808 sec.
iter 50020 || Loss: 1.0303 || timer: 0.0821 sec.
iter 50030 || Loss: 1.4324 || timer: 0.0983 sec.
iter 50040 || Loss: 0.9296 || timer: 0.0851 sec.
iter 50050 || Loss: 1.1328 || timer: 0.0224 sec.
iter 50060 || Loss: 0.6235 || timer: 0.0834 sec.
iter 50070 || Loss: 1.0374 || timer: 0.0897 sec.
iter 50080 || Loss: 1.3822 || timer: 0.0884 sec.
iter 50090 || Loss: 1.1443 || timer: 0.0816 sec.
iter 50100 || Loss: 1.4020 || timer: 0.0855 sec.
iter 50110 || Loss: 1.4925 || timer: 0.0816 sec.
iter 50120 || Loss: 1.2252 || timer: 0.0934 sec.
iter 50130 || Loss: 1.1265 || timer: 0.0901 sec.
iter 50140 || Loss: 1.1531 || timer: 0.0884 sec.
iter 50150 || Loss: 1.1451 || timer: 0.0932 sec.
iter 50160 || Loss: 1.1194 || timer: 0.0979 sec.
iter 50170 || Loss: 1.2880 || timer: 0.0907 sec.
iter 50180 || Loss: 1.4034 || timer: 0.0861 sec.
iter 50190 || Loss: 1.1305 || timer: 0.0882 sec.
iter 50200 || Loss: 0.8899 || timer: 0.1137 sec.
iter 50210 || Loss: 1.5162 || timer: 0.0900 sec.
iter 50220 || Loss: 1.2749 || timer: 0.1010 sec.
iter 50230 || Loss: 1.0762 || timer: 0.0994 sec.
iter 50240 || Loss: 1.2967 || timer: 0.0917 sec.
iter 50250 || Loss: 1.2547 || timer: 0.0822 sec.
iter 50260 || Loss: 0.9237 || timer: 0.0888 sec.
iter 50270 || Loss: 1.2361 || timer: 0.0823 sec.
iter 50280 || Loss: 0.9410 || timer: 0.0823 sec.
iter 50290 || Loss: 1.0955 || timer: 0.0899 sec.
iter 50300 || Loss: 1.6292 || timer: 0.0820 sec.
iter 50310 || Loss: 1.5416 || timer: 0.0891 sec.
iter 50320 || Loss: 1.1462 || timer: 0.0907 sec.
iter 50330 || Loss: 0.9094 || timer: 0.0882 sec.
iter 50340 || Loss: 1.2102 || timer: 0.0810 sec.
iter 50350 || Loss: 1.4046 || timer: 0.0814 sec.
iter 50360 || Loss: 1.1738 || timer: 0.0878 sec.
iter 50370 || Loss: 1.0013 || timer: 0.0894 sec.
iter 50380 || Loss: 1.5800 || timer: 0.0243 sec.
iter 50390 || Loss: 0.1678 || timer: 0.0900 sec.
iter 50400 || Loss: 1.0966 || timer: 0.0903 sec.
iter 50410 || Loss: 1.3516 || timer: 0.0966 sec.
iter 50420 || Loss: 1.0465 || timer: 0.0838 sec.
iter 50430 || Loss: 1.1717 || timer: 0.0913 sec.
iter 50440 || Loss: 1.6186 || timer: 0.0751 sec.
iter 50450 || Loss: 1.0561 || timer: 0.0825 sec.
iter 50460 || Loss: 1.2334 || timer: 0.0933 sec.
iter 50470 || Loss: 1.9409 || timer: 0.1040 sec.
iter 50480 || Loss: 1.0220 || timer: 0.0937 sec.
iter 50490 || Loss: 1.0835 || timer: 0.0978 sec.
iter 50500 || Loss: 1.3189 || timer: 0.0905 sec.
iter 50510 || Loss: 1.0378 || timer: 0.0931 sec.
iter 50520 || Loss: 1.2957 || timer: 0.0879 sec.
iter 50530 || Loss: 0.9932 || timer: 0.0888 sec.
iter 50540 || Loss: 1.1114 || timer: 0.0901 sec.
iter 50550 || Loss: 1.3920 || timer: 0.0831 sec.
iter 50560 || Loss: 1.1790 || timer: 0.0917 sec.
iter 50570 || Loss: 1.1127 || timer: 0.0825 sec.
iter 50580 || Loss: 1.5743 || timer: 0.0909 sec.
iter 50590 || Loss: 1.1080 || timer: 0.1062 sec.
iter 50600 || Loss: 1.2182 || timer: 0.0883 sec.
iter 50610 || Loss: 1.0403 || timer: 0.0877 sec.
iter 50620 || Loss: 1.1415 || timer: 0.1031 sec.
iter 50630 || Loss: 1.8273 || timer: 0.1116 sec.
iter 50640 || Loss: 1.2132 || timer: 0.0932 sec.
iter 50650 || Loss: 1.9005 || timer: 0.0843 sec.
iter 50660 || Loss: 1.1338 || timer: 0.0916 sec.
iter 50670 || Loss: 1.9696 || timer: 0.0844 sec.
iter 50680 || Loss: 1.2911 || timer: 0.0818 sec.
iter 50690 || Loss: 1.1390 || timer: 0.0991 sec.
iter 50700 || Loss: 1.4097 || timer: 0.1280 sec.
iter 50710 || Loss: 1.5602 || timer: 0.0193 sec.
iter 50720 || Loss: 1.4607 || timer: 0.1141 sec.
iter 50730 || Loss: 0.8292 || timer: 0.1201 sec.
iter 50740 || Loss: 1.1410 || timer: 0.0893 sec.
iter 50750 || Loss: 1.5080 || timer: 0.0969 sec.
iter 50760 || Loss: 1.1683 || timer: 0.0929 sec.
iter 50770 || Loss: 1.3065 || timer: 0.0817 sec.
iter 50780 || Loss: 1.1127 || timer: 0.0808 sec.
iter 50790 || Loss: 1.3662 || timer: 0.0809 sec.
iter 50800 || Loss: 1.2038 || timer: 0.0909 sec.
iter 50810 || Loss: 1.5317 || timer: 0.0984 sec.
iter 50820 || Loss: 1.0787 || timer: 0.0754 sec.
iter 50830 || Loss: 1.1797 || timer: 0.1016 sec.
iter 50840 || Loss: 1.7298 || timer: 0.0748 sec.
iter 50850 || Loss: 0.9770 || timer: 0.0824 sec.
iter 50860 || Loss: 1.5621 || timer: 0.0911 sec.
iter 50870 || Loss: 2.1165 || timer: 0.1040 sec.
iter 50880 || Loss: 1.5919 || timer: 0.0819 sec.
iter 50890 || Loss: 1.8794 || timer: 0.0756 sec.
iter 50900 || Loss: 1.5659 || timer: 0.0883 sec.
iter 50910 || Loss: 1.3922 || timer: 0.0879 sec.
iter 50920 || Loss: 1.0757 || timer: 0.0983 sec.
iter 50930 || Loss: 1.2575 || timer: 0.0896 sec.
iter 50940 || Loss: 1.2401 || timer: 0.0825 sec.
iter 50950 || Loss: 1.1113 || timer: 0.0827 sec.
iter 50960 || Loss: 1.1082 || timer: 0.0832 sec.
iter 50970 || Loss: 0.9667 || timer: 0.0912 sec.
iter 50980 || Loss: 1.0604 || timer: 0.0835 sec.
iter 50990 || Loss: 1.1727 || timer: 0.1062 sec.
iter 51000 || Loss: 0.9291 || timer: 0.0892 sec.
iter 51010 || Loss: 0.9017 || timer: 0.0829 sec.
iter 51020 || Loss: 1.0788 || timer: 0.0905 sec.
iter 51030 || Loss: 1.9842 || timer: 0.1033 sec.
iter 51040 || Loss: 1.3021 || timer: 0.0292 sec.
iter 51050 || Loss: 0.8298 || timer: 0.0899 sec.
iter 51060 || Loss: 1.0442 || timer: 0.0819 sec.
iter 51070 || Loss: 1.1373 || timer: 0.0898 sec.
iter 51080 || Loss: 0.8737 || timer: 0.0895 sec.
iter 51090 || Loss: 1.4819 || timer: 0.0832 sec.
iter 51100 || Loss: 1.0598 || timer: 0.0899 sec.
iter 51110 || Loss: 1.5232 || timer: 0.0898 sec.
iter 51120 || Loss: 1.2905 || timer: 0.1013 sec.
iter 51130 || Loss: 1.1340 || timer: 0.0907 sec.
iter 51140 || Loss: 1.3590 || timer: 0.1119 sec.
iter 51150 || Loss: 1.1822 || timer: 0.0902 sec.
iter 51160 || Loss: 0.9301 || timer: 0.0858 sec.
iter 51170 || Loss: 1.1915 || timer: 0.0878 sec.
iter 51180 || Loss: 0.8266 || timer: 0.0755 sec.
iter 51190 || Loss: 1.6081 || timer: 0.0814 sec.
iter 51200 || Loss: 1.0727 || timer: 0.0818 sec.
iter 51210 || Loss: 1.1289 || timer: 0.0910 sec.
iter 51220 || Loss: 1.5973 || timer: 0.0876 sec.
iter 51230 || Loss: 1.3446 || timer: 0.0874 sec.
iter 51240 || Loss: 1.4104 || timer: 0.0812 sec.
iter 51250 || Loss: 1.1013 || timer: 0.0892 sec.
iter 51260 || Loss: 1.0536 || timer: 0.0883 sec.
iter 51270 || Loss: 1.2525 || timer: 0.0823 sec.
iter 51280 || Loss: 1.5565 || timer: 0.0958 sec.
iter 51290 || Loss: 1.0765 || timer: 0.1025 sec.
iter 51300 || Loss: 1.0361 || timer: 0.1036 sec.
iter 51310 || Loss: 0.9967 || timer: 0.0870 sec.
iter 51320 || Loss: 1.2864 || timer: 0.0894 sec.
iter 51330 || Loss: 1.6925 || timer: 0.0820 sec.
iter 51340 || Loss: 1.1356 || timer: 0.0932 sec.
iter 51350 || Loss: 1.5063 || timer: 0.0864 sec.
iter 51360 || Loss: 1.2038 || timer: 0.0892 sec.
iter 51370 || Loss: 1.0873 || timer: 0.0245 sec.
iter 51380 || Loss: 0.8988 || timer: 0.0878 sec.
iter 51390 || Loss: 1.5127 || timer: 0.0880 sec.
iter 51400 || Loss: 1.4238 || timer: 0.0967 sec.
iter 51410 || Loss: 1.5906 || timer: 0.0920 sec.
iter 51420 || Loss: 1.5473 || timer: 0.0964 sec.
iter 51430 || Loss: 1.3628 || timer: 0.0902 sec.
iter 51440 || Loss: 1.4837 || timer: 0.1056 sec.
iter 51450 || Loss: 1.3889 || timer: 0.0898 sec.
iter 51460 || Loss: 1.3702 || timer: 0.0893 sec.
iter 51470 || Loss: 1.0272 || timer: 0.0955 sec.
iter 51480 || Loss: 0.9516 || timer: 0.0774 sec.
iter 51490 || Loss: 1.2568 || timer: 0.0824 sec.
iter 51500 || Loss: 1.0716 || timer: 0.0772 sec.
iter 51510 || Loss: 0.9412 || timer: 0.0989 sec.
iter 51520 || Loss: 1.4883 || timer: 0.0900 sec.
iter 51530 || Loss: 1.4339 || timer: 0.1047 sec.
iter 51540 || Loss: 1.4090 || timer: 0.1039 sec.
iter 51550 || Loss: 1.3442 || timer: 0.0939 sec.
iter 51560 || Loss: 1.2850 || timer: 0.0821 sec.
iter 51570 || Loss: 0.8007 || timer: 0.0821 sec.
iter 51580 || Loss: 1.4009 || timer: 0.0754 sec.
iter 51590 || Loss: 1.5020 || timer: 0.0905 sec.
iter 51600 || Loss: 1.0198 || timer: 0.0905 sec.
iter 51610 || Loss: 1.1649 || timer: 0.0923 sec.
iter 51620 || Loss: 1.3207 || timer: 0.0984 sec.
iter 51630 || Loss: 1.2228 || timer: 0.0955 sec.
iter 51640 || Loss: 1.1640 || timer: 0.0854 sec.
iter 51650 || Loss: 1.2315 || timer: 0.0749 sec.
iter 51660 || Loss: 1.1459 || timer: 0.1035 sec.
iter 51670 || Loss: 0.9357 || timer: 0.0919 sec.
iter 51680 || Loss: 1.2361 || timer: 0.0918 sec.
iter 51690 || Loss: 1.3295 || timer: 0.0824 sec.
iter 51700 || Loss: 1.2050 || timer: 0.0190 sec.
iter 51710 || Loss: 2.0688 || timer: 0.0896 sec.
iter 51720 || Loss: 1.5604 || timer: 0.0832 sec.
iter 51730 || Loss: 1.5098 || timer: 0.0897 sec.
iter 51740 || Loss: 1.3930 || timer: 0.0829 sec.
iter 51750 || Loss: 1.5414 || timer: 0.0933 sec.
iter 51760 || Loss: 1.4560 || timer: 0.0896 sec.
iter 51770 || Loss: 1.0470 || timer: 0.0867 sec.
iter 51780 || Loss: 0.9905 || timer: 0.0845 sec.
iter 51790 || Loss: 1.7794 || timer: 0.0891 sec.
iter 51800 || Loss: 1.4152 || timer: 0.0947 sec.
iter 51810 || Loss: 1.2478 || timer: 0.1065 sec.
iter 51820 || Loss: 1.8539 || timer: 0.0882 sec.
iter 51830 || Loss: 1.3069 || timer: 0.0914 sec.
iter 51840 || Loss: 1.1069 || timer: 0.0823 sec.
iter 51850 || Loss: 1.2255 || timer: 0.0820 sec.
iter 51860 || Loss: 0.8450 || timer: 0.0892 sec.
iter 51870 || Loss: 1.6919 || timer: 0.0919 sec.
iter 51880 || Loss: 1.3488 || timer: 0.0904 sec.
iter 51890 || Loss: 1.2341 || timer: 0.0740 sec.
iter 51900 || Loss: 0.9297 || timer: 0.1012 sec.
iter 51910 || Loss: 1.3009 || timer: 0.0861 sec.
iter 51920 || Loss: 1.3246 || timer: 0.0887 sec.
iter 51930 || Loss: 0.9827 || timer: 0.0897 sec.
iter 51940 || Loss: 1.1105 || timer: 0.1113 sec.
iter 51950 || Loss: 1.2719 || timer: 0.1043 sec.
iter 51960 || Loss: 1.1839 || timer: 0.1034 sec.
iter 51970 || Loss: 1.4635 || timer: 0.1060 sec.
iter 51980 || Loss: 1.4410 || timer: 0.0875 sec.
iter 51990 || Loss: 0.9458 || timer: 0.1103 sec.
iter 52000 || Loss: 1.0750 || timer: 0.0822 sec.
iter 52010 || Loss: 1.4398 || timer: 0.0969 sec.
iter 52020 || Loss: 1.3553 || timer: 0.0753 sec.
iter 52030 || Loss: 1.1021 || timer: 0.0251 sec.
iter 52040 || Loss: 0.5872 || timer: 0.1083 sec.
iter 52050 || Loss: 1.2536 || timer: 0.0817 sec.
iter 52060 || Loss: 1.2777 || timer: 0.0881 sec.
iter 52070 || Loss: 1.2105 || timer: 0.0896 sec.
iter 52080 || Loss: 1.2417 || timer: 0.0854 sec.
iter 52090 || Loss: 1.3615 || timer: 0.0822 sec.
iter 52100 || Loss: 1.3870 || timer: 0.1027 sec.
iter 52110 || Loss: 1.1347 || timer: 0.0902 sec.
iter 52120 || Loss: 1.4054 || timer: 0.0936 sec.
iter 52130 || Loss: 1.4907 || timer: 0.1245 sec.
iter 52140 || Loss: 1.3065 || timer: 0.0975 sec.
iter 52150 || Loss: 1.0334 || timer: 0.0897 sec.
iter 52160 || Loss: 1.0401 || timer: 0.0849 sec.
iter 52170 || Loss: 1.0354 || timer: 0.0968 sec.
iter 52180 || Loss: 1.2313 || timer: 0.0904 sec.
iter 52190 || Loss: 1.3623 || timer: 0.0805 sec.
iter 52200 || Loss: 0.8611 || timer: 0.0905 sec.
iter 52210 || Loss: 1.2654 || timer: 0.0854 sec.
iter 52220 || Loss: 1.3764 || timer: 0.0811 sec.
iter 52230 || Loss: 0.8257 || timer: 0.0899 sec.
iter 52240 || Loss: 1.6465 || timer: 0.0926 sec.
iter 52250 || Loss: 1.3693 || timer: 0.0823 sec.
iter 52260 || Loss: 1.3372 || timer: 0.0820 sec.
iter 52270 || Loss: 1.2167 || timer: 0.0898 sec.
iter 52280 || Loss: 1.1482 || timer: 0.0896 sec.
iter 52290 || Loss: 1.2003 || timer: 0.0913 sec.
iter 52300 || Loss: 1.2279 || timer: 0.0820 sec.
iter 52310 || Loss: 1.0057 || timer: 0.1011 sec.
iter 52320 || Loss: 1.1107 || timer: 0.1003 sec.
iter 52330 || Loss: 1.4291 || timer: 0.0823 sec.
iter 52340 || Loss: 1.2091 || timer: 0.0919 sec.
iter 52350 || Loss: 1.4201 || timer: 0.0901 sec.
iter 52360 || Loss: 1.3332 || timer: 0.0235 sec.
iter 52370 || Loss: 0.8777 || timer: 0.0916 sec.
iter 52380 || Loss: 1.3870 || timer: 0.0913 sec.
iter 52390 || Loss: 1.5181 || timer: 0.0888 sec.
iter 52400 || Loss: 1.0651 || timer: 0.0864 sec.
iter 52410 || Loss: 1.1982 || timer: 0.0810 sec.
iter 52420 || Loss: 1.1061 || timer: 0.0875 sec.
iter 52430 || Loss: 1.5301 || timer: 0.0749 sec.
iter 52440 || Loss: 1.4462 || timer: 0.0899 sec.
iter 52450 || Loss: 1.3039 || timer: 0.0826 sec.
iter 52460 || Loss: 0.9981 || timer: 0.0967 sec.
iter 52470 || Loss: 1.4384 || timer: 0.0877 sec.
iter 52480 || Loss: 1.2971 || timer: 0.0901 sec.
iter 52490 || Loss: 1.2045 || timer: 0.0906 sec.
iter 52500 || Loss: 1.1313 || timer: 0.0901 sec.
iter 52510 || Loss: 1.5868 || timer: 0.1101 sec.
iter 52520 || Loss: 1.4157 || timer: 0.0817 sec.
iter 52530 || Loss: 1.2647 || timer: 0.0883 sec.
iter 52540 || Loss: 1.3006 || timer: 0.0826 sec.
iter 52550 || Loss: 1.2589 || timer: 0.1165 sec.
iter 52560 || Loss: 1.1921 || timer: 0.0782 sec.
iter 52570 || Loss: 1.3933 || timer: 0.0758 sec.
iter 52580 || Loss: 1.1373 || timer: 0.0828 sec.
iter 52590 || Loss: 1.4147 || timer: 0.0860 sec.
iter 52600 || Loss: 1.1855 || timer: 0.0913 sec.
iter 52610 || Loss: 1.2562 || timer: 0.0894 sec.
iter 52620 || Loss: 1.1710 || timer: 0.0865 sec.
iter 52630 || Loss: 1.1230 || timer: 0.0825 sec.
iter 52640 || Loss: 2.2840 || timer: 0.1023 sec.
iter 52650 || Loss: 1.4090 || timer: 0.0822 sec.
iter 52660 || Loss: 1.3070 || timer: 0.0879 sec.
iter 52670 || Loss: 1.2374 || timer: 0.0894 sec.
iter 52680 || Loss: 1.6190 || timer: 0.0824 sec.
iter 52690 || Loss: 1.2193 || timer: 0.0221 sec.
iter 52700 || Loss: 0.9481 || timer: 0.0828 sec.
iter 52710 || Loss: 0.8736 || timer: 0.0760 sec.
iter 52720 || Loss: 0.9299 || timer: 0.0904 sec.
iter 52730 || Loss: 1.1099 || timer: 0.0930 sec.
iter 52740 || Loss: 1.1281 || timer: 0.0894 sec.
iter 52750 || Loss: 0.9154 || timer: 0.1042 sec.
iter 52760 || Loss: 1.1895 || timer: 0.0822 sec.
iter 52770 || Loss: 1.4944 || timer: 0.0918 sec.
iter 52780 || Loss: 1.5691 || timer: 0.0980 sec.
iter 52790 || Loss: 1.1149 || timer: 0.1076 sec.
iter 52800 || Loss: 1.2010 || timer: 0.0895 sec.
iter 52810 || Loss: 1.8494 || timer: 0.1049 sec.
iter 52820 || Loss: 1.1281 || timer: 0.0827 sec.
iter 52830 || Loss: 1.1986 || timer: 0.0901 sec.
iter 52840 || Loss: 1.4901 || timer: 0.0940 sec.
iter 52850 || Loss: 1.5439 || timer: 0.0896 sec.
iter 52860 || Loss: 1.4407 || timer: 0.0915 sec.
iter 52870 || Loss: 1.4134 || timer: 0.0755 sec.
iter 52880 || Loss: 1.1721 || timer: 0.0758 sec.
iter 52890 || Loss: 1.2889 || timer: 0.0749 sec.
iter 52900 || Loss: 0.8952 || timer: 0.1054 sec.
iter 52910 || Loss: 1.1724 || timer: 0.0739 sec.
iter 52920 || Loss: 1.4829 || timer: 0.0735 sec.
iter 52930 || Loss: 1.2941 || timer: 0.1234 sec.
iter 52940 || Loss: 1.4081 || timer: 0.0784 sec.
iter 52950 || Loss: 1.3631 || timer: 0.0823 sec.
iter 52960 || Loss: 1.2235 || timer: 0.0879 sec.
iter 52970 || Loss: 1.0213 || timer: 0.0938 sec.
iter 52980 || Loss: 1.7223 || timer: 0.0900 sec.
iter 52990 || Loss: 1.1320 || timer: 0.0822 sec.
iter 53000 || Loss: 1.3580 || timer: 0.0863 sec.
iter 53010 || Loss: 1.3857 || timer: 0.0953 sec.
iter 53020 || Loss: 1.5203 || timer: 0.0312 sec.
iter 53030 || Loss: 1.0758 || timer: 0.1231 sec.
iter 53040 || Loss: 1.2523 || timer: 0.1115 sec.
iter 53050 || Loss: 1.4071 || timer: 0.0808 sec.
iter 53060 || Loss: 2.0837 || timer: 0.0921 sec.
iter 53070 || Loss: 1.6217 || timer: 0.0917 sec.
iter 53080 || Loss: 1.2973 || timer: 0.0926 sec.
iter 53090 || Loss: 1.1992 || timer: 0.0892 sec.
iter 53100 || Loss: 1.2446 || timer: 0.0900 sec.
iter 53110 || Loss: 1.0386 || timer: 0.0918 sec.
iter 53120 || Loss: 1.1631 || timer: 0.1401 sec.
iter 53130 || Loss: 1.0086 || timer: 0.0823 sec.
iter 53140 || Loss: 1.1598 || timer: 0.0917 sec.
iter 53150 || Loss: 1.8657 || timer: 0.0886 sec.
iter 53160 || Loss: 1.3413 || timer: 0.1132 sec.
iter 53170 || Loss: 1.2645 || timer: 0.0892 sec.
iter 53180 || Loss: 1.3249 || timer: 0.0888 sec.
iter 53190 || Loss: 1.1509 || timer: 0.0984 sec.
iter 53200 || Loss: 1.1158 || timer: 0.0830 sec.
iter 53210 || Loss: 1.5654 || timer: 0.1030 sec.
iter 53220 || Loss: 1.0816 || timer: 0.1126 sec.
iter 53230 || Loss: 1.2412 || timer: 0.1134 sec.
iter 53240 || Loss: 1.1178 || timer: 0.1056 sec.
iter 53250 || Loss: 0.9459 || timer: 0.0911 sec.
iter 53260 || Loss: 1.0750 || timer: 0.1151 sec.
iter 53270 || Loss: 1.3496 || timer: 0.0832 sec.
iter 53280 || Loss: 1.5442 || timer: 0.0830 sec.
iter 53290 || Loss: 1.1957 || timer: 0.0920 sec.
iter 53300 || Loss: 1.4671 || timer: 0.0953 sec.
iter 53310 || Loss: 1.3106 || timer: 0.0984 sec.
iter 53320 || Loss: 1.3656 || timer: 0.0826 sec.
iter 53330 || Loss: 1.0114 || timer: 0.0823 sec.
iter 53340 || Loss: 1.0368 || timer: 0.0896 sec.
iter 53350 || Loss: 1.0778 || timer: 0.0260 sec.
iter 53360 || Loss: 3.1659 || timer: 0.0842 sec.
iter 53370 || Loss: 1.1415 || timer: 0.0909 sec.
iter 53380 || Loss: 1.0327 || timer: 0.0834 sec.
iter 53390 || Loss: 1.4238 || timer: 0.0985 sec.
iter 53400 || Loss: 1.0348 || timer: 0.0915 sec.
iter 53410 || Loss: 1.2215 || timer: 0.0845 sec.
iter 53420 || Loss: 1.2503 || timer: 0.0896 sec.
iter 53430 || Loss: 1.1691 || timer: 0.0863 sec.
iter 53440 || Loss: 1.2085 || timer: 0.0918 sec.
iter 53450 || Loss: 1.0479 || timer: 0.1121 sec.
iter 53460 || Loss: 1.2353 || timer: 0.0878 sec.
iter 53470 || Loss: 1.0776 || timer: 0.0922 sec.
iter 53480 || Loss: 1.1153 || timer: 0.0824 sec.
iter 53490 || Loss: 1.2329 || timer: 0.0907 sec.
iter 53500 || Loss: 0.8400 || timer: 0.0929 sec.
iter 53510 || Loss: 1.6709 || timer: 0.0942 sec.
iter 53520 || Loss: 1.1192 || timer: 0.0850 sec.
iter 53530 || Loss: 1.3364 || timer: 0.0890 sec.
iter 53540 || Loss: 1.1348 || timer: 0.1019 sec.
iter 53550 || Loss: 1.4151 || timer: 0.0856 sec.
iter 53560 || Loss: 1.1931 || timer: 0.0970 sec.
iter 53570 || Loss: 1.5087 || timer: 0.0828 sec.
iter 53580 || Loss: 1.0517 || timer: 0.0936 sec.
iter 53590 || Loss: 1.0709 || timer: 0.0915 sec.
iter 53600 || Loss: 1.9030 || timer: 0.0828 sec.
iter 53610 || Loss: 1.5298 || timer: 0.0899 sec.
iter 53620 || Loss: 1.1755 || timer: 0.0836 sec.
iter 53630 || Loss: 1.4321 || timer: 0.0926 sec.
iter 53640 || Loss: 1.3013 || timer: 0.1096 sec.
iter 53650 || Loss: 1.4568 || timer: 0.0912 sec.
iter 53660 || Loss: 1.4186 || timer: 0.0936 sec.
iter 53670 || Loss: 1.5336 || timer: 0.0836 sec.
iter 53680 || Loss: 1.2915 || timer: 0.0282 sec.
iter 53690 || Loss: 1.0964 || timer: 0.0903 sec.
iter 53700 || Loss: 0.8390 || timer: 0.0819 sec.
iter 53710 || Loss: 1.3174 || timer: 0.1068 sec.
iter 53720 || Loss: 1.1566 || timer: 0.0837 sec.
iter 53730 || Loss: 1.0770 || timer: 0.0974 sec.
iter 53740 || Loss: 1.1572 || timer: 0.0901 sec.
iter 53750 || Loss: 1.1298 || timer: 0.0858 sec.
iter 53760 || Loss: 1.4028 || timer: 0.0888 sec.
iter 53770 || Loss: 1.0823 || timer: 0.0903 sec.
iter 53780 || Loss: 1.0586 || timer: 0.1035 sec.
iter 53790 || Loss: 1.2435 || timer: 0.0918 sec.
iter 53800 || Loss: 0.9217 || timer: 0.0896 sec.
iter 53810 || Loss: 1.2106 || timer: 0.0914 sec.
iter 53820 || Loss: 1.1491 || timer: 0.1033 sec.
iter 53830 || Loss: 1.3306 || timer: 0.1063 sec.
iter 53840 || Loss: 1.1312 || timer: 0.1093 sec.
iter 53850 || Loss: 1.3709 || timer: 0.1155 sec.
iter 53860 || Loss: 1.3370 || timer: 0.0914 sec.
iter 53870 || Loss: 1.2718 || timer: 0.1077 sec.
iter 53880 || Loss: 1.2766 || timer: 0.1094 sec.
iter 53890 || Loss: 1.4418 || timer: 0.0845 sec.
iter 53900 || Loss: 1.5165 || timer: 0.0768 sec.
iter 53910 || Loss: 1.0640 || timer: 0.0791 sec.
iter 53920 || Loss: 1.1446 || timer: 0.0897 sec.
iter 53930 || Loss: 1.1009 || timer: 0.0837 sec.
iter 53940 || Loss: 0.9379 || timer: 0.1064 sec.
iter 53950 || Loss: 0.9002 || timer: 0.0843 sec.
iter 53960 || Loss: 1.2110 || timer: 0.0889 sec.
iter 53970 || Loss: 1.2369 || timer: 0.1128 sec.
iter 53980 || Loss: 1.2842 || timer: 0.0939 sec.
iter 53990 || Loss: 1.2753 || timer: 0.0971 sec.
iter 54000 || Loss: 0.9629 || timer: 0.1047 sec.
iter 54010 || Loss: 1.2007 || timer: 0.0285 sec.
iter 54020 || Loss: 1.0465 || timer: 0.0840 sec.
iter 54030 || Loss: 0.9936 || timer: 0.1051 sec.
iter 54040 || Loss: 1.3483 || timer: 0.0848 sec.
iter 54050 || Loss: 1.2233 || timer: 0.0905 sec.
iter 54060 || Loss: 1.0847 || timer: 0.0772 sec.
iter 54070 || Loss: 0.9488 || timer: 0.0827 sec.
iter 54080 || Loss: 1.1048 || timer: 0.0890 sec.
iter 54090 || Loss: 1.6162 || timer: 0.0874 sec.
iter 54100 || Loss: 1.1518 || timer: 0.0920 sec.
iter 54110 || Loss: 0.8790 || timer: 0.1076 sec.
iter 54120 || Loss: 1.1511 || timer: 0.0930 sec.
iter 54130 || Loss: 1.2709 || timer: 0.0907 sec.
iter 54140 || Loss: 1.1538 || timer: 0.0919 sec.
iter 54150 || Loss: 1.4894 || timer: 0.0947 sec.
iter 54160 || Loss: 1.2917 || timer: 0.0844 sec.
iter 54170 || Loss: 1.0702 || timer: 0.0922 sec.
iter 54180 || Loss: 1.0297 || timer: 0.1171 sec.
iter 54190 || Loss: 1.6402 || timer: 0.1021 sec.
iter 54200 || Loss: 1.9604 || timer: 0.1013 sec.
iter 54210 || Loss: 0.9847 || timer: 0.0940 sec.
iter 54220 || Loss: 1.2205 || timer: 0.0900 sec.
iter 54230 || Loss: 1.4935 || timer: 0.0850 sec.
iter 54240 || Loss: 1.2140 || timer: 0.0904 sec.
iter 54250 || Loss: 1.1190 || timer: 0.0759 sec.
iter 54260 || Loss: 1.2860 || timer: 0.0835 sec.
iter 54270 || Loss: 1.5318 || timer: 0.0837 sec.
iter 54280 || Loss: 1.0644 || timer: 0.0768 sec.
iter 54290 || Loss: 1.5159 || timer: 0.0772 sec.
iter 54300 || Loss: 1.3443 || timer: 0.0895 sec.
iter 54310 || Loss: 1.1325 || timer: 0.0937 sec.
iter 54320 || Loss: 1.3566 || timer: 0.0907 sec.
iter 54330 || Loss: 1.2498 || timer: 0.0850 sec.
iter 54340 || Loss: 1.5455 || timer: 0.0201 sec.
iter 54350 || Loss: 2.2735 || timer: 0.0847 sec.
iter 54360 || Loss: 1.3196 || timer: 0.0843 sec.
iter 54370 || Loss: 1.3779 || timer: 0.0908 sec.
iter 54380 || Loss: 1.3978 || timer: 0.0842 sec.
iter 54390 || Loss: 0.7378 || timer: 0.1021 sec.
iter 54400 || Loss: 1.5228 || timer: 0.0842 sec.
iter 54410 || Loss: 1.0026 || timer: 0.0842 sec.
iter 54420 || Loss: 1.3526 || timer: 0.0997 sec.
iter 54430 || Loss: 1.5296 || timer: 0.0925 sec.
iter 54440 || Loss: 1.2004 || timer: 0.1204 sec.
iter 54450 || Loss: 1.7744 || timer: 0.0910 sec.
iter 54460 || Loss: 1.1491 || timer: 0.0847 sec.
iter 54470 || Loss: 1.4064 || timer: 0.0909 sec.
iter 54480 || Loss: 1.0797 || timer: 0.0887 sec.
iter 54490 || Loss: 1.1223 || timer: 0.0770 sec.
iter 54500 || Loss: 1.4114 || timer: 0.1079 sec.
iter 54510 || Loss: 1.1832 || timer: 0.0914 sec.
iter 54520 || Loss: 1.2892 || timer: 0.0848 sec.
iter 54530 || Loss: 1.2605 || timer: 0.0956 sec.
iter 54540 || Loss: 1.2819 || timer: 0.1067 sec.
iter 54550 || Loss: 1.3017 || timer: 0.0846 sec.
iter 54560 || Loss: 1.2905 || timer: 0.0907 sec.
iter 54570 || Loss: 1.3486 || timer: 0.0922 sec.
iter 54580 || Loss: 0.9864 || timer: 0.1093 sec.
iter 54590 || Loss: 1.0604 || timer: 0.0857 sec.
iter 54600 || Loss: 0.9141 || timer: 0.0902 sec.
iter 54610 || Loss: 1.1895 || timer: 0.1156 sec.
iter 54620 || Loss: 0.8732 || timer: 0.0911 sec.
iter 54630 || Loss: 1.2418 || timer: 0.0843 sec.
iter 54640 || Loss: 1.4772 || timer: 0.0934 sec.
iter 54650 || Loss: 1.0419 || timer: 0.0914 sec.
iter 54660 || Loss: 1.5861 || timer: 0.1022 sec.
iter 54670 || Loss: 0.9385 || timer: 0.0319 sec.
iter 54680 || Loss: 2.4881 || timer: 0.0837 sec.
iter 54690 || Loss: 1.2522 || timer: 0.1028 sec.
iter 54700 || Loss: 1.2156 || timer: 0.0757 sec.
iter 54710 || Loss: 1.2410 || timer: 0.0851 sec.
iter 54720 || Loss: 1.1670 || timer: 0.0856 sec.
iter 54730 || Loss: 1.1369 || timer: 0.0926 sec.
iter 54740 || Loss: 1.1925 || timer: 0.0842 sec.
iter 54750 || Loss: 0.9689 || timer: 0.0909 sec.
iter 54760 || Loss: 1.2857 || timer: 0.1183 sec.
iter 54770 || Loss: 1.4463 || timer: 0.1003 sec.
iter 54780 || Loss: 1.0122 || timer: 0.0837 sec.
iter 54790 || Loss: 1.3058 || timer: 0.0866 sec.
iter 54800 || Loss: 1.1885 || timer: 0.0760 sec.
iter 54810 || Loss: 1.0284 || timer: 0.0797 sec.
iter 54820 || Loss: 1.3452 || timer: 0.1114 sec.
iter 54830 || Loss: 1.2332 || timer: 0.0897 sec.
iter 54840 || Loss: 0.9848 || timer: 0.0776 sec.
iter 54850 || Loss: 1.1389 || timer: 0.0848 sec.
iter 54860 || Loss: 1.4050 || timer: 0.0931 sec.
iter 54870 || Loss: 1.2250 || timer: 0.1023 sec.
iter 54880 || Loss: 1.0423 || timer: 0.1012 sec.
iter 54890 || Loss: 1.4673 || timer: 0.0853 sec.
iter 54900 || Loss: 1.0786 || timer: 0.0844 sec.
iter 54910 || Loss: 0.7837 || timer: 0.0917 sec.
iter 54920 || Loss: 1.3183 || timer: 0.0948 sec.
iter 54930 || Loss: 1.4520 || timer: 0.0950 sec.
iter 54940 || Loss: 1.8384 || timer: 0.0985 sec.
iter 54950 || Loss: 1.0076 || timer: 0.0853 sec.
iter 54960 || Loss: 1.1919 || timer: 0.0841 sec.
iter 54970 || Loss: 1.0266 || timer: 0.0863 sec.
iter 54980 || Loss: 1.1714 || timer: 0.1204 sec.
iter 54990 || Loss: 1.3880 || timer: 0.0906 sec.
iter 55000 || Loss: 1.3589 || Saving state, iter: 55000
timer: 0.0310 sec.
iter 55010 || Loss: 1.6446 || timer: 0.0859 sec.
iter 55020 || Loss: 1.2263 || timer: 0.0845 sec.
iter 55030 || Loss: 1.6866 || timer: 0.0851 sec.
iter 55040 || Loss: 1.3658 || timer: 0.0907 sec.
iter 55050 || Loss: 1.0928 || timer: 0.1009 sec.
iter 55060 || Loss: 1.0683 || timer: 0.0909 sec.
iter 55070 || Loss: 1.4681 || timer: 0.0840 sec.
iter 55080 || Loss: 1.2457 || timer: 0.0947 sec.
iter 55090 || Loss: 1.2855 || timer: 0.0923 sec.
iter 55100 || Loss: 1.1345 || timer: 0.1024 sec.
iter 55110 || Loss: 1.0358 || timer: 0.0785 sec.
iter 55120 || Loss: 1.1616 || timer: 0.0937 sec.
iter 55130 || Loss: 0.9550 || timer: 0.0909 sec.
iter 55140 || Loss: 1.0757 || timer: 0.0899 sec.
iter 55150 || Loss: 1.5415 || timer: 0.0855 sec.
iter 55160 || Loss: 0.9744 || timer: 0.0834 sec.
iter 55170 || Loss: 0.9714 || timer: 0.0889 sec.
iter 55180 || Loss: 1.2628 || timer: 0.1055 sec.
iter 55190 || Loss: 0.9798 || timer: 0.0878 sec.
iter 55200 || Loss: 1.3029 || timer: 0.0840 sec.
iter 55210 || Loss: 1.2945 || timer: 0.0831 sec.
iter 55220 || Loss: 1.4822 || timer: 0.1008 sec.
iter 55230 || Loss: 1.6050 || timer: 0.0831 sec.
iter 55240 || Loss: 1.5137 || timer: 0.0894 sec.
iter 55250 || Loss: 1.1186 || timer: 0.0836 sec.
iter 55260 || Loss: 1.5259 || timer: 0.0954 sec.
iter 55270 || Loss: 1.5420 || timer: 0.0849 sec.
iter 55280 || Loss: 1.2075 || timer: 0.0774 sec.
iter 55290 || Loss: 0.8803 || timer: 0.1032 sec.
iter 55300 || Loss: 1.3453 || timer: 0.0929 sec.
iter 55310 || Loss: 1.4479 || timer: 0.0914 sec.
iter 55320 || Loss: 0.9565 || timer: 0.1122 sec.
iter 55330 || Loss: 1.0740 || timer: 0.0170 sec.
iter 55340 || Loss: 0.1297 || timer: 0.0873 sec.
iter 55350 || Loss: 1.5181 || timer: 0.0860 sec.
iter 55360 || Loss: 1.4651 || timer: 0.0926 sec.
iter 55370 || Loss: 1.3025 || timer: 0.0845 sec.
iter 55380 || Loss: 0.8605 || timer: 0.0903 sec.
iter 55390 || Loss: 1.2247 || timer: 0.0842 sec.
iter 55400 || Loss: 0.9597 || timer: 0.0914 sec.
iter 55410 || Loss: 0.9697 || timer: 0.1062 sec.
iter 55420 || Loss: 1.9218 || timer: 0.0926 sec.
iter 55430 || Loss: 0.8866 || timer: 0.1007 sec.
iter 55440 || Loss: 0.9248 || timer: 0.0895 sec.
iter 55450 || Loss: 1.2411 || timer: 0.0847 sec.
iter 55460 || Loss: 1.2198 || timer: 0.0935 sec.
iter 55470 || Loss: 0.9865 || timer: 0.0838 sec.
iter 55480 || Loss: 1.8608 || timer: 0.0855 sec.
iter 55490 || Loss: 0.9383 || timer: 0.0987 sec.
iter 55500 || Loss: 1.5436 || timer: 0.0998 sec.
iter 55510 || Loss: 0.9690 || timer: 0.0766 sec.
iter 55520 || Loss: 1.4489 || timer: 0.0849 sec.
iter 55530 || Loss: 1.2920 || timer: 0.0771 sec.
iter 55540 || Loss: 1.0039 || timer: 0.0843 sec.
iter 55550 || Loss: 1.2081 || timer: 0.0913 sec.
iter 55560 || Loss: 1.2237 || timer: 0.0857 sec.
iter 55570 || Loss: 1.0445 || timer: 0.1055 sec.
iter 55580 || Loss: 1.2910 || timer: 0.0835 sec.
iter 55590 || Loss: 1.4251 || timer: 0.0915 sec.
iter 55600 || Loss: 0.9403 || timer: 0.0918 sec.
iter 55610 || Loss: 1.5374 || timer: 0.1082 sec.
iter 55620 || Loss: 1.0825 || timer: 0.1048 sec.
iter 55630 || Loss: 1.0806 || timer: 0.0930 sec.
iter 55640 || Loss: 1.5092 || timer: 0.1041 sec.
iter 55650 || Loss: 1.1321 || timer: 0.0940 sec.
iter 55660 || Loss: 1.0431 || timer: 0.0213 sec.
iter 55670 || Loss: 0.6652 || timer: 0.0902 sec.
iter 55680 || Loss: 1.2748 || timer: 0.0843 sec.
iter 55690 || Loss: 1.6612 || timer: 0.1016 sec.
iter 55700 || Loss: 1.0684 || timer: 0.0919 sec.
iter 55710 || Loss: 1.1063 || timer: 0.0850 sec.
iter 55720 || Loss: 0.9953 || timer: 0.0851 sec.
iter 55730 || Loss: 1.0302 || timer: 0.0844 sec.
iter 55740 || Loss: 0.8511 || timer: 0.0846 sec.
iter 55750 || Loss: 1.1193 || timer: 0.0921 sec.
iter 55760 || Loss: 1.5425 || timer: 0.1185 sec.
iter 55770 || Loss: 1.5108 || timer: 0.0776 sec.
iter 55780 || Loss: 1.0137 || timer: 0.0859 sec.
iter 55790 || Loss: 1.7790 || timer: 0.0832 sec.
iter 55800 || Loss: 1.2154 || timer: 0.0841 sec.
iter 55810 || Loss: 1.0622 || timer: 0.0923 sec.
iter 55820 || Loss: 1.3418 || timer: 0.0900 sec.
iter 55830 || Loss: 1.5954 || timer: 0.0938 sec.
iter 55840 || Loss: 1.3280 || timer: 0.0849 sec.
iter 55850 || Loss: 0.9994 || timer: 0.0930 sec.
iter 55860 || Loss: 1.2306 || timer: 0.0969 sec.
iter 55870 || Loss: 1.0621 || timer: 0.0945 sec.
iter 55880 || Loss: 1.4199 || timer: 0.1102 sec.
iter 55890 || Loss: 1.6000 || timer: 0.0753 sec.
iter 55900 || Loss: 1.3734 || timer: 0.0904 sec.
iter 55910 || Loss: 1.4709 || timer: 0.0843 sec.
iter 55920 || Loss: 0.9729 || timer: 0.0832 sec.
iter 55930 || Loss: 1.0288 || timer: 0.0864 sec.
iter 55940 || Loss: 1.0765 || timer: 0.0874 sec.
iter 55950 || Loss: 1.3500 || timer: 0.0769 sec.
iter 55960 || Loss: 0.9417 || timer: 0.0832 sec.
iter 55970 || Loss: 1.1833 || timer: 0.0890 sec.
iter 55980 || Loss: 1.3536 || timer: 0.0946 sec.
iter 55990 || Loss: 1.3277 || timer: 0.0164 sec.
iter 56000 || Loss: 0.8927 || timer: 0.0868 sec.
iter 56010 || Loss: 1.1886 || timer: 0.0922 sec.
iter 56020 || Loss: 1.4281 || timer: 0.0928 sec.
iter 56030 || Loss: 1.1856 || timer: 0.1019 sec.
iter 56040 || Loss: 0.8292 || timer: 0.0932 sec.
iter 56050 || Loss: 1.4493 || timer: 0.0913 sec.
iter 56060 || Loss: 1.0684 || timer: 0.0961 sec.
iter 56070 || Loss: 1.1102 || timer: 0.0901 sec.
iter 56080 || Loss: 1.7041 || timer: 0.0918 sec.
iter 56090 || Loss: 1.0931 || timer: 0.1014 sec.
iter 56100 || Loss: 1.1699 || timer: 0.0847 sec.
iter 56110 || Loss: 1.0818 || timer: 0.0872 sec.
iter 56120 || Loss: 0.9636 || timer: 0.0925 sec.
iter 56130 || Loss: 1.0542 || timer: 0.0761 sec.
iter 56140 || Loss: 0.9771 || timer: 0.0855 sec.
iter 56150 || Loss: 1.1467 || timer: 0.0871 sec.
iter 56160 || Loss: 1.3579 || timer: 0.0843 sec.
iter 56170 || Loss: 1.4568 || timer: 0.0997 sec.
iter 56180 || Loss: 1.2235 || timer: 0.0831 sec.
iter 56190 || Loss: 1.0258 || timer: 0.0934 sec.
iter 56200 || Loss: 0.8899 || timer: 0.0908 sec.
iter 56210 || Loss: 1.3087 || timer: 0.0912 sec.
iter 56220 || Loss: 1.2893 || timer: 0.0843 sec.
iter 56230 || Loss: 1.5174 || timer: 0.0843 sec.
iter 56240 || Loss: 0.8565 || timer: 0.0904 sec.
iter 56250 || Loss: 1.0154 || timer: 0.0906 sec.
iter 56260 || Loss: 1.0107 || timer: 0.0876 sec.
iter 56270 || Loss: 1.2902 || timer: 0.0903 sec.
iter 56280 || Loss: 1.4170 || timer: 0.0985 sec.
iter 56290 || Loss: 1.1644 || timer: 0.0841 sec.
iter 56300 || Loss: 1.0588 || timer: 0.0759 sec.
iter 56310 || Loss: 1.1208 || timer: 0.1075 sec.
iter 56320 || Loss: 0.9532 || timer: 0.0157 sec.
iter 56330 || Loss: 0.6819 || timer: 0.1046 sec.
iter 56340 || Loss: 1.0196 || timer: 0.0922 sec.
iter 56350 || Loss: 1.4334 || timer: 0.0906 sec.
iter 56360 || Loss: 1.1241 || timer: 0.0883 sec.
iter 56370 || Loss: 1.0765 || timer: 0.0913 sec.
iter 56380 || Loss: 1.4562 || timer: 0.0899 sec.
iter 56390 || Loss: 1.1428 || timer: 0.0828 sec.
iter 56400 || Loss: 1.1811 || timer: 0.0853 sec.
iter 56410 || Loss: 1.4074 || timer: 0.0819 sec.
iter 56420 || Loss: 1.0530 || timer: 0.0819 sec.
iter 56430 || Loss: 1.3669 || timer: 0.0842 sec.
iter 56440 || Loss: 1.1161 || timer: 0.0880 sec.
iter 56450 || Loss: 1.3394 || timer: 0.0889 sec.
iter 56460 || Loss: 1.0873 || timer: 0.0841 sec.
iter 56470 || Loss: 1.2486 || timer: 0.0815 sec.
iter 56480 || Loss: 1.4206 || timer: 0.0895 sec.
iter 56490 || Loss: 1.0403 || timer: 0.0943 sec.
iter 56500 || Loss: 1.1806 || timer: 0.0929 sec.
iter 56510 || Loss: 0.9989 || timer: 0.0854 sec.
iter 56520 || Loss: 1.3147 || timer: 0.1129 sec.
iter 56530 || Loss: 1.2224 || timer: 0.0925 sec.
iter 56540 || Loss: 1.4187 || timer: 0.0912 sec.
iter 56550 || Loss: 1.1710 || timer: 0.0913 sec.
iter 56560 || Loss: 1.0853 || timer: 0.1225 sec.
iter 56570 || Loss: 1.2645 || timer: 0.0908 sec.
iter 56580 || Loss: 1.2003 || timer: 0.0841 sec.
iter 56590 || Loss: 1.1026 || timer: 0.0766 sec.
iter 56600 || Loss: 1.1962 || timer: 0.0843 sec.
iter 56610 || Loss: 0.9702 || timer: 0.0815 sec.
iter 56620 || Loss: 1.0830 || timer: 0.1051 sec.
iter 56630 || Loss: 1.2362 || timer: 0.0848 sec.
iter 56640 || Loss: 1.6189 || timer: 0.0912 sec.
iter 56650 || Loss: 1.0100 || timer: 0.0213 sec.
iter 56660 || Loss: 3.8048 || timer: 0.0894 sec.
iter 56670 || Loss: 1.1048 || timer: 0.0895 sec.
iter 56680 || Loss: 1.0690 || timer: 0.0900 sec.
iter 56690 || Loss: 1.4947 || timer: 0.0929 sec.
iter 56700 || Loss: 1.2450 || timer: 0.0890 sec.
iter 56710 || Loss: 1.2079 || timer: 0.0905 sec.
iter 56720 || Loss: 1.0928 || timer: 0.0918 sec.
iter 56730 || Loss: 1.7773 || timer: 0.1039 sec.
iter 56740 || Loss: 1.3870 || timer: 0.0827 sec.
iter 56750 || Loss: 1.2925 || timer: 0.1154 sec.
iter 56760 || Loss: 1.0152 || timer: 0.0897 sec.
iter 56770 || Loss: 1.4077 || timer: 0.1038 sec.
iter 56780 || Loss: 1.2994 || timer: 0.0837 sec.
iter 56790 || Loss: 1.2697 || timer: 0.0855 sec.
iter 56800 || Loss: 0.8274 || timer: 0.0909 sec.
iter 56810 || Loss: 1.1488 || timer: 0.0990 sec.
iter 56820 || Loss: 1.0396 || timer: 0.0904 sec.
iter 56830 || Loss: 1.3550 || timer: 0.0941 sec.
iter 56840 || Loss: 1.3703 || timer: 0.0819 sec.
iter 56850 || Loss: 1.2302 || timer: 0.0826 sec.
iter 56860 || Loss: 1.2848 || timer: 0.0816 sec.
iter 56870 || Loss: 1.3289 || timer: 0.0828 sec.
iter 56880 || Loss: 1.4467 || timer: 0.0899 sec.
iter 56890 || Loss: 1.0813 || timer: 0.0980 sec.
iter 56900 || Loss: 1.0588 || timer: 0.0890 sec.
iter 56910 || Loss: 1.4624 || timer: 0.1013 sec.
iter 56920 || Loss: 1.0621 || timer: 0.0879 sec.
iter 56930 || Loss: 1.1721 || timer: 0.0812 sec.
iter 56940 || Loss: 1.2823 || timer: 0.0872 sec.
iter 56950 || Loss: 0.9220 || timer: 0.0908 sec.
iter 56960 || Loss: 1.2782 || timer: 0.1032 sec.
iter 56970 || Loss: 1.2533 || timer: 0.0823 sec.
iter 56980 || Loss: 1.0646 || timer: 0.0250 sec.
iter 56990 || Loss: 0.7677 || timer: 0.0968 sec.
iter 57000 || Loss: 1.4480 || timer: 0.0903 sec.
iter 57010 || Loss: 1.0502 || timer: 0.0842 sec.
iter 57020 || Loss: 1.1403 || timer: 0.0878 sec.
iter 57030 || Loss: 1.7699 || timer: 0.0835 sec.
iter 57040 || Loss: 1.0700 || timer: 0.0843 sec.
iter 57050 || Loss: 1.0102 || timer: 0.0832 sec.
iter 57060 || Loss: 0.9896 || timer: 0.0906 sec.
iter 57070 || Loss: 1.0890 || timer: 0.0921 sec.
iter 57080 || Loss: 1.4887 || timer: 0.0939 sec.
iter 57090 || Loss: 0.9687 || timer: 0.0886 sec.
iter 57100 || Loss: 1.2635 || timer: 0.0897 sec.
iter 57110 || Loss: 1.2466 || timer: 0.0928 sec.
iter 57120 || Loss: 1.5612 || timer: 0.0889 sec.
iter 57130 || Loss: 0.9911 || timer: 0.0960 sec.
iter 57140 || Loss: 0.9975 || timer: 0.0827 sec.
iter 57150 || Loss: 1.0743 || timer: 0.0901 sec.
iter 57160 || Loss: 0.8764 || timer: 0.0908 sec.
iter 57170 || Loss: 0.9092 || timer: 0.0836 sec.
iter 57180 || Loss: 0.9202 || timer: 0.0915 sec.
iter 57190 || Loss: 1.2619 || timer: 0.0849 sec.
iter 57200 || Loss: 1.8416 || timer: 0.0832 sec.
iter 57210 || Loss: 1.1963 || timer: 0.0828 sec.
iter 57220 || Loss: 1.0576 || timer: 0.0909 sec.
iter 57230 || Loss: 1.1255 || timer: 0.0823 sec.
iter 57240 || Loss: 0.9063 || timer: 0.0963 sec.
iter 57250 || Loss: 1.2016 || timer: 0.0881 sec.
iter 57260 || Loss: 0.9263 || timer: 0.0909 sec.
iter 57270 || Loss: 1.1393 || timer: 0.0884 sec.
iter 57280 || Loss: 1.0534 || timer: 0.0832 sec.
iter 57290 || Loss: 1.0515 || timer: 0.0831 sec.
iter 57300 || Loss: 1.4380 || timer: 0.0903 sec.
iter 57310 || Loss: 1.3339 || timer: 0.0234 sec.
iter 57320 || Loss: 1.0036 || timer: 0.0822 sec.
iter 57330 || Loss: 1.0173 || timer: 0.0880 sec.
iter 57340 || Loss: 1.1148 || timer: 0.0915 sec.
iter 57350 || Loss: 0.9116 || timer: 0.0882 sec.
iter 57360 || Loss: 1.0338 || timer: 0.0825 sec.
iter 57370 || Loss: 1.3118 || timer: 0.0823 sec.
iter 57380 || Loss: 1.1294 || timer: 0.0849 sec.
iter 57390 || Loss: 1.3317 || timer: 0.0844 sec.
iter 57400 || Loss: 0.9685 || timer: 0.0809 sec.
iter 57410 || Loss: 1.0989 || timer: 0.0935 sec.
iter 57420 || Loss: 1.3712 || timer: 0.0910 sec.
iter 57430 || Loss: 1.2539 || timer: 0.0877 sec.
iter 57440 || Loss: 1.4973 || timer: 0.0885 sec.
iter 57450 || Loss: 1.0057 || timer: 0.0901 sec.
iter 57460 || Loss: 1.0192 || timer: 0.0841 sec.
iter 57470 || Loss: 1.3522 || timer: 0.0822 sec.
iter 57480 || Loss: 1.2126 || timer: 0.0905 sec.
iter 57490 || Loss: 0.9783 || timer: 0.0889 sec.
iter 57500 || Loss: 1.1409 || timer: 0.0932 sec.
iter 57510 || Loss: 1.7260 || timer: 0.0872 sec.
iter 57520 || Loss: 1.2134 || timer: 0.1036 sec.
iter 57530 || Loss: 2.0507 || timer: 0.1071 sec.
iter 57540 || Loss: 1.3952 || timer: 0.0831 sec.
iter 57550 || Loss: 1.4031 || timer: 0.1030 sec.
iter 57560 || Loss: 1.0884 || timer: 0.0892 sec.
iter 57570 || Loss: 1.2192 || timer: 0.0896 sec.
iter 57580 || Loss: 1.3439 || timer: 0.0909 sec.
iter 57590 || Loss: 1.3011 || timer: 0.1098 sec.
iter 57600 || Loss: 1.5111 || timer: 0.0833 sec.
iter 57610 || Loss: 1.3009 || timer: 0.0895 sec.
iter 57620 || Loss: 1.4714 || timer: 0.1129 sec.
iter 57630 || Loss: 1.0673 || timer: 0.0827 sec.
iter 57640 || Loss: 0.9697 || timer: 0.0233 sec.
iter 57650 || Loss: 0.3168 || timer: 0.1196 sec.
iter 57660 || Loss: 1.3764 || timer: 0.1009 sec.
iter 57670 || Loss: 1.2192 || timer: 0.0901 sec.
iter 57680 || Loss: 1.1617 || timer: 0.0864 sec.
iter 57690 || Loss: 1.6726 || timer: 0.0899 sec.
iter 57700 || Loss: 1.3469 || timer: 0.0898 sec.
iter 57710 || Loss: 1.1031 || timer: 0.0830 sec.
iter 57720 || Loss: 1.0767 || timer: 0.0906 sec.
iter 57730 || Loss: 1.1239 || timer: 0.0836 sec.
iter 57740 || Loss: 1.0098 || timer: 0.1018 sec.
iter 57750 || Loss: 1.0449 || timer: 0.0825 sec.
iter 57760 || Loss: 1.3230 || timer: 0.0947 sec.
iter 57770 || Loss: 1.1188 || timer: 0.0879 sec.
iter 57780 || Loss: 1.0438 || timer: 0.0884 sec.
iter 57790 || Loss: 1.0215 || timer: 0.0885 sec.
iter 57800 || Loss: 0.9167 || timer: 0.0908 sec.
iter 57810 || Loss: 1.2886 || timer: 0.0861 sec.
iter 57820 || Loss: 1.7190 || timer: 0.0840 sec.
iter 57830 || Loss: 0.6289 || timer: 0.0819 sec.
iter 57840 || Loss: 1.2109 || timer: 0.0847 sec.
iter 57850 || Loss: 1.2567 || timer: 0.1026 sec.
iter 57860 || Loss: 1.0828 || timer: 0.0821 sec.
iter 57870 || Loss: 1.0384 || timer: 0.0887 sec.
iter 57880 || Loss: 1.2644 || timer: 0.0934 sec.
iter 57890 || Loss: 1.1731 || timer: 0.0900 sec.
iter 57900 || Loss: 1.1536 || timer: 0.0932 sec.
iter 57910 || Loss: 1.4874 || timer: 0.0886 sec.
iter 57920 || Loss: 1.0242 || timer: 0.0900 sec.
iter 57930 || Loss: 0.8518 || timer: 0.0890 sec.
iter 57940 || Loss: 1.2653 || timer: 0.0892 sec.
iter 57950 || Loss: 0.8173 || timer: 0.0903 sec.
iter 57960 || Loss: 1.2396 || timer: 0.1020 sec.
iter 57970 || Loss: 1.3943 || timer: 0.0244 sec.
iter 57980 || Loss: 2.1352 || timer: 0.0828 sec.
iter 57990 || Loss: 1.1640 || timer: 0.0930 sec.
iter 58000 || Loss: 1.2655 || timer: 0.0919 sec.
iter 58010 || Loss: 1.2328 || timer: 0.0922 sec.
iter 58020 || Loss: 1.0155 || timer: 0.0849 sec.
iter 58030 || Loss: 1.6781 || timer: 0.0904 sec.
iter 58040 || Loss: 1.1933 || timer: 0.0873 sec.
iter 58050 || Loss: 4.3896 || timer: 0.0985 sec.
iter 58060 || Loss: 4.2496 || timer: 0.0881 sec.
iter 58070 || Loss: 4.3564 || timer: 0.1086 sec.
iter 58080 || Loss: 3.6549 || timer: 0.0828 sec.
iter 58090 || Loss: 2.2681 || timer: 0.0905 sec.
iter 58100 || Loss: 2.3741 || timer: 0.0868 sec.
iter 58110 || Loss: 2.0187 || timer: 0.0887 sec.
iter 58120 || Loss: 1.5627 || timer: 0.1147 sec.
iter 58130 || Loss: 1.6992 || timer: 0.0823 sec.
iter 58140 || Loss: 1.9776 || timer: 0.1214 sec.
iter 58150 || Loss: 1.2231 || timer: 0.1022 sec.
iter 58160 || Loss: 1.2207 || timer: 0.0933 sec.
iter 58170 || Loss: 1.5843 || timer: 0.0922 sec.
iter 58180 || Loss: 1.4221 || timer: 0.0914 sec.
iter 58190 || Loss: 1.4186 || timer: 0.0874 sec.
iter 58200 || Loss: 1.6175 || timer: 0.0901 sec.
iter 58210 || Loss: 0.9893 || timer: 0.0829 sec.
iter 58220 || Loss: 1.5761 || timer: 0.0888 sec.
iter 58230 || Loss: 1.7620 || timer: 0.1003 sec.
iter 58240 || Loss: 1.8602 || timer: 0.1034 sec.
iter 58250 || Loss: 1.8304 || timer: 0.0891 sec.
iter 58260 || Loss: 1.3893 || timer: 0.0960 sec.
iter 58270 || Loss: 1.5628 || timer: 0.0955 sec.
iter 58280 || Loss: 1.2725 || timer: 0.0948 sec.
iter 58290 || Loss: 1.8042 || timer: 0.0892 sec.
iter 58300 || Loss: 1.4010 || timer: 0.0174 sec.
iter 58310 || Loss: 0.6930 || timer: 0.1038 sec.
iter 58320 || Loss: 1.2510 || timer: 0.1126 sec.
iter 58330 || Loss: 1.2962 || timer: 0.1063 sec.
iter 58340 || Loss: 1.2827 || timer: 0.0940 sec.
iter 58350 || Loss: 1.6846 || timer: 0.0884 sec.
iter 58360 || Loss: 1.3272 || timer: 0.0827 sec.
iter 58370 || Loss: 1.3163 || timer: 0.1000 sec.
iter 58380 || Loss: 1.3471 || timer: 0.0885 sec.
iter 58390 || Loss: 1.7352 || timer: 0.0881 sec.
iter 58400 || Loss: 1.3527 || timer: 0.1356 sec.
iter 58410 || Loss: 1.0217 || timer: 0.0896 sec.
iter 58420 || Loss: 1.0466 || timer: 0.0886 sec.
iter 58430 || Loss: 1.4230 || timer: 0.0820 sec.
iter 58440 || Loss: 1.3214 || timer: 0.0832 sec.
iter 58450 || Loss: 1.2210 || timer: 0.0870 sec.
iter 58460 || Loss: 1.6006 || timer: 0.1182 sec.
iter 58470 || Loss: 1.1557 || timer: 0.0911 sec.
iter 58480 || Loss: 1.3332 || timer: 0.0870 sec.
iter 58490 || Loss: 1.4526 || timer: 0.0950 sec.
iter 58500 || Loss: 1.2929 || timer: 0.0914 sec.
iter 58510 || Loss: 1.1679 || timer: 0.0937 sec.
iter 58520 || Loss: 1.1565 || timer: 0.0894 sec.
iter 58530 || Loss: 1.1173 || timer: 0.0818 sec.
iter 58540 || Loss: 1.2915 || timer: 0.0889 sec.
iter 58550 || Loss: 1.1477 || timer: 0.0818 sec.
iter 58560 || Loss: 1.3093 || timer: 0.0839 sec.
iter 58570 || Loss: 1.2018 || timer: 0.0954 sec.
iter 58580 || Loss: 1.2610 || timer: 0.0827 sec.
iter 58590 || Loss: 1.2791 || timer: 0.0904 sec.
iter 58600 || Loss: 1.2582 || timer: 0.0905 sec.
iter 58610 || Loss: 1.1138 || timer: 0.1168 sec.
iter 58620 || Loss: 1.6164 || timer: 0.0904 sec.
iter 58630 || Loss: 2.0302 || timer: 0.0274 sec.
iter 58640 || Loss: 1.8417 || timer: 0.0882 sec.
iter 58650 || Loss: 1.4113 || timer: 0.0881 sec.
iter 58660 || Loss: 1.1551 || timer: 0.0776 sec.
iter 58670 || Loss: 1.1493 || timer: 0.1094 sec.
iter 58680 || Loss: 1.7815 || timer: 0.0878 sec.
iter 58690 || Loss: 1.6531 || timer: 0.0838 sec.
iter 58700 || Loss: 1.1433 || timer: 0.0754 sec.
iter 58710 || Loss: 1.0761 || timer: 0.0878 sec.
iter 58720 || Loss: 1.3800 || timer: 0.0838 sec.
iter 58730 || Loss: 1.4709 || timer: 0.0984 sec.
iter 58740 || Loss: 1.3931 || timer: 0.0808 sec.
iter 58750 || Loss: 1.5415 || timer: 0.1040 sec.
iter 58760 || Loss: 1.2637 || timer: 0.0836 sec.
iter 58770 || Loss: 1.2881 || timer: 0.1172 sec.
iter 58780 || Loss: 1.0606 || timer: 0.0848 sec.
iter 58790 || Loss: 1.4344 || timer: 0.1034 sec.
iter 58800 || Loss: 0.8910 || timer: 0.0842 sec.
iter 58810 || Loss: 1.3995 || timer: 0.0890 sec.
iter 58820 || Loss: 1.1671 || timer: 0.1080 sec.
iter 58830 || Loss: 1.2476 || timer: 0.1048 sec.
iter 58840 || Loss: 1.4077 || timer: 0.0830 sec.
iter 58850 || Loss: 1.3820 || timer: 0.0890 sec.
iter 58860 || Loss: 1.4637 || timer: 0.0910 sec.
iter 58870 || Loss: 1.1806 || timer: 0.1089 sec.
iter 58880 || Loss: 1.1152 || timer: 0.0816 sec.
iter 58890 || Loss: 0.9105 || timer: 0.1210 sec.
iter 58900 || Loss: 1.0604 || timer: 0.0894 sec.
iter 58910 || Loss: 2.1257 || timer: 0.0820 sec.
iter 58920 || Loss: 1.5135 || timer: 0.0827 sec.
iter 58930 || Loss: 0.8562 || timer: 0.0883 sec.
iter 58940 || Loss: 0.9746 || timer: 0.0984 sec.
iter 58950 || Loss: 1.6910 || timer: 0.0891 sec.
iter 58960 || Loss: 1.4472 || timer: 0.0265 sec.
iter 58970 || Loss: 0.5105 || timer: 0.0901 sec.
iter 58980 || Loss: 1.1551 || timer: 0.0886 sec.
iter 58990 || Loss: 1.2333 || timer: 0.0921 sec.
iter 59000 || Loss: 1.3202 || timer: 0.0897 sec.
iter 59010 || Loss: 1.3618 || timer: 0.0817 sec.
iter 59020 || Loss: 1.2923 || timer: 0.0816 sec.
iter 59030 || Loss: 0.9821 || timer: 0.0913 sec.
iter 59040 || Loss: 1.4399 || timer: 0.0889 sec.
iter 59050 || Loss: 1.2193 || timer: 0.0957 sec.
iter 59060 || Loss: 1.1186 || timer: 0.0934 sec.
iter 59070 || Loss: 1.3362 || timer: 0.0888 sec.
iter 59080 || Loss: 1.3476 || timer: 0.0993 sec.
iter 59090 || Loss: 1.2461 || timer: 0.0818 sec.
iter 59100 || Loss: 1.1122 || timer: 0.0901 sec.
iter 59110 || Loss: 1.1187 || timer: 0.1045 sec.
iter 59120 || Loss: 1.3291 || timer: 0.0822 sec.
iter 59130 || Loss: 1.1507 || timer: 0.0885 sec.
iter 59140 || Loss: 1.3440 || timer: 0.1160 sec.
iter 59150 || Loss: 1.2443 || timer: 0.0854 sec.
iter 59160 || Loss: 1.2601 || timer: 0.0960 sec.
iter 59170 || Loss: 1.2271 || timer: 0.0817 sec.
iter 59180 || Loss: 1.1539 || timer: 0.0807 sec.
iter 59190 || Loss: 0.9597 || timer: 0.0819 sec.
iter 59200 || Loss: 1.1606 || timer: 0.0807 sec.
iter 59210 || Loss: 1.2614 || timer: 0.0898 sec.
iter 59220 || Loss: 1.1939 || timer: 0.0898 sec.
iter 59230 || Loss: 1.1344 || timer: 0.0890 sec.
iter 59240 || Loss: 1.0547 || timer: 0.0828 sec.
iter 59250 || Loss: 1.6171 || timer: 0.0918 sec.
iter 59260 || Loss: 1.0431 || timer: 0.0806 sec.
iter 59270 || Loss: 1.0364 || timer: 0.1177 sec.
iter 59280 || Loss: 1.3213 || timer: 0.0839 sec.
iter 59290 || Loss: 1.6544 || timer: 0.0162 sec.
iter 59300 || Loss: 0.2700 || timer: 0.0929 sec.
iter 59310 || Loss: 1.1600 || timer: 0.0909 sec.
iter 59320 || Loss: 1.3508 || timer: 0.1120 sec.
iter 59330 || Loss: 1.0396 || timer: 0.0891 sec.
iter 59340 || Loss: 1.2496 || timer: 0.0885 sec.
iter 59350 || Loss: 0.7738 || timer: 0.0980 sec.
iter 59360 || Loss: 1.2955 || timer: 0.0807 sec.
iter 59370 || Loss: 0.8669 || timer: 0.0810 sec.
iter 59380 || Loss: 1.3418 || timer: 0.0890 sec.
iter 59390 || Loss: 1.2024 || timer: 0.1099 sec.
iter 59400 || Loss: 1.1373 || timer: 0.0807 sec.
iter 59410 || Loss: 1.2696 || timer: 0.0889 sec.
iter 59420 || Loss: 1.2977 || timer: 0.0942 sec.
iter 59430 || Loss: 1.3567 || timer: 0.0916 sec.
iter 59440 || Loss: 1.1565 || timer: 0.0966 sec.
iter 59450 || Loss: 1.2332 || timer: 0.0899 sec.
iter 59460 || Loss: 1.1028 || timer: 0.0886 sec.
iter 59470 || Loss: 1.0155 || timer: 0.0802 sec.
iter 59480 || Loss: 1.1301 || timer: 0.0904 sec.
iter 59490 || Loss: 1.2896 || timer: 0.0907 sec.
iter 59500 || Loss: 1.3651 || timer: 0.0888 sec.
iter 59510 || Loss: 1.0200 || timer: 0.1190 sec.
iter 59520 || Loss: 1.2392 || timer: 0.0818 sec.
iter 59530 || Loss: 0.9863 || timer: 0.1031 sec.
iter 59540 || Loss: 1.5129 || timer: 0.1297 sec.
iter 59550 || Loss: 1.3769 || timer: 0.0802 sec.
iter 59560 || Loss: 0.9019 || timer: 0.0742 sec.
iter 59570 || Loss: 1.5188 || timer: 0.0894 sec.
iter 59580 || Loss: 1.8404 || timer: 0.0875 sec.
iter 59590 || Loss: 1.6074 || timer: 0.0883 sec.
iter 59600 || Loss: 1.3606 || timer: 0.0901 sec.
iter 59610 || Loss: 1.4572 || timer: 0.0901 sec.
iter 59620 || Loss: 1.3120 || timer: 0.0220 sec.
iter 59630 || Loss: 0.7544 || timer: 0.0884 sec.
iter 59640 || Loss: 1.0644 || timer: 0.0851 sec.
iter 59650 || Loss: 1.2597 || timer: 0.0878 sec.
iter 59660 || Loss: 0.9328 || timer: 0.0812 sec.
iter 59670 || Loss: 1.0272 || timer: 0.1208 sec.
iter 59680 || Loss: 0.9526 || timer: 0.0809 sec.
iter 59690 || Loss: 0.9917 || timer: 0.1035 sec.
iter 59700 || Loss: 1.3760 || timer: 0.0870 sec.
iter 59710 || Loss: 1.2419 || timer: 0.0911 sec.
iter 59720 || Loss: 1.4151 || timer: 0.1064 sec.
iter 59730 || Loss: 1.1372 || timer: 0.0810 sec.
iter 59740 || Loss: 1.7251 || timer: 0.1079 sec.
iter 59750 || Loss: 1.0987 || timer: 0.1016 sec.
iter 59760 || Loss: 0.9807 || timer: 0.0983 sec.
iter 59770 || Loss: 1.3141 || timer: 0.0971 sec.
iter 59780 || Loss: 1.0931 || timer: 0.0821 sec.
iter 59790 || Loss: 1.1875 || timer: 0.1041 sec.
iter 59800 || Loss: 1.2544 || timer: 0.0943 sec.
iter 59810 || Loss: 1.0078 || timer: 0.1054 sec.
iter 59820 || Loss: 1.3001 || timer: 0.0951 sec.
iter 59830 || Loss: 1.0316 || timer: 0.0887 sec.
iter 59840 || Loss: 1.3359 || timer: 0.0806 sec.
iter 59850 || Loss: 1.4412 || timer: 0.0872 sec.
iter 59860 || Loss: 1.0035 || timer: 0.0909 sec.
iter 59870 || Loss: 1.0414 || timer: 0.0813 sec.
iter 59880 || Loss: 1.3802 || timer: 0.0907 sec.
iter 59890 || Loss: 1.0969 || timer: 0.0890 sec.
iter 59900 || Loss: 1.1917 || timer: 0.0812 sec.
iter 59910 || Loss: 1.1179 || timer: 0.0817 sec.
iter 59920 || Loss: 1.1823 || timer: 0.0807 sec.
iter 59930 || Loss: 1.1010 || timer: 0.0859 sec.
iter 59940 || Loss: 1.3479 || timer: 0.0899 sec.
iter 59950 || Loss: 1.3385 || timer: 0.0172 sec.
iter 59960 || Loss: 0.7515 || timer: 0.0909 sec.
iter 59970 || Loss: 0.9613 || timer: 0.0900 sec.
iter 59980 || Loss: 1.2307 || timer: 0.0815 sec.
iter 59990 || Loss: 1.1837 || timer: 0.0947 sec.
iter 60000 || Loss: 1.0786 || Saving state, iter: 60000
timer: 0.1081 sec.
iter 60010 || Loss: 0.9840 || timer: 0.0841 sec.
iter 60020 || Loss: 1.2189 || timer: 0.0928 sec.
iter 60030 || Loss: 0.9435 || timer: 0.0841 sec.
iter 60040 || Loss: 1.3297 || timer: 0.0923 sec.
iter 60050 || Loss: 1.2975 || timer: 0.1071 sec.
iter 60060 || Loss: 1.3756 || timer: 0.0816 sec.
iter 60070 || Loss: 1.0756 || timer: 0.0909 sec.
iter 60080 || Loss: 1.3611 || timer: 0.0822 sec.
iter 60090 || Loss: 1.0617 || timer: 0.0854 sec.
iter 60100 || Loss: 1.3729 || timer: 0.0798 sec.
iter 60110 || Loss: 1.3465 || timer: 0.1078 sec.
iter 60120 || Loss: 1.2240 || timer: 0.0811 sec.
iter 60130 || Loss: 1.6599 || timer: 0.0902 sec.
iter 60140 || Loss: 1.1720 || timer: 0.0842 sec.
iter 60150 || Loss: 1.3488 || timer: 0.0899 sec.
iter 60160 || Loss: 1.4125 || timer: 0.1039 sec.
iter 60170 || Loss: 1.1906 || timer: 0.0812 sec.
iter 60180 || Loss: 1.0556 || timer: 0.0805 sec.
iter 60190 || Loss: 1.5508 || timer: 0.0827 sec.
iter 60200 || Loss: 1.5444 || timer: 0.0978 sec.
iter 60210 || Loss: 0.8960 || timer: 0.1211 sec.
iter 60220 || Loss: 1.8542 || timer: 0.0853 sec.
iter 60230 || Loss: 0.9368 || timer: 0.0901 sec.
iter 60240 || Loss: 1.5959 || timer: 0.0896 sec.
iter 60250 || Loss: 1.4044 || timer: 0.0822 sec.
iter 60260 || Loss: 1.1780 || timer: 0.0897 sec.
iter 60270 || Loss: 1.1847 || timer: 0.0806 sec.
iter 60280 || Loss: 1.2931 || timer: 0.0163 sec.
iter 60290 || Loss: 1.4576 || timer: 0.0889 sec.
iter 60300 || Loss: 1.2852 || timer: 0.0820 sec.
iter 60310 || Loss: 1.5889 || timer: 0.0818 sec.
iter 60320 || Loss: 1.2949 || timer: 0.0897 sec.
iter 60330 || Loss: 1.4147 || timer: 0.1101 sec.
iter 60340 || Loss: 1.0832 || timer: 0.0891 sec.
iter 60350 || Loss: 1.4866 || timer: 0.0915 sec.
iter 60360 || Loss: 1.4309 || timer: 0.0821 sec.
iter 60370 || Loss: 1.5536 || timer: 0.1038 sec.
iter 60380 || Loss: 1.2389 || timer: 0.1024 sec.
iter 60390 || Loss: 1.4617 || timer: 0.0982 sec.
iter 60400 || Loss: 1.7366 || timer: 0.0884 sec.
iter 60410 || Loss: 1.2732 || timer: 0.0816 sec.
iter 60420 || Loss: 1.2076 || timer: 0.1126 sec.
iter 60430 || Loss: 1.2966 || timer: 0.0884 sec.
iter 60440 || Loss: 1.0366 || timer: 0.1059 sec.
iter 60450 || Loss: 1.2127 || timer: 0.0887 sec.
iter 60460 || Loss: 1.2463 || timer: 0.1128 sec.
iter 60470 || Loss: 1.8897 || timer: 0.0815 sec.
iter 60480 || Loss: 1.0948 || timer: 0.1002 sec.
iter 60490 || Loss: 1.5826 || timer: 0.0847 sec.
iter 60500 || Loss: 1.0803 || timer: 0.0925 sec.
iter 60510 || Loss: 1.0400 || timer: 0.0897 sec.
iter 60520 || Loss: 1.3782 || timer: 0.0878 sec.
iter 60530 || Loss: 0.9730 || timer: 0.0824 sec.
iter 60540 || Loss: 1.0037 || timer: 0.0911 sec.
iter 60550 || Loss: 1.1577 || timer: 0.1119 sec.
iter 60560 || Loss: 1.4077 || timer: 0.0902 sec.
iter 60570 || Loss: 1.4717 || timer: 0.0936 sec.
iter 60580 || Loss: 1.3155 || timer: 0.0805 sec.
iter 60590 || Loss: 1.3489 || timer: 0.0828 sec.
iter 60600 || Loss: 1.2543 || timer: 0.0903 sec.
iter 60610 || Loss: 1.2244 || timer: 0.0237 sec.
iter 60620 || Loss: 2.0087 || timer: 0.0818 sec.
iter 60630 || Loss: 1.7679 || timer: 0.0896 sec.
iter 60640 || Loss: 1.2816 || timer: 0.0823 sec.
iter 60650 || Loss: 1.4321 || timer: 0.0886 sec.
iter 60660 || Loss: 1.3798 || timer: 0.0892 sec.
iter 60670 || Loss: 1.1337 || timer: 0.0899 sec.
iter 60680 || Loss: 1.3348 || timer: 0.0904 sec.
iter 60690 || Loss: 1.8670 || timer: 0.0808 sec.
iter 60700 || Loss: 1.4983 || timer: 0.0845 sec.
iter 60710 || Loss: 1.5296 || timer: 0.1280 sec.
iter 60720 || Loss: 0.9407 || timer: 0.0950 sec.
iter 60730 || Loss: 1.3881 || timer: 0.0860 sec.
iter 60740 || Loss: 1.1362 || timer: 0.0828 sec.
iter 60750 || Loss: 1.3556 || timer: 0.0824 sec.
iter 60760 || Loss: 0.9592 || timer: 0.1052 sec.
iter 60770 || Loss: 1.2540 || timer: 0.0818 sec.
iter 60780 || Loss: 1.5103 || timer: 0.1059 sec.
iter 60790 || Loss: 1.3575 || timer: 0.0873 sec.
iter 60800 || Loss: 1.2065 || timer: 0.0935 sec.
iter 60810 || Loss: 0.9472 || timer: 0.1052 sec.
iter 60820 || Loss: 1.4656 || timer: 0.0901 sec.
iter 60830 || Loss: 1.4167 || timer: 0.0859 sec.
iter 60840 || Loss: 1.2955 || timer: 0.0910 sec.
iter 60850 || Loss: 1.5418 || timer: 0.0806 sec.
iter 60860 || Loss: 1.5335 || timer: 0.1106 sec.
iter 60870 || Loss: 1.1091 || timer: 0.0929 sec.
iter 60880 || Loss: 1.4238 || timer: 0.0914 sec.
iter 60890 || Loss: 1.2110 || timer: 0.0891 sec.
iter 60900 || Loss: 1.1708 || timer: 0.0805 sec.
iter 60910 || Loss: 1.5617 || timer: 0.0926 sec.
iter 60920 || Loss: 0.9970 || timer: 0.0911 sec.
iter 60930 || Loss: 1.5167 || timer: 0.0856 sec.
iter 60940 || Loss: 1.2502 || timer: 0.0160 sec.
iter 60950 || Loss: 0.8486 || timer: 0.0881 sec.
iter 60960 || Loss: 0.9002 || timer: 0.0899 sec.
iter 60970 || Loss: 1.2868 || timer: 0.0910 sec.
iter 60980 || Loss: 1.1259 || timer: 0.0814 sec.
iter 60990 || Loss: 1.3988 || timer: 0.0915 sec.
iter 61000 || Loss: 1.7469 || timer: 0.0869 sec.
iter 61010 || Loss: 1.3376 || timer: 0.0906 sec.
iter 61020 || Loss: 1.0711 || timer: 0.0815 sec.
iter 61030 || Loss: 1.2985 || timer: 0.0892 sec.
iter 61040 || Loss: 1.3378 || timer: 0.1060 sec.
iter 61050 || Loss: 1.5431 || timer: 0.0880 sec.
iter 61060 || Loss: 1.2128 || timer: 0.0884 sec.
iter 61070 || Loss: 1.0543 || timer: 0.0874 sec.
iter 61080 || Loss: 1.3970 || timer: 0.0888 sec.
iter 61090 || Loss: 1.3214 || timer: 0.1103 sec.
iter 61100 || Loss: 1.2574 || timer: 0.0936 sec.
iter 61110 || Loss: 1.0823 || timer: 0.0813 sec.
iter 61120 || Loss: 1.2778 || timer: 0.0818 sec.
iter 61130 || Loss: 0.8257 || timer: 0.1144 sec.
iter 61140 || Loss: 1.1921 || timer: 0.0820 sec.
iter 61150 || Loss: 1.0618 || timer: 0.1052 sec.
iter 61160 || Loss: 1.2819 || timer: 0.0812 sec.
iter 61170 || Loss: 1.0912 || timer: 0.0800 sec.
iter 61180 || Loss: 0.9189 || timer: 0.0809 sec.
iter 61190 || Loss: 1.4416 || timer: 0.0895 sec.
iter 61200 || Loss: 1.3461 || timer: 0.0892 sec.
iter 61210 || Loss: 1.5598 || timer: 0.0898 sec.
iter 61220 || Loss: 1.3091 || timer: 0.0918 sec.
iter 61230 || Loss: 0.8939 || timer: 0.0888 sec.
iter 61240 || Loss: 1.2565 || timer: 0.0912 sec.
iter 61250 || Loss: 1.4781 || timer: 0.1008 sec.
iter 61260 || Loss: 1.2878 || timer: 0.0938 sec.
iter 61270 || Loss: 1.0957 || timer: 0.0161 sec.
iter 61280 || Loss: 0.7515 || timer: 0.0817 sec.
iter 61290 || Loss: 1.2336 || timer: 0.0927 sec.
iter 61300 || Loss: 1.3386 || timer: 0.0807 sec.
iter 61310 || Loss: 1.0531 || timer: 0.0816 sec.
iter 61320 || Loss: 1.2181 || timer: 0.0870 sec.
iter 61330 || Loss: 1.2244 || timer: 0.0897 sec.
iter 61340 || Loss: 1.0386 || timer: 0.0867 sec.
iter 61350 || Loss: 1.0649 || timer: 0.0878 sec.
iter 61360 || Loss: 0.9676 || timer: 0.0868 sec.
iter 61370 || Loss: 0.7939 || timer: 0.1080 sec.
iter 61380 || Loss: 1.1432 || timer: 0.0856 sec.
iter 61390 || Loss: 1.2680 || timer: 0.1025 sec.
iter 61400 || Loss: 0.8850 || timer: 0.0810 sec.
iter 61410 || Loss: 1.4919 || timer: 0.0914 sec.
iter 61420 || Loss: 1.1563 || timer: 0.1061 sec.
iter 61430 || Loss: 1.4073 || timer: 0.0953 sec.
iter 61440 || Loss: 1.2448 || timer: 0.0882 sec.
iter 61450 || Loss: 1.5878 || timer: 0.0892 sec.
iter 61460 || Loss: 1.1655 || timer: 0.0878 sec.
iter 61470 || Loss: 1.2568 || timer: 0.1160 sec.
iter 61480 || Loss: 1.1344 || timer: 0.0821 sec.
iter 61490 || Loss: 1.4555 || timer: 0.0821 sec.
iter 61500 || Loss: 1.2277 || timer: 0.0823 sec.
iter 61510 || Loss: 1.2380 || timer: 0.0890 sec.
iter 61520 || Loss: 1.4351 || timer: 0.0991 sec.
iter 61530 || Loss: 1.2272 || timer: 0.0892 sec.
iter 61540 || Loss: 1.4847 || timer: 0.0904 sec.
iter 61550 || Loss: 1.3339 || timer: 0.0862 sec.
iter 61560 || Loss: 1.9344 || timer: 0.0808 sec.
iter 61570 || Loss: 0.9540 || timer: 0.0895 sec.
iter 61580 || Loss: 1.0607 || timer: 0.0816 sec.
iter 61590 || Loss: 0.9907 || timer: 0.1144 sec.
iter 61600 || Loss: 1.2029 || timer: 0.0160 sec.
iter 61610 || Loss: 0.8351 || timer: 0.0941 sec.
iter 61620 || Loss: 1.2592 || timer: 0.0883 sec.
iter 61630 || Loss: 1.2140 || timer: 0.0901 sec.
iter 61640 || Loss: 0.9964 || timer: 0.0798 sec.
iter 61650 || Loss: 1.1336 || timer: 0.0869 sec.
iter 61660 || Loss: 1.3264 || timer: 0.0808 sec.
iter 61670 || Loss: 1.3624 || timer: 0.1193 sec.
iter 61680 || Loss: 1.0618 || timer: 0.0861 sec.
iter 61690 || Loss: 1.0881 || timer: 0.0813 sec.
iter 61700 || Loss: 1.1891 || timer: 0.0987 sec.
iter 61710 || Loss: 1.1018 || timer: 0.0815 sec.
iter 61720 || Loss: 1.0444 || timer: 0.0882 sec.
iter 61730 || Loss: 1.4665 || timer: 0.0894 sec.
iter 61740 || Loss: 1.2602 || timer: 0.0887 sec.
iter 61750 || Loss: 1.3215 || timer: 0.0858 sec.
iter 61760 || Loss: 1.1413 || timer: 0.0896 sec.
iter 61770 || Loss: 1.4056 || timer: 0.0817 sec.
iter 61780 || Loss: 1.0250 || timer: 0.0890 sec.
iter 61790 || Loss: 1.1354 || timer: 0.0902 sec.
iter 61800 || Loss: 1.0188 || timer: 0.0921 sec.
iter 61810 || Loss: 0.9039 || timer: 0.1051 sec.
iter 61820 || Loss: 1.2611 || timer: 0.0850 sec.
iter 61830 || Loss: 1.4517 || timer: 0.0808 sec.
iter 61840 || Loss: 1.4253 || timer: 0.0906 sec.
iter 61850 || Loss: 1.1746 || timer: 0.0911 sec.
iter 61860 || Loss: 1.1558 || timer: 0.1134 sec.
iter 61870 || Loss: 1.0645 || timer: 0.0900 sec.
iter 61880 || Loss: 1.4176 || timer: 0.0815 sec.
iter 61890 || Loss: 0.8142 || timer: 0.0858 sec.
iter 61900 || Loss: 1.2213 || timer: 0.0809 sec.
iter 61910 || Loss: 1.0775 || timer: 0.0901 sec.
iter 61920 || Loss: 0.8725 || timer: 0.0883 sec.
iter 61930 || Loss: 1.3728 || timer: 0.0236 sec.
iter 61940 || Loss: 0.6371 || timer: 0.0818 sec.
iter 61950 || Loss: 1.2056 || timer: 0.0957 sec.
iter 61960 || Loss: 1.0593 || timer: 0.0923 sec.
iter 61970 || Loss: 0.9692 || timer: 0.0806 sec.
iter 61980 || Loss: 1.3246 || timer: 0.0894 sec.
iter 61990 || Loss: 1.0400 || timer: 0.1077 sec.
iter 62000 || Loss: 1.1381 || timer: 0.0819 sec.
iter 62010 || Loss: 1.2405 || timer: 0.1103 sec.
iter 62020 || Loss: 1.3293 || timer: 0.0971 sec.
iter 62030 || Loss: 1.0357 || timer: 0.1088 sec.
iter 62040 || Loss: 1.0999 || timer: 0.1052 sec.
iter 62050 || Loss: 1.1299 || timer: 0.0830 sec.
iter 62060 || Loss: 1.0658 || timer: 0.0829 sec.
iter 62070 || Loss: 1.3722 || timer: 0.0888 sec.
iter 62080 || Loss: 1.4130 || timer: 0.0819 sec.
iter 62090 || Loss: 1.0484 || timer: 0.0882 sec.
iter 62100 || Loss: 0.7160 || timer: 0.0811 sec.
iter 62110 || Loss: 1.0022 || timer: 0.0888 sec.
iter 62120 || Loss: 1.1844 || timer: 0.0813 sec.
iter 62130 || Loss: 1.3348 || timer: 0.0816 sec.
iter 62140 || Loss: 1.2481 || timer: 0.1132 sec.
iter 62150 || Loss: 1.6509 || timer: 0.0878 sec.
iter 62160 || Loss: 1.8250 || timer: 0.0814 sec.
iter 62170 || Loss: 1.2071 || timer: 0.0856 sec.
iter 62180 || Loss: 0.9891 || timer: 0.0876 sec.
iter 62190 || Loss: 1.3360 || timer: 0.0877 sec.
iter 62200 || Loss: 1.1982 || timer: 0.1075 sec.
iter 62210 || Loss: 1.4820 || timer: 0.0898 sec.
iter 62220 || Loss: 1.4009 || timer: 0.1257 sec.
iter 62230 || Loss: 1.3873 || timer: 0.0844 sec.
iter 62240 || Loss: 1.1736 || timer: 0.0893 sec.
iter 62250 || Loss: 1.2088 || timer: 0.0825 sec.
iter 62260 || Loss: 1.0103 || timer: 0.0159 sec.
iter 62270 || Loss: 0.7828 || timer: 0.0836 sec.
iter 62280 || Loss: 1.2717 || timer: 0.1077 sec.
iter 62290 || Loss: 1.0363 || timer: 0.0816 sec.
iter 62300 || Loss: 1.2509 || timer: 0.0913 sec.
iter 62310 || Loss: 1.3147 || timer: 0.0896 sec.
iter 62320 || Loss: 1.2928 || timer: 0.1103 sec.
iter 62330 || Loss: 0.9476 || timer: 0.0885 sec.
iter 62340 || Loss: 1.2272 || timer: 0.0956 sec.
iter 62350 || Loss: 1.0100 || timer: 0.0867 sec.
iter 62360 || Loss: 1.5063 || timer: 0.1415 sec.
iter 62370 || Loss: 1.3197 || timer: 0.1163 sec.
iter 62380 || Loss: 1.4773 || timer: 0.0857 sec.
iter 62390 || Loss: 1.0539 || timer: 0.0861 sec.
iter 62400 || Loss: 1.0823 || timer: 0.0820 sec.
iter 62410 || Loss: 1.4440 || timer: 0.0914 sec.
iter 62420 || Loss: 1.2189 || timer: 0.0897 sec.
iter 62430 || Loss: 1.2330 || timer: 0.0916 sec.
iter 62440 || Loss: 1.5623 || timer: 0.0876 sec.
iter 62450 || Loss: 1.8968 || timer: 0.0898 sec.
iter 62460 || Loss: 0.9971 || timer: 0.0886 sec.
iter 62470 || Loss: 0.8905 || timer: 0.1092 sec.
iter 62480 || Loss: 1.0037 || timer: 0.0894 sec.
iter 62490 || Loss: 1.8056 || timer: 0.0900 sec.
iter 62500 || Loss: 1.0863 || timer: 0.0804 sec.
iter 62510 || Loss: 1.4627 || timer: 0.1112 sec.
iter 62520 || Loss: 1.2461 || timer: 0.0973 sec.
iter 62530 || Loss: 1.1970 || timer: 0.0843 sec.
iter 62540 || Loss: 1.3525 || timer: 0.0974 sec.
iter 62550 || Loss: 1.4064 || timer: 0.0930 sec.
iter 62560 || Loss: 1.1174 || timer: 0.0903 sec.
iter 62570 || Loss: 1.2195 || timer: 0.0894 sec.
iter 62580 || Loss: 1.1292 || timer: 0.0906 sec.
iter 62590 || Loss: 1.5371 || timer: 0.0258 sec.
iter 62600 || Loss: 0.8982 || timer: 0.0927 sec.
iter 62610 || Loss: 1.1612 || timer: 0.1270 sec.
iter 62620 || Loss: 1.3034 || timer: 0.0876 sec.
iter 62630 || Loss: 1.2185 || timer: 0.1027 sec.
iter 62640 || Loss: 1.4073 || timer: 0.0994 sec.
iter 62650 || Loss: 1.8433 || timer: 0.0917 sec.
iter 62660 || Loss: 1.2740 || timer: 0.0870 sec.
iter 62670 || Loss: 0.9919 || timer: 0.0904 sec.
iter 62680 || Loss: 0.9291 || timer: 0.0816 sec.
iter 62690 || Loss: 1.3610 || timer: 0.0952 sec.
iter 62700 || Loss: 1.0019 || timer: 0.0915 sec.
iter 62710 || Loss: 1.1276 || timer: 0.0962 sec.
iter 62720 || Loss: 1.0038 || timer: 0.0824 sec.
iter 62730 || Loss: 1.0119 || timer: 0.0893 sec.
iter 62740 || Loss: 1.0264 || timer: 0.1004 sec.
iter 62750 || Loss: 1.3170 || timer: 0.0803 sec.
iter 62760 || Loss: 1.7165 || timer: 0.0884 sec.
iter 62770 || Loss: 1.4713 || timer: 0.0822 sec.
iter 62780 || Loss: 1.3902 || timer: 0.1112 sec.
iter 62790 || Loss: 1.2840 || timer: 0.0832 sec.
iter 62800 || Loss: 1.4735 || timer: 0.0905 sec.
iter 62810 || Loss: 1.1408 || timer: 0.0824 sec.
iter 62820 || Loss: 1.2187 || timer: 0.0898 sec.
iter 62830 || Loss: 0.7887 || timer: 0.0869 sec.
iter 62840 || Loss: 1.2848 || timer: 0.0898 sec.
iter 62850 || Loss: 1.5111 || timer: 0.0896 sec.
iter 62860 || Loss: 1.1860 || timer: 0.0838 sec.
iter 62870 || Loss: 0.9405 || timer: 0.0923 sec.
iter 62880 || Loss: 1.2105 || timer: 0.0829 sec.
iter 62890 || Loss: 0.7928 || timer: 0.0910 sec.
iter 62900 || Loss: 1.0884 || timer: 0.0888 sec.
iter 62910 || Loss: 1.0136 || timer: 0.0902 sec.
iter 62920 || Loss: 1.5396 || timer: 0.0262 sec.
iter 62930 || Loss: 0.8182 || timer: 0.0871 sec.
iter 62940 || Loss: 2.0941 || timer: 0.0926 sec.
iter 62950 || Loss: 1.2202 || timer: 0.0922 sec.
iter 62960 || Loss: 0.8883 || timer: 0.0911 sec.
iter 62970 || Loss: 1.0323 || timer: 0.0948 sec.
iter 62980 || Loss: 1.5645 || timer: 0.0876 sec.
iter 62990 || Loss: 1.4567 || timer: 0.0877 sec.
iter 63000 || Loss: 1.5919 || timer: 0.0985 sec.
iter 63010 || Loss: 1.4826 || timer: 0.0916 sec.
iter 63020 || Loss: 1.0276 || timer: 0.0983 sec.
iter 63030 || Loss: 1.1174 || timer: 0.0829 sec.
iter 63040 || Loss: 1.2193 || timer: 0.0811 sec.
iter 63050 || Loss: 1.3639 || timer: 0.0832 sec.
iter 63060 || Loss: 1.2190 || timer: 0.0863 sec.
iter 63070 || Loss: 1.0858 || timer: 0.0931 sec.
iter 63080 || Loss: 1.2788 || timer: 0.0832 sec.
iter 63090 || Loss: 1.3501 || timer: 0.0880 sec.
iter 63100 || Loss: 1.1439 || timer: 0.1054 sec.
iter 63110 || Loss: 1.2831 || timer: 0.0939 sec.
iter 63120 || Loss: 1.2180 || timer: 0.1080 sec.
iter 63130 || Loss: 0.9656 || timer: 0.0906 sec.
iter 63140 || Loss: 1.2036 || timer: 0.0892 sec.
iter 63150 || Loss: 1.1468 || timer: 0.0747 sec.
iter 63160 || Loss: 0.9334 || timer: 0.1011 sec.
iter 63170 || Loss: 0.9077 || timer: 0.1153 sec.
iter 63180 || Loss: 1.0514 || timer: 0.0825 sec.
iter 63190 || Loss: 1.4898 || timer: 0.0886 sec.
iter 63200 || Loss: 1.2462 || timer: 0.0975 sec.
iter 63210 || Loss: 0.9655 || timer: 0.0911 sec.
iter 63220 || Loss: 1.2666 || timer: 0.0883 sec.
iter 63230 || Loss: 1.0079 || timer: 0.0897 sec.
iter 63240 || Loss: 0.9232 || timer: 0.0933 sec.
iter 63250 || Loss: 1.5029 || timer: 0.0146 sec.
iter 63260 || Loss: 0.5835 || timer: 0.0807 sec.
iter 63270 || Loss: 1.3038 || timer: 0.1056 sec.
iter 63280 || Loss: 1.0002 || timer: 0.1236 sec.
iter 63290 || Loss: 0.9524 || timer: 0.0755 sec.
iter 63300 || Loss: 1.4657 || timer: 0.1037 sec.
iter 63310 || Loss: 1.5831 || timer: 0.0824 sec.
iter 63320 || Loss: 1.5324 || timer: 0.0823 sec.
iter 63330 || Loss: 1.1062 || timer: 0.1100 sec.
iter 63340 || Loss: 1.0209 || timer: 0.1131 sec.
iter 63350 || Loss: 1.4839 || timer: 0.1186 sec.
iter 63360 || Loss: 1.2566 || timer: 0.0898 sec.
iter 63370 || Loss: 1.4646 || timer: 0.0893 sec.
iter 63380 || Loss: 0.9087 || timer: 0.0834 sec.
iter 63390 || Loss: 1.2005 || timer: 0.0855 sec.
iter 63400 || Loss: 1.1254 || timer: 0.0826 sec.
iter 63410 || Loss: 0.8004 || timer: 0.1044 sec.
iter 63420 || Loss: 1.0279 || timer: 0.0896 sec.
iter 63430 || Loss: 1.1251 || timer: 0.0890 sec.
iter 63440 || Loss: 1.1237 || timer: 0.0929 sec.
iter 63450 || Loss: 0.8490 || timer: 0.1222 sec.
iter 63460 || Loss: 1.2137 || timer: 0.0944 sec.
iter 63470 || Loss: 1.3324 || timer: 0.0823 sec.
iter 63480 || Loss: 1.3580 || timer: 0.0824 sec.
iter 63490 || Loss: 1.0406 || timer: 0.0916 sec.
iter 63500 || Loss: 1.2194 || timer: 0.0800 sec.
iter 63510 || Loss: 0.8631 || timer: 0.0879 sec.
iter 63520 || Loss: 0.9428 || timer: 0.0939 sec.
iter 63530 || Loss: 1.2474 || timer: 0.0894 sec.
iter 63540 || Loss: 1.1502 || timer: 0.0998 sec.
iter 63550 || Loss: 1.3619 || timer: 0.0822 sec.
iter 63560 || Loss: 1.0264 || timer: 0.0930 sec.
iter 63570 || Loss: 1.8021 || timer: 0.0897 sec.
iter 63580 || Loss: 1.2431 || timer: 0.0229 sec.
iter 63590 || Loss: 0.4922 || timer: 0.0930 sec.
iter 63600 || Loss: 0.8586 || timer: 0.0945 sec.
iter 63610 || Loss: 0.9089 || timer: 0.0812 sec.
iter 63620 || Loss: 1.1398 || timer: 0.0896 sec.
iter 63630 || Loss: 1.1827 || timer: 0.0814 sec.
iter 63640 || Loss: 1.1883 || timer: 0.0822 sec.
iter 63650 || Loss: 1.2478 || timer: 0.0815 sec.
iter 63660 || Loss: 1.4083 || timer: 0.0984 sec.
iter 63670 || Loss: 1.2463 || timer: 0.1052 sec.
iter 63680 || Loss: 0.9661 || timer: 0.0962 sec.
iter 63690 || Loss: 0.9577 || timer: 0.0924 sec.
iter 63700 || Loss: 1.0699 || timer: 0.0979 sec.
iter 63710 || Loss: 1.4841 || timer: 0.0883 sec.
iter 63720 || Loss: 1.1991 || timer: 0.0887 sec.
iter 63730 || Loss: 1.2743 || timer: 0.0839 sec.
iter 63740 || Loss: 0.8794 || timer: 0.0796 sec.
iter 63750 || Loss: 0.9986 || timer: 0.0842 sec.
iter 63760 || Loss: 1.1530 || timer: 0.0894 sec.
iter 63770 || Loss: 1.0739 || timer: 0.1006 sec.
iter 63780 || Loss: 1.1260 || timer: 0.0916 sec.
iter 63790 || Loss: 1.5282 || timer: 0.0904 sec.
iter 63800 || Loss: 1.0750 || timer: 0.1118 sec.
iter 63810 || Loss: 0.9846 || timer: 0.0900 sec.
iter 63820 || Loss: 0.9943 || timer: 0.0963 sec.
iter 63830 || Loss: 1.1475 || timer: 0.0818 sec.
iter 63840 || Loss: 1.2359 || timer: 0.0841 sec.
iter 63850 || Loss: 1.1885 || timer: 0.1086 sec.
iter 63860 || Loss: 1.2051 || timer: 0.0824 sec.
iter 63870 || Loss: 1.8350 || timer: 0.0910 sec.
iter 63880 || Loss: 1.0817 || timer: 0.0921 sec.
iter 63890 || Loss: 1.1447 || timer: 0.0838 sec.
iter 63900 || Loss: 1.0028 || timer: 0.0910 sec.
iter 63910 || Loss: 1.4160 || timer: 0.0265 sec.
iter 63920 || Loss: 2.8333 || timer: 0.0894 sec.
iter 63930 || Loss: 1.2623 || timer: 0.0844 sec.
iter 63940 || Loss: 1.2037 || timer: 0.0883 sec.
iter 63950 || Loss: 1.1664 || timer: 0.0904 sec.
iter 63960 || Loss: 1.3596 || timer: 0.0997 sec.
iter 63970 || Loss: 1.1472 || timer: 0.0821 sec.
iter 63980 || Loss: 1.3335 || timer: 0.0928 sec.
iter 63990 || Loss: 1.5041 || timer: 0.0896 sec.
iter 64000 || Loss: 1.4189 || timer: 0.0825 sec.
iter 64010 || Loss: 1.1383 || timer: 0.0968 sec.
iter 64020 || Loss: 1.1126 || timer: 0.0868 sec.
iter 64030 || Loss: 1.5010 || timer: 0.0809 sec.
iter 64040 || Loss: 1.1135 || timer: 0.0910 sec.
iter 64050 || Loss: 0.8617 || timer: 0.0894 sec.
iter 64060 || Loss: 0.8604 || timer: 0.0964 sec.
iter 64070 || Loss: 1.3413 || timer: 0.0847 sec.
iter 64080 || Loss: 1.1057 || timer: 0.0913 sec.
iter 64090 || Loss: 1.3998 || timer: 0.1054 sec.
iter 64100 || Loss: 1.4510 || timer: 0.0898 sec.
iter 64110 || Loss: 0.9402 || timer: 0.0894 sec.
iter 64120 || Loss: 1.0892 || timer: 0.0808 sec.
iter 64130 || Loss: 1.2180 || timer: 0.0879 sec.
iter 64140 || Loss: 1.3997 || timer: 0.0822 sec.
iter 64150 || Loss: 1.2740 || timer: 0.0895 sec.
iter 64160 || Loss: 1.3374 || timer: 0.1081 sec.
iter 64170 || Loss: 1.4446 || timer: 0.0818 sec.
iter 64180 || Loss: 0.8241 || timer: 0.0829 sec.
iter 64190 || Loss: 1.1045 || timer: 0.0922 sec.
iter 64200 || Loss: 0.9346 || timer: 0.0813 sec.
iter 64210 || Loss: 1.0818 || timer: 0.0827 sec.
iter 64220 || Loss: 1.3376 || timer: 0.0819 sec.
iter 64230 || Loss: 1.1541 || timer: 0.0829 sec.
iter 64240 || Loss: 1.1229 || timer: 0.0198 sec.
iter 64250 || Loss: 0.9544 || timer: 0.0892 sec.
iter 64260 || Loss: 1.1860 || timer: 0.1269 sec.
iter 64270 || Loss: 1.3319 || timer: 0.1043 sec.
iter 64280 || Loss: 0.7087 || timer: 0.1143 sec.
iter 64290 || Loss: 1.3227 || timer: 0.0941 sec.
iter 64300 || Loss: 1.2116 || timer: 0.0885 sec.
iter 64310 || Loss: 1.3073 || timer: 0.0821 sec.
iter 64320 || Loss: 1.1159 || timer: 0.0841 sec.
iter 64330 || Loss: 1.1244 || timer: 0.0922 sec.
iter 64340 || Loss: 0.8871 || timer: 0.1072 sec.
iter 64350 || Loss: 0.8833 || timer: 0.0869 sec.
iter 64360 || Loss: 0.9110 || timer: 0.0906 sec.
iter 64370 || Loss: 1.4607 || timer: 0.0838 sec.
iter 64380 || Loss: 1.3101 || timer: 0.0818 sec.
iter 64390 || Loss: 0.7229 || timer: 0.0858 sec.
iter 64400 || Loss: 1.2266 || timer: 0.0867 sec.
iter 64410 || Loss: 1.2586 || timer: 0.0863 sec.
iter 64420 || Loss: 1.4069 || timer: 0.1001 sec.
iter 64430 || Loss: 1.0694 || timer: 0.1100 sec.
iter 64440 || Loss: 0.9609 || timer: 0.0849 sec.
iter 64450 || Loss: 1.2718 || timer: 0.0908 sec.
iter 64460 || Loss: 1.1942 || timer: 0.0757 sec.
iter 64470 || Loss: 1.2355 || timer: 0.0830 sec.
iter 64480 || Loss: 1.3050 || timer: 0.0821 sec.
iter 64490 || Loss: 1.1608 || timer: 0.0928 sec.
iter 64500 || Loss: 1.8346 || timer: 0.0934 sec.
iter 64510 || Loss: 0.7745 || timer: 0.0895 sec.
iter 64520 || Loss: 1.6862 || timer: 0.0826 sec.
iter 64530 || Loss: 1.6642 || timer: 0.0824 sec.
iter 64540 || Loss: 2.4336 || timer: 0.1056 sec.
iter 64550 || Loss: 1.1839 || timer: 0.1047 sec.
iter 64560 || Loss: 1.2492 || timer: 0.0894 sec.
iter 64570 || Loss: 1.0551 || timer: 0.0193 sec.
iter 64580 || Loss: 1.4450 || timer: 0.0892 sec.
iter 64590 || Loss: 1.0436 || timer: 0.0875 sec.
iter 64600 || Loss: 1.3312 || timer: 0.0854 sec.
iter 64610 || Loss: 0.7801 || timer: 0.0935 sec.
iter 64620 || Loss: 0.8949 || timer: 0.0989 sec.
iter 64630 || Loss: 1.4711 || timer: 0.0900 sec.
iter 64640 || Loss: 1.1686 || timer: 0.0841 sec.
iter 64650 || Loss: 1.0662 || timer: 0.1107 sec.
iter 64660 || Loss: 1.2425 || timer: 0.0772 sec.
iter 64670 || Loss: 1.0337 || timer: 0.0876 sec.
iter 64680 || Loss: 0.9170 || timer: 0.0808 sec.
iter 64690 || Loss: 1.8273 || timer: 0.1033 sec.
iter 64700 || Loss: 1.2630 || timer: 0.0822 sec.
iter 64710 || Loss: 0.9562 || timer: 0.0825 sec.
iter 64720 || Loss: 1.3722 || timer: 0.0929 sec.
iter 64730 || Loss: 1.2521 || timer: 0.0826 sec.
iter 64740 || Loss: 1.1015 || timer: 0.0894 sec.
iter 64750 || Loss: 1.3432 || timer: 0.1079 sec.
iter 64760 || Loss: 1.0925 || timer: 0.0833 sec.
iter 64770 || Loss: 1.4485 || timer: 0.1005 sec.
iter 64780 || Loss: 1.0665 || timer: 0.1085 sec.
iter 64790 || Loss: 0.9051 || timer: 0.0858 sec.
iter 64800 || Loss: 1.2212 || timer: 0.0906 sec.
iter 64810 || Loss: 1.1337 || timer: 0.0952 sec.
iter 64820 || Loss: 1.0708 || timer: 0.0825 sec.
iter 64830 || Loss: 1.6651 || timer: 0.0808 sec.
iter 64840 || Loss: 1.1419 || timer: 0.1080 sec.
iter 64850 || Loss: 2.2443 || timer: 0.0817 sec.
iter 64860 || Loss: 1.9966 || timer: 0.0900 sec.
iter 64870 || Loss: 1.7723 || timer: 0.0833 sec.
iter 64880 || Loss: 1.0186 || timer: 0.1217 sec.
iter 64890 || Loss: 1.4963 || timer: 0.0827 sec.
iter 64900 || Loss: 1.2424 || timer: 0.0208 sec.
iter 64910 || Loss: 0.6459 || timer: 0.0920 sec.
iter 64920 || Loss: 1.6446 || timer: 0.0846 sec.
iter 64930 || Loss: 1.3665 || timer: 0.0831 sec.
iter 64940 || Loss: 1.3270 || timer: 0.0913 sec.
iter 64950 || Loss: 1.4356 || timer: 0.0915 sec.
iter 64960 || Loss: 1.1757 || timer: 0.0923 sec.
iter 64970 || Loss: 1.0994 || timer: 0.0914 sec.
iter 64980 || Loss: 1.6440 || timer: 0.0922 sec.
iter 64990 || Loss: 1.8506 || timer: 0.1010 sec.
iter 65000 || Loss: 1.3223 || Saving state, iter: 65000
timer: 0.1137 sec.
iter 65010 || Loss: 1.2331 || timer: 0.0860 sec.
iter 65020 || Loss: 1.1152 || timer: 0.0955 sec.
iter 65030 || Loss: 1.0240 || timer: 0.0898 sec.
iter 65040 || Loss: 1.1412 || timer: 0.0903 sec.
iter 65050 || Loss: 1.4509 || timer: 0.0904 sec.
iter 65060 || Loss: 1.0116 || timer: 0.0900 sec.
iter 65070 || Loss: 1.1102 || timer: 0.0911 sec.
iter 65080 || Loss: 1.1774 || timer: 0.0823 sec.
iter 65090 || Loss: 1.1770 || timer: 0.0898 sec.
iter 65100 || Loss: 1.3079 || timer: 0.0840 sec.
iter 65110 || Loss: 1.6382 || timer: 0.0833 sec.
iter 65120 || Loss: 1.0535 || timer: 0.0903 sec.
iter 65130 || Loss: 1.3184 || timer: 0.0903 sec.
iter 65140 || Loss: 1.1923 || timer: 0.0962 sec.
iter 65150 || Loss: 1.2942 || timer: 0.0840 sec.
iter 65160 || Loss: 1.1242 || timer: 0.0891 sec.
iter 65170 || Loss: 1.0518 || timer: 0.0910 sec.
iter 65180 || Loss: 1.5125 || timer: 0.0838 sec.
iter 65190 || Loss: 0.9952 || timer: 0.0831 sec.
iter 65200 || Loss: 1.1479 || timer: 0.0918 sec.
iter 65210 || Loss: 1.8569 || timer: 0.1054 sec.
iter 65220 || Loss: 0.8720 || timer: 0.1001 sec.
iter 65230 || Loss: 0.9125 || timer: 0.0241 sec.
iter 65240 || Loss: 0.4011 || timer: 0.1076 sec.
iter 65250 || Loss: 0.9573 || timer: 0.0846 sec.
iter 65260 || Loss: 1.0581 || timer: 0.0828 sec.
iter 65270 || Loss: 1.0393 || timer: 0.0908 sec.
iter 65280 || Loss: 1.2071 || timer: 0.0839 sec.
iter 65290 || Loss: 1.2725 || timer: 0.0912 sec.
iter 65300 || Loss: 1.5307 || timer: 0.0925 sec.
iter 65310 || Loss: 1.4458 || timer: 0.0933 sec.
iter 65320 || Loss: 1.2698 || timer: 0.0901 sec.
iter 65330 || Loss: 1.0187 || timer: 0.0949 sec.
iter 65340 || Loss: 1.4731 || timer: 0.0847 sec.
iter 65350 || Loss: 0.8579 || timer: 0.1116 sec.
iter 65360 || Loss: 0.9159 || timer: 0.0854 sec.
iter 65370 || Loss: 1.3651 || timer: 0.0935 sec.
iter 65380 || Loss: 0.8558 || timer: 0.0885 sec.
iter 65390 || Loss: 1.3115 || timer: 0.0908 sec.
iter 65400 || Loss: 1.0619 || timer: 0.0840 sec.
iter 65410 || Loss: 1.1356 || timer: 0.0924 sec.
iter 65420 || Loss: 1.0561 || timer: 0.0915 sec.
iter 65430 || Loss: 1.0776 || timer: 0.1031 sec.
iter 65440 || Loss: 1.2996 || timer: 0.0838 sec.
iter 65450 || Loss: 1.1971 || timer: 0.0886 sec.
iter 65460 || Loss: 1.0865 || timer: 0.0913 sec.
iter 65470 || Loss: 1.3649 || timer: 0.0920 sec.
iter 65480 || Loss: 0.9570 || timer: 0.0932 sec.
iter 65490 || Loss: 1.2982 || timer: 0.0825 sec.
iter 65500 || Loss: 0.8796 || timer: 0.0840 sec.
iter 65510 || Loss: 1.2075 || timer: 0.0853 sec.
iter 65520 || Loss: 1.1092 || timer: 0.1072 sec.
iter 65530 || Loss: 1.3463 || timer: 0.0956 sec.
iter 65540 || Loss: 1.4814 || timer: 0.1081 sec.
iter 65550 || Loss: 1.2146 || timer: 0.1101 sec.
iter 65560 || Loss: 1.0879 || timer: 0.0182 sec.
iter 65570 || Loss: 0.5973 || timer: 0.0941 sec.
iter 65580 || Loss: 1.6968 || timer: 0.0942 sec.
iter 65590 || Loss: 1.5387 || timer: 0.0827 sec.
iter 65600 || Loss: 0.9694 || timer: 0.1070 sec.
iter 65610 || Loss: 1.0738 || timer: 0.0901 sec.
iter 65620 || Loss: 1.3468 || timer: 0.1106 sec.
iter 65630 || Loss: 1.1229 || timer: 0.0823 sec.
iter 65640 || Loss: 1.1068 || timer: 0.0893 sec.
iter 65650 || Loss: 1.0038 || timer: 0.0900 sec.
iter 65660 || Loss: 1.7875 || timer: 0.1126 sec.
iter 65670 || Loss: 0.9987 || timer: 0.0826 sec.
iter 65680 || Loss: 1.5205 || timer: 0.0910 sec.
iter 65690 || Loss: 0.8590 || timer: 0.0865 sec.
iter 65700 || Loss: 0.9034 || timer: 0.0902 sec.
iter 65710 || Loss: 1.0838 || timer: 0.0899 sec.
iter 65720 || Loss: 1.1873 || timer: 0.0856 sec.
iter 65730 || Loss: 1.2002 || timer: 0.0845 sec.
iter 65740 || Loss: 1.2963 || timer: 0.0929 sec.
iter 65750 || Loss: 1.3357 || timer: 0.0903 sec.
iter 65760 || Loss: 1.3546 || timer: 0.0840 sec.
iter 65770 || Loss: 1.1742 || timer: 0.0837 sec.
iter 65780 || Loss: 1.5299 || timer: 0.0827 sec.
iter 65790 || Loss: 1.1861 || timer: 0.0845 sec.
iter 65800 || Loss: 1.2172 || timer: 0.0883 sec.
iter 65810 || Loss: 2.1703 || timer: 0.0839 sec.
iter 65820 || Loss: 1.2439 || timer: 0.0843 sec.
iter 65830 || Loss: 0.9684 || timer: 0.0814 sec.
iter 65840 || Loss: 1.7413 || timer: 0.0914 sec.
iter 65850 || Loss: 1.2485 || timer: 0.0899 sec.
iter 65860 || Loss: 1.3137 || timer: 0.0916 sec.
iter 65870 || Loss: 1.3331 || timer: 0.0927 sec.
iter 65880 || Loss: 1.1277 || timer: 0.0962 sec.
iter 65890 || Loss: 1.0737 || timer: 0.0274 sec.
iter 65900 || Loss: 1.1091 || timer: 0.0988 sec.
iter 65910 || Loss: 1.2580 || timer: 0.0919 sec.
iter 65920 || Loss: 0.8803 || timer: 0.0832 sec.
iter 65930 || Loss: 1.0052 || timer: 0.0837 sec.
iter 65940 || Loss: 1.3866 || timer: 0.1019 sec.
iter 65950 || Loss: 1.5433 || timer: 0.0909 sec.
iter 65960 || Loss: 1.1899 || timer: 0.0848 sec.
iter 65970 || Loss: 1.1434 || timer: 0.0921 sec.
iter 65980 || Loss: 1.1739 || timer: 0.0832 sec.
iter 65990 || Loss: 1.4629 || timer: 0.0884 sec.
iter 66000 || Loss: 1.2377 || timer: 0.0897 sec.
iter 66010 || Loss: 1.0212 || timer: 0.0831 sec.
iter 66020 || Loss: 1.1920 || timer: 0.0895 sec.
iter 66030 || Loss: 0.6961 || timer: 0.0891 sec.
iter 66040 || Loss: 1.2427 || timer: 0.0887 sec.
iter 66050 || Loss: 1.4304 || timer: 0.0912 sec.
iter 66060 || Loss: 1.0263 || timer: 0.0819 sec.
iter 66070 || Loss: 1.6172 || timer: 0.1059 sec.
iter 66080 || Loss: 1.1097 || timer: 0.1255 sec.
iter 66090 || Loss: 1.0068 || timer: 0.0838 sec.
iter 66100 || Loss: 1.1140 || timer: 0.0882 sec.
iter 66110 || Loss: 1.2024 || timer: 0.0944 sec.
iter 66120 || Loss: 1.4710 || timer: 0.0895 sec.
iter 66130 || Loss: 1.5900 || timer: 0.0906 sec.
iter 66140 || Loss: 1.5163 || timer: 0.0920 sec.
iter 66150 || Loss: 1.0106 || timer: 0.0928 sec.
iter 66160 || Loss: 1.3669 || timer: 0.1036 sec.
iter 66170 || Loss: 1.6177 || timer: 0.0908 sec.
iter 66180 || Loss: 1.2083 || timer: 0.0959 sec.
iter 66190 || Loss: 1.1720 || timer: 0.0966 sec.
iter 66200 || Loss: 1.2177 || timer: 0.0939 sec.
iter 66210 || Loss: 0.9506 || timer: 0.0853 sec.
iter 66220 || Loss: 1.4003 || timer: 0.0165 sec.
iter 66230 || Loss: 0.5921 || timer: 0.0928 sec.
iter 66240 || Loss: 1.2222 || timer: 0.0898 sec.
iter 66250 || Loss: 1.1478 || timer: 0.1053 sec.
iter 66260 || Loss: 1.2629 || timer: 0.0833 sec.
iter 66270 || Loss: 1.2029 || timer: 0.1004 sec.
iter 66280 || Loss: 1.1468 || timer: 0.0927 sec.
iter 66290 || Loss: 1.0754 || timer: 0.0898 sec.
iter 66300 || Loss: 1.0738 || timer: 0.0896 sec.
iter 66310 || Loss: 1.2128 || timer: 0.0895 sec.
iter 66320 || Loss: 1.0257 || timer: 0.1248 sec.
iter 66330 || Loss: 0.9986 || timer: 0.1035 sec.
iter 66340 || Loss: 1.1101 || timer: 0.0903 sec.
iter 66350 || Loss: 1.7196 || timer: 0.0830 sec.
iter 66360 || Loss: 1.1864 || timer: 0.0911 sec.
iter 66370 || Loss: 1.1477 || timer: 0.0903 sec.
iter 66380 || Loss: 1.2599 || timer: 0.0927 sec.
iter 66390 || Loss: 1.2068 || timer: 0.0916 sec.
iter 66400 || Loss: 1.2410 || timer: 0.0974 sec.
iter 66410 || Loss: 1.1641 || timer: 0.0832 sec.
iter 66420 || Loss: 1.3413 || timer: 0.0842 sec.
iter 66430 || Loss: 1.3982 || timer: 0.0976 sec.
iter 66440 || Loss: 0.9561 || timer: 0.0977 sec.
iter 66450 || Loss: 1.4092 || timer: 0.0824 sec.
iter 66460 || Loss: 1.3703 || timer: 0.0906 sec.
iter 66470 || Loss: 1.2081 || timer: 0.0902 sec.
iter 66480 || Loss: 1.0211 || timer: 0.0851 sec.
iter 66490 || Loss: 1.0325 || timer: 0.0905 sec.
iter 66500 || Loss: 0.9268 || timer: 0.0897 sec.
iter 66510 || Loss: 1.1229 || timer: 0.0830 sec.
iter 66520 || Loss: 1.0615 || timer: 0.0919 sec.
iter 66530 || Loss: 1.2478 || timer: 0.0903 sec.
iter 66540 || Loss: 1.7714 || timer: 0.0898 sec.
iter 66550 || Loss: 1.3082 || timer: 0.0265 sec.
iter 66560 || Loss: 0.8539 || timer: 0.0831 sec.
iter 66570 || Loss: 1.1445 || timer: 0.0825 sec.
iter 66580 || Loss: 1.1858 || timer: 0.0901 sec.
iter 66590 || Loss: 0.9613 || timer: 0.0848 sec.
iter 66600 || Loss: 1.1877 || timer: 0.0878 sec.
iter 66610 || Loss: 0.9311 || timer: 0.0909 sec.
iter 66620 || Loss: 1.2778 || timer: 0.1028 sec.
iter 66630 || Loss: 1.4968 || timer: 0.0824 sec.
iter 66640 || Loss: 0.7934 || timer: 0.0813 sec.
iter 66650 || Loss: 1.2365 || timer: 0.1417 sec.
iter 66660 || Loss: 1.2963 || timer: 0.0923 sec.
iter 66670 || Loss: 1.8949 || timer: 0.1084 sec.
iter 66680 || Loss: 1.2997 || timer: 0.0914 sec.
iter 66690 || Loss: 1.0755 || timer: 0.1113 sec.
iter 66700 || Loss: 1.3033 || timer: 0.1013 sec.
iter 66710 || Loss: 1.0247 || timer: 0.0938 sec.
iter 66720 || Loss: 1.1154 || timer: 0.0915 sec.
iter 66730 || Loss: 1.4765 || timer: 0.0903 sec.
iter 66740 || Loss: 2.1569 || timer: 0.0968 sec.
iter 66750 || Loss: 1.2306 || timer: 0.0884 sec.
iter 66760 || Loss: 1.2446 || timer: 0.0901 sec.
iter 66770 || Loss: 1.1031 || timer: 0.0839 sec.
iter 66780 || Loss: 1.2601 || timer: 0.0904 sec.
iter 66790 || Loss: 1.0454 || timer: 0.0820 sec.
iter 66800 || Loss: 1.2835 || timer: 0.0829 sec.
iter 66810 || Loss: 1.0654 || timer: 0.0995 sec.
iter 66820 || Loss: 0.8426 || timer: 0.0838 sec.
iter 66830 || Loss: 1.2627 || timer: 0.0903 sec.
iter 66840 || Loss: 1.2469 || timer: 0.0883 sec.
iter 66850 || Loss: 1.2343 || timer: 0.0846 sec.
iter 66860 || Loss: 1.4667 || timer: 0.0903 sec.
iter 66870 || Loss: 1.1961 || timer: 0.0909 sec.
iter 66880 || Loss: 1.8244 || timer: 0.0223 sec.
iter 66890 || Loss: 0.9180 || timer: 0.0913 sec.
iter 66900 || Loss: 1.5757 || timer: 0.0871 sec.
iter 66910 || Loss: 0.7533 || timer: 0.0909 sec.
iter 66920 || Loss: 0.9179 || timer: 0.0924 sec.
iter 66930 || Loss: 1.0206 || timer: 0.0980 sec.
iter 66940 || Loss: 1.1785 || timer: 0.0985 sec.
iter 66950 || Loss: 1.1235 || timer: 0.0857 sec.
iter 66960 || Loss: 1.2159 || timer: 0.0905 sec.
iter 66970 || Loss: 1.1770 || timer: 0.0836 sec.
iter 66980 || Loss: 1.0632 || timer: 0.0960 sec.
iter 66990 || Loss: 0.9474 || timer: 0.0895 sec.
iter 67000 || Loss: 1.2404 || timer: 0.0881 sec.
iter 67010 || Loss: 0.9427 || timer: 0.0836 sec.
iter 67020 || Loss: 0.9841 || timer: 0.0927 sec.
iter 67030 || Loss: 1.6012 || timer: 0.0831 sec.
iter 67040 || Loss: 0.9760 || timer: 0.0903 sec.
iter 67050 || Loss: 1.0513 || timer: 0.0926 sec.
iter 67060 || Loss: 1.0132 || timer: 0.0972 sec.
iter 67070 || Loss: 1.0335 || timer: 0.0877 sec.
iter 67080 || Loss: 1.0473 || timer: 0.0933 sec.
iter 67090 || Loss: 1.0848 || timer: 0.1051 sec.
iter 67100 || Loss: 0.7444 || timer: 0.0823 sec.
iter 67110 || Loss: 1.3578 || timer: 0.0945 sec.
iter 67120 || Loss: 0.9212 || timer: 0.0846 sec.
iter 67130 || Loss: 1.1426 || timer: 0.1041 sec.
iter 67140 || Loss: 0.9155 || timer: 0.0898 sec.
iter 67150 || Loss: 1.0163 || timer: 0.0899 sec.
iter 67160 || Loss: 1.0014 || timer: 0.0831 sec.
iter 67170 || Loss: 0.9761 || timer: 0.0836 sec.
iter 67180 || Loss: 1.1184 || timer: 0.0828 sec.
iter 67190 || Loss: 1.1248 || timer: 0.0825 sec.
iter 67200 || Loss: 1.5958 || timer: 0.0831 sec.
iter 67210 || Loss: 1.3831 || timer: 0.0179 sec.
iter 67220 || Loss: 0.4067 || timer: 0.0900 sec.
iter 67230 || Loss: 1.1172 || timer: 0.0866 sec.
iter 67240 || Loss: 1.3922 || timer: 0.1073 sec.
iter 67250 || Loss: 1.0897 || timer: 0.0891 sec.
iter 67260 || Loss: 0.9819 || timer: 0.0916 sec.
iter 67270 || Loss: 0.9227 || timer: 0.1089 sec.
iter 67280 || Loss: 1.0255 || timer: 0.0883 sec.
iter 67290 || Loss: 1.1478 || timer: 0.0985 sec.
iter 67300 || Loss: 1.2402 || timer: 0.0900 sec.
iter 67310 || Loss: 1.4676 || timer: 0.0951 sec.
iter 67320 || Loss: 0.8239 || timer: 0.0922 sec.
iter 67330 || Loss: 0.9931 || timer: 0.1039 sec.
iter 67340 || Loss: 1.1157 || timer: 0.0823 sec.
iter 67350 || Loss: 1.0190 || timer: 0.1235 sec.
iter 67360 || Loss: 0.9681 || timer: 0.0866 sec.
iter 67370 || Loss: 0.9623 || timer: 0.1002 sec.
iter 67380 || Loss: 1.1599 || timer: 0.1031 sec.
iter 67390 || Loss: 1.2594 || timer: 0.0900 sec.
iter 67400 || Loss: 1.0395 || timer: 0.0819 sec.
iter 67410 || Loss: 1.3141 || timer: 0.0915 sec.
iter 67420 || Loss: 1.0405 || timer: 0.0904 sec.
iter 67430 || Loss: 0.8889 || timer: 0.0905 sec.
iter 67440 || Loss: 1.0678 || timer: 0.0842 sec.
iter 67450 || Loss: 0.9660 || timer: 0.0914 sec.
iter 67460 || Loss: 1.1949 || timer: 0.0886 sec.
iter 67470 || Loss: 1.2674 || timer: 0.0963 sec.
iter 67480 || Loss: 1.0389 || timer: 0.0824 sec.
iter 67490 || Loss: 0.8094 || timer: 0.0847 sec.
iter 67500 || Loss: 1.3993 || timer: 0.0833 sec.
iter 67510 || Loss: 0.8504 || timer: 0.0861 sec.
iter 67520 || Loss: 1.1552 || timer: 0.1011 sec.
iter 67530 || Loss: 1.2280 || timer: 0.0889 sec.
iter 67540 || Loss: 1.5375 || timer: 0.0248 sec.
iter 67550 || Loss: 1.7646 || timer: 0.0823 sec.
iter 67560 || Loss: 1.3562 || timer: 0.1000 sec.
iter 67570 || Loss: 1.4346 || timer: 0.1123 sec.
iter 67580 || Loss: 0.9962 || timer: 0.1182 sec.
iter 67590 || Loss: 0.9991 || timer: 0.0904 sec.
iter 67600 || Loss: 1.1293 || timer: 0.0834 sec.
iter 67610 || Loss: 1.2519 || timer: 0.0909 sec.
iter 67620 || Loss: 1.1582 || timer: 0.0898 sec.
iter 67630 || Loss: 1.1254 || timer: 0.0904 sec.
iter 67640 || Loss: 0.9372 || timer: 0.0954 sec.
iter 67650 || Loss: 1.2406 || timer: 0.1029 sec.
iter 67660 || Loss: 1.0633 || timer: 0.0910 sec.
iter 67670 || Loss: 1.2141 || timer: 0.0868 sec.
iter 67680 || Loss: 1.3930 || timer: 0.1148 sec.
iter 67690 || Loss: 1.2097 || timer: 0.0905 sec.
iter 67700 || Loss: 1.3938 || timer: 0.0858 sec.
iter 67710 || Loss: 1.0568 || timer: 0.1317 sec.
iter 67720 || Loss: 0.9848 || timer: 0.1126 sec.
iter 67730 || Loss: 1.2317 || timer: 0.0904 sec.
iter 67740 || Loss: 1.2746 || timer: 0.0827 sec.
iter 67750 || Loss: 1.0515 || timer: 0.0908 sec.
iter 67760 || Loss: 1.0618 || timer: 0.0885 sec.
iter 67770 || Loss: 0.9031 || timer: 0.0909 sec.
iter 67780 || Loss: 1.0339 || timer: 0.0885 sec.
iter 67790 || Loss: 1.1782 || timer: 0.0843 sec.
iter 67800 || Loss: 1.0095 || timer: 0.0880 sec.
iter 67810 || Loss: 0.9717 || timer: 0.0908 sec.
iter 67820 || Loss: 1.1496 || timer: 0.0928 sec.
iter 67830 || Loss: 1.1138 || timer: 0.1037 sec.
iter 67840 || Loss: 1.5733 || timer: 0.0872 sec.
iter 67850 || Loss: 0.9168 || timer: 0.0889 sec.
iter 67860 || Loss: 1.4791 || timer: 0.0872 sec.
iter 67870 || Loss: 1.3576 || timer: 0.0272 sec.
iter 67880 || Loss: 0.8003 || timer: 0.0907 sec.
iter 67890 || Loss: 1.3043 || timer: 0.1024 sec.
iter 67900 || Loss: 1.7418 || timer: 0.0893 sec.
iter 67910 || Loss: 1.0927 || timer: 0.0919 sec.
iter 67920 || Loss: 1.1206 || timer: 0.0919 sec.
iter 67930 || Loss: 1.2035 || timer: 0.0991 sec.
iter 67940 || Loss: 1.1729 || timer: 0.0886 sec.
iter 67950 || Loss: 0.9099 || timer: 0.0833 sec.
iter 67960 || Loss: 1.0264 || timer: 0.0828 sec.
iter 67970 || Loss: 1.1873 || timer: 0.1005 sec.
iter 67980 || Loss: 1.1572 || timer: 0.0823 sec.
iter 67990 || Loss: 1.2742 || timer: 0.0905 sec.
iter 68000 || Loss: 1.2742 || timer: 0.0916 sec.
iter 68010 || Loss: 1.3245 || timer: 0.0914 sec.
iter 68020 || Loss: 1.1187 || timer: 0.0905 sec.
iter 68030 || Loss: 1.3712 || timer: 0.0902 sec.
iter 68040 || Loss: 1.3150 || timer: 0.0854 sec.
iter 68050 || Loss: 1.2039 || timer: 0.0951 sec.
iter 68060 || Loss: 1.2997 || timer: 0.0901 sec.
iter 68070 || Loss: 0.9620 || timer: 0.0905 sec.
iter 68080 || Loss: 0.9324 || timer: 0.0847 sec.
iter 68090 || Loss: 1.4376 || timer: 0.0841 sec.
iter 68100 || Loss: 1.3423 || timer: 0.0922 sec.
iter 68110 || Loss: 1.3940 || timer: 0.0932 sec.
iter 68120 || Loss: 0.8341 || timer: 0.0835 sec.
iter 68130 || Loss: 1.2745 || timer: 0.1045 sec.
iter 68140 || Loss: 1.0867 || timer: 0.0903 sec.
iter 68150 || Loss: 1.0848 || timer: 0.1086 sec.
iter 68160 || Loss: 0.9328 || timer: 0.1185 sec.
iter 68170 || Loss: 0.8724 || timer: 0.0826 sec.
iter 68180 || Loss: 1.0527 || timer: 0.0922 sec.
iter 68190 || Loss: 1.2861 || timer: 0.1057 sec.
iter 68200 || Loss: 1.1419 || timer: 0.0283 sec.
iter 68210 || Loss: 1.7203 || timer: 0.0821 sec.
iter 68220 || Loss: 1.1559 || timer: 0.0910 sec.
iter 68230 || Loss: 0.9035 || timer: 0.0829 sec.
iter 68240 || Loss: 1.4020 || timer: 0.0903 sec.
iter 68250 || Loss: 1.4720 || timer: 0.0841 sec.
iter 68260 || Loss: 1.1425 || timer: 0.0825 sec.
iter 68270 || Loss: 1.0394 || timer: 0.1026 sec.
iter 68280 || Loss: 1.0002 || timer: 0.0963 sec.
iter 68290 || Loss: 1.3645 || timer: 0.0828 sec.
iter 68300 || Loss: 1.7043 || timer: 0.0981 sec.
iter 68310 || Loss: 1.3511 || timer: 0.0810 sec.
iter 68320 || Loss: 1.0201 || timer: 0.1048 sec.
iter 68330 || Loss: 1.0220 || timer: 0.0901 sec.
iter 68340 || Loss: 1.2912 || timer: 0.0918 sec.
iter 68350 || Loss: 1.2871 || timer: 0.0903 sec.
iter 68360 || Loss: 0.9859 || timer: 0.0885 sec.
iter 68370 || Loss: 1.5079 || timer: 0.0890 sec.
iter 68380 || Loss: 1.1631 || timer: 0.0807 sec.
iter 68390 || Loss: 1.1652 || timer: 0.1049 sec.
iter 68400 || Loss: 1.1422 || timer: 0.0921 sec.
iter 68410 || Loss: 1.3119 || timer: 0.0882 sec.
iter 68420 || Loss: 1.1182 || timer: 0.0905 sec.
iter 68430 || Loss: 1.3074 || timer: 0.0906 sec.
iter 68440 || Loss: 1.3216 || timer: 0.0911 sec.
iter 68450 || Loss: 1.0330 || timer: 0.0835 sec.
iter 68460 || Loss: 1.3041 || timer: 0.0906 sec.
iter 68470 || Loss: 1.0199 || timer: 0.1069 sec.
iter 68480 || Loss: 0.9724 || timer: 0.0861 sec.
iter 68490 || Loss: 0.9965 || timer: 0.0906 sec.
iter 68500 || Loss: 0.9465 || timer: 0.0902 sec.
iter 68510 || Loss: 1.2264 || timer: 0.0831 sec.
iter 68520 || Loss: 0.9126 || timer: 0.0840 sec.
iter 68530 || Loss: 1.0985 || timer: 0.0208 sec.
iter 68540 || Loss: 0.2585 || timer: 0.0838 sec.
iter 68550 || Loss: 1.1421 || timer: 0.0843 sec.
iter 68560 || Loss: 1.3496 || timer: 0.0848 sec.
iter 68570 || Loss: 1.4316 || timer: 0.1043 sec.
iter 68580 || Loss: 1.2985 || timer: 0.0811 sec.
iter 68590 || Loss: 0.8900 || timer: 0.0845 sec.
iter 68600 || Loss: 0.9985 || timer: 0.0922 sec.
iter 68610 || Loss: 0.7756 || timer: 0.0839 sec.
iter 68620 || Loss: 1.2223 || timer: 0.0836 sec.
iter 68630 || Loss: 1.3882 || timer: 0.0969 sec.
iter 68640 || Loss: 0.8908 || timer: 0.0864 sec.
iter 68650 || Loss: 0.9270 || timer: 0.0915 sec.
iter 68660 || Loss: 1.4488 || timer: 0.0837 sec.
iter 68670 || Loss: 1.0284 || timer: 0.0883 sec.
iter 68680 || Loss: 1.1656 || timer: 0.0909 sec.
iter 68690 || Loss: 1.1761 || timer: 0.0910 sec.
iter 68700 || Loss: 0.9667 || timer: 0.0935 sec.
iter 68710 || Loss: 0.8414 || timer: 0.0846 sec.
iter 68720 || Loss: 1.0468 || timer: 0.0841 sec.
iter 68730 || Loss: 1.7283 || timer: 0.0881 sec.
iter 68740 || Loss: 0.9959 || timer: 0.0764 sec.
iter 68750 || Loss: 1.1122 || timer: 0.0840 sec.
iter 68760 || Loss: 1.1244 || timer: 0.0923 sec.
iter 68770 || Loss: 1.1000 || timer: 0.1028 sec.
iter 68780 || Loss: 1.0539 || timer: 0.1045 sec.
iter 68790 || Loss: 0.9282 || timer: 0.0847 sec.
iter 68800 || Loss: 1.0602 || timer: 0.0920 sec.
iter 68810 || Loss: 0.9701 || timer: 0.0842 sec.
iter 68820 || Loss: 0.8184 || timer: 0.0906 sec.
iter 68830 || Loss: 1.1690 || timer: 0.0858 sec.
iter 68840 || Loss: 1.3967 || timer: 0.0827 sec.
iter 68850 || Loss: 1.0357 || timer: 0.0897 sec.
iter 68860 || Loss: 1.1428 || timer: 0.0209 sec.
iter 68870 || Loss: 0.7469 || timer: 0.0844 sec.
iter 68880 || Loss: 0.8462 || timer: 0.0934 sec.
iter 68890 || Loss: 1.0747 || timer: 0.0934 sec.
iter 68900 || Loss: 1.3736 || timer: 0.0828 sec.
iter 68910 || Loss: 1.2302 || timer: 0.0915 sec.
iter 68920 || Loss: 1.3104 || timer: 0.0940 sec.
iter 68930 || Loss: 1.1786 || timer: 0.0907 sec.
iter 68940 || Loss: 1.0467 || timer: 0.0839 sec.
iter 68950 || Loss: 1.2041 || timer: 0.0844 sec.
iter 68960 || Loss: 1.1207 || timer: 0.0982 sec.
iter 68970 || Loss: 1.0695 || timer: 0.0835 sec.
iter 68980 || Loss: 0.8982 || timer: 0.0845 sec.
iter 68990 || Loss: 1.4174 || timer: 0.0823 sec.
iter 69000 || Loss: 1.4873 || timer: 0.0904 sec.
iter 69010 || Loss: 0.8481 || timer: 0.1029 sec.
iter 69020 || Loss: 0.9002 || timer: 0.0902 sec.
iter 69030 || Loss: 0.9386 || timer: 0.0916 sec.
iter 69040 || Loss: 1.1331 || timer: 0.0858 sec.
iter 69050 || Loss: 0.9407 || timer: 0.0947 sec.
iter 69060 || Loss: 1.2952 || timer: 0.0901 sec.
iter 69070 || Loss: 1.2656 || timer: 0.0915 sec.
iter 69080 || Loss: 1.2575 || timer: 0.0836 sec.
iter 69090 || Loss: 1.1048 || timer: 0.0895 sec.
iter 69100 || Loss: 0.9142 || timer: 0.0883 sec.
iter 69110 || Loss: 1.7771 || timer: 0.1006 sec.
iter 69120 || Loss: 1.6500 || timer: 0.1016 sec.
iter 69130 || Loss: 1.4693 || timer: 0.0911 sec.
iter 69140 || Loss: 1.1819 || timer: 0.0775 sec.
iter 69150 || Loss: 1.5341 || timer: 0.0842 sec.
iter 69160 || Loss: 1.0940 || timer: 0.0933 sec.
iter 69170 || Loss: 1.3120 || timer: 0.0830 sec.
iter 69180 || Loss: 1.3096 || timer: 0.0835 sec.
iter 69190 || Loss: 1.0480 || timer: 0.0278 sec.
iter 69200 || Loss: 2.1119 || timer: 0.1106 sec.
iter 69210 || Loss: 1.1507 || timer: 0.0926 sec.
iter 69220 || Loss: 1.5997 || timer: 0.0805 sec.
iter 69230 || Loss: 1.4342 || timer: 0.0817 sec.
iter 69240 || Loss: 1.1217 || timer: 0.0908 sec.
iter 69250 || Loss: 0.9667 || timer: 0.0836 sec.
iter 69260 || Loss: 0.9607 || timer: 0.0960 sec.
iter 69270 || Loss: 1.6347 || timer: 0.0843 sec.
iter 69280 || Loss: 1.1711 || timer: 0.0904 sec.
iter 69290 || Loss: 1.5656 || timer: 0.1056 sec.
iter 69300 || Loss: 0.9421 || timer: 0.0836 sec.
iter 69310 || Loss: 1.0024 || timer: 0.0835 sec.
iter 69320 || Loss: 1.3542 || timer: 0.0928 sec.
iter 69330 || Loss: 1.1007 || timer: 0.1065 sec.
iter 69340 || Loss: 1.1634 || timer: 0.1010 sec.
iter 69350 || Loss: 1.1171 || timer: 0.0769 sec.
iter 69360 || Loss: 1.4318 || timer: 0.0998 sec.
iter 69370 || Loss: 0.9446 || timer: 0.1082 sec.
iter 69380 || Loss: 1.0093 || timer: 0.0912 sec.
iter 69390 || Loss: 1.0817 || timer: 0.0880 sec.
iter 69400 || Loss: 1.1013 || timer: 0.0840 sec.
iter 69410 || Loss: 1.2781 || timer: 0.0894 sec.
iter 69420 || Loss: 1.0448 || timer: 0.0828 sec.
iter 69430 || Loss: 1.8443 || timer: 0.0839 sec.
iter 69440 || Loss: 1.7391 || timer: 0.0953 sec.
iter 69450 || Loss: 1.2702 || timer: 0.0737 sec.
iter 69460 || Loss: 1.0135 || timer: 0.0771 sec.
iter 69470 || Loss: 1.2832 || timer: 0.1092 sec.
iter 69480 || Loss: 1.2662 || timer: 0.0909 sec.
iter 69490 || Loss: 1.5704 || timer: 0.0910 sec.
iter 69500 || Loss: 1.1904 || timer: 0.0842 sec.
iter 69510 || Loss: 0.9758 || timer: 0.0816 sec.
iter 69520 || Loss: 1.1780 || timer: 0.0190 sec.
iter 69530 || Loss: 3.2343 || timer: 0.0819 sec.
iter 69540 || Loss: 1.1277 || timer: 0.0926 sec.
iter 69550 || Loss: 1.2361 || timer: 0.1039 sec.
iter 69560 || Loss: 1.2456 || timer: 0.0902 sec.
iter 69570 || Loss: 1.1513 || timer: 0.0966 sec.
iter 69580 || Loss: 1.3669 || timer: 0.0836 sec.
iter 69590 || Loss: 1.8259 || timer: 0.0772 sec.
iter 69600 || Loss: 0.9260 || timer: 0.0769 sec.
iter 69610 || Loss: 1.2423 || timer: 0.0840 sec.
iter 69620 || Loss: 1.0686 || timer: 0.1026 sec.
iter 69630 || Loss: 0.9382 || timer: 0.0843 sec.
iter 69640 || Loss: 1.1444 || timer: 0.1156 sec.
iter 69650 || Loss: 0.9978 || timer: 0.0838 sec.
iter 69660 || Loss: 1.1270 || timer: 0.0923 sec.
iter 69670 || Loss: 1.2877 || timer: 0.1129 sec.
iter 69680 || Loss: 1.0471 || timer: 0.0887 sec.
iter 69690 || Loss: 1.6017 || timer: 0.0836 sec.
iter 69700 || Loss: 0.9228 || timer: 0.0988 sec.
iter 69710 || Loss: 0.8452 || timer: 0.0775 sec.
iter 69720 || Loss: 0.9017 || timer: 0.0826 sec.
iter 69730 || Loss: 1.0855 || timer: 0.0803 sec.
iter 69740 || Loss: 1.1932 || timer: 0.1017 sec.
iter 69750 || Loss: 1.3107 || timer: 0.0899 sec.
iter 69760 || Loss: 0.9391 || timer: 0.0973 sec.
iter 69770 || Loss: 1.0700 || timer: 0.0836 sec.
iter 69780 || Loss: 1.1352 || timer: 0.0995 sec.
iter 69790 || Loss: 1.4610 || timer: 0.0926 sec.
iter 69800 || Loss: 1.2466 || timer: 0.0846 sec.
iter 69810 || Loss: 0.9027 || timer: 0.0937 sec.
iter 69820 || Loss: 0.9984 || timer: 0.0886 sec.
iter 69830 || Loss: 0.9025 || timer: 0.0915 sec.
iter 69840 || Loss: 1.1860 || timer: 0.0913 sec.
iter 69850 || Loss: 0.9143 || timer: 0.0210 sec.
iter 69860 || Loss: 2.5261 || timer: 0.0905 sec.
iter 69870 || Loss: 1.0706 || timer: 0.0902 sec.
iter 69880 || Loss: 1.4525 || timer: 0.0932 sec.
iter 69890 || Loss: 0.9393 || timer: 0.0919 sec.
iter 69900 || Loss: 1.0966 || timer: 0.1157 sec.
iter 69910 || Loss: 1.0157 || timer: 0.1046 sec.
iter 69920 || Loss: 1.4010 || timer: 0.0912 sec.
iter 69930 || Loss: 1.3408 || timer: 0.0984 sec.
iter 69940 || Loss: 1.3152 || timer: 0.0843 sec.
iter 69950 || Loss: 1.0649 || timer: 0.1199 sec.
iter 69960 || Loss: 1.2877 || timer: 0.0930 sec.
iter 69970 || Loss: 1.2066 || timer: 0.0907 sec.
iter 69980 || Loss: 1.1493 || timer: 0.0896 sec.
iter 69990 || Loss: 1.4967 || timer: 0.0843 sec.
iter 70000 || Loss: 0.9595 || Saving state, iter: 70000
timer: 0.0902 sec.
iter 70010 || Loss: 1.1167 || timer: 0.0849 sec.
iter 70020 || Loss: 1.6872 || timer: 0.0900 sec.
iter 70030 || Loss: 0.8321 || timer: 0.0927 sec.
iter 70040 || Loss: 1.1680 || timer: 0.0828 sec.
iter 70050 || Loss: 1.1414 || timer: 0.0844 sec.
iter 70060 || Loss: 1.2093 || timer: 0.1098 sec.
iter 70070 || Loss: 1.2786 || timer: 0.0935 sec.
iter 70080 || Loss: 0.9325 || timer: 0.0918 sec.
iter 70090 || Loss: 1.0017 || timer: 0.0903 sec.
iter 70100 || Loss: 1.1514 || timer: 0.0873 sec.
iter 70110 || Loss: 0.9744 || timer: 0.1045 sec.
iter 70120 || Loss: 1.1836 || timer: 0.0841 sec.
iter 70130 || Loss: 1.3591 || timer: 0.1081 sec.
iter 70140 || Loss: 1.2616 || timer: 0.0915 sec.
iter 70150 || Loss: 0.8738 || timer: 0.0842 sec.
iter 70160 || Loss: 1.2625 || timer: 0.0892 sec.
iter 70170 || Loss: 1.0667 || timer: 0.0843 sec.
iter 70180 || Loss: 1.4827 || timer: 0.0270 sec.
iter 70190 || Loss: 0.2073 || timer: 0.0891 sec.
iter 70200 || Loss: 0.8281 || timer: 0.0837 sec.
iter 70210 || Loss: 0.8403 || timer: 0.0832 sec.
iter 70220 || Loss: 0.8983 || timer: 0.0944 sec.
iter 70230 || Loss: 0.9302 || timer: 0.0906 sec.
iter 70240 || Loss: 0.7131 || timer: 0.0884 sec.
iter 70250 || Loss: 0.9442 || timer: 0.0888 sec.
iter 70260 || Loss: 1.3219 || timer: 0.0922 sec.
iter 70270 || Loss: 1.2611 || timer: 0.0894 sec.
iter 70280 || Loss: 1.0384 || timer: 0.1087 sec.
iter 70290 || Loss: 1.2761 || timer: 0.0922 sec.
iter 70300 || Loss: 1.2056 || timer: 0.0919 sec.
iter 70310 || Loss: 1.3626 || timer: 0.0826 sec.
iter 70320 || Loss: 1.3051 || timer: 0.0851 sec.
iter 70330 || Loss: 1.4125 || timer: 0.0927 sec.
iter 70340 || Loss: 1.0707 || timer: 0.0910 sec.
iter 70350 || Loss: 1.3366 || timer: 0.0876 sec.
iter 70360 || Loss: 0.7570 || timer: 0.0869 sec.
iter 70370 || Loss: 0.8454 || timer: 0.0927 sec.
iter 70380 || Loss: 1.0210 || timer: 0.0836 sec.
iter 70390 || Loss: 0.9989 || timer: 0.0911 sec.
iter 70400 || Loss: 1.2503 || timer: 0.0898 sec.
iter 70410 || Loss: 0.9820 || timer: 0.0926 sec.
iter 70420 || Loss: 1.1108 || timer: 0.0910 sec.
iter 70430 || Loss: 0.8467 || timer: 0.0937 sec.
iter 70440 || Loss: 1.3746 || timer: 0.1154 sec.
iter 70450 || Loss: 1.2899 || timer: 0.0831 sec.
iter 70460 || Loss: 1.1963 || timer: 0.0986 sec.
iter 70470 || Loss: 1.0630 || timer: 0.0835 sec.
iter 70480 || Loss: 1.3704 || timer: 0.0913 sec.
iter 70490 || Loss: 1.3126 || timer: 0.1099 sec.
iter 70500 || Loss: 0.9216 || timer: 0.0917 sec.
iter 70510 || Loss: 1.4383 || timer: 0.0261 sec.
iter 70520 || Loss: 0.5046 || timer: 0.0899 sec.
iter 70530 || Loss: 1.0028 || timer: 0.0896 sec.
iter 70540 || Loss: 1.3835 || timer: 0.1045 sec.
iter 70550 || Loss: 1.1274 || timer: 0.0836 sec.
iter 70560 || Loss: 0.9369 || timer: 0.0911 sec.
iter 70570 || Loss: 1.0073 || timer: 0.0866 sec.
iter 70580 || Loss: 1.2196 || timer: 0.0839 sec.
iter 70590 || Loss: 0.9889 || timer: 0.0837 sec.
iter 70600 || Loss: 1.0166 || timer: 0.0991 sec.
iter 70610 || Loss: 1.1282 || timer: 0.0864 sec.
iter 70620 || Loss: 0.8950 || timer: 0.1004 sec.
iter 70630 || Loss: 1.2397 || timer: 0.0892 sec.
iter 70640 || Loss: 0.9769 || timer: 0.0756 sec.
iter 70650 || Loss: 1.0577 || timer: 0.0843 sec.
iter 70660 || Loss: 0.8825 || timer: 0.0889 sec.
iter 70670 || Loss: 1.3177 || timer: 0.0834 sec.
iter 70680 || Loss: 1.2779 || timer: 0.0901 sec.
iter 70690 || Loss: 1.2053 || timer: 0.0777 sec.
iter 70700 || Loss: 0.8981 || timer: 0.0943 sec.
iter 70710 || Loss: 0.8618 || timer: 0.1063 sec.
iter 70720 || Loss: 1.5434 || timer: 0.1162 sec.
iter 70730 || Loss: 0.8257 || timer: 0.0871 sec.
iter 70740 || Loss: 1.4128 || timer: 0.0924 sec.
iter 70750 || Loss: 0.9734 || timer: 0.0903 sec.
iter 70760 || Loss: 1.4324 || timer: 0.1119 sec.
iter 70770 || Loss: 1.0902 || timer: 0.0921 sec.
iter 70780 || Loss: 1.0088 || timer: 0.0996 sec.
iter 70790 || Loss: 1.0179 || timer: 0.1034 sec.
iter 70800 || Loss: 0.8802 || timer: 0.1011 sec.
iter 70810 || Loss: 1.1072 || timer: 0.1027 sec.
iter 70820 || Loss: 1.2569 || timer: 0.0828 sec.
iter 70830 || Loss: 0.8392 || timer: 0.0854 sec.
iter 70840 || Loss: 1.0901 || timer: 0.0192 sec.
iter 70850 || Loss: 0.7881 || timer: 0.0841 sec.
iter 70860 || Loss: 1.4665 || timer: 0.0909 sec.
iter 70870 || Loss: 0.9946 || timer: 0.0899 sec.
iter 70880 || Loss: 1.2316 || timer: 0.0917 sec.
iter 70890 || Loss: 0.8131 || timer: 0.0900 sec.
iter 70900 || Loss: 1.1856 || timer: 0.1202 sec.
iter 70910 || Loss: 0.7547 || timer: 0.1027 sec.
iter 70920 || Loss: 1.0403 || timer: 0.0934 sec.
iter 70930 || Loss: 0.8160 || timer: 0.0898 sec.
iter 70940 || Loss: 0.8082 || timer: 0.1143 sec.
iter 70950 || Loss: 1.0439 || timer: 0.0825 sec.
iter 70960 || Loss: 1.6911 || timer: 0.0856 sec.
iter 70970 || Loss: 1.3220 || timer: 0.0913 sec.
iter 70980 || Loss: 1.2313 || timer: 0.1020 sec.
iter 70990 || Loss: 0.9664 || timer: 0.0903 sec.
iter 71000 || Loss: 1.1283 || timer: 0.0778 sec.
iter 71010 || Loss: 0.9478 || timer: 0.0841 sec.
iter 71020 || Loss: 1.3789 || timer: 0.0837 sec.
iter 71030 || Loss: 1.2671 || timer: 0.0904 sec.
iter 71040 || Loss: 0.8699 || timer: 0.0879 sec.
iter 71050 || Loss: 1.3226 || timer: 0.1027 sec.
iter 71060 || Loss: 1.0242 || timer: 0.0837 sec.
iter 71070 || Loss: 1.1875 || timer: 0.0915 sec.
iter 71080 || Loss: 1.3088 || timer: 0.0827 sec.
iter 71090 || Loss: 1.3059 || timer: 0.1136 sec.
iter 71100 || Loss: 0.8329 || timer: 0.0912 sec.
iter 71110 || Loss: 1.1166 || timer: 0.0904 sec.
iter 71120 || Loss: 1.3753 || timer: 0.1013 sec.
iter 71130 || Loss: 1.0084 || timer: 0.0831 sec.
iter 71140 || Loss: 1.1758 || timer: 0.0910 sec.
iter 71150 || Loss: 1.1453 || timer: 0.0829 sec.
iter 71160 || Loss: 0.9117 || timer: 0.0833 sec.
iter 71170 || Loss: 1.2968 || timer: 0.0208 sec.
iter 71180 || Loss: 1.4499 || timer: 0.0861 sec.
iter 71190 || Loss: 1.1033 || timer: 0.0901 sec.
iter 71200 || Loss: 1.3844 || timer: 0.1116 sec.
iter 71210 || Loss: 1.3288 || timer: 0.0851 sec.
iter 71220 || Loss: 0.8860 || timer: 0.0957 sec.
iter 71230 || Loss: 1.0898 || timer: 0.1067 sec.
iter 71240 || Loss: 1.1011 || timer: 0.0918 sec.
iter 71250 || Loss: 1.0140 || timer: 0.1047 sec.
iter 71260 || Loss: 1.0261 || timer: 0.0834 sec.
iter 71270 || Loss: 0.9082 || timer: 0.0986 sec.
iter 71280 || Loss: 1.0504 || timer: 0.1028 sec.
iter 71290 || Loss: 1.2015 || timer: 0.0841 sec.
iter 71300 || Loss: 0.9255 || timer: 0.0894 sec.
iter 71310 || Loss: 1.0185 || timer: 0.1074 sec.
iter 71320 || Loss: 1.1519 || timer: 0.0923 sec.
iter 71330 || Loss: 0.9003 || timer: 0.0965 sec.
iter 71340 || Loss: 1.0499 || timer: 0.1124 sec.
iter 71350 || Loss: 1.3384 || timer: 0.0914 sec.
iter 71360 || Loss: 1.1740 || timer: 0.0888 sec.
iter 71370 || Loss: 1.4400 || timer: 0.0957 sec.
iter 71380 || Loss: 0.9692 || timer: 0.0795 sec.
iter 71390 || Loss: 1.4879 || timer: 0.0868 sec.
iter 71400 || Loss: 1.1910 || timer: 0.0913 sec.
iter 71410 || Loss: 0.9632 || timer: 0.0825 sec.
iter 71420 || Loss: 1.2983 || timer: 0.1176 sec.
iter 71430 || Loss: 1.0262 || timer: 0.0859 sec.
iter 71440 || Loss: 1.0501 || timer: 0.0886 sec.
iter 71450 || Loss: 1.2049 || timer: 0.0922 sec.
iter 71460 || Loss: 1.1861 || timer: 0.1049 sec.
iter 71470 || Loss: 1.2828 || timer: 0.1047 sec.
iter 71480 || Loss: 1.1047 || timer: 0.0850 sec.
iter 71490 || Loss: 1.2655 || timer: 0.1029 sec.
iter 71500 || Loss: 1.0183 || timer: 0.0207 sec.
iter 71510 || Loss: 0.8864 || timer: 0.0903 sec.
iter 71520 || Loss: 1.3254 || timer: 0.1086 sec.
iter 71530 || Loss: 1.4105 || timer: 0.0923 sec.
iter 71540 || Loss: 1.1951 || timer: 0.0904 sec.
iter 71550 || Loss: 1.0214 || timer: 0.0832 sec.
iter 71560 || Loss: 0.8612 || timer: 0.0919 sec.
iter 71570 || Loss: 1.2564 || timer: 0.0841 sec.
iter 71580 || Loss: 1.7448 || timer: 0.0838 sec.
iter 71590 || Loss: 1.2758 || timer: 0.1103 sec.
iter 71600 || Loss: 1.1276 || timer: 0.1151 sec.
iter 71610 || Loss: 0.8966 || timer: 0.0844 sec.
iter 71620 || Loss: 1.1777 || timer: 0.0923 sec.
iter 71630 || Loss: 1.2651 || timer: 0.0911 sec.
iter 71640 || Loss: 1.1865 || timer: 0.1045 sec.
iter 71650 || Loss: 0.9284 || timer: 0.0908 sec.
iter 71660 || Loss: 1.3049 || timer: 0.0982 sec.
iter 71670 || Loss: 1.4343 || timer: 0.0921 sec.
iter 71680 || Loss: 1.5781 || timer: 0.0916 sec.
iter 71690 || Loss: 0.9252 || timer: 0.0903 sec.
iter 71700 || Loss: 1.1149 || timer: 0.0823 sec.
iter 71710 || Loss: 1.0331 || timer: 0.1159 sec.
iter 71720 || Loss: 0.8383 || timer: 0.1040 sec.
iter 71730 || Loss: 1.1874 || timer: 0.0848 sec.
iter 71740 || Loss: 1.1118 || timer: 0.1139 sec.
iter 71750 || Loss: 0.7847 || timer: 0.1100 sec.
iter 71760 || Loss: 1.4834 || timer: 0.0850 sec.
iter 71770 || Loss: 1.2828 || timer: 0.0836 sec.
iter 71780 || Loss: 1.2092 || timer: 0.0904 sec.
iter 71790 || Loss: 1.1907 || timer: 0.0846 sec.
iter 71800 || Loss: 0.8470 || timer: 0.0768 sec.
iter 71810 || Loss: 1.2521 || timer: 0.0836 sec.
iter 71820 || Loss: 1.1214 || timer: 0.0840 sec.
iter 71830 || Loss: 0.8486 || timer: 0.0206 sec.
iter 71840 || Loss: 0.5572 || timer: 0.0967 sec.
iter 71850 || Loss: 1.2922 || timer: 0.1102 sec.
iter 71860 || Loss: 1.1346 || timer: 0.0918 sec.
iter 71870 || Loss: 0.9403 || timer: 0.0896 sec.
iter 71880 || Loss: 1.2887 || timer: 0.0901 sec.
iter 71890 || Loss: 1.0008 || timer: 0.0927 sec.
iter 71900 || Loss: 1.1744 || timer: 0.1060 sec.
iter 71910 || Loss: 1.1578 || timer: 0.0768 sec.
iter 71920 || Loss: 1.0746 || timer: 0.0963 sec.
iter 71930 || Loss: 1.0287 || timer: 0.1153 sec.
iter 71940 || Loss: 0.9195 || timer: 0.0841 sec.
iter 71950 || Loss: 0.9753 || timer: 0.0837 sec.
iter 71960 || Loss: 1.2661 || timer: 0.0913 sec.
iter 71970 || Loss: 0.9840 || timer: 0.0852 sec.
iter 71980 || Loss: 1.2885 || timer: 0.1098 sec.
iter 71990 || Loss: 1.1361 || timer: 0.1030 sec.
iter 72000 || Loss: 1.0825 || timer: 0.0837 sec.
iter 72010 || Loss: 1.1015 || timer: 0.1155 sec.
iter 72020 || Loss: 1.3622 || timer: 0.0894 sec.
iter 72030 || Loss: 1.1088 || timer: 0.0911 sec.
iter 72040 || Loss: 1.0676 || timer: 0.0944 sec.
iter 72050 || Loss: 1.4806 || timer: 0.1069 sec.
iter 72060 || Loss: 1.0162 || timer: 0.0841 sec.
iter 72070 || Loss: 1.2003 || timer: 0.0839 sec.
iter 72080 || Loss: 1.5143 || timer: 0.0846 sec.
iter 72090 || Loss: 1.4153 || timer: 0.0870 sec.
iter 72100 || Loss: 0.9315 || timer: 0.0898 sec.
iter 72110 || Loss: 1.1328 || timer: 0.1073 sec.
iter 72120 || Loss: 1.4279 || timer: 0.0915 sec.
iter 72130 || Loss: 1.0210 || timer: 0.0925 sec.
iter 72140 || Loss: 1.2520 || timer: 0.0897 sec.
iter 72150 || Loss: 1.0630 || timer: 0.0921 sec.
iter 72160 || Loss: 0.9489 || timer: 0.0155 sec.
iter 72170 || Loss: 0.6031 || timer: 0.0922 sec.
iter 72180 || Loss: 0.9223 || timer: 0.0823 sec.
iter 72190 || Loss: 1.0886 || timer: 0.0955 sec.
iter 72200 || Loss: 0.7887 || timer: 0.0907 sec.
iter 72210 || Loss: 0.9408 || timer: 0.0911 sec.
iter 72220 || Loss: 1.2074 || timer: 0.0850 sec.
iter 72230 || Loss: 1.1170 || timer: 0.0894 sec.
iter 72240 || Loss: 1.2494 || timer: 0.1258 sec.
iter 72250 || Loss: 1.3240 || timer: 0.0901 sec.
iter 72260 || Loss: 1.3008 || timer: 0.1201 sec.
iter 72270 || Loss: 1.0544 || timer: 0.0991 sec.
iter 72280 || Loss: 1.1010 || timer: 0.0874 sec.
iter 72290 || Loss: 1.1249 || timer: 0.1048 sec.
iter 72300 || Loss: 0.9097 || timer: 0.0941 sec.
iter 72310 || Loss: 0.8805 || timer: 0.0822 sec.
iter 72320 || Loss: 0.9870 || timer: 0.0833 sec.
iter 72330 || Loss: 1.0945 || timer: 0.0874 sec.
iter 72340 || Loss: 1.2832 || timer: 0.1012 sec.
iter 72350 || Loss: 1.0609 || timer: 0.0917 sec.
iter 72360 || Loss: 0.9476 || timer: 0.0833 sec.
iter 72370 || Loss: 1.3208 || timer: 0.1367 sec.
iter 72380 || Loss: 0.9480 || timer: 0.0886 sec.
iter 72390 || Loss: 1.3447 || timer: 0.0870 sec.
iter 72400 || Loss: 1.4292 || timer: 0.0901 sec.
iter 72410 || Loss: 1.1975 || timer: 0.1103 sec.
iter 72420 || Loss: 1.3663 || timer: 0.0893 sec.
iter 72430 || Loss: 1.1731 || timer: 0.0890 sec.
iter 72440 || Loss: 1.2131 || timer: 0.1135 sec.
iter 72450 || Loss: 1.3507 || timer: 0.0891 sec.
iter 72460 || Loss: 1.0776 || timer: 0.0818 sec.
iter 72470 || Loss: 1.3953 || timer: 0.1163 sec.
iter 72480 || Loss: 1.0921 || timer: 0.0884 sec.
iter 72490 || Loss: 1.0537 || timer: 0.0187 sec.
iter 72500 || Loss: 0.7620 || timer: 0.0822 sec.
iter 72510 || Loss: 1.5148 || timer: 0.0883 sec.
iter 72520 || Loss: 1.0475 || timer: 0.0820 sec.
iter 72530 || Loss: 1.1170 || timer: 0.0901 sec.
iter 72540 || Loss: 0.9426 || timer: 0.0960 sec.
iter 72550 || Loss: 0.9852 || timer: 0.0879 sec.
iter 72560 || Loss: 1.2081 || timer: 0.0975 sec.
iter 72570 || Loss: 1.1129 || timer: 0.0827 sec.
iter 72580 || Loss: 1.4767 || timer: 0.0824 sec.
iter 72590 || Loss: 1.2009 || timer: 0.1087 sec.
iter 72600 || Loss: 1.6177 || timer: 0.1102 sec.
iter 72610 || Loss: 1.0520 || timer: 0.0885 sec.
iter 72620 || Loss: 0.9644 || timer: 0.0837 sec.
iter 72630 || Loss: 0.9578 || timer: 0.0823 sec.
iter 72640 || Loss: 0.9295 || timer: 0.1051 sec.
iter 72650 || Loss: 1.1595 || timer: 0.0844 sec.
iter 72660 || Loss: 0.8632 || timer: 0.0868 sec.
iter 72670 || Loss: 1.1005 || timer: 0.0898 sec.
iter 72680 || Loss: 0.9132 || timer: 0.0946 sec.
iter 72690 || Loss: 1.0902 || timer: 0.0845 sec.
iter 72700 || Loss: 1.0316 || timer: 0.0822 sec.
iter 72710 || Loss: 1.1258 || timer: 0.1068 sec.
iter 72720 || Loss: 1.3492 || timer: 0.0817 sec.
iter 72730 || Loss: 0.9870 || timer: 0.0843 sec.
iter 72740 || Loss: 1.1224 || timer: 0.0823 sec.
iter 72750 || Loss: 1.0521 || timer: 0.1043 sec.
iter 72760 || Loss: 1.1564 || timer: 0.0815 sec.
iter 72770 || Loss: 0.8868 || timer: 0.0761 sec.
iter 72780 || Loss: 1.0430 || timer: 0.0817 sec.
iter 72790 || Loss: 1.0946 || timer: 0.0824 sec.
iter 72800 || Loss: 1.0787 || timer: 0.0824 sec.
iter 72810 || Loss: 0.8213 || timer: 0.0875 sec.
iter 72820 || Loss: 0.9172 || timer: 0.0229 sec.
iter 72830 || Loss: 1.5893 || timer: 0.0808 sec.
iter 72840 || Loss: 1.3532 || timer: 0.0958 sec.
iter 72850 || Loss: 0.8789 || timer: 0.0823 sec.
iter 72860 || Loss: 0.9223 || timer: 0.0952 sec.
iter 72870 || Loss: 0.9009 || timer: 0.0827 sec.
iter 72880 || Loss: 1.5813 || timer: 0.0832 sec.
iter 72890 || Loss: 1.2519 || timer: 0.0881 sec.
iter 72900 || Loss: 1.1720 || timer: 0.0896 sec.
iter 72910 || Loss: 1.1202 || timer: 0.0928 sec.
iter 72920 || Loss: 0.9538 || timer: 0.0964 sec.
iter 72930 || Loss: 1.0556 || timer: 0.0817 sec.
iter 72940 || Loss: 1.4837 || timer: 0.0876 sec.
iter 72950 || Loss: 1.1400 || timer: 0.0860 sec.
iter 72960 || Loss: 1.3109 || timer: 0.0933 sec.
iter 72970 || Loss: 1.1285 || timer: 0.0995 sec.
iter 72980 || Loss: 0.9592 || timer: 0.0885 sec.
iter 72990 || Loss: 1.4552 || timer: 0.0892 sec.
iter 73000 || Loss: 1.1752 || timer: 0.0942 sec.
iter 73010 || Loss: 1.0211 || timer: 0.1102 sec.
iter 73020 || Loss: 1.2649 || timer: 0.0809 sec.
iter 73030 || Loss: 1.3448 || timer: 0.0810 sec.
iter 73040 || Loss: 1.0262 || timer: 0.0900 sec.
iter 73050 || Loss: 1.2750 || timer: 0.0811 sec.
iter 73060 || Loss: 1.2034 || timer: 0.0814 sec.
iter 73070 || Loss: 0.8518 || timer: 0.0811 sec.
iter 73080 || Loss: 0.8663 || timer: 0.0973 sec.
iter 73090 || Loss: 1.2863 || timer: 0.0808 sec.
iter 73100 || Loss: 1.2692 || timer: 0.0847 sec.
iter 73110 || Loss: 0.9989 || timer: 0.1217 sec.
iter 73120 || Loss: 0.7941 || timer: 0.0870 sec.
iter 73130 || Loss: 1.4163 || timer: 0.0902 sec.
iter 73140 || Loss: 1.9508 || timer: 0.0851 sec.
iter 73150 || Loss: 1.0314 || timer: 0.0167 sec.
iter 73160 || Loss: 1.3881 || timer: 0.0808 sec.
iter 73170 || Loss: 1.7473 || timer: 0.1211 sec.
iter 73180 || Loss: 1.0811 || timer: 0.1014 sec.
iter 73190 || Loss: 1.0603 || timer: 0.0887 sec.
iter 73200 || Loss: 1.1546 || timer: 0.0915 sec.
iter 73210 || Loss: 1.1515 || timer: 0.1038 sec.
iter 73220 || Loss: 1.5888 || timer: 0.0820 sec.
iter 73230 || Loss: 1.3003 || timer: 0.1042 sec.
iter 73240 || Loss: 0.8585 || timer: 0.0955 sec.
iter 73250 || Loss: 1.2780 || timer: 0.1066 sec.
iter 73260 || Loss: 1.1977 || timer: 0.0876 sec.
iter 73270 || Loss: 1.0530 || timer: 0.1038 sec.
iter 73280 || Loss: 0.9507 || timer: 0.0863 sec.
iter 73290 || Loss: 1.2083 || timer: 0.0813 sec.
iter 73300 || Loss: 1.1219 || timer: 0.0920 sec.
iter 73310 || Loss: 0.9638 || timer: 0.0836 sec.
iter 73320 || Loss: 1.1120 || timer: 0.0899 sec.
iter 73330 || Loss: 1.0204 || timer: 0.0887 sec.
iter 73340 || Loss: 1.2805 || timer: 0.0865 sec.
iter 73350 || Loss: 1.1657 || timer: 0.0822 sec.
iter 73360 || Loss: 1.1735 || timer: 0.0887 sec.
iter 73370 || Loss: 1.1279 || timer: 0.0827 sec.
iter 73380 || Loss: 1.1861 || timer: 0.0896 sec.
iter 73390 || Loss: 1.0413 || timer: 0.0915 sec.
iter 73400 || Loss: 1.0035 || timer: 0.0904 sec.
iter 73410 || Loss: 1.0401 || timer: 0.0894 sec.
iter 73420 || Loss: 1.2817 || timer: 0.1012 sec.
iter 73430 || Loss: 1.5147 || timer: 0.0805 sec.
iter 73440 || Loss: 0.8734 || timer: 0.0838 sec.
iter 73450 || Loss: 0.9554 || timer: 0.0907 sec.
iter 73460 || Loss: 0.7762 || timer: 0.1073 sec.
iter 73470 || Loss: 0.8751 || timer: 0.0910 sec.
iter 73480 || Loss: 1.1179 || timer: 0.0160 sec.
iter 73490 || Loss: 1.5464 || timer: 0.0930 sec.
iter 73500 || Loss: 1.3880 || timer: 0.0964 sec.
iter 73510 || Loss: 1.2301 || timer: 0.0895 sec.
iter 73520 || Loss: 1.3221 || timer: 0.0812 sec.
iter 73530 || Loss: 1.1418 || timer: 0.0803 sec.
iter 73540 || Loss: 0.8807 || timer: 0.0899 sec.
iter 73550 || Loss: 1.0816 || timer: 0.0834 sec.
iter 73560 || Loss: 1.1288 || timer: 0.0925 sec.
iter 73570 || Loss: 1.3959 || timer: 0.0935 sec.
iter 73580 || Loss: 0.9717 || timer: 0.0966 sec.
iter 73590 || Loss: 1.0057 || timer: 0.0810 sec.
iter 73600 || Loss: 1.1097 || timer: 0.0907 sec.
iter 73610 || Loss: 1.2169 || timer: 0.0908 sec.
iter 73620 || Loss: 0.8223 || timer: 0.0807 sec.
iter 73630 || Loss: 0.8198 || timer: 0.0909 sec.
iter 73640 || Loss: 1.2042 || timer: 0.0812 sec.
iter 73650 || Loss: 0.9167 || timer: 0.0817 sec.
iter 73660 || Loss: 1.0029 || timer: 0.0993 sec.
iter 73670 || Loss: 1.0723 || timer: 0.0896 sec.
iter 73680 || Loss: 0.9512 || timer: 0.0815 sec.
iter 73690 || Loss: 1.3157 || timer: 0.1108 sec.
iter 73700 || Loss: 1.3799 || timer: 0.0904 sec.
iter 73710 || Loss: 1.1820 || timer: 0.0880 sec.
iter 73720 || Loss: 1.0250 || timer: 0.0931 sec.
iter 73730 || Loss: 1.1865 || timer: 0.0914 sec.
iter 73740 || Loss: 1.0747 || timer: 0.0826 sec.
iter 73750 || Loss: 1.1447 || timer: 0.0818 sec.
iter 73760 || Loss: 0.7636 || timer: 0.0826 sec.
iter 73770 || Loss: 1.0411 || timer: 0.0824 sec.
iter 73780 || Loss: 1.0899 || timer: 0.0839 sec.
iter 73790 || Loss: 0.9380 || timer: 0.0883 sec.
iter 73800 || Loss: 1.4602 || timer: 0.0962 sec.
iter 73810 || Loss: 1.2758 || timer: 0.0163 sec.
iter 73820 || Loss: 1.1220 || timer: 0.1034 sec.
iter 73830 || Loss: 1.0951 || timer: 0.0833 sec.
iter 73840 || Loss: 1.5948 || timer: 0.0881 sec.
iter 73850 || Loss: 0.9468 || timer: 0.0875 sec.
iter 73860 || Loss: 1.1416 || timer: 0.0883 sec.
iter 73870 || Loss: 1.1431 || timer: 0.1015 sec.
iter 73880 || Loss: 1.0066 || timer: 0.0899 sec.
iter 73890 || Loss: 1.4633 || timer: 0.0821 sec.
iter 73900 || Loss: 1.1286 || timer: 0.0877 sec.
iter 73910 || Loss: 1.2452 || timer: 0.1078 sec.
iter 73920 || Loss: 1.3092 || timer: 0.0805 sec.
iter 73930 || Loss: 0.9277 || timer: 0.0809 sec.
iter 73940 || Loss: 1.2731 || timer: 0.1012 sec.
iter 73950 || Loss: 0.8789 || timer: 0.1157 sec.
iter 73960 || Loss: 1.6555 || timer: 0.0823 sec.
iter 73970 || Loss: 1.3021 || timer: 0.0824 sec.
iter 73980 || Loss: 1.1643 || timer: 0.0875 sec.
iter 73990 || Loss: 1.6666 || timer: 0.0962 sec.
iter 74000 || Loss: 1.1705 || timer: 0.0803 sec.
iter 74010 || Loss: 0.9891 || timer: 0.0805 sec.
iter 74020 || Loss: 1.1217 || timer: 0.0812 sec.
iter 74030 || Loss: 2.0556 || timer: 0.0817 sec.
iter 74040 || Loss: 1.5673 || timer: 0.0886 sec.
iter 74050 || Loss: 1.4730 || timer: 0.0890 sec.
iter 74060 || Loss: 1.2531 || timer: 0.0913 sec.
iter 74070 || Loss: 1.0704 || timer: 0.0809 sec.
iter 74080 || Loss: 1.1359 || timer: 0.0866 sec.
iter 74090 || Loss: 1.3081 || timer: 0.0915 sec.
iter 74100 || Loss: 1.1852 || timer: 0.0822 sec.
iter 74110 || Loss: 1.7916 || timer: 0.0888 sec.
iter 74120 || Loss: 1.0182 || timer: 0.0900 sec.
iter 74130 || Loss: 1.2200 || timer: 0.0894 sec.
iter 74140 || Loss: 0.8019 || timer: 0.0163 sec.
iter 74150 || Loss: 0.5355 || timer: 0.0908 sec.
iter 74160 || Loss: 1.0909 || timer: 0.1149 sec.
iter 74170 || Loss: 1.0784 || timer: 0.0872 sec.
iter 74180 || Loss: 1.1331 || timer: 0.0966 sec.
iter 74190 || Loss: 1.1178 || timer: 0.0824 sec.
iter 74200 || Loss: 1.3992 || timer: 0.0906 sec.
iter 74210 || Loss: 1.4939 || timer: 0.0913 sec.
iter 74220 || Loss: 0.8300 || timer: 0.0822 sec.
iter 74230 || Loss: 1.2240 || timer: 0.0876 sec.
iter 74240 || Loss: 1.3539 || timer: 0.0955 sec.
iter 74250 || Loss: 0.7631 || timer: 0.0871 sec.
iter 74260 || Loss: 1.3280 || timer: 0.0831 sec.
iter 74270 || Loss: 0.7264 || timer: 0.0826 sec.
iter 74280 || Loss: 0.9120 || timer: 0.0814 sec.
iter 74290 || Loss: 1.5534 || timer: 0.0911 sec.
iter 74300 || Loss: 1.0770 || timer: 0.0896 sec.
iter 74310 || Loss: 0.7363 || timer: 0.0931 sec.
iter 74320 || Loss: 1.4186 || timer: 0.0849 sec.
iter 74330 || Loss: 1.1879 || timer: 0.0955 sec.
iter 74340 || Loss: 1.0268 || timer: 0.0812 sec.
iter 74350 || Loss: 1.1091 || timer: 0.0901 sec.
iter 74360 || Loss: 0.9351 || timer: 0.0914 sec.
iter 74370 || Loss: 1.3551 || timer: 0.0895 sec.
iter 74380 || Loss: 0.8752 || timer: 0.0807 sec.
iter 74390 || Loss: 1.1032 || timer: 0.0903 sec.
iter 74400 || Loss: 1.1325 || timer: 0.0898 sec.
iter 74410 || Loss: 0.8341 || timer: 0.0850 sec.
iter 74420 || Loss: 1.4760 || timer: 0.0818 sec.
iter 74430 || Loss: 1.3562 || timer: 0.0824 sec.
iter 74440 || Loss: 1.1216 || timer: 0.0920 sec.
iter 74450 || Loss: 1.3310 || timer: 0.0827 sec.
iter 74460 || Loss: 1.4905 || timer: 0.0917 sec.
iter 74470 || Loss: 1.1656 || timer: 0.0205 sec.
iter 74480 || Loss: 0.7618 || timer: 0.0884 sec.
iter 74490 || Loss: 1.6091 || timer: 0.0876 sec.
iter 74500 || Loss: 0.9800 || timer: 0.0909 sec.
iter 74510 || Loss: 0.9215 || timer: 0.0880 sec.
iter 74520 || Loss: 1.0066 || timer: 0.0927 sec.
iter 74530 || Loss: 1.1981 || timer: 0.0823 sec.
iter 74540 || Loss: 0.8860 || timer: 0.0887 sec.
iter 74550 || Loss: 0.9458 || timer: 0.0916 sec.
iter 74560 || Loss: 1.3533 || timer: 0.0821 sec.
iter 74570 || Loss: 1.0892 || timer: 0.0983 sec.
iter 74580 || Loss: 1.2322 || timer: 0.0858 sec.
iter 74590 || Loss: 1.1667 || timer: 0.0824 sec.
iter 74600 || Loss: 0.9717 || timer: 0.0863 sec.
iter 74610 || Loss: 1.2436 || timer: 0.0907 sec.
iter 74620 || Loss: 1.1866 || timer: 0.0982 sec.
iter 74630 || Loss: 1.1426 || timer: 0.0940 sec.
iter 74640 || Loss: 1.0578 || timer: 0.0802 sec.
iter 74650 || Loss: 1.0712 || timer: 0.1121 sec.
iter 74660 || Loss: 1.0346 || timer: 0.0887 sec.
iter 74670 || Loss: 1.1946 || timer: 0.1015 sec.
iter 74680 || Loss: 1.1164 || timer: 0.1033 sec.
iter 74690 || Loss: 0.9579 || timer: 0.0885 sec.
iter 74700 || Loss: 1.1177 || timer: 0.0860 sec.
iter 74710 || Loss: 1.1891 || timer: 0.0874 sec.
iter 74720 || Loss: 0.9412 || timer: 0.0969 sec.
iter 74730 || Loss: 0.8568 || timer: 0.0948 sec.
iter 74740 || Loss: 1.0337 || timer: 0.1092 sec.
iter 74750 || Loss: 1.5032 || timer: 0.1092 sec.
iter 74760 || Loss: 0.8623 || timer: 0.0883 sec.
iter 74770 || Loss: 1.4155 || timer: 0.0830 sec.
iter 74780 || Loss: 1.0114 || timer: 0.0822 sec.
iter 74790 || Loss: 0.9613 || timer: 0.0823 sec.
iter 74800 || Loss: 1.3721 || timer: 0.0173 sec.
iter 74810 || Loss: 0.9034 || timer: 0.0869 sec.
iter 74820 || Loss: 1.1142 || timer: 0.0967 sec.
iter 74830 || Loss: 1.2304 || timer: 0.0832 sec.
iter 74840 || Loss: 0.9448 || timer: 0.0877 sec.
iter 74850 || Loss: 0.7637 || timer: 0.0963 sec.
iter 74860 || Loss: 1.1258 || timer: 0.0818 sec.
iter 74870 || Loss: 1.0479 || timer: 0.0883 sec.
iter 74880 || Loss: 0.8134 || timer: 0.0853 sec.
iter 74890 || Loss: 0.8802 || timer: 0.0823 sec.
iter 74900 || Loss: 1.0203 || timer: 0.1236 sec.
iter 74910 || Loss: 1.1604 || timer: 0.0889 sec.
iter 74920 || Loss: 1.2021 || timer: 0.1021 sec.
iter 74930 || Loss: 1.2038 || timer: 0.0885 sec.
iter 74940 || Loss: 1.6464 || timer: 0.0880 sec.
iter 74950 || Loss: 1.1032 || timer: 0.0948 sec.
iter 74960 || Loss: 1.1130 || timer: 0.1031 sec.
iter 74970 || Loss: 1.1340 || timer: 0.0910 sec.
iter 74980 || Loss: 1.2485 || timer: 0.0813 sec.
iter 74990 || Loss: 1.3427 || timer: 0.1198 sec.
iter 75000 || Loss: 1.1313 || Saving state, iter: 75000
timer: 0.0899 sec.
iter 75010 || Loss: 1.3220 || timer: 0.0889 sec.
iter 75020 || Loss: 1.0691 || timer: 0.0910 sec.
iter 75030 || Loss: 1.6171 || timer: 0.0834 sec.
iter 75040 || Loss: 0.9780 || timer: 0.0894 sec.
iter 75050 || Loss: 1.5126 || timer: 0.0930 sec.
iter 75060 || Loss: 1.7693 || timer: 0.0889 sec.
iter 75070 || Loss: 1.0837 || timer: 0.0874 sec.
iter 75080 || Loss: 1.4156 || timer: 0.0852 sec.
iter 75090 || Loss: 1.1092 || timer: 0.0895 sec.
iter 75100 || Loss: 1.2360 || timer: 0.1113 sec.
iter 75110 || Loss: 0.9847 || timer: 0.0911 sec.
iter 75120 || Loss: 1.0256 || timer: 0.0875 sec.
iter 75130 || Loss: 1.2829 || timer: 0.0160 sec.
iter 75140 || Loss: 0.5259 || timer: 0.0823 sec.
iter 75150 || Loss: 1.1262 || timer: 0.1007 sec.
iter 75160 || Loss: 1.1955 || timer: 0.0854 sec.
iter 75170 || Loss: 1.0992 || timer: 0.0887 sec.
iter 75180 || Loss: 1.3446 || timer: 0.0890 sec.
iter 75190 || Loss: 1.2866 || timer: 0.0880 sec.
iter 75200 || Loss: 0.9772 || timer: 0.1030 sec.
iter 75210 || Loss: 1.0326 || timer: 0.0874 sec.
iter 75220 || Loss: 1.1238 || timer: 0.0821 sec.
iter 75230 || Loss: 0.8642 || timer: 0.1012 sec.
iter 75240 || Loss: 1.1790 || timer: 0.0857 sec.
iter 75250 || Loss: 1.5751 || timer: 0.0896 sec.
iter 75260 || Loss: 1.4067 || timer: 0.0893 sec.
iter 75270 || Loss: 1.0316 || timer: 0.0980 sec.
iter 75280 || Loss: 0.8662 || timer: 0.0821 sec.
iter 75290 || Loss: 1.2488 || timer: 0.0978 sec.
iter 75300 || Loss: 1.3286 || timer: 0.0881 sec.
iter 75310 || Loss: 1.1083 || timer: 0.0886 sec.
iter 75320 || Loss: 1.0536 || timer: 0.0816 sec.
iter 75330 || Loss: 1.1225 || timer: 0.0848 sec.
iter 75340 || Loss: 1.1359 || timer: 0.0813 sec.
iter 75350 || Loss: 1.1240 || timer: 0.0815 sec.
iter 75360 || Loss: 0.9884 || timer: 0.0852 sec.
iter 75370 || Loss: 1.2214 || timer: 0.0876 sec.
iter 75380 || Loss: 1.3374 || timer: 0.0907 sec.
iter 75390 || Loss: 0.9266 || timer: 0.0811 sec.
iter 75400 || Loss: 1.2126 || timer: 0.0809 sec.
iter 75410 || Loss: 0.8970 || timer: 0.0899 sec.
iter 75420 || Loss: 0.8721 || timer: 0.0954 sec.
iter 75430 || Loss: 1.0801 || timer: 0.0886 sec.
iter 75440 || Loss: 1.2093 || timer: 0.0830 sec.
iter 75450 || Loss: 1.1674 || timer: 0.0835 sec.
iter 75460 || Loss: 1.1774 || timer: 0.0238 sec.
iter 75470 || Loss: 2.1786 || timer: 0.1012 sec.
iter 75480 || Loss: 1.2070 || timer: 0.0804 sec.
iter 75490 || Loss: 0.9051 || timer: 0.1013 sec.
iter 75500 || Loss: 1.0847 || timer: 0.0972 sec.
iter 75510 || Loss: 1.1954 || timer: 0.0893 sec.
iter 75520 || Loss: 1.3200 || timer: 0.0809 sec.
iter 75530 || Loss: 1.3282 || timer: 0.1069 sec.
iter 75540 || Loss: 0.9564 || timer: 0.0926 sec.
iter 75550 || Loss: 0.9375 || timer: 0.0815 sec.
iter 75560 || Loss: 1.0147 || timer: 0.0961 sec.
iter 75570 || Loss: 1.0752 || timer: 0.0951 sec.
iter 75580 || Loss: 0.7991 || timer: 0.0890 sec.
iter 75590 || Loss: 0.9026 || timer: 0.0882 sec.
iter 75600 || Loss: 0.9433 || timer: 0.0872 sec.
iter 75610 || Loss: 1.2324 || timer: 0.0808 sec.
iter 75620 || Loss: 1.0565 || timer: 0.0866 sec.
iter 75630 || Loss: 0.9983 || timer: 0.0810 sec.
iter 75640 || Loss: 0.9043 || timer: 0.0898 sec.
iter 75650 || Loss: 1.2256 || timer: 0.0877 sec.
iter 75660 || Loss: 0.9919 || timer: 0.0861 sec.
iter 75670 || Loss: 1.2930 || timer: 0.0896 sec.
iter 75680 || Loss: 1.2717 || timer: 0.0820 sec.
iter 75690 || Loss: 1.2720 || timer: 0.0889 sec.
iter 75700 || Loss: 1.0559 || timer: 0.0822 sec.
iter 75710 || Loss: 1.2441 || timer: 0.1007 sec.
iter 75720 || Loss: 1.3386 || timer: 0.0900 sec.
iter 75730 || Loss: 0.7432 || timer: 0.1262 sec.
iter 75740 || Loss: 0.8427 || timer: 0.1023 sec.
iter 75750 || Loss: 1.2749 || timer: 0.0804 sec.
iter 75760 || Loss: 1.1021 || timer: 0.0896 sec.
iter 75770 || Loss: 1.0478 || timer: 0.1012 sec.
iter 75780 || Loss: 1.2902 || timer: 0.0808 sec.
iter 75790 || Loss: 0.7861 || timer: 0.0160 sec.
iter 75800 || Loss: 2.2383 || timer: 0.0939 sec.
iter 75810 || Loss: 1.3179 || timer: 0.0903 sec.
iter 75820 || Loss: 1.3659 || timer: 0.0819 sec.
iter 75830 || Loss: 1.0118 || timer: 0.0810 sec.
iter 75840 || Loss: 1.2155 || timer: 0.0810 sec.
iter 75850 || Loss: 1.0009 || timer: 0.0810 sec.
iter 75860 || Loss: 1.2544 || timer: 0.0906 sec.
iter 75870 || Loss: 0.9434 || timer: 0.0888 sec.
iter 75880 || Loss: 0.9431 || timer: 0.0897 sec.
iter 75890 || Loss: 0.9107 || timer: 0.1120 sec.
iter 75900 || Loss: 0.9424 || timer: 0.0899 sec.
iter 75910 || Loss: 1.1349 || timer: 0.0817 sec.
iter 75920 || Loss: 1.1690 || timer: 0.0893 sec.
iter 75930 || Loss: 1.3549 || timer: 0.1186 sec.
iter 75940 || Loss: 0.9051 || timer: 0.0813 sec.
iter 75950 || Loss: 1.2087 || timer: 0.1049 sec.
iter 75960 || Loss: 1.1273 || timer: 0.0927 sec.
iter 75970 || Loss: 1.3675 || timer: 0.0816 sec.
iter 75980 || Loss: 0.9793 || timer: 0.0868 sec.
iter 75990 || Loss: 0.9731 || timer: 0.0814 sec.
iter 76000 || Loss: 1.0801 || timer: 0.0845 sec.
iter 76010 || Loss: 0.9303 || timer: 0.0878 sec.
iter 76020 || Loss: 0.8203 || timer: 0.0812 sec.
iter 76030 || Loss: 1.1123 || timer: 0.1174 sec.
iter 76040 || Loss: 1.0026 || timer: 0.0909 sec.
iter 76050 || Loss: 1.3823 || timer: 0.1153 sec.
iter 76060 || Loss: 0.8326 || timer: 0.0875 sec.
iter 76070 || Loss: 1.1576 || timer: 0.1121 sec.
iter 76080 || Loss: 1.3058 || timer: 0.0801 sec.
iter 76090 || Loss: 1.0002 || timer: 0.0889 sec.
iter 76100 || Loss: 1.0548 || timer: 0.1051 sec.
iter 76110 || Loss: 1.2363 || timer: 0.0850 sec.
iter 76120 || Loss: 1.3696 || timer: 0.0219 sec.
iter 76130 || Loss: 2.0106 || timer: 0.0898 sec.
iter 76140 || Loss: 1.2220 || timer: 0.1023 sec.
iter 76150 || Loss: 1.7905 || timer: 0.0920 sec.
iter 76160 || Loss: 2.0104 || timer: 0.0889 sec.
iter 76170 || Loss: 1.4145 || timer: 0.1046 sec.
iter 76180 || Loss: 1.1178 || timer: 0.0894 sec.
iter 76190 || Loss: 0.7308 || timer: 0.0821 sec.
iter 76200 || Loss: 0.8674 || timer: 0.0969 sec.
iter 76210 || Loss: 1.2113 || timer: 0.0943 sec.
iter 76220 || Loss: 2.2374 || timer: 0.0953 sec.
iter 76230 || Loss: 1.2106 || timer: 0.0875 sec.
iter 76240 || Loss: 1.2114 || timer: 0.0891 sec.
iter 76250 || Loss: 1.4924 || timer: 0.0924 sec.
iter 76260 || Loss: 1.1515 || timer: 0.0904 sec.
iter 76270 || Loss: 1.1962 || timer: 0.1104 sec.
iter 76280 || Loss: 1.4586 || timer: 0.0817 sec.
iter 76290 || Loss: 1.4858 || timer: 0.0933 sec.
iter 76300 || Loss: 1.2529 || timer: 0.1122 sec.
iter 76310 || Loss: 1.1455 || timer: 0.1242 sec.
iter 76320 || Loss: 1.3963 || timer: 0.0913 sec.
iter 76330 || Loss: 0.8878 || timer: 0.0908 sec.
iter 76340 || Loss: 1.2807 || timer: 0.0882 sec.
iter 76350 || Loss: 1.2394 || timer: 0.0925 sec.
iter 76360 || Loss: 0.9800 || timer: 0.1098 sec.
iter 76370 || Loss: 1.0593 || timer: 0.0914 sec.
iter 76380 || Loss: 1.2486 || timer: 0.0983 sec.
iter 76390 || Loss: 0.7411 || timer: 0.0745 sec.
iter 76400 || Loss: 1.2259 || timer: 0.0846 sec.
iter 76410 || Loss: 0.8922 || timer: 0.0840 sec.
iter 76420 || Loss: 1.1640 || timer: 0.0996 sec.
iter 76430 || Loss: 1.1394 || timer: 0.0928 sec.
iter 76440 || Loss: 1.1104 || timer: 0.0891 sec.
iter 76450 || Loss: 0.9130 || timer: 0.0222 sec.
iter 76460 || Loss: 0.5774 || timer: 0.0953 sec.
iter 76470 || Loss: 1.2144 || timer: 0.0848 sec.
iter 76480 || Loss: 1.2023 || timer: 0.0831 sec.
iter 76490 || Loss: 1.1050 || timer: 0.0771 sec.
iter 76500 || Loss: 1.1642 || timer: 0.0769 sec.
iter 76510 || Loss: 1.2239 || timer: 0.0803 sec.
iter 76520 || Loss: 1.2941 || timer: 0.1025 sec.
iter 76530 || Loss: 0.7929 || timer: 0.0857 sec.
iter 76540 || Loss: 0.8752 || timer: 0.0772 sec.
iter 76550 || Loss: 1.8212 || timer: 0.1082 sec.
iter 76560 || Loss: 0.9929 || timer: 0.0932 sec.
iter 76570 || Loss: 1.4182 || timer: 0.0897 sec.
iter 76580 || Loss: 1.1564 || timer: 0.0902 sec.
iter 76590 || Loss: 1.0381 || timer: 0.0944 sec.
iter 76600 || Loss: 1.3628 || timer: 0.0777 sec.
iter 76610 || Loss: 1.1834 || timer: 0.0918 sec.
iter 76620 || Loss: 1.3817 || timer: 0.0912 sec.
iter 76630 || Loss: 1.0085 || timer: 0.0910 sec.
iter 76640 || Loss: 0.8381 || timer: 0.0826 sec.
iter 76650 || Loss: 1.1509 || timer: 0.0913 sec.
iter 76660 || Loss: 1.3621 || timer: 0.0847 sec.
iter 76670 || Loss: 1.2191 || timer: 0.0868 sec.
iter 76680 || Loss: 1.1586 || timer: 0.0851 sec.
iter 76690 || Loss: 1.3300 || timer: 0.1006 sec.
iter 76700 || Loss: 1.1551 || timer: 0.0890 sec.
iter 76710 || Loss: 1.1922 || timer: 0.0925 sec.
iter 76720 || Loss: 0.9557 || timer: 0.0870 sec.
iter 76730 || Loss: 0.9713 || timer: 0.0899 sec.
iter 76740 || Loss: 1.1049 || timer: 0.0801 sec.
iter 76750 || Loss: 1.0626 || timer: 0.0824 sec.
iter 76760 || Loss: 1.4797 || timer: 0.0830 sec.
iter 76770 || Loss: 1.1326 || timer: 0.0839 sec.
iter 76780 || Loss: 1.0024 || timer: 0.0239 sec.
iter 76790 || Loss: 0.2165 || timer: 0.0994 sec.
iter 76800 || Loss: 1.1994 || timer: 0.0963 sec.
iter 76810 || Loss: 1.0333 || timer: 0.0855 sec.
iter 76820 || Loss: 1.2035 || timer: 0.0823 sec.
iter 76830 || Loss: 0.8816 || timer: 0.0885 sec.
iter 76840 || Loss: 1.1793 || timer: 0.0824 sec.
iter 76850 || Loss: 1.4928 || timer: 0.1136 sec.
iter 76860 || Loss: 0.8796 || timer: 0.0936 sec.
iter 76870 || Loss: 0.8505 || timer: 0.0876 sec.
iter 76880 || Loss: 1.0672 || timer: 0.0934 sec.
iter 76890 || Loss: 0.9639 || timer: 0.0813 sec.
iter 76900 || Loss: 1.1689 || timer: 0.0999 sec.
iter 76910 || Loss: 0.9300 || timer: 0.0900 sec.
iter 76920 || Loss: 0.8927 || timer: 0.0891 sec.
iter 76930 || Loss: 1.3225 || timer: 0.0823 sec.
iter 76940 || Loss: 1.0730 || timer: 0.1008 sec.
iter 76950 || Loss: 1.0985 || timer: 0.0887 sec.
iter 76960 || Loss: 0.7912 || timer: 0.0810 sec.
iter 76970 || Loss: 1.0936 || timer: 0.0815 sec.
iter 76980 || Loss: 0.9009 || timer: 0.0824 sec.
iter 76990 || Loss: 1.1667 || timer: 0.0904 sec.
iter 77000 || Loss: 1.2131 || timer: 0.0815 sec.
iter 77010 || Loss: 1.1780 || timer: 0.0902 sec.
iter 77020 || Loss: 1.2883 || timer: 0.0796 sec.
iter 77030 || Loss: 0.8999 || timer: 0.1197 sec.
iter 77040 || Loss: 0.9699 || timer: 0.0799 sec.
iter 77050 || Loss: 1.1042 || timer: 0.0963 sec.
iter 77060 || Loss: 1.1522 || timer: 0.0884 sec.
iter 77070 || Loss: 1.1380 || timer: 0.0813 sec.
iter 77080 || Loss: 1.0345 || timer: 0.0922 sec.
iter 77090 || Loss: 1.2017 || timer: 0.0893 sec.
iter 77100 || Loss: 1.1325 || timer: 0.0820 sec.
iter 77110 || Loss: 1.2166 || timer: 0.0262 sec.
iter 77120 || Loss: 0.5851 || timer: 0.0919 sec.
iter 77130 || Loss: 1.1191 || timer: 0.0903 sec.
iter 77140 || Loss: 1.1915 || timer: 0.0889 sec.
iter 77150 || Loss: 0.7983 || timer: 0.0871 sec.
iter 77160 || Loss: 1.1933 || timer: 0.0848 sec.
iter 77170 || Loss: 0.9926 || timer: 0.0841 sec.
iter 77180 || Loss: 1.0683 || timer: 0.0842 sec.
iter 77190 || Loss: 0.8143 || timer: 0.0848 sec.
iter 77200 || Loss: 1.4829 || timer: 0.0901 sec.
iter 77210 || Loss: 0.9321 || timer: 0.1212 sec.
iter 77220 || Loss: 1.0415 || timer: 0.1046 sec.
iter 77230 || Loss: 0.9001 || timer: 0.0809 sec.
iter 77240 || Loss: 0.7784 || timer: 0.0920 sec.
iter 77250 || Loss: 0.8200 || timer: 0.0875 sec.
iter 77260 || Loss: 1.4176 || timer: 0.0900 sec.
iter 77270 || Loss: 0.8246 || timer: 0.1093 sec.
iter 77280 || Loss: 1.2044 || timer: 0.0915 sec.
iter 77290 || Loss: 1.2812 || timer: 0.0822 sec.
iter 77300 || Loss: 1.2592 || timer: 0.1040 sec.
iter 77310 || Loss: 1.0569 || timer: 0.0840 sec.
iter 77320 || Loss: 1.3235 || timer: 0.0918 sec.
iter 77330 || Loss: 1.2920 || timer: 0.0971 sec.
iter 77340 || Loss: 1.8954 || timer: 0.0879 sec.
iter 77350 || Loss: 1.1281 || timer: 0.0962 sec.
iter 77360 || Loss: 1.1118 || timer: 0.0817 sec.
iter 77370 || Loss: 1.6495 || timer: 0.0817 sec.
iter 77380 || Loss: 0.9963 || timer: 0.0834 sec.
iter 77390 || Loss: 1.2210 || timer: 0.1402 sec.
iter 77400 || Loss: 0.8437 || timer: 0.0887 sec.
iter 77410 || Loss: 0.9492 || timer: 0.0893 sec.
iter 77420 || Loss: 1.2470 || timer: 0.0876 sec.
iter 77430 || Loss: 1.2294 || timer: 0.0897 sec.
iter 77440 || Loss: 1.3769 || timer: 0.0167 sec.
iter 77450 || Loss: 0.4468 || timer: 0.0886 sec.
iter 77460 || Loss: 1.1477 || timer: 0.0810 sec.
iter 77470 || Loss: 1.2217 || timer: 0.0887 sec.
iter 77480 || Loss: 1.2377 || timer: 0.0847 sec.
iter 77490 || Loss: 1.6114 || timer: 0.1047 sec.
iter 77500 || Loss: 1.2562 || timer: 0.1062 sec.
iter 77510 || Loss: 1.3135 || timer: 0.0838 sec.
iter 77520 || Loss: 1.2960 || timer: 0.1037 sec.
iter 77530 || Loss: 1.1180 || timer: 0.0816 sec.
iter 77540 || Loss: 1.7553 || timer: 0.0981 sec.
iter 77550 || Loss: 0.9519 || timer: 0.0902 sec.
iter 77560 || Loss: 1.4740 || timer: 0.0839 sec.
iter 77570 || Loss: 0.8423 || timer: 0.0882 sec.
iter 77580 || Loss: 1.1996 || timer: 0.1030 sec.
iter 77590 || Loss: 1.2147 || timer: 0.0898 sec.
iter 77600 || Loss: 0.6892 || timer: 0.0885 sec.
iter 77610 || Loss: 1.1483 || timer: 0.0907 sec.
iter 77620 || Loss: 1.2611 || timer: 0.0913 sec.
iter 77630 || Loss: 0.9775 || timer: 0.0888 sec.
iter 77640 || Loss: 1.5869 || timer: 0.1078 sec.
iter 77650 || Loss: 1.4309 || timer: 0.0863 sec.
iter 77660 || Loss: 0.8529 || timer: 0.1359 sec.
iter 77670 || Loss: 1.1443 || timer: 0.0881 sec.
iter 77680 || Loss: 0.8188 || timer: 0.0867 sec.
iter 77690 || Loss: 1.1074 || timer: 0.0831 sec.
iter 77700 || Loss: 1.3138 || timer: 0.0881 sec.
iter 77710 || Loss: 1.1485 || timer: 0.0814 sec.
iter 77720 || Loss: 1.3830 || timer: 0.0856 sec.
iter 77730 || Loss: 1.2580 || timer: 0.0804 sec.
iter 77740 || Loss: 0.7353 || timer: 0.0887 sec.
iter 77750 || Loss: 1.3468 || timer: 0.0895 sec.
iter 77760 || Loss: 1.0099 || timer: 0.0830 sec.
iter 77770 || Loss: 1.2609 || timer: 0.0205 sec.
iter 77780 || Loss: 2.8295 || timer: 0.0826 sec.
iter 77790 || Loss: 0.7915 || timer: 0.0948 sec.
iter 77800 || Loss: 1.0405 || timer: 0.1010 sec.
iter 77810 || Loss: 1.3929 || timer: 0.0915 sec.
iter 77820 || Loss: 1.3023 || timer: 0.0883 sec.
iter 77830 || Loss: 1.3807 || timer: 0.1037 sec.
iter 77840 || Loss: 0.9266 || timer: 0.0894 sec.
iter 77850 || Loss: 1.1943 || timer: 0.0890 sec.
iter 77860 || Loss: 0.9242 || timer: 0.0804 sec.
iter 77870 || Loss: 1.1728 || timer: 0.0990 sec.
iter 77880 || Loss: 1.1916 || timer: 0.0833 sec.
iter 77890 || Loss: 1.1353 || timer: 0.0996 sec.
iter 77900 || Loss: 1.2073 || timer: 0.1041 sec.
iter 77910 || Loss: 1.0696 || timer: 0.0873 sec.
iter 77920 || Loss: 1.0915 || timer: 0.0987 sec.
iter 77930 || Loss: 1.1737 || timer: 0.0820 sec.
iter 77940 || Loss: 1.0141 || timer: 0.0891 sec.
iter 77950 || Loss: 1.3011 || timer: 0.1078 sec.
iter 77960 || Loss: 1.1281 || timer: 0.0877 sec.
iter 77970 || Loss: 1.0435 || timer: 0.0918 sec.
iter 77980 || Loss: 1.0262 || timer: 0.0888 sec.
iter 77990 || Loss: 1.0173 || timer: 0.0846 sec.
iter 78000 || Loss: 1.0681 || timer: 0.0887 sec.
iter 78010 || Loss: 1.3673 || timer: 0.0813 sec.
iter 78020 || Loss: 1.2830 || timer: 0.0867 sec.
iter 78030 || Loss: 1.3322 || timer: 0.0871 sec.
iter 78040 || Loss: 1.0149 || timer: 0.0908 sec.
iter 78050 || Loss: 1.2306 || timer: 0.0842 sec.
iter 78060 || Loss: 1.1418 || timer: 0.0828 sec.
iter 78070 || Loss: 1.2736 || timer: 0.0827 sec.
iter 78080 || Loss: 1.2031 || timer: 0.0911 sec.
iter 78090 || Loss: 0.9854 || timer: 0.0960 sec.
iter 78100 || Loss: 0.8877 || timer: 0.0158 sec.
iter 78110 || Loss: 0.8162 || timer: 0.1082 sec.
iter 78120 || Loss: 1.2554 || timer: 0.0821 sec.
iter 78130 || Loss: 0.9090 || timer: 0.0812 sec.
iter 78140 || Loss: 0.8599 || timer: 0.0905 sec.
iter 78150 || Loss: 1.0611 || timer: 0.0813 sec.
iter 78160 || Loss: 1.0087 || timer: 0.0906 sec.
iter 78170 || Loss: 1.0532 || timer: 0.0889 sec.
iter 78180 || Loss: 0.7702 || timer: 0.0880 sec.
iter 78190 || Loss: 1.3657 || timer: 0.0892 sec.
iter 78200 || Loss: 0.9760 || timer: 0.0935 sec.
iter 78210 || Loss: 1.1544 || timer: 0.0814 sec.
iter 78220 || Loss: 1.0161 || timer: 0.0885 sec.
iter 78230 || Loss: 0.9042 || timer: 0.0889 sec.
iter 78240 || Loss: 1.0850 || timer: 0.0889 sec.
iter 78250 || Loss: 1.1431 || timer: 0.1108 sec.
iter 78260 || Loss: 1.0456 || timer: 0.0888 sec.
iter 78270 || Loss: 0.9885 || timer: 0.0885 sec.
iter 78280 || Loss: 0.9974 || timer: 0.0823 sec.
iter 78290 || Loss: 1.1300 || timer: 0.0830 sec.
iter 78300 || Loss: 0.9452 || timer: 0.1049 sec.
iter 78310 || Loss: 1.1647 || timer: 0.0821 sec.
iter 78320 || Loss: 1.0310 || timer: 0.0898 sec.
iter 78330 || Loss: 0.8778 || timer: 0.0891 sec.
iter 78340 || Loss: 1.2220 || timer: 0.1053 sec.
iter 78350 || Loss: 1.1411 || timer: 0.0893 sec.
iter 78360 || Loss: 1.0872 || timer: 0.0883 sec.
iter 78370 || Loss: 1.2077 || timer: 0.0854 sec.
iter 78380 || Loss: 1.4533 || timer: 0.1009 sec.
iter 78390 || Loss: 0.8880 || timer: 0.0898 sec.
iter 78400 || Loss: 0.9386 || timer: 0.0899 sec.
iter 78410 || Loss: 1.2132 || timer: 0.1026 sec.
iter 78420 || Loss: 1.6373 || timer: 0.0817 sec.
iter 78430 || Loss: 1.3084 || timer: 0.0238 sec.
iter 78440 || Loss: 0.5064 || timer: 0.0930 sec.
iter 78450 || Loss: 1.1708 || timer: 0.0837 sec.
iter 78460 || Loss: 1.0761 || timer: 0.0820 sec.
iter 78470 || Loss: 0.9980 || timer: 0.1021 sec.
iter 78480 || Loss: 1.2079 || timer: 0.0904 sec.
iter 78490 || Loss: 0.9956 || timer: 0.1124 sec.
iter 78500 || Loss: 0.9957 || timer: 0.0830 sec.
iter 78510 || Loss: 0.9770 || timer: 0.0888 sec.
iter 78520 || Loss: 0.8716 || timer: 0.0809 sec.
iter 78530 || Loss: 0.9702 || timer: 0.0997 sec.
iter 78540 || Loss: 0.8697 || timer: 0.0877 sec.
iter 78550 || Loss: 0.9320 || timer: 0.0957 sec.
iter 78560 || Loss: 1.2093 || timer: 0.0928 sec.
iter 78570 || Loss: 1.2029 || timer: 0.0894 sec.
iter 78580 || Loss: 1.0712 || timer: 0.0881 sec.
iter 78590 || Loss: 1.0309 || timer: 0.0933 sec.
iter 78600 || Loss: 0.8672 || timer: 0.1018 sec.
iter 78610 || Loss: 1.3861 || timer: 0.1038 sec.
iter 78620 || Loss: 0.7260 || timer: 0.0860 sec.
iter 78630 || Loss: 1.1835 || timer: 0.0887 sec.
iter 78640 || Loss: 0.7076 || timer: 0.0898 sec.
iter 78650 || Loss: 0.8215 || timer: 0.0897 sec.
iter 78660 || Loss: 1.0025 || timer: 0.0804 sec.
iter 78670 || Loss: 1.3205 || timer: 0.0971 sec.
iter 78680 || Loss: 1.0271 || timer: 0.0901 sec.
iter 78690 || Loss: 1.0119 || timer: 0.1251 sec.
iter 78700 || Loss: 0.9741 || timer: 0.0906 sec.
iter 78710 || Loss: 1.1772 || timer: 0.0805 sec.
iter 78720 || Loss: 0.9952 || timer: 0.1062 sec.
iter 78730 || Loss: 0.9178 || timer: 0.0808 sec.
iter 78740 || Loss: 1.3417 || timer: 0.0815 sec.
iter 78750 || Loss: 0.9743 || timer: 0.0815 sec.
iter 78760 || Loss: 1.1679 || timer: 0.0229 sec.
iter 78770 || Loss: 0.9423 || timer: 0.0903 sec.
iter 78780 || Loss: 0.9367 || timer: 0.0815 sec.
iter 78790 || Loss: 0.9099 || timer: 0.0828 sec.
iter 78800 || Loss: 0.9171 || timer: 0.0875 sec.
iter 78810 || Loss: 0.9377 || timer: 0.1004 sec.
iter 78820 || Loss: 0.9103 || timer: 0.0940 sec.
iter 78830 || Loss: 0.8009 || timer: 0.0809 sec.
iter 78840 || Loss: 1.1567 || timer: 0.0962 sec.
iter 78850 || Loss: 1.1065 || timer: 0.1065 sec.
iter 78860 || Loss: 0.9113 || timer: 0.1068 sec.
iter 78870 || Loss: 1.0059 || timer: 0.0858 sec.
iter 78880 || Loss: 1.0101 || timer: 0.0895 sec.
iter 78890 || Loss: 0.9505 || timer: 0.0839 sec.
iter 78900 || Loss: 0.9248 || timer: 0.0888 sec.
iter 78910 || Loss: 1.3978 || timer: 0.1058 sec.
iter 78920 || Loss: 1.2214 || timer: 0.0806 sec.
iter 78930 || Loss: 1.1028 || timer: 0.0901 sec.
iter 78940 || Loss: 1.1384 || timer: 0.0827 sec.
iter 78950 || Loss: 1.2611 || timer: 0.0914 sec.
iter 78960 || Loss: 1.1456 || timer: 0.0850 sec.
iter 78970 || Loss: 0.9444 || timer: 0.0846 sec.
iter 78980 || Loss: 0.9955 || timer: 0.1044 sec.
iter 78990 || Loss: 0.9509 || timer: 0.0861 sec.
iter 79000 || Loss: 0.9332 || timer: 0.0875 sec.
iter 79010 || Loss: 1.1083 || timer: 0.0875 sec.
iter 79020 || Loss: 0.6941 || timer: 0.0923 sec.
iter 79030 || Loss: 1.1792 || timer: 0.0872 sec.
iter 79040 || Loss: 1.1195 || timer: 0.0919 sec.
iter 79050 || Loss: 0.8788 || timer: 0.0866 sec.
iter 79060 || Loss: 1.4006 || timer: 0.0873 sec.
iter 79070 || Loss: 1.1945 || timer: 0.0810 sec.
iter 79080 || Loss: 1.0981 || timer: 0.0895 sec.
iter 79090 || Loss: 1.0219 || timer: 0.0223 sec.
iter 79100 || Loss: 0.7044 || timer: 0.0806 sec.
iter 79110 || Loss: 0.8765 || timer: 0.0931 sec.
iter 79120 || Loss: 1.0996 || timer: 0.0810 sec.
iter 79130 || Loss: 0.9564 || timer: 0.0937 sec.
iter 79140 || Loss: 0.6351 || timer: 0.1089 sec.
iter 79150 || Loss: 1.3739 || timer: 0.0907 sec.
iter 79160 || Loss: 0.9634 || timer: 0.0828 sec.
iter 79170 || Loss: 1.0708 || timer: 0.0818 sec.
iter 79180 || Loss: 1.0323 || timer: 0.0954 sec.
iter 79190 || Loss: 1.0114 || timer: 0.1008 sec.
iter 79200 || Loss: 1.0874 || timer: 0.0807 sec.
iter 79210 || Loss: 1.0486 || timer: 0.0879 sec.
iter 79220 || Loss: 0.8665 || timer: 0.0826 sec.
iter 79230 || Loss: 1.0260 || timer: 0.0988 sec.
iter 79240 || Loss: 1.3200 || timer: 0.0924 sec.
iter 79250 || Loss: 1.2410 || timer: 0.0959 sec.
iter 79260 || Loss: 0.9176 || timer: 0.0916 sec.
iter 79270 || Loss: 1.1933 || timer: 0.0827 sec.
iter 79280 || Loss: 0.9545 || timer: 0.0903 sec.
iter 79290 || Loss: 0.9568 || timer: 0.0820 sec.
iter 79300 || Loss: 1.0817 || timer: 0.0855 sec.
iter 79310 || Loss: 1.3785 || timer: 0.0876 sec.
iter 79320 || Loss: 0.9997 || timer: 0.0822 sec.
iter 79330 || Loss: 1.0778 || timer: 0.0873 sec.
iter 79340 || Loss: 1.1468 || timer: 0.0891 sec.
iter 79350 || Loss: 1.3507 || timer: 0.1120 sec.
iter 79360 || Loss: 1.2416 || timer: 0.0906 sec.
iter 79370 || Loss: 1.0202 || timer: 0.0821 sec.
iter 79380 || Loss: 1.1212 || timer: 0.0883 sec.
iter 79390 || Loss: 1.2311 || timer: 0.0839 sec.
iter 79400 || Loss: 1.1089 || timer: 0.0894 sec.
iter 79410 || Loss: 1.3445 || timer: 0.0880 sec.
iter 79420 || Loss: 1.0161 || timer: 0.0254 sec.
iter 79430 || Loss: 0.8565 || timer: 0.0917 sec.
iter 79440 || Loss: 1.0373 || timer: 0.0995 sec.
iter 79450 || Loss: 0.9757 || timer: 0.0898 sec.
iter 79460 || Loss: 0.8921 || timer: 0.1039 sec.
iter 79470 || Loss: 1.1645 || timer: 0.0874 sec.
iter 79480 || Loss: 1.2922 || timer: 0.0894 sec.
iter 79490 || Loss: 0.9855 || timer: 0.0903 sec.
iter 79500 || Loss: 0.8239 || timer: 0.0883 sec.
iter 79510 || Loss: 1.2585 || timer: 0.1463 sec.
iter 79520 || Loss: 1.3670 || timer: 0.0937 sec.
iter 79530 || Loss: 0.8526 || timer: 0.0910 sec.
iter 79540 || Loss: 1.3650 || timer: 0.0820 sec.
iter 79550 || Loss: 0.6894 || timer: 0.0859 sec.
iter 79560 || Loss: 0.9809 || timer: 0.0905 sec.
iter 79570 || Loss: 1.1596 || timer: 0.0814 sec.
iter 79580 || Loss: 1.2241 || timer: 0.0836 sec.
iter 79590 || Loss: 1.1849 || timer: 0.0818 sec.
iter 79600 || Loss: 0.9811 || timer: 0.1041 sec.
iter 79610 || Loss: 0.9614 || timer: 0.0807 sec.
iter 79620 || Loss: 1.2152 || timer: 0.0750 sec.
iter 79630 || Loss: 1.1014 || timer: 0.0748 sec.
iter 79640 || Loss: 0.9711 || timer: 0.0757 sec.
iter 79650 || Loss: 1.1310 || timer: 0.0851 sec.
iter 79660 || Loss: 1.0464 || timer: 0.0802 sec.
iter 79670 || Loss: 1.2317 || timer: 0.1017 sec.
iter 79680 || Loss: 0.9308 || timer: 0.0893 sec.
iter 79690 || Loss: 1.0150 || timer: 0.0812 sec.
iter 79700 || Loss: 0.9983 || timer: 0.0878 sec.
iter 79710 || Loss: 1.0039 || timer: 0.1153 sec.
iter 79720 || Loss: 1.2999 || timer: 0.0868 sec.
iter 79730 || Loss: 1.1095 || timer: 0.0813 sec.
iter 79740 || Loss: 1.3930 || timer: 0.0818 sec.
iter 79750 || Loss: 1.1638 || timer: 0.0263 sec.
iter 79760 || Loss: 0.9274 || timer: 0.0807 sec.
iter 79770 || Loss: 0.9113 || timer: 0.0876 sec.
iter 79780 || Loss: 0.9516 || timer: 0.0899 sec.
iter 79790 || Loss: 0.9427 || timer: 0.0889 sec.
iter 79800 || Loss: 1.2393 || timer: 0.0908 sec.
iter 79810 || Loss: 0.9451 || timer: 0.0809 sec.
iter 79820 || Loss: 0.8087 || timer: 0.0810 sec.
iter 79830 || Loss: 1.2575 || timer: 0.0919 sec.
iter 79840 || Loss: 1.1848 || timer: 0.0890 sec.
iter 79850 || Loss: 1.3539 || timer: 0.1483 sec.
iter 79860 || Loss: 0.7052 || timer: 0.0889 sec.
iter 79870 || Loss: 0.8940 || timer: 0.0851 sec.
iter 79880 || Loss: 1.0163 || timer: 0.0792 sec.
iter 79890 || Loss: 0.6607 || timer: 0.0817 sec.
iter 79900 || Loss: 1.2492 || timer: 0.0924 sec.
iter 79910 || Loss: 1.1605 || timer: 0.0880 sec.
iter 79920 || Loss: 0.6872 || timer: 0.0886 sec.
iter 79930 || Loss: 0.9180 || timer: 0.1049 sec.
iter 79940 || Loss: 1.2505 || timer: 0.0896 sec.
iter 79950 || Loss: 1.2549 || timer: 0.1083 sec.
iter 79960 || Loss: 1.3171 || timer: 0.0868 sec.
iter 79970 || Loss: 1.0961 || timer: 0.0815 sec.
iter 79980 || Loss: 1.0414 || timer: 0.0910 sec.
iter 79990 || Loss: 0.8908 || timer: 0.0999 sec.
iter 80000 || Loss: 1.1759 || Saving state, iter: 80000
timer: 0.0929 sec.
iter 80010 || Loss: 1.1686 || timer: 0.0912 sec.
iter 80020 || Loss: 0.9960 || timer: 0.0897 sec.
iter 80030 || Loss: 1.3853 || timer: 0.0876 sec.
iter 80040 || Loss: 1.0139 || timer: 0.0956 sec.
iter 80050 || Loss: 1.3322 || timer: 0.0876 sec.
iter 80060 || Loss: 1.3205 || timer: 0.0879 sec.
iter 80070 || Loss: 1.3647 || timer: 0.0894 sec.
iter 80080 || Loss: 1.0749 || timer: 0.0162 sec.
iter 80090 || Loss: 1.1725 || timer: 0.0810 sec.
iter 80100 || Loss: 1.1321 || timer: 0.0815 sec.
iter 80110 || Loss: 1.0401 || timer: 0.0814 sec.
iter 80120 || Loss: 1.2296 || timer: 0.0883 sec.
iter 80130 || Loss: 1.3799 || timer: 0.0872 sec.
iter 80140 || Loss: 1.2430 || timer: 0.0898 sec.
iter 80150 || Loss: 1.3567 || timer: 0.0823 sec.
iter 80160 || Loss: 1.2145 || timer: 0.0905 sec.
iter 80170 || Loss: 1.0199 || timer: 0.0824 sec.
iter 80180 || Loss: 1.4108 || timer: 0.0999 sec.
iter 80190 || Loss: 1.4961 || timer: 0.0979 sec.
iter 80200 || Loss: 1.3213 || timer: 0.0881 sec.
iter 80210 || Loss: 0.9527 || timer: 0.0906 sec.
iter 80220 || Loss: 1.0326 || timer: 0.1186 sec.
iter 80230 || Loss: 1.1121 || timer: 0.0820 sec.
iter 80240 || Loss: 1.2148 || timer: 0.0870 sec.
iter 80250 || Loss: 0.9519 || timer: 0.0861 sec.
iter 80260 || Loss: 1.2938 || timer: 0.0877 sec.
iter 80270 || Loss: 1.2530 || timer: 0.0821 sec.
iter 80280 || Loss: 0.8043 || timer: 0.0836 sec.
iter 80290 || Loss: 0.8493 || timer: 0.0821 sec.
iter 80300 || Loss: 0.9826 || timer: 0.0942 sec.
iter 80310 || Loss: 0.9647 || timer: 0.0878 sec.
iter 80320 || Loss: 0.9684 || timer: 0.0878 sec.
iter 80330 || Loss: 1.0309 || timer: 0.0884 sec.
iter 80340 || Loss: 1.1219 || timer: 0.0906 sec.
iter 80350 || Loss: 1.0360 || timer: 0.0798 sec.
iter 80360 || Loss: 0.9113 || timer: 0.0814 sec.
iter 80370 || Loss: 1.1909 || timer: 0.0849 sec.
iter 80380 || Loss: 1.1365 || timer: 0.0904 sec.
iter 80390 || Loss: 1.0936 || timer: 0.0854 sec.
iter 80400 || Loss: 1.0079 || timer: 0.0825 sec.
iter 80410 || Loss: 0.8738 || timer: 0.0223 sec.
iter 80420 || Loss: 2.1824 || timer: 0.0806 sec.
iter 80430 || Loss: 1.0644 || timer: 0.0919 sec.
iter 80440 || Loss: 0.9481 || timer: 0.0980 sec.
iter 80450 || Loss: 1.0293 || timer: 0.0854 sec.
iter 80460 || Loss: 1.7379 || timer: 0.0930 sec.
iter 80470 || Loss: 1.1321 || timer: 0.0884 sec.
iter 80480 || Loss: 1.2846 || timer: 0.1129 sec.
iter 80490 || Loss: 0.8974 || timer: 0.0849 sec.
iter 80500 || Loss: 1.2400 || timer: 0.1072 sec.
iter 80510 || Loss: 1.7447 || timer: 0.1163 sec.
iter 80520 || Loss: 1.0554 || timer: 0.0808 sec.
iter 80530 || Loss: 1.0451 || timer: 0.0847 sec.
iter 80540 || Loss: 1.3689 || timer: 0.0815 sec.
iter 80550 || Loss: 0.9905 || timer: 0.1079 sec.
iter 80560 || Loss: 0.9607 || timer: 0.0893 sec.
iter 80570 || Loss: 0.9094 || timer: 0.0994 sec.
iter 80580 || Loss: 1.1830 || timer: 0.1283 sec.
iter 80590 || Loss: 1.3188 || timer: 0.0832 sec.
iter 80600 || Loss: 0.8473 || timer: 0.0821 sec.
iter 80610 || Loss: 1.2453 || timer: 0.0915 sec.
iter 80620 || Loss: 1.0865 || timer: 0.0889 sec.
iter 80630 || Loss: 0.9183 || timer: 0.0810 sec.
iter 80640 || Loss: 0.9887 || timer: 0.0849 sec.
iter 80650 || Loss: 1.1382 || timer: 0.0880 sec.
iter 80660 || Loss: 0.9140 || timer: 0.1097 sec.
iter 80670 || Loss: 1.2838 || timer: 0.0823 sec.
iter 80680 || Loss: 0.8836 || timer: 0.1040 sec.
iter 80690 || Loss: 1.9277 || timer: 0.0914 sec.
iter 80700 || Loss: 1.3195 || timer: 0.0840 sec.
iter 80710 || Loss: 1.2296 || timer: 0.0864 sec.
iter 80720 || Loss: 1.3819 || timer: 0.0927 sec.
iter 80730 || Loss: 1.0892 || timer: 0.0813 sec.
iter 80740 || Loss: 0.9799 || timer: 0.0250 sec.
iter 80750 || Loss: 0.8636 || timer: 0.1133 sec.
iter 80760 || Loss: 1.6420 || timer: 0.0952 sec.
iter 80770 || Loss: 0.7504 || timer: 0.0920 sec.
iter 80780 || Loss: 1.0724 || timer: 0.0812 sec.
iter 80790 || Loss: 1.2634 || timer: 0.0829 sec.
iter 80800 || Loss: 0.8931 || timer: 0.0851 sec.
iter 80810 || Loss: 1.1368 || timer: 0.0886 sec.
iter 80820 || Loss: 1.2145 || timer: 0.0926 sec.
iter 80830 || Loss: 0.9161 || timer: 0.0857 sec.
iter 80840 || Loss: 1.3639 || timer: 0.1208 sec.
iter 80850 || Loss: 1.4024 || timer: 0.0900 sec.
iter 80860 || Loss: 1.3747 || timer: 0.0903 sec.
iter 80870 || Loss: 1.1271 || timer: 0.0916 sec.
iter 80880 || Loss: 1.2754 || timer: 0.0906 sec.
iter 80890 || Loss: 1.0920 || timer: 0.0887 sec.
iter 80900 || Loss: 1.2333 || timer: 0.0925 sec.
iter 80910 || Loss: 1.0245 || timer: 0.1010 sec.
iter 80920 || Loss: 1.1047 || timer: 0.0810 sec.
iter 80930 || Loss: 1.0704 || timer: 0.0819 sec.
iter 80940 || Loss: 1.4066 || timer: 0.0811 sec.
iter 80950 || Loss: 0.9344 || timer: 0.0905 sec.
iter 80960 || Loss: 1.0716 || timer: 0.0806 sec.
iter 80970 || Loss: 0.6945 || timer: 0.0816 sec.
iter 80980 || Loss: 0.9352 || timer: 0.1106 sec.
iter 80990 || Loss: 1.2488 || timer: 0.0941 sec.
iter 81000 || Loss: 1.3478 || timer: 0.0890 sec.
iter 81010 || Loss: 0.9223 || timer: 0.0859 sec.
iter 81020 || Loss: 1.2167 || timer: 0.1084 sec.
iter 81030 || Loss: 0.8841 || timer: 0.0812 sec.
iter 81040 || Loss: 1.4382 || timer: 0.0880 sec.
iter 81050 || Loss: 0.8220 || timer: 0.0880 sec.
iter 81060 || Loss: 1.2983 || timer: 0.0811 sec.
iter 81070 || Loss: 1.1383 || timer: 0.0172 sec.
iter 81080 || Loss: 0.4539 || timer: 0.0905 sec.
iter 81090 || Loss: 1.2332 || timer: 0.0902 sec.
iter 81100 || Loss: 1.0134 || timer: 0.0833 sec.
iter 81110 || Loss: 0.9038 || timer: 0.1050 sec.
iter 81120 || Loss: 0.9488 || timer: 0.0881 sec.
iter 81130 || Loss: 1.2892 || timer: 0.0879 sec.
iter 81140 || Loss: 1.0543 || timer: 0.0821 sec.
iter 81150 || Loss: 1.0483 || timer: 0.0817 sec.
iter 81160 || Loss: 1.0555 || timer: 0.0920 sec.
iter 81170 || Loss: 1.4521 || timer: 0.0938 sec.
iter 81180 || Loss: 1.1054 || timer: 0.0828 sec.
iter 81190 || Loss: 1.1442 || timer: 0.1044 sec.
iter 81200 || Loss: 1.1756 || timer: 0.0882 sec.
iter 81210 || Loss: 0.8558 || timer: 0.1098 sec.
iter 81220 || Loss: 1.2235 || timer: 0.0909 sec.
iter 81230 || Loss: 1.3460 || timer: 0.0981 sec.
iter 81240 || Loss: 0.9682 || timer: 0.0904 sec.
iter 81250 || Loss: 1.0582 || timer: 0.0970 sec.
iter 81260 || Loss: 0.8728 || timer: 0.0812 sec.
iter 81270 || Loss: 1.2512 || timer: 0.0808 sec.
iter 81280 || Loss: 0.9184 || timer: 0.1010 sec.
iter 81290 || Loss: 1.4614 || timer: 0.0906 sec.
iter 81300 || Loss: 0.8184 || timer: 0.0850 sec.
iter 81310 || Loss: 0.8779 || timer: 0.0894 sec.
iter 81320 || Loss: 1.1900 || timer: 0.0963 sec.
iter 81330 || Loss: 0.8310 || timer: 0.0820 sec.
iter 81340 || Loss: 1.1099 || timer: 0.1023 sec.
iter 81350 || Loss: 1.2602 || timer: 0.0870 sec.
iter 81360 || Loss: 1.0613 || timer: 0.0805 sec.
iter 81370 || Loss: 1.5327 || timer: 0.0818 sec.
iter 81380 || Loss: 1.3135 || timer: 0.0905 sec.
iter 81390 || Loss: 1.2908 || timer: 0.1063 sec.
iter 81400 || Loss: 1.2795 || timer: 0.0171 sec.
iter 81410 || Loss: 0.1539 || timer: 0.0811 sec.
iter 81420 || Loss: 1.0239 || timer: 0.0855 sec.
iter 81430 || Loss: 1.0079 || timer: 0.1003 sec.
iter 81440 || Loss: 1.0084 || timer: 0.0904 sec.
iter 81450 || Loss: 0.6748 || timer: 0.0968 sec.
iter 81460 || Loss: 0.9376 || timer: 0.0827 sec.
iter 81470 || Loss: 1.2534 || timer: 0.0894 sec.
iter 81480 || Loss: 0.5601 || timer: 0.0887 sec.
iter 81490 || Loss: 1.0093 || timer: 0.0930 sec.
iter 81500 || Loss: 1.2116 || timer: 0.1071 sec.
iter 81510 || Loss: 0.8826 || timer: 0.0886 sec.
iter 81520 || Loss: 1.0738 || timer: 0.0804 sec.
iter 81530 || Loss: 0.9958 || timer: 0.0889 sec.
iter 81540 || Loss: 1.1752 || timer: 0.0881 sec.
iter 81550 || Loss: 0.9248 || timer: 0.0822 sec.
iter 81560 || Loss: 1.0468 || timer: 0.1091 sec.
iter 81570 || Loss: 1.1181 || timer: 0.1045 sec.
iter 81580 || Loss: 0.8255 || timer: 0.0882 sec.
iter 81590 || Loss: 1.1010 || timer: 0.0813 sec.
iter 81600 || Loss: 1.3613 || timer: 0.0810 sec.
iter 81610 || Loss: 1.7161 || timer: 0.0975 sec.
iter 81620 || Loss: 1.4142 || timer: 0.0821 sec.
iter 81630 || Loss: 0.8567 || timer: 0.0878 sec.
iter 81640 || Loss: 1.0142 || timer: 0.0811 sec.
iter 81650 || Loss: 0.9934 || timer: 0.0824 sec.
iter 81660 || Loss: 1.0444 || timer: 0.0811 sec.
iter 81670 || Loss: 1.0991 || timer: 0.0990 sec.
iter 81680 || Loss: 1.1217 || timer: 0.0887 sec.
iter 81690 || Loss: 1.3252 || timer: 0.0895 sec.
iter 81700 || Loss: 1.0738 || timer: 0.0811 sec.
iter 81710 || Loss: 1.4140 || timer: 0.0865 sec.
iter 81720 || Loss: 1.2049 || timer: 0.0888 sec.
iter 81730 || Loss: 0.9610 || timer: 0.0157 sec.
iter 81740 || Loss: 1.6871 || timer: 0.0896 sec.
iter 81750 || Loss: 1.2472 || timer: 0.1051 sec.
iter 81760 || Loss: 1.1234 || timer: 0.1289 sec.
iter 81770 || Loss: 1.5829 || timer: 0.1012 sec.
iter 81780 || Loss: 1.2903 || timer: 0.0813 sec.
iter 81790 || Loss: 0.8549 || timer: 0.0824 sec.
iter 81800 || Loss: 1.0256 || timer: 0.0814 sec.
iter 81810 || Loss: 1.2594 || timer: 0.0733 sec.
iter 81820 || Loss: 1.0508 || timer: 0.0854 sec.
iter 81830 || Loss: 0.9242 || timer: 0.0998 sec.
iter 81840 || Loss: 0.9111 || timer: 0.0882 sec.
iter 81850 || Loss: 1.3101 || timer: 0.1035 sec.
iter 81860 || Loss: 1.0764 || timer: 0.0943 sec.
iter 81870 || Loss: 1.1198 || timer: 0.0820 sec.
iter 81880 || Loss: 1.0228 || timer: 0.0813 sec.
iter 81890 || Loss: 1.4297 || timer: 0.0822 sec.
iter 81900 || Loss: 1.3522 || timer: 0.1065 sec.
iter 81910 || Loss: 1.1444 || timer: 0.0880 sec.
iter 81920 || Loss: 0.7988 || timer: 0.0909 sec.
iter 81930 || Loss: 1.3437 || timer: 0.0824 sec.
iter 81940 || Loss: 1.3432 || timer: 0.0811 sec.
iter 81950 || Loss: 1.5628 || timer: 0.0809 sec.
iter 81960 || Loss: 1.1107 || timer: 0.0855 sec.
iter 81970 || Loss: 0.8980 || timer: 0.0997 sec.
iter 81980 || Loss: 0.8032 || timer: 0.0810 sec.
iter 81990 || Loss: 1.0444 || timer: 0.1277 sec.
iter 82000 || Loss: 1.0916 || timer: 0.0916 sec.
iter 82010 || Loss: 1.0710 || timer: 0.1052 sec.
iter 82020 || Loss: 1.0225 || timer: 0.0880 sec.
iter 82030 || Loss: 1.1726 || timer: 0.0868 sec.
iter 82040 || Loss: 0.9139 || timer: 0.1045 sec.
iter 82050 || Loss: 1.1400 || timer: 0.0810 sec.
iter 82060 || Loss: 1.1061 || timer: 0.0161 sec.
iter 82070 || Loss: 1.0682 || timer: 0.0822 sec.
iter 82080 || Loss: 0.8971 || timer: 0.0883 sec.
iter 82090 || Loss: 0.7964 || timer: 0.0826 sec.
iter 82100 || Loss: 1.2236 || timer: 0.0906 sec.
iter 82110 || Loss: 1.1912 || timer: 0.0893 sec.
iter 82120 || Loss: 0.9715 || timer: 0.0844 sec.
iter 82130 || Loss: 1.5077 || timer: 0.0878 sec.
iter 82140 || Loss: 0.9997 || timer: 0.0923 sec.
iter 82150 || Loss: 1.3517 || timer: 0.0886 sec.
iter 82160 || Loss: 1.0471 || timer: 0.1015 sec.
iter 82170 || Loss: 0.9665 || timer: 0.0878 sec.
iter 82180 || Loss: 1.1284 || timer: 0.0914 sec.
iter 82190 || Loss: 1.0495 || timer: 0.0941 sec.
iter 82200 || Loss: 1.1632 || timer: 0.0808 sec.
iter 82210 || Loss: 1.2365 || timer: 0.0813 sec.
iter 82220 || Loss: 1.4834 || timer: 0.0814 sec.
iter 82230 || Loss: 1.1422 || timer: 0.0836 sec.
iter 82240 || Loss: 1.1050 || timer: 0.0918 sec.
iter 82250 || Loss: 1.0138 || timer: 0.0920 sec.
iter 82260 || Loss: 0.8292 || timer: 0.0862 sec.
iter 82270 || Loss: 0.8034 || timer: 0.0794 sec.
iter 82280 || Loss: 1.5366 || timer: 0.0888 sec.
iter 82290 || Loss: 1.2009 || timer: 0.0815 sec.
iter 82300 || Loss: 0.7857 || timer: 0.0924 sec.
iter 82310 || Loss: 0.9782 || timer: 0.0990 sec.
iter 82320 || Loss: 0.8246 || timer: 0.0884 sec.
iter 82330 || Loss: 0.8071 || timer: 0.0818 sec.
iter 82340 || Loss: 0.9422 || timer: 0.0891 sec.
iter 82350 || Loss: 0.9993 || timer: 0.0835 sec.
iter 82360 || Loss: 1.1686 || timer: 0.0891 sec.
iter 82370 || Loss: 0.9447 || timer: 0.0903 sec.
iter 82380 || Loss: 0.9554 || timer: 0.0837 sec.
iter 82390 || Loss: 0.8173 || timer: 0.0166 sec.
iter 82400 || Loss: 0.4889 || timer: 0.0816 sec.
iter 82410 || Loss: 1.1009 || timer: 0.1158 sec.
iter 82420 || Loss: 1.0264 || timer: 0.0891 sec.
iter 82430 || Loss: 1.0089 || timer: 0.0898 sec.
iter 82440 || Loss: 0.9832 || timer: 0.0887 sec.
iter 82450 || Loss: 0.8152 || timer: 0.0824 sec.
iter 82460 || Loss: 1.0034 || timer: 0.1208 sec.
iter 82470 || Loss: 1.5019 || timer: 0.0894 sec.
iter 82480 || Loss: 0.8208 || timer: 0.0908 sec.
iter 82490 || Loss: 0.8023 || timer: 0.0935 sec.
iter 82500 || Loss: 0.7240 || timer: 0.0888 sec.
iter 82510 || Loss: 0.8606 || timer: 0.0881 sec.
iter 82520 || Loss: 0.9025 || timer: 0.0897 sec.
iter 82530 || Loss: 1.0235 || timer: 0.1081 sec.
iter 82540 || Loss: 0.9568 || timer: 0.0909 sec.
iter 82550 || Loss: 0.8305 || timer: 0.0832 sec.
iter 82560 || Loss: 1.1078 || timer: 0.0825 sec.
iter 82570 || Loss: 1.2681 || timer: 0.0890 sec.
iter 82580 || Loss: 1.0495 || timer: 0.0925 sec.
iter 82590 || Loss: 1.1555 || timer: 0.0978 sec.
iter 82600 || Loss: 1.1166 || timer: 0.0805 sec.
iter 82610 || Loss: 1.1710 || timer: 0.0823 sec.
iter 82620 || Loss: 1.0967 || timer: 0.0885 sec.
iter 82630 || Loss: 0.9403 || timer: 0.0996 sec.
iter 82640 || Loss: 1.0699 || timer: 0.0808 sec.
iter 82650 || Loss: 1.3057 || timer: 0.0888 sec.
iter 82660 || Loss: 1.2469 || timer: 0.0906 sec.
iter 82670 || Loss: 1.4334 || timer: 0.0905 sec.
iter 82680 || Loss: 1.1877 || timer: 0.0826 sec.
iter 82690 || Loss: 1.3158 || timer: 0.0825 sec.
iter 82700 || Loss: 0.8562 || timer: 0.1162 sec.
iter 82710 || Loss: 1.1162 || timer: 0.1087 sec.
iter 82720 || Loss: 1.1799 || timer: 0.0169 sec.
iter 82730 || Loss: 0.7018 || timer: 0.0895 sec.
iter 82740 || Loss: 1.3591 || timer: 0.0809 sec.
iter 82750 || Loss: 0.8511 || timer: 0.0901 sec.
iter 82760 || Loss: 0.7466 || timer: 0.0896 sec.
iter 82770 || Loss: 1.2930 || timer: 0.0820 sec.
iter 82780 || Loss: 1.2126 || timer: 0.0825 sec.
iter 82790 || Loss: 1.1823 || timer: 0.0947 sec.
iter 82800 || Loss: 1.2714 || timer: 0.0956 sec.
iter 82810 || Loss: 1.1642 || timer: 0.0822 sec.
iter 82820 || Loss: 1.3530 || timer: 0.0970 sec.
iter 82830 || Loss: 1.0093 || timer: 0.1023 sec.
iter 82840 || Loss: 1.2602 || timer: 0.1180 sec.
iter 82850 || Loss: 0.8653 || timer: 0.0861 sec.
iter 82860 || Loss: 0.9404 || timer: 0.0883 sec.
iter 82870 || Loss: 0.8806 || timer: 0.0878 sec.
iter 82880 || Loss: 1.2530 || timer: 0.0866 sec.
iter 82890 || Loss: 1.1599 || timer: 0.0957 sec.
iter 82900 || Loss: 1.0518 || timer: 0.1027 sec.
iter 82910 || Loss: 1.1050 || timer: 0.1040 sec.
iter 82920 || Loss: 1.1261 || timer: 0.0813 sec.
iter 82930 || Loss: 0.9335 || timer: 0.0873 sec.
iter 82940 || Loss: 0.8996 || timer: 0.0816 sec.
iter 82950 || Loss: 1.1462 || timer: 0.0813 sec.
iter 82960 || Loss: 0.9556 || timer: 0.0805 sec.
iter 82970 || Loss: 1.0197 || timer: 0.0918 sec.
iter 82980 || Loss: 1.0480 || timer: 0.0811 sec.
iter 82990 || Loss: 1.1107 || timer: 0.0991 sec.
iter 83000 || Loss: 1.1486 || timer: 0.0866 sec.
iter 83010 || Loss: 1.5344 || timer: 0.0848 sec.
iter 83020 || Loss: 1.0424 || timer: 0.0874 sec.
iter 83030 || Loss: 1.0284 || timer: 0.0866 sec.
iter 83040 || Loss: 1.2289 || timer: 0.0918 sec.
iter 83050 || Loss: 1.0206 || timer: 0.0217 sec.
iter 83060 || Loss: 0.9454 || timer: 0.0987 sec.
iter 83070 || Loss: 1.0581 || timer: 0.1096 sec.
iter 83080 || Loss: 0.8975 || timer: 0.0807 sec.
iter 83090 || Loss: 0.7828 || timer: 0.0834 sec.
iter 83100 || Loss: 0.9262 || timer: 0.0946 sec.
iter 83110 || Loss: 1.4690 || timer: 0.0884 sec.
iter 83120 || Loss: 0.9451 || timer: 0.0881 sec.
iter 83130 || Loss: 1.1659 || timer: 0.0833 sec.
iter 83140 || Loss: 1.1660 || timer: 0.0949 sec.
iter 83150 || Loss: 0.7662 || timer: 0.1416 sec.
iter 83160 || Loss: 0.9329 || timer: 0.0914 sec.
iter 83170 || Loss: 1.3342 || timer: 0.0809 sec.
iter 83180 || Loss: 1.1016 || timer: 0.0745 sec.
iter 83190 || Loss: 1.1569 || timer: 0.1029 sec.
iter 83200 || Loss: 1.2427 || timer: 0.0903 sec.
iter 83210 || Loss: 1.0788 || timer: 0.1032 sec.
iter 83220 || Loss: 1.3997 || timer: 0.1203 sec.
iter 83230 || Loss: 0.8499 || timer: 0.0889 sec.
iter 83240 || Loss: 1.2052 || timer: 0.1090 sec.
iter 83250 || Loss: 1.1869 || timer: 0.0902 sec.
iter 83260 || Loss: 1.3433 || timer: 0.0935 sec.
iter 83270 || Loss: 0.9773 || timer: 0.0896 sec.
iter 83280 || Loss: 1.2421 || timer: 0.0820 sec.
iter 83290 || Loss: 1.5477 || timer: 0.0907 sec.
iter 83300 || Loss: 0.9320 || timer: 0.0815 sec.
iter 83310 || Loss: 0.8940 || timer: 0.1208 sec.
iter 83320 || Loss: 1.0546 || timer: 0.1027 sec.
iter 83330 || Loss: 1.1253 || timer: 0.1002 sec.
iter 83340 || Loss: 1.0418 || timer: 0.0919 sec.
iter 83350 || Loss: 1.1268 || timer: 0.0905 sec.
iter 83360 || Loss: 0.8922 || timer: 0.0876 sec.
iter 83370 || Loss: 1.2589 || timer: 0.0930 sec.
iter 83380 || Loss: 0.9960 || timer: 0.0159 sec.
iter 83390 || Loss: 0.5057 || timer: 0.0921 sec.
iter 83400 || Loss: 1.3407 || timer: 0.0905 sec.
iter 83410 || Loss: 1.4162 || timer: 0.0883 sec.
iter 83420 || Loss: 0.8249 || timer: 0.0751 sec.
iter 83430 || Loss: 1.1923 || timer: 0.0844 sec.
iter 83440 || Loss: 0.9050 || timer: 0.0940 sec.
iter 83450 || Loss: 1.1402 || timer: 0.0818 sec.
iter 83460 || Loss: 1.0211 || timer: 0.0903 sec.
iter 83470 || Loss: 1.0079 || timer: 0.0863 sec.
iter 83480 || Loss: 1.0130 || timer: 0.1011 sec.
iter 83490 || Loss: 0.9724 || timer: 0.1134 sec.
iter 83500 || Loss: 1.0237 || timer: 0.1077 sec.
iter 83510 || Loss: 0.8800 || timer: 0.0921 sec.
iter 83520 || Loss: 1.0270 || timer: 0.0886 sec.
iter 83530 || Loss: 1.7264 || timer: 0.0845 sec.
iter 83540 || Loss: 1.0842 || timer: 0.0822 sec.
iter 83550 || Loss: 0.9022 || timer: 0.0899 sec.
iter 83560 || Loss: 1.0790 || timer: 0.0910 sec.
iter 83570 || Loss: 0.9421 || timer: 0.0837 sec.
iter 83580 || Loss: 1.0411 || timer: 0.0816 sec.
iter 83590 || Loss: 1.5810 || timer: 0.0809 sec.
iter 83600 || Loss: 0.8040 || timer: 0.0810 sec.
iter 83610 || Loss: 0.9647 || timer: 0.0813 sec.
iter 83620 || Loss: 1.2074 || timer: 0.0905 sec.
iter 83630 || Loss: 1.2890 || timer: 0.0909 sec.
iter 83640 || Loss: 1.3928 || timer: 0.0893 sec.
iter 83650 || Loss: 0.9727 || timer: 0.0802 sec.
iter 83660 || Loss: 1.0133 || timer: 0.0880 sec.
iter 83670 || Loss: 0.9933 || timer: 0.0822 sec.
iter 83680 || Loss: 0.9849 || timer: 0.0807 sec.
iter 83690 || Loss: 1.0091 || timer: 0.0857 sec.
iter 83700 || Loss: 0.9731 || timer: 0.0958 sec.
iter 83710 || Loss: 1.1562 || timer: 0.0326 sec.
iter 83720 || Loss: 0.5123 || timer: 0.1105 sec.
iter 83730 || Loss: 1.3914 || timer: 0.0833 sec.
iter 83740 || Loss: 0.7699 || timer: 0.0901 sec.
iter 83750 || Loss: 1.2379 || timer: 0.0805 sec.
iter 83760 || Loss: 1.2712 || timer: 0.0898 sec.
iter 83770 || Loss: 1.5538 || timer: 0.0875 sec.
iter 83780 || Loss: 1.3996 || timer: 0.0887 sec.
iter 83790 || Loss: 1.6299 || timer: 0.0815 sec.
iter 83800 || Loss: 1.1462 || timer: 0.0892 sec.
iter 83810 || Loss: 1.2322 || timer: 0.0946 sec.
iter 83820 || Loss: 0.9625 || timer: 0.1059 sec.
iter 83830 || Loss: 1.4044 || timer: 0.0875 sec.
iter 83840 || Loss: 0.9653 || timer: 0.0806 sec.
iter 83850 || Loss: 1.0114 || timer: 0.0814 sec.
iter 83860 || Loss: 1.1869 || timer: 0.1066 sec.
iter 83870 || Loss: 0.9129 || timer: 0.0807 sec.
iter 83880 || Loss: 0.9026 || timer: 0.0863 sec.
iter 83890 || Loss: 1.1051 || timer: 0.1054 sec.
iter 83900 || Loss: 1.0690 || timer: 0.0937 sec.
iter 83910 || Loss: 1.0947 || timer: 0.0805 sec.
iter 83920 || Loss: 0.9902 || timer: 0.0918 sec.
iter 83930 || Loss: 1.4409 || timer: 0.0881 sec.
iter 83940 || Loss: 1.1635 || timer: 0.0896 sec.
iter 83950 || Loss: 0.7079 || timer: 0.1036 sec.
iter 83960 || Loss: 1.7147 || timer: 0.0824 sec.
iter 83970 || Loss: 1.1476 || timer: 0.0825 sec.
iter 83980 || Loss: 1.1850 || timer: 0.0903 sec.
iter 83990 || Loss: 0.9531 || timer: 0.0946 sec.
iter 84000 || Loss: 1.3461 || timer: 0.0867 sec.
iter 84010 || Loss: 0.9521 || timer: 0.0899 sec.
iter 84020 || Loss: 1.2642 || timer: 0.0818 sec.
iter 84030 || Loss: 1.0803 || timer: 0.1015 sec.
iter 84040 || Loss: 0.9476 || timer: 0.0193 sec.
iter 84050 || Loss: 0.5477 || timer: 0.0838 sec.
iter 84060 || Loss: 0.7902 || timer: 0.0895 sec.
iter 84070 || Loss: 0.9560 || timer: 0.0844 sec.
iter 84080 || Loss: 1.0244 || timer: 0.0934 sec.
iter 84090 || Loss: 0.8340 || timer: 0.1022 sec.
iter 84100 || Loss: 1.0464 || timer: 0.0809 sec.
iter 84110 || Loss: 0.7940 || timer: 0.0882 sec.
iter 84120 || Loss: 0.9995 || timer: 0.0805 sec.
iter 84130 || Loss: 1.2113 || timer: 0.0812 sec.
iter 84140 || Loss: 0.9104 || timer: 0.1120 sec.
iter 84150 || Loss: 0.8152 || timer: 0.0881 sec.
iter 84160 || Loss: 0.9358 || timer: 0.0822 sec.
iter 84170 || Loss: 1.0002 || timer: 0.0883 sec.
iter 84180 || Loss: 0.9675 || timer: 0.1016 sec.
iter 84190 || Loss: 0.9957 || timer: 0.0909 sec.
iter 84200 || Loss: 0.8312 || timer: 0.1157 sec.
iter 84210 || Loss: 1.2036 || timer: 0.0932 sec.
iter 84220 || Loss: 1.0529 || timer: 0.0811 sec.
iter 84230 || Loss: 1.1318 || timer: 0.0904 sec.
iter 84240 || Loss: 0.9085 || timer: 0.0815 sec.
iter 84250 || Loss: 1.0854 || timer: 0.0824 sec.
iter 84260 || Loss: 0.8921 || timer: 0.1033 sec.
iter 84270 || Loss: 0.9474 || timer: 0.0853 sec.
iter 84280 || Loss: 1.2203 || timer: 0.0871 sec.
iter 84290 || Loss: 1.4971 || timer: 0.0973 sec.
iter 84300 || Loss: 0.9745 || timer: 0.0912 sec.
iter 84310 || Loss: 0.8711 || timer: 0.1291 sec.
iter 84320 || Loss: 1.3143 || timer: 0.0864 sec.
iter 84330 || Loss: 1.0340 || timer: 0.0889 sec.
iter 84340 || Loss: 0.9366 || timer: 0.0882 sec.
iter 84350 || Loss: 0.9717 || timer: 0.0852 sec.
iter 84360 || Loss: 1.1195 || timer: 0.0891 sec.
iter 84370 || Loss: 0.9632 || timer: 0.0270 sec.
iter 84380 || Loss: 2.3649 || timer: 0.0821 sec.
iter 84390 || Loss: 1.1106 || timer: 0.0824 sec.
iter 84400 || Loss: 1.1978 || timer: 0.0887 sec.
iter 84410 || Loss: 0.9242 || timer: 0.0916 sec.
iter 84420 || Loss: 0.9779 || timer: 0.0876 sec.
iter 84430 || Loss: 1.1543 || timer: 0.0992 sec.
iter 84440 || Loss: 1.2187 || timer: 0.0888 sec.
iter 84450 || Loss: 1.1085 || timer: 0.0875 sec.
iter 84460 || Loss: 1.1462 || timer: 0.1063 sec.
iter 84470 || Loss: 1.1623 || timer: 0.0951 sec.
iter 84480 || Loss: 1.0046 || timer: 0.0898 sec.
iter 84490 || Loss: 1.5083 || timer: 0.0891 sec.
iter 84500 || Loss: 1.1398 || timer: 0.0911 sec.
iter 84510 || Loss: 1.1715 || timer: 0.1073 sec.
iter 84520 || Loss: 1.1075 || timer: 0.0920 sec.
iter 84530 || Loss: 1.0760 || timer: 0.0850 sec.
iter 84540 || Loss: 1.2156 || timer: 0.0889 sec.
iter 84550 || Loss: 1.1100 || timer: 0.1121 sec.
iter 84560 || Loss: 1.0777 || timer: 0.0899 sec.
iter 84570 || Loss: 0.8877 || timer: 0.0804 sec.
iter 84580 || Loss: 1.0767 || timer: 0.0809 sec.
iter 84590 || Loss: 1.0194 || timer: 0.0898 sec.
iter 84600 || Loss: 0.9502 || timer: 0.0884 sec.
iter 84610 || Loss: 1.1396 || timer: 0.0966 sec.
iter 84620 || Loss: 1.4333 || timer: 0.0919 sec.
iter 84630 || Loss: 1.0459 || timer: 0.0809 sec.
iter 84640 || Loss: 1.3980 || timer: 0.0807 sec.
iter 84650 || Loss: 1.1357 || timer: 0.0891 sec.
iter 84660 || Loss: 1.2739 || timer: 0.0953 sec.
iter 84670 || Loss: 0.8779 || timer: 0.0869 sec.
iter 84680 || Loss: 1.2416 || timer: 0.0889 sec.
iter 84690 || Loss: 0.8953 || timer: 0.0816 sec.
iter 84700 || Loss: 1.1892 || timer: 0.0159 sec.
iter 84710 || Loss: 0.2579 || timer: 0.0885 sec.
iter 84720 || Loss: 0.9829 || timer: 0.0812 sec.
iter 84730 || Loss: 1.2443 || timer: 0.0892 sec.
iter 84740 || Loss: 1.0895 || timer: 0.0887 sec.
iter 84750 || Loss: 1.2197 || timer: 0.0806 sec.
iter 84760 || Loss: 1.2564 || timer: 0.0928 sec.
iter 84770 || Loss: 1.0617 || timer: 0.0904 sec.
iter 84780 || Loss: 1.1673 || timer: 0.0981 sec.
iter 84790 || Loss: 0.9620 || timer: 0.0996 sec.
iter 84800 || Loss: 1.5306 || timer: 0.1186 sec.
iter 84810 || Loss: 0.9341 || timer: 0.0873 sec.
iter 84820 || Loss: 1.0279 || timer: 0.0888 sec.
iter 84830 || Loss: 1.0712 || timer: 0.0906 sec.
iter 84840 || Loss: 0.9359 || timer: 0.0902 sec.
iter 84850 || Loss: 0.9025 || timer: 0.0913 sec.
iter 84860 || Loss: 0.9255 || timer: 0.0796 sec.
iter 84870 || Loss: 1.3176 || timer: 0.0914 sec.
iter 84880 || Loss: 1.0940 || timer: 0.0789 sec.
iter 84890 || Loss: 1.1485 || timer: 0.0811 sec.
iter 84900 || Loss: 1.1001 || timer: 0.0983 sec.
iter 84910 || Loss: 0.9229 || timer: 0.0919 sec.
iter 84920 || Loss: 0.8993 || timer: 0.0935 sec.
iter 84930 || Loss: 1.1636 || timer: 0.0814 sec.
iter 84940 || Loss: 0.9145 || timer: 0.0898 sec.
iter 84950 || Loss: 1.0478 || timer: 0.0818 sec.
iter 84960 || Loss: 0.9609 || timer: 0.0843 sec.
iter 84970 || Loss: 1.0731 || timer: 0.0958 sec.
iter 84980 || Loss: 1.0197 || timer: 0.0893 sec.
iter 84990 || Loss: 1.0599 || timer: 0.0817 sec.
iter 85000 || Loss: 1.1103 || Saving state, iter: 85000
timer: 0.0808 sec.
iter 85010 || Loss: 1.1170 || timer: 0.0867 sec.
iter 85020 || Loss: 1.2127 || timer: 0.0949 sec.
iter 85030 || Loss: 0.9040 || timer: 0.0250 sec.
iter 85040 || Loss: 1.3419 || timer: 0.0863 sec.
iter 85050 || Loss: 0.9848 || timer: 0.0807 sec.
iter 85060 || Loss: 1.2147 || timer: 0.0892 sec.
iter 85070 || Loss: 1.1878 || timer: 0.0816 sec.
iter 85080 || Loss: 0.9187 || timer: 0.0888 sec.
iter 85090 || Loss: 1.1822 || timer: 0.0924 sec.
iter 85100 || Loss: 1.2605 || timer: 0.0824 sec.
iter 85110 || Loss: 1.0445 || timer: 0.0839 sec.
iter 85120 || Loss: 0.8056 || timer: 0.0812 sec.
iter 85130 || Loss: 0.7727 || timer: 0.0946 sec.
iter 85140 || Loss: 1.0705 || timer: 0.0837 sec.
iter 85150 || Loss: 0.7852 || timer: 0.0974 sec.
iter 85160 || Loss: 0.9315 || timer: 0.0903 sec.
iter 85170 || Loss: 0.9913 || timer: 0.0881 sec.
iter 85180 || Loss: 1.2182 || timer: 0.0822 sec.
iter 85190 || Loss: 1.0723 || timer: 0.0822 sec.
iter 85200 || Loss: 1.1569 || timer: 0.0843 sec.
iter 85210 || Loss: 0.9554 || timer: 0.1024 sec.
iter 85220 || Loss: 0.9592 || timer: 0.0735 sec.
iter 85230 || Loss: 1.3528 || timer: 0.0923 sec.
iter 85240 || Loss: 1.0121 || timer: 0.0913 sec.
iter 85250 || Loss: 1.0393 || timer: 0.0880 sec.
iter 85260 || Loss: 1.1138 || timer: 0.0810 sec.
iter 85270 || Loss: 1.0642 || timer: 0.0882 sec.
iter 85280 || Loss: 1.2572 || timer: 0.0817 sec.
iter 85290 || Loss: 1.5250 || timer: 0.0984 sec.
iter 85300 || Loss: 1.5194 || timer: 0.0939 sec.
iter 85310 || Loss: 0.9582 || timer: 0.0820 sec.
iter 85320 || Loss: 1.1669 || timer: 0.0882 sec.
iter 85330 || Loss: 0.8376 || timer: 0.0890 sec.
iter 85340 || Loss: 1.0769 || timer: 0.0886 sec.
iter 85350 || Loss: 1.2314 || timer: 0.1022 sec.
iter 85360 || Loss: 1.1206 || timer: 0.0232 sec.
iter 85370 || Loss: 1.2609 || timer: 0.0883 sec.
iter 85380 || Loss: 1.0246 || timer: 0.0899 sec.
iter 85390 || Loss: 1.1250 || timer: 0.0899 sec.
iter 85400 || Loss: 0.9855 || timer: 0.0807 sec.
iter 85410 || Loss: 2.0471 || timer: 0.1234 sec.
iter 85420 || Loss: 1.4522 || timer: 0.0901 sec.
iter 85430 || Loss: 1.6234 || timer: 0.1038 sec.
iter 85440 || Loss: 1.7820 || timer: 0.1121 sec.
iter 85450 || Loss: 0.9699 || timer: 0.0893 sec.
iter 85460 || Loss: 1.0865 || timer: 0.1172 sec.
iter 85470 || Loss: 0.8629 || timer: 0.1178 sec.
iter 85480 || Loss: 1.2276 || timer: 0.0884 sec.
iter 85490 || Loss: 0.7846 || timer: 0.0905 sec.
iter 85500 || Loss: 1.2223 || timer: 0.0819 sec.
iter 85510 || Loss: 1.8095 || timer: 0.1085 sec.
iter 85520 || Loss: 1.0719 || timer: 0.1112 sec.
iter 85530 || Loss: 1.3990 || timer: 0.0915 sec.
iter 85540 || Loss: 1.2445 || timer: 0.0887 sec.
iter 85550 || Loss: 1.3747 || timer: 0.1028 sec.
iter 85560 || Loss: 1.3669 || timer: 0.0844 sec.
iter 85570 || Loss: 1.2608 || timer: 0.0893 sec.
iter 85580 || Loss: 0.9756 || timer: 0.0913 sec.
iter 85590 || Loss: 1.1980 || timer: 0.0887 sec.
iter 85600 || Loss: 1.1149 || timer: 0.0890 sec.
iter 85610 || Loss: 0.9543 || timer: 0.1098 sec.
iter 85620 || Loss: 1.3048 || timer: 0.0935 sec.
iter 85630 || Loss: 0.9339 || timer: 0.1311 sec.
iter 85640 || Loss: 1.0526 || timer: 0.0962 sec.
iter 85650 || Loss: 1.2802 || timer: 0.0892 sec.
iter 85660 || Loss: 1.0143 || timer: 0.0810 sec.
iter 85670 || Loss: 1.2030 || timer: 0.0898 sec.
iter 85680 || Loss: 1.0490 || timer: 0.1065 sec.
iter 85690 || Loss: 0.7752 || timer: 0.0296 sec.
iter 85700 || Loss: 0.4342 || timer: 0.1503 sec.
iter 85710 || Loss: 1.1888 || timer: 0.1029 sec.
iter 85720 || Loss: 0.8607 || timer: 0.0865 sec.
iter 85730 || Loss: 1.0616 || timer: 0.0853 sec.
iter 85740 || Loss: 0.9740 || timer: 0.1008 sec.
iter 85750 || Loss: 1.0911 || timer: 0.1110 sec.
iter 85760 || Loss: 1.2250 || timer: 0.1078 sec.
iter 85770 || Loss: 1.0479 || timer: 0.0888 sec.
iter 85780 || Loss: 0.8834 || timer: 0.0994 sec.
iter 85790 || Loss: 1.1539 || timer: 0.1149 sec.
iter 85800 || Loss: 1.2391 || timer: 0.0921 sec.
iter 85810 || Loss: 0.9501 || timer: 0.0900 sec.
iter 85820 || Loss: 1.4178 || timer: 0.0890 sec.
iter 85830 || Loss: 1.3165 || timer: 0.0820 sec.
iter 85840 || Loss: 1.3709 || timer: 0.0843 sec.
iter 85850 || Loss: 1.2038 || timer: 0.0895 sec.
iter 85860 || Loss: 1.0991 || timer: 0.0907 sec.
iter 85870 || Loss: 1.1190 || timer: 0.0906 sec.
iter 85880 || Loss: 1.1104 || timer: 0.0811 sec.
iter 85890 || Loss: 1.1404 || timer: 0.0813 sec.
iter 85900 || Loss: 1.2291 || timer: 0.0804 sec.
iter 85910 || Loss: 0.8018 || timer: 0.0897 sec.
iter 85920 || Loss: 0.9029 || timer: 0.0902 sec.
iter 85930 || Loss: 1.1763 || timer: 0.0888 sec.
iter 85940 || Loss: 0.8764 || timer: 0.1030 sec.
iter 85950 || Loss: 0.8746 || timer: 0.0897 sec.
iter 85960 || Loss: 0.8991 || timer: 0.0902 sec.
iter 85970 || Loss: 1.1963 || timer: 0.0908 sec.
iter 85980 || Loss: 0.9264 || timer: 0.0822 sec.
iter 85990 || Loss: 0.9654 || timer: 0.0903 sec.
iter 86000 || Loss: 0.8788 || timer: 0.0811 sec.
iter 86010 || Loss: 1.0075 || timer: 0.0808 sec.
iter 86020 || Loss: 0.8748 || timer: 0.0260 sec.
iter 86030 || Loss: 0.4252 || timer: 0.1155 sec.
iter 86040 || Loss: 1.1960 || timer: 0.0878 sec.
iter 86050 || Loss: 1.1738 || timer: 0.0960 sec.
iter 86060 || Loss: 1.2342 || timer: 0.0904 sec.
iter 86070 || Loss: 1.0650 || timer: 0.0981 sec.
iter 86080 || Loss: 1.1940 || timer: 0.0888 sec.
iter 86090 || Loss: 1.0594 || timer: 0.0879 sec.
iter 86100 || Loss: 1.2133 || timer: 0.1332 sec.
iter 86110 || Loss: 1.1297 || timer: 0.0847 sec.
iter 86120 || Loss: 1.1128 || timer: 0.1224 sec.
iter 86130 || Loss: 1.0344 || timer: 0.0853 sec.
iter 86140 || Loss: 1.2319 || timer: 0.0939 sec.
iter 86150 || Loss: 1.1566 || timer: 0.0812 sec.
iter 86160 || Loss: 0.9638 || timer: 0.0929 sec.
iter 86170 || Loss: 0.9861 || timer: 0.0882 sec.
iter 86180 || Loss: 0.9771 || timer: 0.0947 sec.
iter 86190 || Loss: 0.8220 || timer: 0.0860 sec.
iter 86200 || Loss: 1.0319 || timer: 0.0920 sec.
iter 86210 || Loss: 1.1469 || timer: 0.0893 sec.
iter 86220 || Loss: 0.9389 || timer: 0.0812 sec.
iter 86230 || Loss: 0.9328 || timer: 0.0812 sec.
iter 86240 || Loss: 1.0115 || timer: 0.1356 sec.
iter 86250 || Loss: 1.1660 || timer: 0.0811 sec.
iter 86260 || Loss: 1.1756 || timer: 0.1034 sec.
iter 86270 || Loss: 1.1484 || timer: 0.0980 sec.
iter 86280 || Loss: 0.9045 || timer: 0.0897 sec.
iter 86290 || Loss: 0.9398 || timer: 0.0815 sec.
iter 86300 || Loss: 0.8634 || timer: 0.0889 sec.
iter 86310 || Loss: 0.9368 || timer: 0.0898 sec.
iter 86320 || Loss: 0.9624 || timer: 0.0875 sec.
iter 86330 || Loss: 2.0465 || timer: 0.0882 sec.
iter 86340 || Loss: 1.8451 || timer: 0.1088 sec.
iter 86350 || Loss: 1.4377 || timer: 0.0254 sec.
iter 86360 || Loss: 1.4865 || timer: 0.0903 sec.
iter 86370 || Loss: 1.0851 || timer: 0.0914 sec.
iter 86380 || Loss: 1.2805 || timer: 0.0960 sec.
iter 86390 || Loss: 1.3550 || timer: 0.0895 sec.
iter 86400 || Loss: 1.2988 || timer: 0.1101 sec.
iter 86410 || Loss: 0.9515 || timer: 0.0907 sec.
iter 86420 || Loss: 1.1830 || timer: 0.0813 sec.
iter 86430 || Loss: 1.3368 || timer: 0.0884 sec.
iter 86440 || Loss: 1.0628 || timer: 0.0890 sec.
iter 86450 || Loss: 0.7696 || timer: 0.1065 sec.
iter 86460 || Loss: 1.2391 || timer: 0.0880 sec.
iter 86470 || Loss: 1.7038 || timer: 0.0902 sec.
iter 86480 || Loss: 1.2582 || timer: 0.1082 sec.
iter 86490 || Loss: 1.0621 || timer: 0.0888 sec.
iter 86500 || Loss: 1.8234 || timer: 0.0835 sec.
iter 86510 || Loss: 1.0164 || timer: 0.0810 sec.
iter 86520 || Loss: 1.5661 || timer: 0.0886 sec.
iter 86530 || Loss: 2.8752 || timer: 0.0805 sec.
iter 86540 || Loss: 1.1821 || timer: 0.0877 sec.
iter 86550 || Loss: 0.9941 || timer: 0.0812 sec.
iter 86560 || Loss: 1.0710 || timer: 0.0807 sec.
iter 86570 || Loss: 1.1377 || timer: 0.0834 sec.
iter 86580 || Loss: 1.0283 || timer: 0.0831 sec.
iter 86590 || Loss: 0.9566 || timer: 0.0800 sec.
iter 86600 || Loss: 1.3431 || timer: 0.1037 sec.
iter 86610 || Loss: 1.0605 || timer: 0.0898 sec.
iter 86620 || Loss: 0.9158 || timer: 0.0829 sec.
iter 86630 || Loss: 1.2864 || timer: 0.0883 sec.
iter 86640 || Loss: 1.1022 || timer: 0.1075 sec.
iter 86650 || Loss: 0.9481 || timer: 0.0935 sec.
iter 86660 || Loss: 1.0249 || timer: 0.0894 sec.
iter 86670 || Loss: 1.0986 || timer: 0.1026 sec.
iter 86680 || Loss: 1.5013 || timer: 0.0156 sec.
iter 86690 || Loss: 0.3530 || timer: 0.0916 sec.
iter 86700 || Loss: 0.9315 || timer: 0.0824 sec.
iter 86710 || Loss: 1.0938 || timer: 0.0882 sec.
iter 86720 || Loss: 0.9789 || timer: 0.0817 sec.
iter 86730 || Loss: 1.2983 || timer: 0.1074 sec.
iter 86740 || Loss: 1.0648 || timer: 0.0816 sec.
iter 86750 || Loss: 1.2944 || timer: 0.1003 sec.
iter 86760 || Loss: 1.3952 || timer: 0.0805 sec.
iter 86770 || Loss: 1.3521 || timer: 0.0873 sec.
iter 86780 || Loss: 0.9171 || timer: 0.0980 sec.
iter 86790 || Loss: 1.1572 || timer: 0.0902 sec.
iter 86800 || Loss: 1.2530 || timer: 0.0878 sec.
iter 86810 || Loss: 1.1147 || timer: 0.0806 sec.
iter 86820 || Loss: 1.3416 || timer: 0.0830 sec.
iter 86830 || Loss: 1.2899 || timer: 0.0879 sec.
iter 86840 || Loss: 0.8482 || timer: 0.0883 sec.
iter 86850 || Loss: 0.9708 || timer: 0.1069 sec.
iter 86860 || Loss: 0.9857 || timer: 0.0896 sec.
iter 86870 || Loss: 1.0386 || timer: 0.0908 sec.
iter 86880 || Loss: 1.0033 || timer: 0.1157 sec.
iter 86890 || Loss: 1.4718 || timer: 0.0815 sec.
iter 86900 || Loss: 1.1665 || timer: 0.0873 sec.
iter 86910 || Loss: 1.0372 || timer: 0.0820 sec.
iter 86920 || Loss: 1.1566 || timer: 0.0838 sec.
iter 86930 || Loss: 1.0153 || timer: 0.0893 sec.
iter 86940 || Loss: 1.1623 || timer: 0.0833 sec.
iter 86950 || Loss: 1.8350 || timer: 0.0883 sec.
iter 86960 || Loss: 0.9486 || timer: 0.0803 sec.
iter 86970 || Loss: 1.4019 || timer: 0.1017 sec.
iter 86980 || Loss: 1.3813 || timer: 0.0806 sec.
iter 86990 || Loss: 1.2080 || timer: 0.0806 sec.
iter 87000 || Loss: 1.1637 || timer: 0.0895 sec.
iter 87010 || Loss: 1.1526 || timer: 0.0172 sec.
iter 87020 || Loss: 1.8408 || timer: 0.0820 sec.
iter 87030 || Loss: 1.3458 || timer: 0.0824 sec.
iter 87040 || Loss: 1.1606 || timer: 0.0832 sec.
iter 87050 || Loss: 1.2921 || timer: 0.0902 sec.
iter 87060 || Loss: 0.9716 || timer: 0.0929 sec.
iter 87070 || Loss: 1.1158 || timer: 0.0813 sec.
iter 87080 || Loss: 0.8858 || timer: 0.0898 sec.
iter 87090 || Loss: 1.4665 || timer: 0.1257 sec.
iter 87100 || Loss: 1.2516 || timer: 0.1115 sec.
iter 87110 || Loss: 1.2003 || timer: 0.1001 sec.
iter 87120 || Loss: 1.2573 || timer: 0.0818 sec.
iter 87130 || Loss: 1.3437 || timer: 0.0945 sec.
iter 87140 || Loss: 1.6518 || timer: 0.0816 sec.
iter 87150 || Loss: 0.9956 || timer: 0.0818 sec.
iter 87160 || Loss: 1.2277 || timer: 0.0889 sec.
iter 87170 || Loss: 1.2028 || timer: 0.0826 sec.
iter 87180 || Loss: 1.1229 || timer: 0.1029 sec.
iter 87190 || Loss: 1.7313 || timer: 0.0871 sec.
iter 87200 || Loss: 1.1765 || timer: 0.0813 sec.
iter 87210 || Loss: 1.2080 || timer: 0.1122 sec.
iter 87220 || Loss: 1.0625 || timer: 0.0886 sec.
iter 87230 || Loss: 1.1047 || timer: 0.1081 sec.
iter 87240 || Loss: 1.1310 || timer: 0.0864 sec.
iter 87250 || Loss: 1.1313 || timer: 0.0912 sec.
iter 87260 || Loss: 0.9516 || timer: 0.0911 sec.
iter 87270 || Loss: 0.9752 || timer: 0.0909 sec.
iter 87280 || Loss: 0.9083 || timer: 0.0879 sec.
iter 87290 || Loss: 0.8707 || timer: 0.0899 sec.
iter 87300 || Loss: 1.5428 || timer: 0.0921 sec.
iter 87310 || Loss: 1.3426 || timer: 0.0983 sec.
iter 87320 || Loss: 1.4572 || timer: 0.0812 sec.
iter 87330 || Loss: 1.1687 || timer: 0.0923 sec.
iter 87340 || Loss: 1.2356 || timer: 0.0228 sec.
iter 87350 || Loss: 0.9428 || timer: 0.0816 sec.
iter 87360 || Loss: 1.1571 || timer: 0.0815 sec.
iter 87370 || Loss: 1.0458 || timer: 0.0841 sec.
iter 87380 || Loss: 1.1831 || timer: 0.0928 sec.
iter 87390 || Loss: 1.1500 || timer: 0.1044 sec.
iter 87400 || Loss: 1.4038 || timer: 0.1067 sec.
iter 87410 || Loss: 1.0145 || timer: 0.0967 sec.
iter 87420 || Loss: 1.0742 || timer: 0.0944 sec.
iter 87430 || Loss: 1.1247 || timer: 0.0807 sec.
iter 87440 || Loss: 1.1848 || timer: 0.1199 sec.
iter 87450 || Loss: 0.7511 || timer: 0.0817 sec.
iter 87460 || Loss: 1.0705 || timer: 0.0879 sec.
iter 87470 || Loss: 0.9876 || timer: 0.0821 sec.
iter 87480 || Loss: 0.7876 || timer: 0.0847 sec.
iter 87490 || Loss: 0.9601 || timer: 0.0893 sec.
iter 87500 || Loss: 0.8638 || timer: 0.0808 sec.
iter 87510 || Loss: 1.5628 || timer: 0.0859 sec.
iter 87520 || Loss: 1.1920 || timer: 0.1087 sec.
iter 87530 || Loss: 1.1396 || timer: 0.0898 sec.
iter 87540 || Loss: 1.1476 || timer: 0.0970 sec.
iter 87550 || Loss: 0.8498 || timer: 0.0813 sec.
iter 87560 || Loss: 1.5080 || timer: 0.0901 sec.
iter 87570 || Loss: 1.3350 || timer: 0.0834 sec.
iter 87580 || Loss: 0.9477 || timer: 0.0807 sec.
iter 87590 || Loss: 1.3268 || timer: 0.0858 sec.
iter 87600 || Loss: 1.5256 || timer: 0.1074 sec.
iter 87610 || Loss: 1.2006 || timer: 0.0837 sec.
iter 87620 || Loss: 0.8342 || timer: 0.0836 sec.
iter 87630 || Loss: 0.9058 || timer: 0.0835 sec.
iter 87640 || Loss: 1.2607 || timer: 0.0909 sec.
iter 87650 || Loss: 0.9666 || timer: 0.0846 sec.
iter 87660 || Loss: 1.0878 || timer: 0.0843 sec.
iter 87670 || Loss: 1.2914 || timer: 0.0218 sec.
iter 87680 || Loss: 0.7234 || timer: 0.0904 sec.
iter 87690 || Loss: 1.2868 || timer: 0.1070 sec.
iter 87700 || Loss: 1.5906 || timer: 0.0936 sec.
iter 87710 || Loss: 1.0826 || timer: 0.0932 sec.
iter 87720 || Loss: 1.1007 || timer: 0.0908 sec.
iter 87730 || Loss: 1.0304 || timer: 0.0844 sec.
iter 87740 || Loss: 1.0293 || timer: 0.0838 sec.
iter 87750 || Loss: 1.2373 || timer: 0.0932 sec.
iter 87760 || Loss: 1.4011 || timer: 0.0925 sec.
iter 87770 || Loss: 1.2898 || timer: 0.1005 sec.
iter 87780 || Loss: 1.0302 || timer: 0.0913 sec.
iter 87790 || Loss: 1.0098 || timer: 0.0842 sec.
iter 87800 || Loss: 1.0836 || timer: 0.0898 sec.
iter 87810 || Loss: 1.2370 || timer: 0.0843 sec.
iter 87820 || Loss: 1.1622 || timer: 0.1283 sec.
iter 87830 || Loss: 1.1908 || timer: 0.1117 sec.
iter 87840 || Loss: 1.0416 || timer: 0.1065 sec.
iter 87850 || Loss: 1.0080 || timer: 0.0917 sec.
iter 87860 || Loss: 0.9386 || timer: 0.0848 sec.
iter 87870 || Loss: 1.1851 || timer: 0.0859 sec.
iter 87880 || Loss: 1.2275 || timer: 0.0833 sec.
iter 87890 || Loss: 0.9055 || timer: 0.1014 sec.
iter 87900 || Loss: 1.2170 || timer: 0.0934 sec.
iter 87910 || Loss: 1.6684 || timer: 0.0910 sec.
iter 87920 || Loss: 1.2952 || timer: 0.0918 sec.
iter 87930 || Loss: 1.1134 || timer: 0.0831 sec.
iter 87940 || Loss: 1.0308 || timer: 0.1148 sec.
iter 87950 || Loss: 1.1969 || timer: 0.0923 sec.
iter 87960 || Loss: 1.1186 || timer: 0.0877 sec.
iter 87970 || Loss: 1.0512 || timer: 0.0913 sec.
iter 87980 || Loss: 0.9386 || timer: 0.1011 sec.
iter 87990 || Loss: 1.0332 || timer: 0.0937 sec.
iter 88000 || Loss: 1.1328 || timer: 0.0279 sec.
iter 88010 || Loss: 1.0029 || timer: 0.0907 sec.
iter 88020 || Loss: 1.1912 || timer: 0.0771 sec.
iter 88030 || Loss: 1.0571 || timer: 0.0839 sec.
iter 88040 || Loss: 0.9861 || timer: 0.0766 sec.
iter 88050 || Loss: 1.2987 || timer: 0.0965 sec.
iter 88060 || Loss: 0.9895 || timer: 0.0836 sec.
iter 88070 || Loss: 0.8816 || timer: 0.1039 sec.
iter 88080 || Loss: 1.2658 || timer: 0.1104 sec.
iter 88090 || Loss: 1.1471 || timer: 0.0838 sec.
iter 88100 || Loss: 0.8136 || timer: 0.0982 sec.
iter 88110 || Loss: 1.1588 || timer: 0.0904 sec.
iter 88120 || Loss: 1.2045 || timer: 0.0837 sec.
iter 88130 || Loss: 1.2012 || timer: 0.0910 sec.
iter 88140 || Loss: 1.1099 || timer: 0.0844 sec.
iter 88150 || Loss: 1.4358 || timer: 0.0835 sec.
iter 88160 || Loss: 1.2818 || timer: 0.0917 sec.
iter 88170 || Loss: 0.8420 || timer: 0.0922 sec.
iter 88180 || Loss: 1.2956 || timer: 0.0837 sec.
iter 88190 || Loss: 0.8710 || timer: 0.0907 sec.
iter 88200 || Loss: 1.0285 || timer: 0.0917 sec.
iter 88210 || Loss: 1.1748 || timer: 0.0911 sec.
iter 88220 || Loss: 0.9458 || timer: 0.0839 sec.
iter 88230 || Loss: 1.4777 || timer: 0.0908 sec.
iter 88240 || Loss: 1.0255 || timer: 0.0937 sec.
iter 88250 || Loss: 1.1518 || timer: 0.0836 sec.
iter 88260 || Loss: 1.7457 || timer: 0.0918 sec.
iter 88270 || Loss: 1.1890 || timer: 0.0766 sec.
iter 88280 || Loss: 0.8931 || timer: 0.0911 sec.
iter 88290 || Loss: 1.3937 || timer: 0.1164 sec.
iter 88300 || Loss: 0.9690 || timer: 0.0873 sec.
iter 88310 || Loss: 1.2963 || timer: 0.0921 sec.
iter 88320 || Loss: 0.9920 || timer: 0.0836 sec.
iter 88330 || Loss: 1.3384 || timer: 0.0210 sec.
iter 88340 || Loss: 0.9011 || timer: 0.0840 sec.
iter 88350 || Loss: 1.0556 || timer: 0.0775 sec.
iter 88360 || Loss: 1.2319 || timer: 0.0924 sec.
iter 88370 || Loss: 1.0978 || timer: 0.0837 sec.
iter 88380 || Loss: 1.5306 || timer: 0.0771 sec.
iter 88390 || Loss: 1.4561 || timer: 0.0837 sec.
iter 88400 || Loss: 0.9148 || timer: 0.0913 sec.
iter 88410 || Loss: 1.3808 || timer: 0.1024 sec.
iter 88420 || Loss: 0.9382 || timer: 0.0913 sec.
iter 88430 || Loss: 1.0786 || timer: 0.1183 sec.
iter 88440 || Loss: 0.9595 || timer: 0.0846 sec.
iter 88450 || Loss: 1.4469 || timer: 0.0900 sec.
iter 88460 || Loss: 1.1716 || timer: 0.0835 sec.
iter 88470 || Loss: 1.1497 || timer: 0.0918 sec.
iter 88480 || Loss: 1.0018 || timer: 0.0868 sec.
iter 88490 || Loss: 1.0090 || timer: 0.0896 sec.
iter 88500 || Loss: 1.0305 || timer: 0.0927 sec.
iter 88510 || Loss: 1.4568 || timer: 0.0835 sec.
iter 88520 || Loss: 1.2074 || timer: 0.0840 sec.
iter 88530 || Loss: 1.0682 || timer: 0.0810 sec.
iter 88540 || Loss: 0.9779 || timer: 0.0768 sec.
iter 88550 || Loss: 1.5602 || timer: 0.1010 sec.
iter 88560 || Loss: 0.9701 || timer: 0.0923 sec.
iter 88570 || Loss: 0.8917 || timer: 0.0852 sec.
iter 88580 || Loss: 1.1058 || timer: 0.0840 sec.
iter 88590 || Loss: 1.0267 || timer: 0.0908 sec.
iter 88600 || Loss: 1.1045 || timer: 0.0937 sec.
iter 88610 || Loss: 1.6546 || timer: 0.0844 sec.
iter 88620 || Loss: 1.2020 || timer: 0.0928 sec.
iter 88630 || Loss: 0.9817 || timer: 0.0938 sec.
iter 88640 || Loss: 0.9402 || timer: 0.0907 sec.
iter 88650 || Loss: 1.0611 || timer: 0.0767 sec.
iter 88660 || Loss: 0.9746 || timer: 0.0151 sec.
iter 88670 || Loss: 1.4565 || timer: 0.0939 sec.
iter 88680 || Loss: 1.2835 || timer: 0.0873 sec.
iter 88690 || Loss: 1.3953 || timer: 0.0939 sec.
iter 88700 || Loss: 1.0004 || timer: 0.1114 sec.
iter 88710 || Loss: 1.0959 || timer: 0.0911 sec.
iter 88720 || Loss: 0.9601 || timer: 0.1231 sec.
iter 88730 || Loss: 1.0586 || timer: 0.1021 sec.
iter 88740 || Loss: 0.9667 || timer: 0.0933 sec.
iter 88750 || Loss: 1.0094 || timer: 0.0894 sec.
iter 88760 || Loss: 0.7660 || timer: 0.1068 sec.
iter 88770 || Loss: 0.7164 || timer: 0.0807 sec.
iter 88780 || Loss: 0.7996 || timer: 0.0857 sec.
iter 88790 || Loss: 0.9248 || timer: 0.0909 sec.
iter 88800 || Loss: 1.0785 || timer: 0.0848 sec.
iter 88810 || Loss: 0.8320 || timer: 0.0928 sec.
iter 88820 || Loss: 0.9561 || timer: 0.0926 sec.
iter 88830 || Loss: 1.1531 || timer: 0.0833 sec.
iter 88840 || Loss: 1.1404 || timer: 0.0873 sec.
iter 88850 || Loss: 0.7762 || timer: 0.0984 sec.
iter 88860 || Loss: 0.7935 || timer: 0.0828 sec.
iter 88870 || Loss: 0.8646 || timer: 0.0928 sec.
iter 88880 || Loss: 0.7383 || timer: 0.0909 sec.
iter 88890 || Loss: 0.7950 || timer: 0.0922 sec.
iter 88900 || Loss: 0.8323 || timer: 0.0913 sec.
iter 88910 || Loss: 1.1767 || timer: 0.0811 sec.
iter 88920 || Loss: 1.1184 || timer: 0.0955 sec.
iter 88930 || Loss: 1.0350 || timer: 0.0910 sec.
iter 88940 || Loss: 0.9447 || timer: 0.0870 sec.
iter 88950 || Loss: 1.0090 || timer: 0.1056 sec.
iter 88960 || Loss: 1.3914 || timer: 0.0897 sec.
iter 88970 || Loss: 1.5412 || timer: 0.0868 sec.
iter 88980 || Loss: 1.1337 || timer: 0.0898 sec.
iter 88990 || Loss: 1.0752 || timer: 0.0161 sec.
iter 89000 || Loss: 0.6441 || timer: 0.0875 sec.
iter 89010 || Loss: 1.3103 || timer: 0.1120 sec.
iter 89020 || Loss: 1.0702 || timer: 0.1132 sec.
iter 89030 || Loss: 1.1541 || timer: 0.0837 sec.
iter 89040 || Loss: 1.1025 || timer: 0.0945 sec.
iter 89050 || Loss: 1.0315 || timer: 0.0818 sec.
iter 89060 || Loss: 1.1331 || timer: 0.0920 sec.
iter 89070 || Loss: 1.0934 || timer: 0.0951 sec.
iter 89080 || Loss: 0.8173 || timer: 0.0821 sec.
iter 89090 || Loss: 0.9950 || timer: 0.1160 sec.
iter 89100 || Loss: 1.2435 || timer: 0.0803 sec.
iter 89110 || Loss: 1.1174 || timer: 0.0853 sec.
iter 89120 || Loss: 0.7762 || timer: 0.0888 sec.
iter 89130 || Loss: 1.3945 || timer: 0.0890 sec.
iter 89140 || Loss: 1.0756 || timer: 0.0948 sec.
iter 89150 || Loss: 1.0439 || timer: 0.0847 sec.
iter 89160 || Loss: 1.3523 || timer: 0.0891 sec.
iter 89170 || Loss: 1.3478 || timer: 0.0813 sec.
iter 89180 || Loss: 1.2507 || timer: 0.1091 sec.
iter 89190 || Loss: 1.1471 || timer: 0.0870 sec.
iter 89200 || Loss: 1.2127 || timer: 0.0826 sec.
iter 89210 || Loss: 1.2208 || timer: 0.0912 sec.
iter 89220 || Loss: 1.2650 || timer: 0.0889 sec.
iter 89230 || Loss: 1.0339 || timer: 0.0897 sec.
iter 89240 || Loss: 0.9837 || timer: 0.0923 sec.
iter 89250 || Loss: 1.0858 || timer: 0.0810 sec.
iter 89260 || Loss: 0.9031 || timer: 0.1294 sec.
iter 89270 || Loss: 1.3428 || timer: 0.0890 sec.
iter 89280 || Loss: 0.7414 || timer: 0.0876 sec.
iter 89290 || Loss: 0.8266 || timer: 0.0807 sec.
iter 89300 || Loss: 1.2927 || timer: 0.0898 sec.
iter 89310 || Loss: 1.1304 || timer: 0.0898 sec.
iter 89320 || Loss: 1.2903 || timer: 0.0173 sec.
iter 89330 || Loss: 1.6156 || timer: 0.0732 sec.
iter 89340 || Loss: 1.0721 || timer: 0.0993 sec.
iter 89350 || Loss: 0.9631 || timer: 0.0873 sec.
iter 89360 || Loss: 1.0734 || timer: 0.0803 sec.
iter 89370 || Loss: 1.4890 || timer: 0.0901 sec.
iter 89380 || Loss: 1.1893 || timer: 0.0888 sec.
iter 89390 || Loss: 1.5286 || timer: 0.0906 sec.
iter 89400 || Loss: 0.9198 || timer: 0.0901 sec.
iter 89410 || Loss: 1.1410 || timer: 0.0947 sec.
iter 89420 || Loss: 1.2286 || timer: 0.1024 sec.
iter 89430 || Loss: 1.0849 || timer: 0.0811 sec.
iter 89440 || Loss: 1.2131 || timer: 0.1001 sec.
iter 89450 || Loss: 1.4605 || timer: 0.0883 sec.
iter 89460 || Loss: 1.2246 || timer: 0.0882 sec.
iter 89470 || Loss: 0.9306 || timer: 0.0810 sec.
iter 89480 || Loss: 1.1354 || timer: 0.0878 sec.
iter 89490 || Loss: 0.8908 || timer: 0.0968 sec.
iter 89500 || Loss: 3.3720 || timer: 0.0821 sec.
iter 89510 || Loss: 2.9870 || timer: 0.0854 sec.
iter 89520 || Loss: 1.5909 || timer: 0.0811 sec.
iter 89530 || Loss: 1.8515 || timer: 0.0964 sec.
iter 89540 || Loss: 1.7155 || timer: 0.0809 sec.
iter 89550 || Loss: 1.1953 || timer: 0.0909 sec.
iter 89560 || Loss: 1.8704 || timer: 0.1039 sec.
iter 89570 || Loss: 1.2975 || timer: 0.1128 sec.
iter 89580 || Loss: 1.4831 || timer: 0.0839 sec.
iter 89590 || Loss: 1.1546 || timer: 0.0973 sec.
iter 89600 || Loss: 2.4300 || timer: 0.0831 sec.
iter 89610 || Loss: 1.9893 || timer: 0.1027 sec.
iter 89620 || Loss: 1.3778 || timer: 0.0894 sec.
iter 89630 || Loss: 1.5684 || timer: 0.0873 sec.
iter 89640 || Loss: 1.2031 || timer: 0.1142 sec.
iter 89650 || Loss: 1.4525 || timer: 0.0290 sec.
iter 89660 || Loss: 0.7005 || timer: 0.0889 sec.
iter 89670 || Loss: 1.1461 || timer: 0.0879 sec.
iter 89680 || Loss: 1.0748 || timer: 0.0974 sec.
iter 89690 || Loss: 1.0261 || timer: 0.0964 sec.
iter 89700 || Loss: 1.3280 || timer: 0.0902 sec.
iter 89710 || Loss: 1.2119 || timer: 0.0918 sec.
iter 89720 || Loss: 1.1720 || timer: 0.0895 sec.
iter 89730 || Loss: 1.3120 || timer: 0.0889 sec.
iter 89740 || Loss: 1.1531 || timer: 0.0883 sec.
iter 89750 || Loss: 1.0157 || timer: 0.0925 sec.
iter 89760 || Loss: 1.3117 || timer: 0.0896 sec.
iter 89770 || Loss: 1.3272 || timer: 0.0915 sec.
iter 89780 || Loss: 1.1533 || timer: 0.0871 sec.
iter 89790 || Loss: 0.9790 || timer: 0.0899 sec.
iter 89800 || Loss: 0.9841 || timer: 0.0881 sec.
iter 89810 || Loss: 1.0447 || timer: 0.0902 sec.
iter 89820 || Loss: 1.0673 || timer: 0.0900 sec.
iter 89830 || Loss: 0.9765 || timer: 0.1053 sec.
iter 89840 || Loss: 1.3948 || timer: 0.1052 sec.
iter 89850 || Loss: 0.9310 || timer: 0.1112 sec.
iter 89860 || Loss: 1.2932 || timer: 0.0902 sec.
iter 89870 || Loss: 1.1136 || timer: 0.0836 sec.
iter 89880 || Loss: 0.9170 || timer: 0.0819 sec.
iter 89890 || Loss: 0.9570 || timer: 0.0886 sec.
iter 89900 || Loss: 1.3538 || timer: 0.1032 sec.
iter 89910 || Loss: 1.2300 || timer: 0.0814 sec.
iter 89920 || Loss: 1.2011 || timer: 0.0794 sec.
iter 89930 || Loss: 1.3811 || timer: 0.0951 sec.
iter 89940 || Loss: 1.2047 || timer: 0.0956 sec.
iter 89950 || Loss: 0.9877 || timer: 0.0890 sec.
iter 89960 || Loss: 1.1999 || timer: 0.0899 sec.
iter 89970 || Loss: 1.4160 || timer: 0.0849 sec.
iter 89980 || Loss: 1.6580 || timer: 0.0224 sec.
iter 89990 || Loss: 1.4400 || timer: 0.0824 sec.
iter 90000 || Loss: 1.1561 || Saving state, iter: 90000
timer: 0.0834 sec.
iter 90010 || Loss: 1.4437 || timer: 0.0823 sec.
iter 90020 || Loss: 1.5044 || timer: 0.0843 sec.
iter 90030 || Loss: 1.3767 || timer: 0.0906 sec.
iter 90040 || Loss: 0.9825 || timer: 0.0856 sec.
iter 90050 || Loss: 1.1637 || timer: 0.0985 sec.
iter 90060 || Loss: 1.1040 || timer: 0.0757 sec.
iter 90070 || Loss: 0.8773 || timer: 0.0936 sec.
iter 90080 || Loss: 1.0534 || timer: 0.0952 sec.
iter 90090 || Loss: 0.9421 || timer: 0.0900 sec.
iter 90100 || Loss: 1.0397 || timer: 0.0912 sec.
iter 90110 || Loss: 0.9353 || timer: 0.0923 sec.
iter 90120 || Loss: 1.0005 || timer: 0.0928 sec.
iter 90130 || Loss: 0.9823 || timer: 0.0900 sec.
iter 90140 || Loss: 1.0755 || timer: 0.0963 sec.
iter 90150 || Loss: 1.0390 || timer: 0.0766 sec.
iter 90160 || Loss: 1.0776 || timer: 0.0916 sec.
iter 90170 || Loss: 1.2887 || timer: 0.0902 sec.
iter 90180 || Loss: 1.2842 || timer: 0.0921 sec.
iter 90190 || Loss: 1.1889 || timer: 0.0927 sec.
iter 90200 || Loss: 1.5555 || timer: 0.0909 sec.
iter 90210 || Loss: 0.8148 || timer: 0.0828 sec.
iter 90220 || Loss: 1.2117 || timer: 0.1003 sec.
iter 90230 || Loss: 0.9060 || timer: 0.1039 sec.
iter 90240 || Loss: 1.2101 || timer: 0.0966 sec.
iter 90250 || Loss: 1.6511 || timer: 0.0911 sec.
iter 90260 || Loss: 1.4597 || timer: 0.0774 sec.
iter 90270 || Loss: 1.2534 || timer: 0.0906 sec.
iter 90280 || Loss: 0.7161 || timer: 0.0846 sec.
iter 90290 || Loss: 0.9374 || timer: 0.0770 sec.
iter 90300 || Loss: 1.2402 || timer: 0.0769 sec.
iter 90310 || Loss: 1.2403 || timer: 0.0196 sec.
iter 90320 || Loss: 1.1207 || timer: 0.0809 sec.
iter 90330 || Loss: 0.9042 || timer: 0.0919 sec.
iter 90340 || Loss: 0.6944 || timer: 0.0834 sec.
iter 90350 || Loss: 0.8532 || timer: 0.0828 sec.
iter 90360 || Loss: 1.0525 || timer: 0.0834 sec.
iter 90370 || Loss: 0.9493 || timer: 0.1070 sec.
iter 90380 || Loss: 1.3671 || timer: 0.1088 sec.
iter 90390 || Loss: 0.9758 || timer: 0.0838 sec.
iter 90400 || Loss: 1.0863 || timer: 0.0918 sec.
iter 90410 || Loss: 1.2338 || timer: 0.0961 sec.
iter 90420 || Loss: 1.0191 || timer: 0.0847 sec.
iter 90430 || Loss: 1.3118 || timer: 0.0890 sec.
iter 90440 || Loss: 0.9511 || timer: 0.0966 sec.
iter 90450 || Loss: 1.3902 || timer: 0.0882 sec.
iter 90460 || Loss: 1.2405 || timer: 0.0903 sec.
iter 90470 || Loss: 1.1478 || timer: 0.0899 sec.
iter 90480 || Loss: 1.3022 || timer: 0.0829 sec.
iter 90490 || Loss: 0.9429 || timer: 0.1051 sec.
iter 90500 || Loss: 1.3451 || timer: 0.0840 sec.
iter 90510 || Loss: 0.9502 || timer: 0.1037 sec.
iter 90520 || Loss: 1.0827 || timer: 0.0843 sec.
iter 90530 || Loss: 0.9578 || timer: 0.0827 sec.
iter 90540 || Loss: 1.2335 || timer: 0.0753 sec.
iter 90550 || Loss: 1.1369 || timer: 0.1122 sec.
iter 90560 || Loss: 1.4515 || timer: 0.0835 sec.
iter 90570 || Loss: 1.3455 || timer: 0.0826 sec.
iter 90580 || Loss: 1.1118 || timer: 0.0839 sec.
iter 90590 || Loss: 1.1025 || timer: 0.1083 sec.
iter 90600 || Loss: 0.9256 || timer: 0.0889 sec.
iter 90610 || Loss: 1.2065 || timer: 0.0908 sec.
iter 90620 || Loss: 1.7243 || timer: 0.0910 sec.
iter 90630 || Loss: 1.0521 || timer: 0.0920 sec.
iter 90640 || Loss: 0.9664 || timer: 0.0236 sec.
iter 90650 || Loss: 0.2959 || timer: 0.0770 sec.
iter 90660 || Loss: 0.8159 || timer: 0.0838 sec.
iter 90670 || Loss: 0.9502 || timer: 0.0796 sec.
iter 90680 || Loss: 1.3696 || timer: 0.0860 sec.
iter 90690 || Loss: 1.3397 || timer: 0.0930 sec.
iter 90700 || Loss: 1.5955 || timer: 0.0883 sec.
iter 90710 || Loss: 1.3133 || timer: 0.0836 sec.
iter 90720 || Loss: 0.8486 || timer: 0.0835 sec.
iter 90730 || Loss: 0.9829 || timer: 0.0841 sec.
iter 90740 || Loss: 0.9306 || timer: 0.1000 sec.
iter 90750 || Loss: 0.9821 || timer: 0.0941 sec.
iter 90760 || Loss: 1.8072 || timer: 0.1051 sec.
iter 90770 || Loss: 1.4648 || timer: 0.0882 sec.
iter 90780 || Loss: 1.2780 || timer: 0.0907 sec.
iter 90790 || Loss: 1.7684 || timer: 0.0893 sec.
iter 90800 || Loss: 1.3163 || timer: 0.1057 sec.
iter 90810 || Loss: 0.9532 || timer: 0.0911 sec.
iter 90820 || Loss: 2.3634 || timer: 0.0901 sec.
iter 90830 || Loss: 1.9765 || timer: 0.0828 sec.
iter 90840 || Loss: 1.4164 || timer: 0.0904 sec.
iter 90850 || Loss: 1.7692 || timer: 0.0906 sec.
iter 90860 || Loss: 1.3076 || timer: 0.0841 sec.
iter 90870 || Loss: 1.5207 || timer: 0.0939 sec.
iter 90880 || Loss: 1.1422 || timer: 0.0908 sec.
iter 90890 || Loss: 1.0423 || timer: 0.0910 sec.
iter 90900 || Loss: 0.9812 || timer: 0.0901 sec.
iter 90910 || Loss: 1.0689 || timer: 0.0880 sec.
iter 90920 || Loss: 1.5295 || timer: 0.1091 sec.
iter 90930 || Loss: 1.2701 || timer: 0.1031 sec.
iter 90940 || Loss: 1.4304 || timer: 0.0839 sec.
iter 90950 || Loss: 1.3881 || timer: 0.0918 sec.
iter 90960 || Loss: 1.7678 || timer: 0.0876 sec.
iter 90970 || Loss: 1.3763 || timer: 0.0207 sec.
iter 90980 || Loss: 6.2311 || timer: 0.0842 sec.
iter 90990 || Loss: 1.2109 || timer: 0.0909 sec.
iter 91000 || Loss: 1.3896 || timer: 0.0904 sec.
iter 91010 || Loss: 1.1686 || timer: 0.1050 sec.
iter 91020 || Loss: 1.4319 || timer: 0.0849 sec.
iter 91030 || Loss: 1.1885 || timer: 0.1012 sec.
iter 91040 || Loss: 1.1573 || timer: 0.0917 sec.
iter 91050 || Loss: 0.8560 || timer: 0.0833 sec.
iter 91060 || Loss: 1.0489 || timer: 0.1044 sec.
iter 91070 || Loss: 0.9382 || timer: 0.0878 sec.
iter 91080 || Loss: 1.5337 || timer: 0.0972 sec.
iter 91090 || Loss: 1.2683 || timer: 0.0971 sec.
iter 91100 || Loss: 1.1034 || timer: 0.0833 sec.
iter 91110 || Loss: 0.8310 || timer: 0.1131 sec.
iter 91120 || Loss: 1.3081 || timer: 0.0976 sec.
iter 91130 || Loss: 1.0250 || timer: 0.0847 sec.
iter 91140 || Loss: 1.2928 || timer: 0.1088 sec.
iter 91150 || Loss: 1.0502 || timer: 0.0915 sec.
iter 91160 || Loss: 1.2759 || timer: 0.0899 sec.
iter 91170 || Loss: 1.2956 || timer: 0.0923 sec.
iter 91180 || Loss: 0.9433 || timer: 0.0774 sec.
iter 91190 || Loss: 1.2873 || timer: 0.0929 sec.
iter 91200 || Loss: 0.9389 || timer: 0.0913 sec.
iter 91210 || Loss: 1.1099 || timer: 0.0935 sec.
iter 91220 || Loss: 1.0069 || timer: 0.0899 sec.
iter 91230 || Loss: 0.8124 || timer: 0.0928 sec.
iter 91240 || Loss: 1.1242 || timer: 0.0997 sec.
iter 91250 || Loss: 0.9837 || timer: 0.0976 sec.
iter 91260 || Loss: 1.3650 || timer: 0.1086 sec.
iter 91270 || Loss: 1.4359 || timer: 0.0837 sec.
iter 91280 || Loss: 0.9879 || timer: 0.1036 sec.
iter 91290 || Loss: 0.8106 || timer: 0.0846 sec.
iter 91300 || Loss: 1.5075 || timer: 0.0205 sec.
iter 91310 || Loss: 0.8793 || timer: 0.0877 sec.
iter 91320 || Loss: 1.4685 || timer: 0.0907 sec.
iter 91330 || Loss: 1.2705 || timer: 0.0844 sec.
iter 91340 || Loss: 0.7728 || timer: 0.0843 sec.
iter 91350 || Loss: 1.4617 || timer: 0.0908 sec.
iter 91360 || Loss: 1.0328 || timer: 0.0841 sec.
iter 91370 || Loss: 0.8261 || timer: 0.0943 sec.
iter 91380 || Loss: 0.8614 || timer: 0.0856 sec.
iter 91390 || Loss: 0.7864 || timer: 0.1013 sec.
iter 91400 || Loss: 1.2285 || timer: 0.0956 sec.
iter 91410 || Loss: 1.1144 || timer: 0.0883 sec.
iter 91420 || Loss: 1.5033 || timer: 0.0922 sec.
iter 91430 || Loss: 1.1874 || timer: 0.0873 sec.
iter 91440 || Loss: 0.9993 || timer: 0.0960 sec.
iter 91450 || Loss: 1.2930 || timer: 0.0913 sec.
iter 91460 || Loss: 1.1030 || timer: 0.0825 sec.
iter 91470 || Loss: 1.3700 || timer: 0.1025 sec.
iter 91480 || Loss: 1.5228 || timer: 0.0775 sec.
iter 91490 || Loss: 0.9310 || timer: 0.0815 sec.
iter 91500 || Loss: 0.9837 || timer: 0.0768 sec.
iter 91510 || Loss: 1.3533 || timer: 0.0839 sec.
iter 91520 || Loss: 0.7826 || timer: 0.0934 sec.
iter 91530 || Loss: 1.1241 || timer: 0.0864 sec.
iter 91540 || Loss: 0.9627 || timer: 0.0896 sec.
iter 91550 || Loss: 1.0698 || timer: 0.0935 sec.
iter 91560 || Loss: 0.8929 || timer: 0.0920 sec.
iter 91570 || Loss: 1.0097 || timer: 0.0844 sec.
iter 91580 || Loss: 0.9448 || timer: 0.0913 sec.
iter 91590 || Loss: 0.8919 || timer: 0.0844 sec.
iter 91600 || Loss: 1.1333 || timer: 0.0818 sec.
iter 91610 || Loss: 0.9885 || timer: 0.0899 sec.
iter 91620 || Loss: 1.3365 || timer: 0.0900 sec.
iter 91630 || Loss: 1.1002 || timer: 0.0256 sec.
iter 91640 || Loss: 0.5906 || timer: 0.1002 sec.
iter 91650 || Loss: 1.1155 || timer: 0.0838 sec.
iter 91660 || Loss: 0.9408 || timer: 0.1058 sec.
iter 91670 || Loss: 0.9167 || timer: 0.0841 sec.
iter 91680 || Loss: 1.1607 || timer: 0.1043 sec.
iter 91690 || Loss: 1.1980 || timer: 0.0832 sec.
iter 91700 || Loss: 1.0362 || timer: 0.0842 sec.
iter 91710 || Loss: 1.3477 || timer: 0.1069 sec.
iter 91720 || Loss: 0.8630 || timer: 0.0884 sec.
iter 91730 || Loss: 1.0291 || timer: 0.0979 sec.
iter 91740 || Loss: 1.1212 || timer: 0.0896 sec.
iter 91750 || Loss: 1.0200 || timer: 0.0812 sec.
iter 91760 || Loss: 1.1236 || timer: 0.0825 sec.
iter 91770 || Loss: 0.9134 || timer: 0.0890 sec.
iter 91780 || Loss: 0.8486 || timer: 0.0921 sec.
iter 91790 || Loss: 1.2659 || timer: 0.0822 sec.
iter 91800 || Loss: 1.1779 || timer: 0.1085 sec.
iter 91810 || Loss: 1.1769 || timer: 0.0895 sec.
iter 91820 || Loss: 1.3052 || timer: 0.0928 sec.
iter 91830 || Loss: 0.9258 || timer: 0.0918 sec.
iter 91840 || Loss: 1.0722 || timer: 0.0907 sec.
iter 91850 || Loss: 0.9589 || timer: 0.0929 sec.
iter 91860 || Loss: 0.9181 || timer: 0.0914 sec.
iter 91870 || Loss: 1.0916 || timer: 0.0897 sec.
iter 91880 || Loss: 0.8612 || timer: 0.0931 sec.
iter 91890 || Loss: 1.0643 || timer: 0.0924 sec.
iter 91900 || Loss: 1.1956 || timer: 0.0934 sec.
iter 91910 || Loss: 1.0888 || timer: 0.0843 sec.
iter 91920 || Loss: 1.0186 || timer: 0.0921 sec.
iter 91930 || Loss: 1.1267 || timer: 0.0927 sec.
iter 91940 || Loss: 1.0426 || timer: 0.1006 sec.
iter 91950 || Loss: 0.9449 || timer: 0.0890 sec.
iter 91960 || Loss: 0.9540 || timer: 0.0209 sec.
iter 91970 || Loss: 2.9065 || timer: 0.0881 sec.
iter 91980 || Loss: 1.3049 || timer: 0.0959 sec.
iter 91990 || Loss: 1.2050 || timer: 0.0904 sec.
iter 92000 || Loss: 1.1278 || timer: 0.0773 sec.
iter 92010 || Loss: 1.1130 || timer: 0.0774 sec.
iter 92020 || Loss: 1.2895 || timer: 0.1019 sec.
iter 92030 || Loss: 0.8963 || timer: 0.0771 sec.
iter 92040 || Loss: 0.8431 || timer: 0.0778 sec.
iter 92050 || Loss: 1.0368 || timer: 0.0917 sec.
iter 92060 || Loss: 1.1508 || timer: 0.1108 sec.
iter 92070 || Loss: 0.8179 || timer: 0.0857 sec.
iter 92080 || Loss: 1.0758 || timer: 0.1018 sec.
iter 92090 || Loss: 0.9344 || timer: 0.0750 sec.
iter 92100 || Loss: 0.9659 || timer: 0.0919 sec.
iter 92110 || Loss: 1.1746 || timer: 0.0834 sec.
iter 92120 || Loss: 0.9511 || timer: 0.0836 sec.
iter 92130 || Loss: 1.0189 || timer: 0.0902 sec.
iter 92140 || Loss: 0.9343 || timer: 0.0892 sec.
iter 92150 || Loss: 1.2364 || timer: 0.0841 sec.
iter 92160 || Loss: 1.0727 || timer: 0.0968 sec.
iter 92170 || Loss: 0.9500 || timer: 0.0826 sec.
iter 92180 || Loss: 0.9972 || timer: 0.0766 sec.
iter 92190 || Loss: 1.4783 || timer: 0.0829 sec.
iter 92200 || Loss: 1.1329 || timer: 0.0885 sec.
iter 92210 || Loss: 0.9604 || timer: 0.0844 sec.
iter 92220 || Loss: 0.9749 || timer: 0.0839 sec.
iter 92230 || Loss: 0.8250 || timer: 0.1070 sec.
iter 92240 || Loss: 1.0736 || timer: 0.0844 sec.
iter 92250 || Loss: 0.8171 || timer: 0.0965 sec.
iter 92260 || Loss: 1.2421 || timer: 0.0917 sec.
iter 92270 || Loss: 1.3254 || timer: 0.0762 sec.
iter 92280 || Loss: 1.0828 || timer: 0.0860 sec.
iter 92290 || Loss: 1.1961 || timer: 0.0273 sec.
iter 92300 || Loss: 3.5373 || timer: 0.0934 sec.
iter 92310 || Loss: 1.3796 || timer: 0.0924 sec.
iter 92320 || Loss: 1.0671 || timer: 0.0929 sec.
iter 92330 || Loss: 1.0575 || timer: 0.0847 sec.
iter 92340 || Loss: 1.2523 || timer: 0.0846 sec.
iter 92350 || Loss: 1.0754 || timer: 0.0919 sec.
iter 92360 || Loss: 0.9239 || timer: 0.0946 sec.
iter 92370 || Loss: 0.8327 || timer: 0.1145 sec.
iter 92380 || Loss: 0.9177 || timer: 0.0978 sec.
iter 92390 || Loss: 1.0911 || timer: 0.0998 sec.
iter 92400 || Loss: 1.0461 || timer: 0.0937 sec.
iter 92410 || Loss: 1.0800 || timer: 0.0868 sec.
iter 92420 || Loss: 0.7724 || timer: 0.0833 sec.
iter 92430 || Loss: 1.2417 || timer: 0.0975 sec.
iter 92440 || Loss: 1.2179 || timer: 0.0874 sec.
iter 92450 || Loss: 0.7326 || timer: 0.0904 sec.
iter 92460 || Loss: 1.0622 || timer: 0.0908 sec.
iter 92470 || Loss: 0.9627 || timer: 0.0887 sec.
iter 92480 || Loss: 0.9949 || timer: 0.1174 sec.
iter 92490 || Loss: 1.2460 || timer: 0.1181 sec.
iter 92500 || Loss: 1.2048 || timer: 0.1089 sec.
iter 92510 || Loss: 1.3742 || timer: 0.0990 sec.
iter 92520 || Loss: 1.2410 || timer: 0.0855 sec.
iter 92530 || Loss: 1.0984 || timer: 0.0775 sec.
iter 92540 || Loss: 1.0672 || timer: 0.0829 sec.
iter 92550 || Loss: 1.0318 || timer: 0.0914 sec.
iter 92560 || Loss: 1.0731 || timer: 0.0840 sec.
iter 92570 || Loss: 1.1290 || timer: 0.0813 sec.
iter 92580 || Loss: 1.0303 || timer: 0.0912 sec.
iter 92590 || Loss: 1.3386 || timer: 0.0924 sec.
iter 92600 || Loss: 1.0512 || timer: 0.1059 sec.
iter 92610 || Loss: 1.3007 || timer: 0.0914 sec.
iter 92620 || Loss: 1.7858 || timer: 0.0257 sec.
iter 92630 || Loss: 1.6683 || timer: 0.0910 sec.
iter 92640 || Loss: 1.1219 || timer: 0.0987 sec.
iter 92650 || Loss: 0.9740 || timer: 0.0900 sec.
iter 92660 || Loss: 1.0075 || timer: 0.0840 sec.
iter 92670 || Loss: 0.8591 || timer: 0.0885 sec.
iter 92680 || Loss: 1.3449 || timer: 0.0935 sec.
iter 92690 || Loss: 1.2943 || timer: 0.0839 sec.
iter 92700 || Loss: 1.4698 || timer: 0.0842 sec.
iter 92710 || Loss: 0.9311 || timer: 0.0914 sec.
iter 92720 || Loss: 0.8196 || timer: 0.1110 sec.
iter 92730 || Loss: 0.9694 || timer: 0.0910 sec.
iter 92740 || Loss: 1.2509 || timer: 0.0826 sec.
iter 92750 || Loss: 1.4656 || timer: 0.0850 sec.
iter 92760 || Loss: 0.9541 || timer: 0.0901 sec.
iter 92770 || Loss: 1.1187 || timer: 0.0837 sec.
iter 92780 || Loss: 0.9241 || timer: 0.0839 sec.
iter 92790 || Loss: 1.4949 || timer: 0.0769 sec.
iter 92800 || Loss: 1.1058 || timer: 0.0838 sec.
iter 92810 || Loss: 1.1171 || timer: 0.0913 sec.
iter 92820 || Loss: 0.7915 || timer: 0.0896 sec.
iter 92830 || Loss: 0.9798 || timer: 0.0838 sec.
iter 92840 || Loss: 0.9279 || timer: 0.1002 sec.
iter 92850 || Loss: 1.1644 || timer: 0.0919 sec.
iter 92860 || Loss: 0.8125 || timer: 0.0904 sec.
iter 92870 || Loss: 1.1936 || timer: 0.0846 sec.
iter 92880 || Loss: 1.5893 || timer: 0.0864 sec.
iter 92890 || Loss: 1.0005 || timer: 0.0911 sec.
iter 92900 || Loss: 1.3321 || timer: 0.0892 sec.
iter 92910 || Loss: 1.2495 || timer: 0.0919 sec.
iter 92920 || Loss: 1.6444 || timer: 0.0912 sec.
iter 92930 || Loss: 0.7756 || timer: 0.0842 sec.
iter 92940 || Loss: 0.9740 || timer: 0.0842 sec.
iter 92950 || Loss: 0.8679 || timer: 0.0153 sec.
iter 92960 || Loss: 1.5179 || timer: 0.0842 sec.
iter 92970 || Loss: 1.0241 || timer: 0.1126 sec.
iter 92980 || Loss: 0.9884 || timer: 0.0906 sec.
iter 92990 || Loss: 1.5221 || timer: 0.0898 sec.
iter 93000 || Loss: 0.8835 || timer: 0.1024 sec.
iter 93010 || Loss: 0.9972 || timer: 0.0920 sec.
iter 93020 || Loss: 1.5768 || timer: 0.0867 sec.
iter 93030 || Loss: 1.1410 || timer: 0.0846 sec.
iter 93040 || Loss: 1.1845 || timer: 0.1067 sec.
iter 93050 || Loss: 1.0064 || timer: 0.1073 sec.
iter 93060 || Loss: 0.9615 || timer: 0.0930 sec.
iter 93070 || Loss: 0.8272 || timer: 0.0917 sec.
iter 93080 || Loss: 0.9645 || timer: 0.0853 sec.
iter 93090 || Loss: 0.9516 || timer: 0.0847 sec.
iter 93100 || Loss: 1.0363 || timer: 0.0836 sec.
iter 93110 || Loss: 1.0086 || timer: 0.0911 sec.
iter 93120 || Loss: 0.7771 || timer: 0.0915 sec.
iter 93130 || Loss: 0.8886 || timer: 0.0822 sec.
iter 93140 || Loss: 0.8691 || timer: 0.0755 sec.
iter 93150 || Loss: 1.1394 || timer: 0.0782 sec.
iter 93160 || Loss: 1.1178 || timer: 0.1043 sec.
iter 93170 || Loss: 0.9365 || timer: 0.0834 sec.
iter 93180 || Loss: 0.9487 || timer: 0.0854 sec.
iter 93190 || Loss: 1.2462 || timer: 0.0925 sec.
iter 93200 || Loss: 1.3480 || timer: 0.0886 sec.
iter 93210 || Loss: 0.9447 || timer: 0.0828 sec.
iter 93220 || Loss: 1.1342 || timer: 0.0891 sec.
iter 93230 || Loss: 0.7471 || timer: 0.0912 sec.
iter 93240 || Loss: 1.1279 || timer: 0.0920 sec.
iter 93250 || Loss: 1.1877 || timer: 0.0938 sec.
iter 93260 || Loss: 0.7507 || timer: 0.0834 sec.
iter 93270 || Loss: 0.8241 || timer: 0.0921 sec.
iter 93280 || Loss: 1.3524 || timer: 0.0208 sec.
iter 93290 || Loss: 0.4900 || timer: 0.0927 sec.
iter 93300 || Loss: 1.4851 || timer: 0.0833 sec.
iter 93310 || Loss: 1.0400 || timer: 0.0830 sec.
iter 93320 || Loss: 1.3595 || timer: 0.1072 sec.
iter 93330 || Loss: 1.0879 || timer: 0.0842 sec.
iter 93340 || Loss: 1.4876 || timer: 0.0841 sec.
iter 93350 || Loss: 1.1634 || timer: 0.0840 sec.
iter 93360 || Loss: 0.8387 || timer: 0.0847 sec.
iter 93370 || Loss: 0.8161 || timer: 0.1024 sec.
iter 93380 || Loss: 0.8822 || timer: 0.1161 sec.
iter 93390 || Loss: 0.9387 || timer: 0.0913 sec.
iter 93400 || Loss: 0.6987 || timer: 0.1012 sec.
iter 93410 || Loss: 1.0527 || timer: 0.0872 sec.
iter 93420 || Loss: 0.8988 || timer: 0.0844 sec.
iter 93430 || Loss: 1.3225 || timer: 0.0843 sec.
iter 93440 || Loss: 1.7397 || timer: 0.0845 sec.
iter 93450 || Loss: 1.0726 || timer: 0.0914 sec.
iter 93460 || Loss: 1.2609 || timer: 0.0916 sec.
iter 93470 || Loss: 0.8037 || timer: 0.0876 sec.
iter 93480 || Loss: 0.9211 || timer: 0.0876 sec.
iter 93490 || Loss: 1.1683 || timer: 0.1065 sec.
iter 93500 || Loss: 1.3263 || timer: 0.0900 sec.
iter 93510 || Loss: 1.1943 || timer: 0.0843 sec.
iter 93520 || Loss: 0.9855 || timer: 0.1189 sec.
iter 93530 || Loss: 0.8825 || timer: 0.0831 sec.
iter 93540 || Loss: 1.5461 || timer: 0.0856 sec.
iter 93550 || Loss: 0.8957 || timer: 0.0848 sec.
iter 93560 || Loss: 1.0168 || timer: 0.0836 sec.
iter 93570 || Loss: 0.9856 || timer: 0.0835 sec.
iter 93580 || Loss: 0.8515 || timer: 0.1159 sec.
iter 93590 || Loss: 1.0114 || timer: 0.0839 sec.
iter 93600 || Loss: 1.2405 || timer: 0.0900 sec.
iter 93610 || Loss: 1.1425 || timer: 0.0293 sec.
iter 93620 || Loss: 0.5704 || timer: 0.0976 sec.
iter 93630 || Loss: 1.1650 || timer: 0.1059 sec.
iter 93640 || Loss: 0.9899 || timer: 0.1017 sec.
iter 93650 || Loss: 0.6757 || timer: 0.0879 sec.
iter 93660 || Loss: 0.9738 || timer: 0.0887 sec.
iter 93670 || Loss: 1.1856 || timer: 0.0871 sec.
iter 93680 || Loss: 1.0044 || timer: 0.0828 sec.
iter 93690 || Loss: 1.3082 || timer: 0.0874 sec.
iter 93700 || Loss: 1.0521 || timer: 0.0838 sec.
iter 93710 || Loss: 1.1523 || timer: 0.0962 sec.
iter 93720 || Loss: 1.2151 || timer: 0.0906 sec.
iter 93730 || Loss: 0.9945 || timer: 0.0899 sec.
iter 93740 || Loss: 1.0253 || timer: 0.0814 sec.
iter 93750 || Loss: 1.4021 || timer: 0.0940 sec.
iter 93760 || Loss: 0.7668 || timer: 0.0911 sec.
iter 93770 || Loss: 1.2424 || timer: 0.1087 sec.
iter 93780 || Loss: 1.1921 || timer: 0.0903 sec.
iter 93790 || Loss: 0.9447 || timer: 0.1058 sec.
iter 93800 || Loss: 1.0702 || timer: 0.0815 sec.
iter 93810 || Loss: 1.4317 || timer: 0.0829 sec.
iter 93820 || Loss: 0.9497 || timer: 0.0881 sec.
iter 93830 || Loss: 1.3446 || timer: 0.0905 sec.
iter 93840 || Loss: 1.0968 || timer: 0.0889 sec.
iter 93850 || Loss: 0.8601 || timer: 0.1050 sec.
iter 93860 || Loss: 1.2029 || timer: 0.0887 sec.
iter 93870 || Loss: 1.1455 || timer: 0.0936 sec.
iter 93880 || Loss: 0.9277 || timer: 0.0921 sec.
iter 93890 || Loss: 1.2459 || timer: 0.0841 sec.
iter 93900 || Loss: 1.5458 || timer: 0.0972 sec.
iter 93910 || Loss: 0.8334 || timer: 0.0838 sec.
iter 93920 || Loss: 0.7263 || timer: 0.0899 sec.
iter 93930 || Loss: 0.8201 || timer: 0.0820 sec.
iter 93940 || Loss: 0.8589 || timer: 0.0168 sec.
iter 93950 || Loss: 0.8802 || timer: 0.0836 sec.
iter 93960 || Loss: 1.4555 || timer: 0.0997 sec.
iter 93970 || Loss: 0.9609 || timer: 0.0892 sec.
iter 93980 || Loss: 1.1580 || timer: 0.0901 sec.
iter 93990 || Loss: 0.9933 || timer: 0.0893 sec.
iter 94000 || Loss: 0.9700 || timer: 0.1022 sec.
iter 94010 || Loss: 0.9132 || timer: 0.0860 sec.
iter 94020 || Loss: 1.2958 || timer: 0.0820 sec.
iter 94030 || Loss: 1.5584 || timer: 0.0900 sec.
iter 94040 || Loss: 0.8263 || timer: 0.1100 sec.
iter 94050 || Loss: 1.1520 || timer: 0.0806 sec.
iter 94060 || Loss: 0.6284 || timer: 0.0888 sec.
iter 94070 || Loss: 1.0012 || timer: 0.0885 sec.
iter 94080 || Loss: 1.2705 || timer: 0.0817 sec.
iter 94090 || Loss: 1.0115 || timer: 0.1036 sec.
iter 94100 || Loss: 0.8613 || timer: 0.1022 sec.
iter 94110 || Loss: 0.8892 || timer: 0.0810 sec.
iter 94120 || Loss: 1.1868 || timer: 0.1096 sec.
iter 94130 || Loss: 1.0109 || timer: 0.0890 sec.
iter 94140 || Loss: 1.2501 || timer: 0.0895 sec.
iter 94150 || Loss: 1.1528 || timer: 0.0822 sec.
iter 94160 || Loss: 1.0744 || timer: 0.0825 sec.
iter 94170 || Loss: 0.8754 || timer: 0.0888 sec.
iter 94180 || Loss: 0.9213 || timer: 0.0908 sec.
iter 94190 || Loss: 1.3056 || timer: 0.0869 sec.
iter 94200 || Loss: 0.7412 || timer: 0.0878 sec.
iter 94210 || Loss: 1.3973 || timer: 0.0814 sec.
iter 94220 || Loss: 1.0130 || timer: 0.1004 sec.
iter 94230 || Loss: 1.1975 || timer: 0.0853 sec.
iter 94240 || Loss: 0.7799 || timer: 0.0893 sec.
iter 94250 || Loss: 0.9509 || timer: 0.0815 sec.
iter 94260 || Loss: 1.1288 || timer: 0.0962 sec.
iter 94270 || Loss: 1.0138 || timer: 0.0178 sec.
iter 94280 || Loss: 2.2338 || timer: 0.0901 sec.
iter 94290 || Loss: 1.3080 || timer: 0.0831 sec.
iter 94300 || Loss: 0.8448 || timer: 0.1003 sec.
iter 94310 || Loss: 1.1023 || timer: 0.0825 sec.
iter 94320 || Loss: 1.2534 || timer: 0.0900 sec.
iter 94330 || Loss: 0.8842 || timer: 0.0824 sec.
iter 94340 || Loss: 1.3461 || timer: 0.0868 sec.
iter 94350 || Loss: 1.3534 || timer: 0.1161 sec.
iter 94360 || Loss: 1.3045 || timer: 0.0859 sec.
iter 94370 || Loss: 1.4905 || timer: 0.0944 sec.
iter 94380 || Loss: 0.9191 || timer: 0.0982 sec.
iter 94390 || Loss: 1.1960 || timer: 0.1060 sec.
iter 94400 || Loss: 1.3238 || timer: 0.0819 sec.
iter 94410 || Loss: 1.1631 || timer: 0.0816 sec.
iter 94420 || Loss: 1.1909 || timer: 0.0889 sec.
iter 94430 || Loss: 0.8366 || timer: 0.1120 sec.
iter 94440 || Loss: 1.0194 || timer: 0.1182 sec.
iter 94450 || Loss: 1.1086 || timer: 0.0938 sec.
iter 94460 || Loss: 1.1236 || timer: 0.0960 sec.
iter 94470 || Loss: 0.9445 || timer: 0.0926 sec.
iter 94480 || Loss: 0.9526 || timer: 0.0858 sec.
iter 94490 || Loss: 0.9595 || timer: 0.0892 sec.
iter 94500 || Loss: 0.6102 || timer: 0.0902 sec.
iter 94510 || Loss: 0.9669 || timer: 0.0818 sec.
iter 94520 || Loss: 0.9113 || timer: 0.0889 sec.
iter 94530 || Loss: 1.2789 || timer: 0.0909 sec.
iter 94540 || Loss: 0.9594 || timer: 0.0912 sec.
iter 94550 || Loss: 0.9449 || timer: 0.0941 sec.
iter 94560 || Loss: 1.0426 || timer: 0.0943 sec.
iter 94570 || Loss: 1.1884 || timer: 0.0896 sec.
iter 94580 || Loss: 1.3520 || timer: 0.0939 sec.
iter 94590 || Loss: 1.1440 || timer: 0.0920 sec.
iter 94600 || Loss: 1.1171 || timer: 0.0256 sec.
iter 94610 || Loss: 2.5322 || timer: 0.0886 sec.
iter 94620 || Loss: 1.0310 || timer: 0.0870 sec.
iter 94630 || Loss: 1.2206 || timer: 0.0806 sec.
iter 94640 || Loss: 0.9439 || timer: 0.0887 sec.
iter 94650 || Loss: 1.1341 || timer: 0.0900 sec.
iter 94660 || Loss: 1.2430 || timer: 0.1085 sec.
iter 94670 || Loss: 0.8264 || timer: 0.0824 sec.
iter 94680 || Loss: 1.5960 || timer: 0.0916 sec.
iter 94690 || Loss: 0.9446 || timer: 0.0961 sec.
iter 94700 || Loss: 1.0336 || timer: 0.0993 sec.
iter 94710 || Loss: 1.1866 || timer: 0.0813 sec.
iter 94720 || Loss: 1.2164 || timer: 0.0821 sec.
iter 94730 || Loss: 1.2935 || timer: 0.0809 sec.
iter 94740 || Loss: 1.4194 || timer: 0.1126 sec.
iter 94750 || Loss: 1.2350 || timer: 0.0825 sec.
iter 94760 || Loss: 1.3821 || timer: 0.1120 sec.
iter 94770 || Loss: 0.8148 || timer: 0.0912 sec.
iter 94780 || Loss: 0.8974 || timer: 0.1098 sec.
iter 94790 || Loss: 0.9405 || timer: 0.0988 sec.
iter 94800 || Loss: 1.0635 || timer: 0.1173 sec.
iter 94810 || Loss: 0.8684 || timer: 0.1107 sec.
iter 94820 || Loss: 1.7484 || timer: 0.0936 sec.
iter 94830 || Loss: 1.4157 || timer: 0.0832 sec.
iter 94840 || Loss: 1.2623 || timer: 0.0816 sec.
iter 94850 || Loss: 1.0149 || timer: 0.0819 sec.
iter 94860 || Loss: 0.8459 || timer: 0.0869 sec.
iter 94870 || Loss: 1.0040 || timer: 0.0806 sec.
iter 94880 || Loss: 0.9258 || timer: 0.1013 sec.
iter 94890 || Loss: 1.0781 || timer: 0.0819 sec.
iter 94900 || Loss: 1.1255 || timer: 0.0915 sec.
iter 94910 || Loss: 1.1135 || timer: 0.0992 sec.
iter 94920 || Loss: 1.2490 || timer: 0.0910 sec.
iter 94930 || Loss: 1.0646 || timer: 0.0147 sec.
iter 94940 || Loss: 0.6471 || timer: 0.0825 sec.
iter 94950 || Loss: 0.8499 || timer: 0.0821 sec.
iter 94960 || Loss: 1.0916 || timer: 0.1062 sec.
iter 94970 || Loss: 0.7580 || timer: 0.0868 sec.
iter 94980 || Loss: 1.0134 || timer: 0.0911 sec.
iter 94990 || Loss: 1.2271 || timer: 0.0881 sec.
iter 95000 || Loss: 1.1348 || Saving state, iter: 95000
timer: 0.0850 sec.
iter 95010 || Loss: 1.3676 || timer: 0.0823 sec.
iter 95020 || Loss: 0.9746 || timer: 0.0834 sec.
iter 95030 || Loss: 1.0030 || timer: 0.1000 sec.
iter 95040 || Loss: 0.8733 || timer: 0.0836 sec.
iter 95050 || Loss: 1.3103 || timer: 0.1117 sec.
iter 95060 || Loss: 0.9152 || timer: 0.0884 sec.
iter 95070 || Loss: 1.1735 || timer: 0.0899 sec.
iter 95080 || Loss: 1.2677 || timer: 0.1019 sec.
iter 95090 || Loss: 1.5291 || timer: 0.0817 sec.
iter 95100 || Loss: 0.9911 || timer: 0.0839 sec.
iter 95110 || Loss: 1.3603 || timer: 0.0938 sec.
iter 95120 || Loss: 0.9715 || timer: 0.0910 sec.
iter 95130 || Loss: 1.0332 || timer: 0.0817 sec.
iter 95140 || Loss: 1.4437 || timer: 0.0888 sec.
iter 95150 || Loss: 0.9705 || timer: 0.0900 sec.
iter 95160 || Loss: 1.1129 || timer: 0.0909 sec.
iter 95170 || Loss: 0.6338 || timer: 0.0920 sec.
iter 95180 || Loss: 0.7221 || timer: 0.0822 sec.
iter 95190 || Loss: 1.1540 || timer: 0.1084 sec.
iter 95200 || Loss: 0.9645 || timer: 0.0877 sec.
iter 95210 || Loss: 1.0881 || timer: 0.1082 sec.
iter 95220 || Loss: 0.9716 || timer: 0.0825 sec.
iter 95230 || Loss: 1.3276 || timer: 0.0899 sec.
iter 95240 || Loss: 1.4899 || timer: 0.1107 sec.
iter 95250 || Loss: 0.9052 || timer: 0.1100 sec.
iter 95260 || Loss: 0.8335 || timer: 0.0173 sec.
iter 95270 || Loss: 0.9726 || timer: 0.0820 sec.
iter 95280 || Loss: 1.3082 || timer: 0.0836 sec.
iter 95290 || Loss: 1.3887 || timer: 0.0892 sec.
iter 95300 || Loss: 1.1627 || timer: 0.0811 sec.
iter 95310 || Loss: 1.0131 || timer: 0.1080 sec.
iter 95320 || Loss: 1.2508 || timer: 0.0885 sec.
iter 95330 || Loss: 0.9133 || timer: 0.0812 sec.
iter 95340 || Loss: 1.0090 || timer: 0.0897 sec.
iter 95350 || Loss: 0.9675 || timer: 0.0803 sec.
iter 95360 || Loss: 0.8769 || timer: 0.0973 sec.
iter 95370 || Loss: 0.9563 || timer: 0.0804 sec.
iter 95380 || Loss: 0.8596 || timer: 0.0885 sec.
iter 95390 || Loss: 1.1649 || timer: 0.0861 sec.
iter 95400 || Loss: 0.9865 || timer: 0.0979 sec.
iter 95410 || Loss: 0.8453 || timer: 0.0886 sec.
iter 95420 || Loss: 1.1461 || timer: 0.0915 sec.
iter 95430 || Loss: 0.7548 || timer: 0.0881 sec.
iter 95440 || Loss: 1.1077 || timer: 0.0967 sec.
iter 95450 || Loss: 1.5195 || timer: 0.0874 sec.
iter 95460 || Loss: 1.1379 || timer: 0.1167 sec.
iter 95470 || Loss: 0.8754 || timer: 0.1000 sec.
iter 95480 || Loss: 1.3374 || timer: 0.0900 sec.
iter 95490 || Loss: 1.2437 || timer: 0.0838 sec.
iter 95500 || Loss: 0.9374 || timer: 0.0879 sec.
iter 95510 || Loss: 1.1625 || timer: 0.0872 sec.
iter 95520 || Loss: 1.4270 || timer: 0.0831 sec.
iter 95530 || Loss: 0.8781 || timer: 0.0855 sec.
iter 95540 || Loss: 1.1567 || timer: 0.0915 sec.
iter 95550 || Loss: 1.0986 || timer: 0.0810 sec.
iter 95560 || Loss: 1.3778 || timer: 0.0898 sec.
iter 95570 || Loss: 1.2672 || timer: 0.0882 sec.
iter 95580 || Loss: 0.9018 || timer: 0.0839 sec.
iter 95590 || Loss: 1.0633 || timer: 0.0235 sec.
iter 95600 || Loss: 0.8825 || timer: 0.1021 sec.
iter 95610 || Loss: 1.2769 || timer: 0.1186 sec.
iter 95620 || Loss: 1.0856 || timer: 0.1043 sec.
iter 95630 || Loss: 0.9356 || timer: 0.0889 sec.
iter 95640 || Loss: 1.0712 || timer: 0.0981 sec.
iter 95650 || Loss: 1.1254 || timer: 0.0881 sec.
iter 95660 || Loss: 0.8438 || timer: 0.1142 sec.
iter 95670 || Loss: 0.8071 || timer: 0.1339 sec.
iter 95680 || Loss: 1.0246 || timer: 0.0891 sec.
iter 95690 || Loss: 0.9621 || timer: 0.1089 sec.
iter 95700 || Loss: 0.9721 || timer: 0.0801 sec.
iter 95710 || Loss: 1.2508 || timer: 0.0819 sec.
iter 95720 || Loss: 1.1347 || timer: 0.0807 sec.
iter 95730 || Loss: 1.0533 || timer: 0.0832 sec.
iter 95740 || Loss: 1.0158 || timer: 0.0885 sec.
iter 95750 || Loss: 1.0995 || timer: 0.0883 sec.
iter 95760 || Loss: 1.0348 || timer: 0.0827 sec.
iter 95770 || Loss: 0.9044 || timer: 0.0982 sec.
iter 95780 || Loss: 1.0099 || timer: 0.0885 sec.
iter 95790 || Loss: 1.1197 || timer: 0.0813 sec.
iter 95800 || Loss: 1.1076 || timer: 0.0907 sec.
iter 95810 || Loss: 0.8025 || timer: 0.0802 sec.
iter 95820 || Loss: 0.9598 || timer: 0.0871 sec.
iter 95830 || Loss: 1.0446 || timer: 0.0857 sec.
iter 95840 || Loss: 0.8640 || timer: 0.0920 sec.
iter 95850 || Loss: 1.1415 || timer: 0.0821 sec.
iter 95860 || Loss: 1.3293 || timer: 0.0820 sec.
iter 95870 || Loss: 0.8649 || timer: 0.0827 sec.
iter 95880 || Loss: 1.0341 || timer: 0.1137 sec.
iter 95890 || Loss: 0.8977 || timer: 0.1086 sec.
iter 95900 || Loss: 1.0385 || timer: 0.0837 sec.
iter 95910 || Loss: 0.9592 || timer: 0.1162 sec.
iter 95920 || Loss: 0.8529 || timer: 0.0230 sec.
iter 95930 || Loss: 1.3973 || timer: 0.0830 sec.
iter 95940 || Loss: 1.1606 || timer: 0.0907 sec.
iter 95950 || Loss: 1.1634 || timer: 0.0816 sec.
iter 95960 || Loss: 1.4257 || timer: 0.0925 sec.
iter 95970 || Loss: 1.2698 || timer: 0.0893 sec.
iter 95980 || Loss: 1.1921 || timer: 0.0959 sec.
iter 95990 || Loss: 1.0605 || timer: 0.0891 sec.
iter 96000 || Loss: 1.0808 || timer: 0.0909 sec.
iter 96010 || Loss: 1.2667 || timer: 0.0960 sec.
iter 96020 || Loss: 0.8841 || timer: 0.0990 sec.
iter 96030 || Loss: 1.1405 || timer: 0.0886 sec.
iter 96040 || Loss: 0.7409 || timer: 0.0878 sec.
iter 96050 || Loss: 0.8696 || timer: 0.0885 sec.
iter 96060 || Loss: 1.3246 || timer: 0.0964 sec.
iter 96070 || Loss: 1.0069 || timer: 0.1002 sec.
iter 96080 || Loss: 0.9895 || timer: 0.0800 sec.
iter 96090 || Loss: 0.8528 || timer: 0.0882 sec.
iter 96100 || Loss: 1.0645 || timer: 0.0872 sec.
iter 96110 || Loss: 1.3512 || timer: 0.0892 sec.
iter 96120 || Loss: 1.5169 || timer: 0.0904 sec.
iter 96130 || Loss: 0.9250 || timer: 0.0833 sec.
iter 96140 || Loss: 0.9927 || timer: 0.0893 sec.
iter 96150 || Loss: 0.8218 || timer: 0.0901 sec.
iter 96160 || Loss: 1.1168 || timer: 0.1045 sec.
iter 96170 || Loss: 1.0191 || timer: 0.0822 sec.
iter 96180 || Loss: 0.8022 || timer: 0.0902 sec.
iter 96190 || Loss: 0.7978 || timer: 0.0812 sec.
iter 96200 || Loss: 1.1335 || timer: 0.0914 sec.
iter 96210 || Loss: 1.0437 || timer: 0.0872 sec.
iter 96220 || Loss: 1.1443 || timer: 0.0906 sec.
iter 96230 || Loss: 1.1225 || timer: 0.0918 sec.
iter 96240 || Loss: 0.8230 || timer: 0.0837 sec.
iter 96250 || Loss: 0.8626 || timer: 0.0233 sec.
iter 96260 || Loss: 0.3384 || timer: 0.0920 sec.
iter 96270 || Loss: 1.2334 || timer: 0.0904 sec.
iter 96280 || Loss: 1.2146 || timer: 0.0810 sec.
iter 96290 || Loss: 1.1412 || timer: 0.0885 sec.
iter 96300 || Loss: 1.2803 || timer: 0.0810 sec.
iter 96310 || Loss: 0.7808 || timer: 0.0886 sec.
iter 96320 || Loss: 0.8567 || timer: 0.0806 sec.
iter 96330 || Loss: 1.0921 || timer: 0.0994 sec.
iter 96340 || Loss: 1.2845 || timer: 0.0863 sec.
iter 96350 || Loss: 1.0923 || timer: 0.1031 sec.
iter 96360 || Loss: 0.8450 || timer: 0.0894 sec.
iter 96370 || Loss: 1.2372 || timer: 0.0798 sec.
iter 96380 || Loss: 1.2850 || timer: 0.0817 sec.
iter 96390 || Loss: 0.8974 || timer: 0.0878 sec.
iter 96400 || Loss: 1.3715 || timer: 0.1034 sec.
iter 96410 || Loss: 0.7711 || timer: 0.0796 sec.
iter 96420 || Loss: 0.8908 || timer: 0.0814 sec.
iter 96430 || Loss: 0.9211 || timer: 0.1147 sec.
iter 96440 || Loss: 1.0325 || timer: 0.1027 sec.
iter 96450 || Loss: 0.9567 || timer: 0.0995 sec.
iter 96460 || Loss: 1.1553 || timer: 0.1049 sec.
iter 96470 || Loss: 0.7977 || timer: 0.0822 sec.
iter 96480 || Loss: 0.9792 || timer: 0.0878 sec.
iter 96490 || Loss: 0.9484 || timer: 0.0886 sec.
iter 96500 || Loss: 0.9261 || timer: 0.0899 sec.
iter 96510 || Loss: 0.9885 || timer: 0.0914 sec.
iter 96520 || Loss: 0.9457 || timer: 0.0893 sec.
iter 96530 || Loss: 1.0441 || timer: 0.1133 sec.
iter 96540 || Loss: 1.7198 || timer: 0.1183 sec.
iter 96550 || Loss: 0.6908 || timer: 0.0807 sec.
iter 96560 || Loss: 1.0530 || timer: 0.1042 sec.
iter 96570 || Loss: 0.9662 || timer: 0.0823 sec.
iter 96580 || Loss: 1.0164 || timer: 0.0172 sec.
iter 96590 || Loss: 0.5028 || timer: 0.0926 sec.
iter 96600 || Loss: 0.8903 || timer: 0.0913 sec.
iter 96610 || Loss: 1.3007 || timer: 0.1022 sec.
iter 96620 || Loss: 1.0954 || timer: 0.0915 sec.
iter 96630 || Loss: 1.0300 || timer: 0.0884 sec.
iter 96640 || Loss: 1.4493 || timer: 0.0883 sec.
iter 96650 || Loss: 1.1070 || timer: 0.0830 sec.
iter 96660 || Loss: 1.1673 || timer: 0.0917 sec.
iter 96670 || Loss: 0.6607 || timer: 0.1054 sec.
iter 96680 || Loss: 1.1476 || timer: 0.1007 sec.
iter 96690 || Loss: 0.7990 || timer: 0.0806 sec.
iter 96700 || Loss: 1.3201 || timer: 0.0810 sec.
iter 96710 || Loss: 1.0766 || timer: 0.0899 sec.
iter 96720 || Loss: 1.0110 || timer: 0.0846 sec.
iter 96730 || Loss: 0.7472 || timer: 0.0871 sec.
iter 96740 || Loss: 1.1555 || timer: 0.0897 sec.
iter 96750 || Loss: 1.1075 || timer: 0.0834 sec.
iter 96760 || Loss: 1.1096 || timer: 0.0877 sec.
iter 96770 || Loss: 0.7722 || timer: 0.0825 sec.
iter 96780 || Loss: 1.0532 || timer: 0.0893 sec.
iter 96790 || Loss: 1.0052 || timer: 0.0899 sec.
iter 96800 || Loss: 1.3402 || timer: 0.0890 sec.
iter 96810 || Loss: 1.1818 || timer: 0.0891 sec.
iter 96820 || Loss: 0.9626 || timer: 0.1081 sec.
iter 96830 || Loss: 1.0630 || timer: 0.0831 sec.
iter 96840 || Loss: 0.7605 || timer: 0.0833 sec.
iter 96850 || Loss: 1.2322 || timer: 0.0911 sec.
iter 96860 || Loss: 1.6174 || timer: 0.0816 sec.
iter 96870 || Loss: 1.5353 || timer: 0.1074 sec.
iter 96880 || Loss: 0.9748 || timer: 0.0886 sec.
iter 96890 || Loss: 1.2871 || timer: 0.0871 sec.
iter 96900 || Loss: 1.6440 || timer: 0.0855 sec.
iter 96910 || Loss: 1.1698 || timer: 0.0168 sec.
iter 96920 || Loss: 2.4128 || timer: 0.0997 sec.
iter 96930 || Loss: 0.9409 || timer: 0.0814 sec.
iter 96940 || Loss: 1.3346 || timer: 0.0810 sec.
iter 96950 || Loss: 1.2017 || timer: 0.0814 sec.
iter 96960 || Loss: 0.9796 || timer: 0.0845 sec.
iter 96970 || Loss: 1.2506 || timer: 0.0877 sec.
iter 96980 || Loss: 1.1231 || timer: 0.0920 sec.
iter 96990 || Loss: 1.0346 || timer: 0.0877 sec.
iter 97000 || Loss: 0.7961 || timer: 0.0991 sec.
iter 97010 || Loss: 0.7279 || timer: 0.0973 sec.
iter 97020 || Loss: 1.2592 || timer: 0.0912 sec.
iter 97030 || Loss: 0.7959 || timer: 0.0827 sec.
iter 97040 || Loss: 1.1674 || timer: 0.0831 sec.
iter 97050 || Loss: 0.8863 || timer: 0.0891 sec.
iter 97060 || Loss: 1.1056 || timer: 0.0884 sec.
iter 97070 || Loss: 0.9701 || timer: 0.0908 sec.
iter 97080 || Loss: 1.1283 || timer: 0.0890 sec.
iter 97090 || Loss: 1.0028 || timer: 0.0880 sec.
iter 97100 || Loss: 0.9419 || timer: 0.0917 sec.
iter 97110 || Loss: 0.8115 || timer: 0.0809 sec.
iter 97120 || Loss: 0.8525 || timer: 0.1124 sec.
iter 97130 || Loss: 1.2696 || timer: 0.0889 sec.
iter 97140 || Loss: 0.8027 || timer: 0.0897 sec.
iter 97150 || Loss: 1.4453 || timer: 0.0910 sec.
iter 97160 || Loss: 1.1635 || timer: 0.1002 sec.
iter 97170 || Loss: 1.3250 || timer: 0.0893 sec.
iter 97180 || Loss: 1.2287 || timer: 0.0831 sec.
iter 97190 || Loss: 1.1871 || timer: 0.0834 sec.
iter 97200 || Loss: 1.0975 || timer: 0.0851 sec.
iter 97210 || Loss: 0.7771 || timer: 0.1018 sec.
iter 97220 || Loss: 1.3118 || timer: 0.0952 sec.
iter 97230 || Loss: 2.0938 || timer: 0.0821 sec.
iter 97240 || Loss: 1.0210 || timer: 0.0160 sec.
iter 97250 || Loss: 0.2513 || timer: 0.0806 sec.
iter 97260 || Loss: 1.0952 || timer: 0.0912 sec.
iter 97270 || Loss: 1.0550 || timer: 0.0902 sec.
iter 97280 || Loss: 1.0946 || timer: 0.1036 sec.
iter 97290 || Loss: 0.8649 || timer: 0.0858 sec.
iter 97300 || Loss: 0.8042 || timer: 0.0958 sec.
iter 97310 || Loss: 1.1227 || timer: 0.1031 sec.
iter 97320 || Loss: 1.4360 || timer: 0.0901 sec.
iter 97330 || Loss: 1.3286 || timer: 0.0826 sec.
iter 97340 || Loss: 1.1522 || timer: 0.0866 sec.
iter 97350 || Loss: 1.0227 || timer: 0.0888 sec.
iter 97360 || Loss: 1.5231 || timer: 0.0894 sec.
iter 97370 || Loss: 1.2114 || timer: 0.0881 sec.
iter 97380 || Loss: 1.4120 || timer: 0.0950 sec.
iter 97390 || Loss: 0.8618 || timer: 0.0819 sec.
iter 97400 || Loss: 1.1068 || timer: 0.0872 sec.
iter 97410 || Loss: 0.9523 || timer: 0.0894 sec.
iter 97420 || Loss: 0.8658 || timer: 0.0912 sec.
iter 97430 || Loss: 0.9035 || timer: 0.0900 sec.
iter 97440 || Loss: 1.2032 || timer: 0.0820 sec.
iter 97450 || Loss: 0.9805 || timer: 0.0833 sec.
iter 97460 || Loss: 1.1288 || timer: 0.0893 sec.
iter 97470 || Loss: 1.1159 || timer: 0.0857 sec.
iter 97480 || Loss: 0.8151 || timer: 0.1122 sec.
iter 97490 || Loss: 1.1167 || timer: 0.0884 sec.
iter 97500 || Loss: 1.4017 || timer: 0.0946 sec.
iter 97510 || Loss: 1.0905 || timer: 0.0807 sec.
iter 97520 || Loss: 1.0412 || timer: 0.0941 sec.
iter 97530 || Loss: 0.6764 || timer: 0.0871 sec.
iter 97540 || Loss: 1.0893 || timer: 0.1174 sec.
iter 97550 || Loss: 1.0784 || timer: 0.1029 sec.
iter 97560 || Loss: 0.8568 || timer: 0.0812 sec.
iter 97570 || Loss: 0.9932 || timer: 0.0161 sec.
iter 97580 || Loss: 0.3553 || timer: 0.0883 sec.
iter 97590 || Loss: 1.1102 || timer: 0.0845 sec.
iter 97600 || Loss: 1.2535 || timer: 0.1035 sec.
iter 97610 || Loss: 0.9102 || timer: 0.1373 sec.
iter 97620 || Loss: 1.0428 || timer: 0.0904 sec.
iter 97630 || Loss: 0.9182 || timer: 0.0824 sec.
iter 97640 || Loss: 1.1161 || timer: 0.0953 sec.
iter 97650 || Loss: 1.1780 || timer: 0.0905 sec.
iter 97660 || Loss: 1.1705 || timer: 0.0977 sec.
iter 97670 || Loss: 1.3335 || timer: 0.0984 sec.
iter 97680 || Loss: 1.1524 || timer: 0.0857 sec.
iter 97690 || Loss: 1.1305 || timer: 0.0914 sec.
iter 97700 || Loss: 1.0070 || timer: 0.0896 sec.
iter 97710 || Loss: 0.9220 || timer: 0.0980 sec.
iter 97720 || Loss: 1.0206 || timer: 0.0826 sec.
iter 97730 || Loss: 1.1535 || timer: 0.0988 sec.
iter 97740 || Loss: 1.0182 || timer: 0.0980 sec.
iter 97750 || Loss: 1.4006 || timer: 0.1021 sec.
iter 97760 || Loss: 1.0772 || timer: 0.0808 sec.
iter 97770 || Loss: 1.0897 || timer: 0.0823 sec.
iter 97780 || Loss: 0.9188 || timer: 0.0926 sec.
iter 97790 || Loss: 1.1151 || timer: 0.0903 sec.
iter 97800 || Loss: 1.2576 || timer: 0.0917 sec.
iter 97810 || Loss: 1.0674 || timer: 0.0908 sec.
iter 97820 || Loss: 1.4703 || timer: 0.0900 sec.
iter 97830 || Loss: 1.2683 || timer: 0.0920 sec.
iter 97840 || Loss: 1.0956 || timer: 0.0901 sec.
iter 97850 || Loss: 2.0137 || timer: 0.0916 sec.
iter 97860 || Loss: 1.1615 || timer: 0.1130 sec.
iter 97870 || Loss: 1.0850 || timer: 0.0815 sec.
iter 97880 || Loss: 1.2216 || timer: 0.0901 sec.
iter 97890 || Loss: 0.8358 || timer: 0.1195 sec.
iter 97900 || Loss: 1.2315 || timer: 0.0271 sec.
iter 97910 || Loss: 3.1837 || timer: 0.0900 sec.
iter 97920 || Loss: 1.6168 || timer: 0.0931 sec.
iter 97930 || Loss: 1.0552 || timer: 0.0902 sec.
iter 97940 || Loss: 1.2627 || timer: 0.1093 sec.
iter 97950 || Loss: 1.3770 || timer: 0.0872 sec.
iter 97960 || Loss: 1.3338 || timer: 0.0984 sec.
iter 97970 || Loss: 1.3780 || timer: 0.0810 sec.
iter 97980 || Loss: 1.1775 || timer: 0.0809 sec.
iter 97990 || Loss: 1.0733 || timer: 0.0919 sec.
iter 98000 || Loss: 1.0148 || timer: 0.0985 sec.
iter 98010 || Loss: 1.1514 || timer: 0.0816 sec.
iter 98020 || Loss: 0.9200 || timer: 0.0891 sec.
iter 98030 || Loss: 1.2608 || timer: 0.0878 sec.
iter 98040 || Loss: 0.9179 || timer: 0.0891 sec.
iter 98050 || Loss: 1.2318 || timer: 0.0803 sec.
iter 98060 || Loss: 0.8358 || timer: 0.0860 sec.
iter 98070 || Loss: 0.6933 || timer: 0.1002 sec.
iter 98080 || Loss: 1.0825 || timer: 0.0915 sec.
iter 98090 || Loss: 0.9224 || timer: 0.0826 sec.
iter 98100 || Loss: 0.6962 || timer: 0.0809 sec.
iter 98110 || Loss: 0.9718 || timer: 0.1349 sec.
iter 98120 || Loss: 1.3227 || timer: 0.0907 sec.
iter 98130 || Loss: 1.1785 || timer: 0.0823 sec.
iter 98140 || Loss: 1.1170 || timer: 0.0880 sec.
iter 98150 || Loss: 0.9575 || timer: 0.0899 sec.
iter 98160 || Loss: 1.1235 || timer: 0.1124 sec.
iter 98170 || Loss: 0.9449 || timer: 0.0802 sec.
iter 98180 || Loss: 1.3514 || timer: 0.0820 sec.
iter 98190 || Loss: 1.0032 || timer: 0.1132 sec.
iter 98200 || Loss: 0.7392 || timer: 0.0897 sec.
iter 98210 || Loss: 1.5283 || timer: 0.1081 sec.
iter 98220 || Loss: 1.2904 || timer: 0.1247 sec.
iter 98230 || Loss: 1.0972 || timer: 0.0166 sec.
iter 98240 || Loss: 2.0475 || timer: 0.0810 sec.
iter 98250 || Loss: 1.7050 || timer: 0.0817 sec.
iter 98260 || Loss: 1.5167 || timer: 0.0900 sec.
iter 98270 || Loss: 0.8745 || timer: 0.1127 sec.
iter 98280 || Loss: 0.9718 || timer: 0.0869 sec.
iter 98290 || Loss: 1.0611 || timer: 0.0887 sec.
iter 98300 || Loss: 0.9651 || timer: 0.0873 sec.
iter 98310 || Loss: 1.0369 || timer: 0.0824 sec.
iter 98320 || Loss: 1.2256 || timer: 0.0877 sec.
iter 98330 || Loss: 1.0454 || timer: 0.1120 sec.
iter 98340 || Loss: 0.8214 || timer: 0.0903 sec.
iter 98350 || Loss: 0.9069 || timer: 0.0877 sec.
iter 98360 || Loss: 0.7961 || timer: 0.1125 sec.
iter 98370 || Loss: 0.9730 || timer: 0.0887 sec.
iter 98380 || Loss: 1.1691 || timer: 0.0811 sec.
iter 98390 || Loss: 1.1703 || timer: 0.1150 sec.
iter 98400 || Loss: 1.1899 || timer: 0.1084 sec.
iter 98410 || Loss: 1.1491 || timer: 0.0815 sec.
iter 98420 || Loss: 1.0863 || timer: 0.0981 sec.
iter 98430 || Loss: 0.6927 || timer: 0.0833 sec.
iter 98440 || Loss: 0.7218 || timer: 0.0925 sec.
iter 98450 || Loss: 1.0826 || timer: 0.0874 sec.
iter 98460 || Loss: 0.9588 || timer: 0.0884 sec.
iter 98470 || Loss: 0.7892 || timer: 0.0845 sec.
iter 98480 || Loss: 0.9265 || timer: 0.1106 sec.
iter 98490 || Loss: 0.8987 || timer: 0.0898 sec.
iter 98500 || Loss: 1.0690 || timer: 0.0878 sec.
iter 98510 || Loss: 1.1640 || timer: 0.1027 sec.
iter 98520 || Loss: 1.0053 || timer: 0.0802 sec.
iter 98530 || Loss: 1.4297 || timer: 0.0812 sec.
iter 98540 || Loss: 1.1841 || timer: 0.0899 sec.
iter 98550 || Loss: 1.0692 || timer: 0.1092 sec.
iter 98560 || Loss: 0.8301 || timer: 0.0289 sec.
iter 98570 || Loss: 1.8326 || timer: 0.1050 sec.
iter 98580 || Loss: 1.2457 || timer: 0.0934 sec.
iter 98590 || Loss: 1.3058 || timer: 0.0833 sec.
iter 98600 || Loss: 1.2577 || timer: 0.0871 sec.
iter 98610 || Loss: 0.9672 || timer: 0.0887 sec.
iter 98620 || Loss: 1.0155 || timer: 0.0823 sec.
iter 98630 || Loss: 0.7682 || timer: 0.0879 sec.
iter 98640 || Loss: 1.0067 || timer: 0.0808 sec.
iter 98650 || Loss: 0.8966 || timer: 0.0943 sec.
iter 98660 || Loss: 0.8495 || timer: 0.0992 sec.
iter 98670 || Loss: 1.2157 || timer: 0.0815 sec.
iter 98680 || Loss: 1.0304 || timer: 0.1127 sec.
iter 98690 || Loss: 1.0198 || timer: 0.0812 sec.
iter 98700 || Loss: 0.7620 || timer: 0.0901 sec.
iter 98710 || Loss: 0.8392 || timer: 0.0820 sec.
iter 98720 || Loss: 0.9824 || timer: 0.0975 sec.
iter 98730 || Loss: 1.0955 || timer: 0.1041 sec.
iter 98740 || Loss: 0.7278 || timer: 0.0835 sec.
iter 98750 || Loss: 0.9365 || timer: 0.1089 sec.
iter 98760 || Loss: 1.2106 || timer: 0.0904 sec.
iter 98770 || Loss: 0.8304 || timer: 0.1111 sec.
iter 98780 || Loss: 1.1614 || timer: 0.0908 sec.
iter 98790 || Loss: 0.8245 || timer: 0.0837 sec.
iter 98800 || Loss: 1.2448 || timer: 0.0896 sec.
iter 98810 || Loss: 1.0173 || timer: 0.0890 sec.
iter 98820 || Loss: 1.1100 || timer: 0.0954 sec.
iter 98830 || Loss: 0.8073 || timer: 0.1073 sec.
iter 98840 || Loss: 1.1686 || timer: 0.0819 sec.
iter 98850 || Loss: 1.1143 || timer: 0.1180 sec.
iter 98860 || Loss: 1.4270 || timer: 0.0835 sec.
iter 98870 || Loss: 0.8795 || timer: 0.0813 sec.
iter 98880 || Loss: 0.7920 || timer: 0.0861 sec.
iter 98890 || Loss: 0.8228 || timer: 0.0160 sec.
iter 98900 || Loss: 0.5706 || timer: 0.0958 sec.
iter 98910 || Loss: 0.8667 || timer: 0.1125 sec.
iter 98920 || Loss: 1.1276 || timer: 0.0914 sec.
iter 98930 || Loss: 0.9952 || timer: 0.0825 sec.
iter 98940 || Loss: 1.2838 || timer: 0.0903 sec.
iter 98950 || Loss: 0.8630 || timer: 0.0819 sec.
iter 98960 || Loss: 1.3323 || timer: 0.0810 sec.
iter 98970 || Loss: 0.8405 || timer: 0.0814 sec.
iter 98980 || Loss: 1.2768 || timer: 0.0913 sec.
iter 98990 || Loss: 1.2250 || timer: 0.1028 sec.
iter 99000 || Loss: 0.8852 || timer: 0.0887 sec.
iter 99010 || Loss: 0.7883 || timer: 0.0914 sec.
iter 99020 || Loss: 0.9486 || timer: 0.0884 sec.
iter 99030 || Loss: 0.7944 || timer: 0.0817 sec.
iter 99040 || Loss: 0.9311 || timer: 0.0902 sec.
iter 99050 || Loss: 0.9483 || timer: 0.0890 sec.
iter 99060 || Loss: 0.9155 || timer: 0.1568 sec.
iter 99070 || Loss: 0.9731 || timer: 0.1115 sec.
iter 99080 || Loss: 0.9390 || timer: 0.0906 sec.
iter 99090 || Loss: 1.2504 || timer: 0.0960 sec.
iter 99100 || Loss: 1.2043 || timer: 0.0829 sec.
iter 99110 || Loss: 1.1637 || timer: 0.0947 sec.
iter 99120 || Loss: 1.1336 || timer: 0.0886 sec.
iter 99130 || Loss: 1.2081 || timer: 0.0910 sec.
iter 99140 || Loss: 0.9487 || timer: 0.0817 sec.
iter 99150 || Loss: 0.9640 || timer: 0.0863 sec.
iter 99160 || Loss: 0.8399 || timer: 0.0816 sec.
iter 99170 || Loss: 1.0227 || timer: 0.1179 sec.
iter 99180 || Loss: 1.2560 || timer: 0.1069 sec.
iter 99190 || Loss: 1.2968 || timer: 0.0887 sec.
iter 99200 || Loss: 0.7762 || timer: 0.0890 sec.
iter 99210 || Loss: 1.0563 || timer: 0.0808 sec.
iter 99220 || Loss: 0.8075 || timer: 0.0192 sec.
iter 99230 || Loss: 0.2486 || timer: 0.0807 sec.
iter 99240 || Loss: 0.9957 || timer: 0.0842 sec.
iter 99250 || Loss: 0.7888 || timer: 0.0915 sec.
iter 99260 || Loss: 0.9574 || timer: 0.0913 sec.
iter 99270 || Loss: 0.7441 || timer: 0.0898 sec.
iter 99280 || Loss: 1.0516 || timer: 0.0985 sec.
iter 99290 || Loss: 0.8975 || timer: 0.1281 sec.
iter 99300 || Loss: 1.1571 || timer: 0.0884 sec.
iter 99310 || Loss: 1.2451 || timer: 0.0834 sec.
iter 99320 || Loss: 0.9849 || timer: 0.1101 sec.
iter 99330 || Loss: 1.0354 || timer: 0.0867 sec.
iter 99340 || Loss: 1.0168 || timer: 0.1067 sec.
iter 99350 || Loss: 0.7320 || timer: 0.0893 sec.
iter 99360 || Loss: 0.9613 || timer: 0.0819 sec.
iter 99370 || Loss: 1.1902 || timer: 0.0878 sec.
iter 99380 || Loss: 1.0594 || timer: 0.0905 sec.
iter 99390 || Loss: 1.1120 || timer: 0.0806 sec.
iter 99400 || Loss: 1.0531 || timer: 0.0885 sec.
iter 99410 || Loss: 0.9890 || timer: 0.0807 sec.
iter 99420 || Loss: 1.0716 || timer: 0.0854 sec.
iter 99430 || Loss: 1.0530 || timer: 0.0834 sec.
iter 99440 || Loss: 0.8151 || timer: 0.0992 sec.
iter 99450 || Loss: 1.2011 || timer: 0.0884 sec.
iter 99460 || Loss: 2.0578 || timer: 0.0916 sec.
iter 99470 || Loss: 1.5345 || timer: 0.0881 sec.
iter 99480 || Loss: 1.1557 || timer: 0.0855 sec.
iter 99490 || Loss: 0.7702 || timer: 0.0908 sec.
iter 99500 || Loss: 0.9250 || timer: 0.0833 sec.
iter 99510 || Loss: 1.3180 || timer: 0.0920 sec.
iter 99520 || Loss: 0.7745 || timer: 0.0890 sec.
iter 99530 || Loss: 1.1541 || timer: 0.0880 sec.
iter 99540 || Loss: 1.5993 || timer: 0.0896 sec.
iter 99550 || Loss: 1.1015 || timer: 0.0167 sec.
iter 99560 || Loss: 1.2914 || timer: 0.1018 sec.
iter 99570 || Loss: 1.5475 || timer: 0.0915 sec.
iter 99580 || Loss: 1.7310 || timer: 0.0895 sec.
iter 99590 || Loss: 1.1467 || timer: 0.0918 sec.
iter 99600 || Loss: 0.9688 || timer: 0.0921 sec.
iter 99610 || Loss: 0.7801 || timer: 0.0891 sec.
iter 99620 || Loss: 1.1061 || timer: 0.0857 sec.
iter 99630 || Loss: 1.9768 || timer: 0.0970 sec.
iter 99640 || Loss: 1.0382 || timer: 0.0950 sec.
iter 99650 || Loss: 0.8264 || timer: 0.1627 sec.
iter 99660 || Loss: 0.7468 || timer: 0.1100 sec.
iter 99670 || Loss: 1.4096 || timer: 0.0888 sec.
iter 99680 || Loss: 1.1539 || timer: 0.0896 sec.
iter 99690 || Loss: 1.0816 || timer: 0.0806 sec.
iter 99700 || Loss: 0.9272 || timer: 0.0885 sec.
iter 99710 || Loss: 1.4630 || timer: 0.0901 sec.
iter 99720 || Loss: 1.5303 || timer: 0.0969 sec.
iter 99730 || Loss: 0.9775 || timer: 0.0890 sec.
iter 99740 || Loss: 0.9367 || timer: 0.0823 sec.
iter 99750 || Loss: 0.8482 || timer: 0.0891 sec.
iter 99760 || Loss: 1.0616 || timer: 0.0970 sec.
iter 99770 || Loss: 1.0808 || timer: 0.0988 sec.
iter 99780 || Loss: 0.8908 || timer: 0.0811 sec.
iter 99790 || Loss: 1.0545 || timer: 0.0897 sec.
iter 99800 || Loss: 1.0096 || timer: 0.0801 sec.
iter 99810 || Loss: 0.9964 || timer: 0.0898 sec.
iter 99820 || Loss: 0.8430 || timer: 0.1091 sec.
iter 99830 || Loss: 1.0689 || timer: 0.0894 sec.
iter 99840 || Loss: 1.1148 || timer: 0.0971 sec.
iter 99850 || Loss: 1.3709 || timer: 0.0884 sec.
iter 99860 || Loss: 0.9995 || timer: 0.0897 sec.
iter 99870 || Loss: 1.4555 || timer: 0.0887 sec.
iter 99880 || Loss: 1.1450 || timer: 0.0231 sec.
iter 99890 || Loss: 0.8916 || timer: 0.0853 sec.
iter 99900 || Loss: 1.2088 || timer: 0.0827 sec.
iter 99910 || Loss: 1.2068 || timer: 0.0906 sec.
iter 99920 || Loss: 0.8379 || timer: 0.0880 sec.
iter 99930 || Loss: 1.8147 || timer: 0.1123 sec.
iter 99940 || Loss: 1.1110 || timer: 0.0808 sec.
iter 99950 || Loss: 0.9011 || timer: 0.1102 sec.
iter 99960 || Loss: 0.9725 || timer: 0.0898 sec.
iter 99970 || Loss: 0.7664 || timer: 0.0897 sec.
iter 99980 || Loss: 1.6647 || timer: 0.1690 sec.
iter 99990 || Loss: 1.2513 || timer: 0.1091 sec.
iter 100000 || Loss: 0.8160 || Saving state, iter: 100000
timer: 0.0887 sec.
iter 100010 || Loss: 1.2723 || timer: 0.0808 sec.
iter 100020 || Loss: 1.2153 || timer: 0.0809 sec.
iter 100030 || Loss: 1.2205 || timer: 0.0799 sec.
iter 100040 || Loss: 0.8321 || timer: 0.1144 sec.
iter 100050 || Loss: 1.3000 || timer: 0.1184 sec.
iter 100060 || Loss: 1.0815 || timer: 0.0886 sec.
iter 100070 || Loss: 0.8491 || timer: 0.0824 sec.
iter 100080 || Loss: 1.2139 || timer: 0.0899 sec.
iter 100090 || Loss: 1.0237 || timer: 0.1033 sec.
iter 100100 || Loss: 0.8866 || timer: 0.0814 sec.
iter 100110 || Loss: 0.9001 || timer: 0.0818 sec.
iter 100120 || Loss: 1.0735 || timer: 0.0880 sec.
iter 100130 || Loss: 0.9195 || timer: 0.0906 sec.
iter 100140 || Loss: 0.6921 || timer: 0.0910 sec.
iter 100150 || Loss: 1.2446 || timer: 0.0886 sec.
iter 100160 || Loss: 0.9012 || timer: 0.0890 sec.
iter 100170 || Loss: 1.2024 || timer: 0.0995 sec.
iter 100180 || Loss: 1.0088 || timer: 0.0871 sec.
iter 100190 || Loss: 1.1232 || timer: 0.1172 sec.
iter 100200 || Loss: 1.2253 || timer: 0.0884 sec.
iter 100210 || Loss: 0.8257 || timer: 0.0250 sec.
iter 100220 || Loss: 0.5054 || timer: 0.0821 sec.
iter 100230 || Loss: 0.6632 || timer: 0.0993 sec.
iter 100240 || Loss: 1.0860 || timer: 0.0869 sec.
iter 100250 || Loss: 0.8126 || timer: 0.0874 sec.
iter 100260 || Loss: 0.7954 || timer: 0.0947 sec.
iter 100270 || Loss: 0.8286 || timer: 0.0907 sec.
iter 100280 || Loss: 0.8814 || timer: 0.0931 sec.
iter 100290 || Loss: 1.0410 || timer: 0.0824 sec.
iter 100300 || Loss: 1.3438 || timer: 0.0807 sec.
iter 100310 || Loss: 1.1185 || timer: 0.0983 sec.
iter 100320 || Loss: 1.3525 || timer: 0.0806 sec.
iter 100330 || Loss: 0.8371 || timer: 0.1100 sec.
iter 100340 || Loss: 1.0408 || timer: 0.0882 sec.
iter 100350 || Loss: 1.0117 || timer: 0.0809 sec.
iter 100360 || Loss: 1.1490 || timer: 0.0805 sec.
iter 100370 || Loss: 0.9071 || timer: 0.1049 sec.
iter 100380 || Loss: 0.7015 || timer: 0.0914 sec.
iter 100390 || Loss: 0.9150 || timer: 0.0889 sec.
iter 100400 || Loss: 0.7807 || timer: 0.0811 sec.
iter 100410 || Loss: 1.2609 || timer: 0.1053 sec.
iter 100420 || Loss: 1.1507 || timer: 0.0924 sec.
iter 100430 || Loss: 1.0374 || timer: 0.0905 sec.
iter 100440 || Loss: 0.9910 || timer: 0.0895 sec.
iter 100450 || Loss: 0.8772 || timer: 0.0915 sec.
iter 100460 || Loss: 1.2248 || timer: 0.0812 sec.
iter 100470 || Loss: 0.8537 || timer: 0.0801 sec.
iter 100480 || Loss: 0.9384 || timer: 0.0820 sec.
iter 100490 || Loss: 1.1385 || timer: 0.0835 sec.
iter 100500 || Loss: 1.7186 || timer: 0.0893 sec.
iter 100510 || Loss: 1.0028 || timer: 0.0906 sec.
iter 100520 || Loss: 1.2109 || timer: 0.0820 sec.
iter 100530 || Loss: 1.0932 || timer: 0.0826 sec.
iter 100540 || Loss: 1.2182 || timer: 0.0237 sec.
iter 100550 || Loss: 2.3646 || timer: 0.0917 sec.
iter 100560 || Loss: 1.1767 || timer: 0.0943 sec.
iter 100570 || Loss: 1.0053 || timer: 0.0838 sec.
iter 100580 || Loss: 1.0254 || timer: 0.1003 sec.
iter 100590 || Loss: 1.1180 || timer: 0.0893 sec.
iter 100600 || Loss: 1.4677 || timer: 0.0815 sec.
iter 100610 || Loss: 1.0607 || timer: 0.0885 sec.
iter 100620 || Loss: 1.1937 || timer: 0.0903 sec.
iter 100630 || Loss: 0.9790 || timer: 0.0900 sec.
iter 100640 || Loss: 1.1723 || timer: 0.0975 sec.
iter 100650 || Loss: 0.8127 || timer: 0.0885 sec.
iter 100660 || Loss: 1.1686 || timer: 0.0951 sec.
iter 100670 || Loss: 0.9065 || timer: 0.0927 sec.
iter 100680 || Loss: 0.9449 || timer: 0.0820 sec.
iter 100690 || Loss: 1.0312 || timer: 0.0915 sec.
iter 100700 || Loss: 1.0395 || timer: 0.0838 sec.
iter 100710 || Loss: 0.9552 || timer: 0.0882 sec.
iter 100720 || Loss: 1.2841 || timer: 0.0914 sec.
iter 100730 || Loss: 1.4019 || timer: 0.0890 sec.
iter 100740 || Loss: 1.0704 || timer: 0.0874 sec.
iter 100750 || Loss: 1.1891 || timer: 0.0983 sec.
iter 100760 || Loss: 1.0530 || timer: 0.0905 sec.
iter 100770 || Loss: 1.1375 || timer: 0.1304 sec.
iter 100780 || Loss: 1.1951 || timer: 0.0892 sec.
iter 100790 || Loss: 1.7985 || timer: 0.1112 sec.
iter 100800 || Loss: 1.0824 || timer: 0.0857 sec.
iter 100810 || Loss: 1.1295 || timer: 0.0826 sec.
iter 100820 || Loss: 0.8529 || timer: 0.0816 sec.
iter 100830 || Loss: 0.9129 || timer: 0.0832 sec.
iter 100840 || Loss: 1.0618 || timer: 0.0865 sec.
iter 100850 || Loss: 1.1496 || timer: 0.0903 sec.
iter 100860 || Loss: 1.2983 || timer: 0.0845 sec.
iter 100870 || Loss: 0.9006 || timer: 0.0253 sec.
iter 100880 || Loss: 1.3213 || timer: 0.0876 sec.
iter 100890 || Loss: 0.9680 || timer: 0.0856 sec.
iter 100900 || Loss: 1.0274 || timer: 0.1013 sec.
iter 100910 || Loss: 1.2512 || timer: 0.1037 sec.
iter 100920 || Loss: 0.7378 || timer: 0.0902 sec.
iter 100930 || Loss: 1.1985 || timer: 0.0897 sec.
iter 100940 || Loss: 1.4872 || timer: 0.0892 sec.
iter 100950 || Loss: 1.5502 || timer: 0.0894 sec.
iter 100960 || Loss: 1.1527 || timer: 0.1095 sec.
iter 100970 || Loss: 0.7947 || timer: 0.1209 sec.
iter 100980 || Loss: 0.8496 || timer: 0.0912 sec.
iter 100990 || Loss: 1.0401 || timer: 0.0951 sec.
iter 101000 || Loss: 1.1035 || timer: 0.0919 sec.
iter 101010 || Loss: 1.4178 || timer: 0.1155 sec.
iter 101020 || Loss: 1.0155 || timer: 0.0846 sec.
iter 101030 || Loss: 1.0432 || timer: 0.0804 sec.
iter 101040 || Loss: 0.8853 || timer: 0.0976 sec.
iter 101050 || Loss: 0.7448 || timer: 0.1034 sec.
iter 101060 || Loss: 0.7981 || timer: 0.0907 sec.
iter 101070 || Loss: 0.9095 || timer: 0.0818 sec.
iter 101080 || Loss: 0.9532 || timer: 0.0957 sec.
iter 101090 || Loss: 1.1976 || timer: 0.0863 sec.
iter 101100 || Loss: 0.9548 || timer: 0.0812 sec.
iter 101110 || Loss: 0.9200 || timer: 0.0818 sec.
iter 101120 || Loss: 0.8341 || timer: 0.1031 sec.
iter 101130 || Loss: 1.2294 || timer: 0.0975 sec.
iter 101140 || Loss: 1.0202 || timer: 0.0839 sec.
iter 101150 || Loss: 1.0927 || timer: 0.0820 sec.
iter 101160 || Loss: 1.0731 || timer: 0.0895 sec.
iter 101170 || Loss: 0.9137 || timer: 0.0968 sec.
iter 101180 || Loss: 1.0963 || timer: 0.0907 sec.
iter 101190 || Loss: 1.1267 || timer: 0.0921 sec.
iter 101200 || Loss: 1.0811 || timer: 0.0268 sec.
iter 101210 || Loss: 2.0430 || timer: 0.0888 sec.
iter 101220 || Loss: 1.0949 || timer: 0.0835 sec.
iter 101230 || Loss: 1.3416 || timer: 0.0827 sec.
iter 101240 || Loss: 1.0705 || timer: 0.0891 sec.
iter 101250 || Loss: 1.5198 || timer: 0.0885 sec.
iter 101260 || Loss: 0.8373 || timer: 0.0885 sec.
iter 101270 || Loss: 0.9336 || timer: 0.1184 sec.
iter 101280 || Loss: 1.2810 || timer: 0.1128 sec.
iter 101290 || Loss: 0.9467 || timer: 0.1037 sec.
iter 101300 || Loss: 1.0355 || timer: 0.1384 sec.
iter 101310 || Loss: 1.4582 || timer: 0.0809 sec.
iter 101320 || Loss: 0.9553 || timer: 0.0888 sec.
iter 101330 || Loss: 0.9991 || timer: 0.0906 sec.
iter 101340 || Loss: 0.9340 || timer: 0.0823 sec.
iter 101350 || Loss: 0.9691 || timer: 0.0828 sec.
iter 101360 || Loss: 0.9036 || timer: 0.0992 sec.
iter 101370 || Loss: 0.8005 || timer: 0.1108 sec.
iter 101380 || Loss: 1.0071 || timer: 0.1163 sec.
iter 101390 || Loss: 0.9268 || timer: 0.0886 sec.
iter 101400 || Loss: 1.1035 || timer: 0.0751 sec.
iter 101410 || Loss: 1.0842 || timer: 0.0824 sec.
iter 101420 || Loss: 1.2174 || timer: 0.0902 sec.
iter 101430 || Loss: 0.8111 || timer: 0.0891 sec.
iter 101440 || Loss: 1.0528 || timer: 0.0840 sec.
iter 101450 || Loss: 1.1429 || timer: 0.1223 sec.
iter 101460 || Loss: 0.7866 || timer: 0.1091 sec.
iter 101470 || Loss: 1.1375 || timer: 0.0804 sec.
iter 101480 || Loss: 1.3740 || timer: 0.0972 sec.
iter 101490 || Loss: 1.1692 || timer: 0.1036 sec.
iter 101500 || Loss: 0.9233 || timer: 0.0846 sec.
iter 101510 || Loss: 1.5298 || timer: 0.0854 sec.
iter 101520 || Loss: 0.6853 || timer: 0.1030 sec.
iter 101530 || Loss: 0.8419 || timer: 0.0277 sec.
iter 101540 || Loss: 0.6506 || timer: 0.0905 sec.
iter 101550 || Loss: 1.3846 || timer: 0.0897 sec.
iter 101560 || Loss: 1.4148 || timer: 0.1253 sec.
iter 101570 || Loss: 1.1773 || timer: 0.1101 sec.
iter 101580 || Loss: 1.1950 || timer: 0.0859 sec.
iter 101590 || Loss: 1.0438 || timer: 0.1220 sec.
iter 101600 || Loss: 1.2105 || timer: 0.0874 sec.
iter 101610 || Loss: 1.0498 || timer: 0.0880 sec.
iter 101620 || Loss: 1.1359 || timer: 0.0893 sec.
iter 101630 || Loss: 0.8237 || timer: 0.1096 sec.
iter 101640 || Loss: 0.9133 || timer: 0.0884 sec.
iter 101650 || Loss: 1.3611 || timer: 0.1046 sec.
iter 101660 || Loss: 1.3892 || timer: 0.0886 sec.
iter 101670 || Loss: 0.8344 || timer: 0.0892 sec.
iter 101680 || Loss: 1.4717 || timer: 0.1038 sec.
iter 101690 || Loss: 1.0551 || timer: 0.0928 sec.
iter 101700 || Loss: 1.0457 || timer: 0.0886 sec.
iter 101710 || Loss: 1.2803 || timer: 0.0867 sec.
iter 101720 || Loss: 1.2779 || timer: 0.0935 sec.
iter 101730 || Loss: 1.0520 || timer: 0.0812 sec.
iter 101740 || Loss: 1.2733 || timer: 0.0814 sec.
iter 101750 || Loss: 1.0754 || timer: 0.0888 sec.
iter 101760 || Loss: 0.8320 || timer: 0.0916 sec.
iter 101770 || Loss: 0.8870 || timer: 0.0933 sec.
iter 101780 || Loss: 1.2740 || timer: 0.0820 sec.
iter 101790 || Loss: 1.1904 || timer: 0.0851 sec.
iter 101800 || Loss: 0.8677 || timer: 0.0814 sec.
iter 101810 || Loss: 1.2236 || timer: 0.0886 sec.
iter 101820 || Loss: 0.8016 || timer: 0.0958 sec.
iter 101830 || Loss: 0.7712 || timer: 0.0818 sec.
iter 101840 || Loss: 0.9185 || timer: 0.1120 sec.
iter 101850 || Loss: 0.9611 || timer: 0.0910 sec.
iter 101860 || Loss: 1.4615 || timer: 0.0198 sec.
iter 101870 || Loss: 0.8238 || timer: 0.0855 sec.
iter 101880 || Loss: 1.1430 || timer: 0.0814 sec.
iter 101890 || Loss: 0.8172 || timer: 0.0808 sec.
iter 101900 || Loss: 0.7137 || timer: 0.0807 sec.
iter 101910 || Loss: 0.8948 || timer: 0.1070 sec.
iter 101920 || Loss: 0.8876 || timer: 0.0951 sec.
iter 101930 || Loss: 0.7791 || timer: 0.0807 sec.
iter 101940 || Loss: 1.0103 || timer: 0.0874 sec.
iter 101950 || Loss: 1.1697 || timer: 0.0872 sec.
iter 101960 || Loss: 1.0967 || timer: 0.0942 sec.
iter 101970 || Loss: 1.0094 || timer: 0.0908 sec.
iter 101980 || Loss: 1.3073 || timer: 0.0922 sec.
iter 101990 || Loss: 1.6316 || timer: 0.0903 sec.
iter 102000 || Loss: 1.0667 || timer: 0.0804 sec.
iter 102010 || Loss: 1.0344 || timer: 0.0913 sec.
iter 102020 || Loss: 1.0738 || timer: 0.0881 sec.
iter 102030 || Loss: 1.2017 || timer: 0.0814 sec.
iter 102040 || Loss: 0.8524 || timer: 0.0905 sec.
iter 102050 || Loss: 1.4114 || timer: 0.0825 sec.
iter 102060 || Loss: 1.0273 || timer: 0.0905 sec.
iter 102070 || Loss: 1.2339 || timer: 0.0932 sec.
iter 102080 || Loss: 1.2190 || timer: 0.0966 sec.
iter 102090 || Loss: 1.3100 || timer: 0.0887 sec.
iter 102100 || Loss: 0.7610 || timer: 0.0918 sec.
iter 102110 || Loss: 1.4875 || timer: 0.0890 sec.
iter 102120 || Loss: 0.9400 || timer: 0.0799 sec.
iter 102130 || Loss: 0.8570 || timer: 0.1036 sec.
iter 102140 || Loss: 1.1573 || timer: 0.0884 sec.
iter 102150 || Loss: 1.6000 || timer: 0.0912 sec.
iter 102160 || Loss: 1.2618 || timer: 0.0824 sec.
iter 102170 || Loss: 1.3936 || timer: 0.0920 sec.
iter 102180 || Loss: 1.2540 || timer: 0.0821 sec.
iter 102190 || Loss: 1.1536 || timer: 0.0240 sec.
iter 102200 || Loss: 0.2661 || timer: 0.1113 sec.
iter 102210 || Loss: 0.9573 || timer: 0.0804 sec.
iter 102220 || Loss: 0.8711 || timer: 0.0897 sec.
iter 102230 || Loss: 0.9265 || timer: 0.0884 sec.
iter 102240 || Loss: 0.9721 || timer: 0.0888 sec.
iter 102250 || Loss: 1.0417 || timer: 0.0894 sec.
iter 102260 || Loss: 0.8939 || timer: 0.0823 sec.
iter 102270 || Loss: 0.9559 || timer: 0.0818 sec.
iter 102280 || Loss: 0.9263 || timer: 0.0904 sec.
iter 102290 || Loss: 1.0329 || timer: 0.1174 sec.
iter 102300 || Loss: 1.4963 || timer: 0.1008 sec.
iter 102310 || Loss: 0.9627 || timer: 0.0920 sec.
iter 102320 || Loss: 1.1737 || timer: 0.0899 sec.
iter 102330 || Loss: 1.2883 || timer: 0.0808 sec.
iter 102340 || Loss: 0.8686 || timer: 0.0885 sec.
iter 102350 || Loss: 1.0185 || timer: 0.1038 sec.
iter 102360 || Loss: 1.3840 || timer: 0.0896 sec.
iter 102370 || Loss: 1.3982 || timer: 0.0819 sec.
iter 102380 || Loss: 0.9434 || timer: 0.0823 sec.
iter 102390 || Loss: 0.7626 || timer: 0.0816 sec.
iter 102400 || Loss: 0.8800 || timer: 0.0890 sec.
iter 102410 || Loss: 1.1949 || timer: 0.0963 sec.
iter 102420 || Loss: 1.1554 || timer: 0.0894 sec.
iter 102430 || Loss: 0.9145 || timer: 0.0908 sec.
iter 102440 || Loss: 1.1282 || timer: 0.1132 sec.
iter 102450 || Loss: 1.0918 || timer: 0.0807 sec.
iter 102460 || Loss: 0.6882 || timer: 0.1016 sec.
iter 102470 || Loss: 1.1596 || timer: 0.0907 sec.
iter 102480 || Loss: 1.0569 || timer: 0.0911 sec.
iter 102490 || Loss: 0.7570 || timer: 0.0913 sec.
iter 102500 || Loss: 1.0802 || timer: 0.0889 sec.
iter 102510 || Loss: 0.9183 || timer: 0.0870 sec.
iter 102520 || Loss: 0.6817 || timer: 0.0182 sec.
iter 102530 || Loss: 0.3729 || timer: 0.0907 sec.
iter 102540 || Loss: 1.1530 || timer: 0.0917 sec.
iter 102550 || Loss: 1.1840 || timer: 0.0865 sec.
iter 102560 || Loss: 0.9183 || timer: 0.1025 sec.
iter 102570 || Loss: 0.9507 || timer: 0.0899 sec.
iter 102580 || Loss: 1.1870 || timer: 0.0923 sec.
iter 102590 || Loss: 0.8937 || timer: 0.0819 sec.
iter 102600 || Loss: 0.7743 || timer: 0.0799 sec.
iter 102610 || Loss: 0.9087 || timer: 0.0840 sec.
iter 102620 || Loss: 1.0061 || timer: 0.1115 sec.
iter 102630 || Loss: 1.0855 || timer: 0.0822 sec.
iter 102640 || Loss: 1.2308 || timer: 0.0899 sec.
iter 102650 || Loss: 1.2362 || timer: 0.0867 sec.
iter 102660 || Loss: 1.4687 || timer: 0.0814 sec.
iter 102670 || Loss: 1.1768 || timer: 0.0920 sec.
iter 102680 || Loss: 1.1285 || timer: 0.0812 sec.
iter 102690 || Loss: 1.2707 || timer: 0.0807 sec.
iter 102700 || Loss: 1.3118 || timer: 0.0862 sec.
iter 102710 || Loss: 1.0872 || timer: 0.0817 sec.
iter 102720 || Loss: 0.9600 || timer: 0.0988 sec.
iter 102730 || Loss: 0.9790 || timer: 0.1099 sec.
iter 102740 || Loss: 1.0324 || timer: 0.0892 sec.
iter 102750 || Loss: 1.3567 || timer: 0.0883 sec.
iter 102760 || Loss: 1.2207 || timer: 0.0806 sec.
iter 102770 || Loss: 0.8550 || timer: 0.0811 sec.
iter 102780 || Loss: 1.2618 || timer: 0.0893 sec.
iter 102790 || Loss: 1.0773 || timer: 0.0813 sec.
iter 102800 || Loss: 1.3308 || timer: 0.1018 sec.
iter 102810 || Loss: 1.2054 || timer: 0.0865 sec.
iter 102820 || Loss: 0.9553 || timer: 0.0813 sec.
iter 102830 || Loss: 0.9409 || timer: 0.1032 sec.
iter 102840 || Loss: 0.9814 || timer: 0.0906 sec.
iter 102850 || Loss: 0.8130 || timer: 0.0284 sec.
iter 102860 || Loss: 0.7482 || timer: 0.0903 sec.
iter 102870 || Loss: 1.0720 || timer: 0.0952 sec.
iter 102880 || Loss: 1.0637 || timer: 0.0907 sec.
iter 102890 || Loss: 0.7317 || timer: 0.0818 sec.
iter 102900 || Loss: 1.1681 || timer: 0.0815 sec.
iter 102910 || Loss: 1.1413 || timer: 0.0841 sec.
iter 102920 || Loss: 0.9247 || timer: 0.0886 sec.
iter 102930 || Loss: 1.1408 || timer: 0.1191 sec.
iter 102940 || Loss: 1.0373 || timer: 0.0823 sec.
iter 102950 || Loss: 1.0962 || timer: 0.1003 sec.
iter 102960 || Loss: 0.9025 || timer: 0.0875 sec.
iter 102970 || Loss: 1.2535 || timer: 0.0926 sec.
iter 102980 || Loss: 1.0972 || timer: 0.0819 sec.
iter 102990 || Loss: 1.2611 || timer: 0.1047 sec.
iter 103000 || Loss: 1.5769 || timer: 0.0895 sec.
iter 103010 || Loss: 1.2919 || timer: 0.1078 sec.
iter 103020 || Loss: 0.8565 || timer: 0.0828 sec.
iter 103030 || Loss: 0.9222 || timer: 0.0896 sec.
iter 103040 || Loss: 0.9882 || timer: 0.0897 sec.
iter 103050 || Loss: 1.0727 || timer: 0.0901 sec.
iter 103060 || Loss: 0.7626 || timer: 0.0910 sec.
iter 103070 || Loss: 0.6579 || timer: 0.0893 sec.
iter 103080 || Loss: 1.0211 || timer: 0.0820 sec.
iter 103090 || Loss: 0.9983 || timer: 0.0891 sec.
iter 103100 || Loss: 0.8806 || timer: 0.0908 sec.
iter 103110 || Loss: 0.7983 || timer: 0.0870 sec.
iter 103120 || Loss: 1.1727 || timer: 0.0907 sec.
iter 103130 || Loss: 0.8334 || timer: 0.0873 sec.
iter 103140 || Loss: 0.8127 || timer: 0.1038 sec.
iter 103150 || Loss: 0.9633 || timer: 0.0853 sec.
iter 103160 || Loss: 0.8612 || timer: 0.0806 sec.
iter 103170 || Loss: 0.7891 || timer: 0.0883 sec.
iter 103180 || Loss: 0.9588 || timer: 0.0231 sec.
iter 103190 || Loss: 0.8857 || timer: 0.0905 sec.
iter 103200 || Loss: 1.0932 || timer: 0.0892 sec.
iter 103210 || Loss: 0.7450 || timer: 0.0882 sec.
iter 103220 || Loss: 0.8458 || timer: 0.0892 sec.
iter 103230 || Loss: 1.0767 || timer: 0.1050 sec.
iter 103240 || Loss: 1.2214 || timer: 0.0831 sec.
iter 103250 || Loss: 1.1064 || timer: 0.0870 sec.
iter 103260 || Loss: 1.2091 || timer: 0.1025 sec.
iter 103270 || Loss: 0.8847 || timer: 0.0889 sec.
iter 103280 || Loss: 1.2155 || timer: 0.0928 sec.
iter 103290 || Loss: 1.0263 || timer: 0.0819 sec.
iter 103300 || Loss: 1.1510 || timer: 0.0958 sec.
iter 103310 || Loss: 1.1245 || timer: 0.0866 sec.
iter 103320 || Loss: 0.9255 || timer: 0.0918 sec.
iter 103330 || Loss: 1.0004 || timer: 0.0883 sec.
iter 103340 || Loss: 1.0058 || timer: 0.0883 sec.
iter 103350 || Loss: 1.0196 || timer: 0.0901 sec.
iter 103360 || Loss: 0.8814 || timer: 0.0881 sec.
iter 103370 || Loss: 0.9290 || timer: 0.0910 sec.
iter 103380 || Loss: 0.9690 || timer: 0.0807 sec.
iter 103390 || Loss: 0.9991 || timer: 0.0811 sec.
iter 103400 || Loss: 1.4216 || timer: 0.0895 sec.
iter 103410 || Loss: 0.9314 || timer: 0.0890 sec.
iter 103420 || Loss: 0.8235 || timer: 0.0807 sec.
iter 103430 || Loss: 0.8885 || timer: 0.0906 sec.
iter 103440 || Loss: 1.0820 || timer: 0.0935 sec.
iter 103450 || Loss: 1.2456 || timer: 0.0872 sec.
iter 103460 || Loss: 1.2094 || timer: 0.0845 sec.
iter 103470 || Loss: 0.6967 || timer: 0.0862 sec.
iter 103480 || Loss: 0.8105 || timer: 0.0904 sec.
iter 103490 || Loss: 1.1215 || timer: 0.0908 sec.
iter 103500 || Loss: 0.9617 || timer: 0.1086 sec.
iter 103510 || Loss: 1.0448 || timer: 0.0177 sec.
iter 103520 || Loss: 0.7117 || timer: 0.0919 sec.
iter 103530 || Loss: 0.9302 || timer: 0.0899 sec.
iter 103540 || Loss: 1.2115 || timer: 0.0818 sec.
iter 103550 || Loss: 1.0408 || timer: 0.0855 sec.
iter 103560 || Loss: 0.9207 || timer: 0.0889 sec.
iter 103570 || Loss: 0.9788 || timer: 0.0913 sec.
iter 103580 || Loss: 0.6516 || timer: 0.0909 sec.
iter 103590 || Loss: 1.2918 || timer: 0.1116 sec.
iter 103600 || Loss: 1.2025 || timer: 0.0903 sec.
iter 103610 || Loss: 1.0275 || timer: 0.1117 sec.
iter 103620 || Loss: 1.5066 || timer: 0.0905 sec.
iter 103630 || Loss: 0.8130 || timer: 0.0882 sec.
iter 103640 || Loss: 0.8170 || timer: 0.0877 sec.
iter 103650 || Loss: 0.9405 || timer: 0.1035 sec.
iter 103660 || Loss: 1.0943 || timer: 0.0878 sec.
iter 103670 || Loss: 1.1236 || timer: 0.0888 sec.
iter 103680 || Loss: 1.1460 || timer: 0.0816 sec.
iter 103690 || Loss: 0.8639 || timer: 0.0883 sec.
iter 103700 || Loss: 0.7372 || timer: 0.0886 sec.
iter 103710 || Loss: 0.8531 || timer: 0.0858 sec.
iter 103720 || Loss: 1.1083 || timer: 0.0807 sec.
iter 103730 || Loss: 0.9621 || timer: 0.0903 sec.
iter 103740 || Loss: 0.9143 || timer: 0.0874 sec.
iter 103750 || Loss: 1.1520 || timer: 0.0864 sec.
iter 103760 || Loss: 0.7969 || timer: 0.0978 sec.
iter 103770 || Loss: 0.6940 || timer: 0.0883 sec.
iter 103780 || Loss: 0.9902 || timer: 0.0918 sec.
iter 103790 || Loss: 1.1423 || timer: 0.0825 sec.
iter 103800 || Loss: 1.3040 || timer: 0.0818 sec.
iter 103810 || Loss: 1.0542 || timer: 0.0806 sec.
iter 103820 || Loss: 1.1503 || timer: 0.1012 sec.
iter 103830 || Loss: 1.0063 || timer: 0.0836 sec.
iter 103840 || Loss: 1.9150 || timer: 0.0203 sec.
iter 103850 || Loss: 0.9984 || timer: 0.0882 sec.
iter 103860 || Loss: 0.9518 || timer: 0.0882 sec.
iter 103870 || Loss: 1.2071 || timer: 0.1087 sec.
iter 103880 || Loss: 1.1622 || timer: 0.0884 sec.
iter 103890 || Loss: 1.6348 || timer: 0.0874 sec.
iter 103900 || Loss: 0.9306 || timer: 0.0881 sec.
iter 103910 || Loss: 0.9599 || timer: 0.0804 sec.
iter 103920 || Loss: 0.7940 || timer: 0.0821 sec.
iter 103930 || Loss: 0.8831 || timer: 0.0901 sec.
iter 103940 || Loss: 1.0409 || timer: 0.1121 sec.
iter 103950 || Loss: 0.8116 || timer: 0.0823 sec.
iter 103960 || Loss: 0.6541 || timer: 0.0899 sec.
iter 103970 || Loss: 1.1338 || timer: 0.0884 sec.
iter 103980 || Loss: 0.8689 || timer: 0.0820 sec.
iter 103990 || Loss: 0.9790 || timer: 0.0828 sec.
iter 104000 || Loss: 1.1294 || timer: 0.0937 sec.
iter 104010 || Loss: 1.0442 || timer: 0.0892 sec.
iter 104020 || Loss: 1.0869 || timer: 0.0990 sec.
iter 104030 || Loss: 0.9963 || timer: 0.1025 sec.
iter 104040 || Loss: 0.8253 || timer: 0.0880 sec.
iter 104050 || Loss: 0.8073 || timer: 0.1370 sec.
iter 104060 || Loss: 0.9004 || timer: 0.1027 sec.
iter 104070 || Loss: 0.9710 || timer: 0.0844 sec.
iter 104080 || Loss: 1.2991 || timer: 0.1035 sec.
iter 104090 || Loss: 0.9688 || timer: 0.0899 sec.
iter 104100 || Loss: 1.1154 || timer: 0.0965 sec.
iter 104110 || Loss: 0.7887 || timer: 0.0852 sec.
iter 104120 || Loss: 0.7347 || timer: 0.1020 sec.
iter 104130 || Loss: 1.1469 || timer: 0.1135 sec.
iter 104140 || Loss: 1.0787 || timer: 0.0886 sec.
iter 104150 || Loss: 0.8520 || timer: 0.1014 sec.
iter 104160 || Loss: 0.9413 || timer: 0.0866 sec.
iter 104170 || Loss: 1.3718 || timer: 0.0162 sec.
iter 104180 || Loss: 0.9511 || timer: 0.0813 sec.
iter 104190 || Loss: 1.0406 || timer: 0.0895 sec.
iter 104200 || Loss: 0.7745 || timer: 0.1028 sec.
iter 104210 || Loss: 0.8906 || timer: 0.0885 sec.
iter 104220 || Loss: 1.3936 || timer: 0.0815 sec.
iter 104230 || Loss: 0.9780 || timer: 0.1042 sec.
iter 104240 || Loss: 1.2544 || timer: 0.0889 sec.
iter 104250 || Loss: 1.0478 || timer: 0.0884 sec.
iter 104260 || Loss: 0.7206 || timer: 0.0822 sec.
iter 104270 || Loss: 0.9490 || timer: 0.0995 sec.
iter 104280 || Loss: 0.8416 || timer: 0.0902 sec.
iter 104290 || Loss: 1.2271 || timer: 0.0900 sec.
iter 104300 || Loss: 1.0704 || timer: 0.1001 sec.
iter 104310 || Loss: 0.8987 || timer: 0.0917 sec.
iter 104320 || Loss: 1.1038 || timer: 0.1267 sec.
iter 104330 || Loss: 0.9709 || timer: 0.0890 sec.
iter 104340 || Loss: 1.1378 || timer: 0.0898 sec.
iter 104350 || Loss: 0.7556 || timer: 0.0937 sec.
iter 104360 || Loss: 0.8646 || timer: 0.0814 sec.
iter 104370 || Loss: 0.9360 || timer: 0.0807 sec.
iter 104380 || Loss: 1.0543 || timer: 0.1027 sec.
iter 104390 || Loss: 1.0582 || timer: 0.0820 sec.
iter 104400 || Loss: 0.8709 || timer: 0.0863 sec.
iter 104410 || Loss: 0.9256 || timer: 0.0821 sec.
iter 104420 || Loss: 1.0445 || timer: 0.0823 sec.
iter 104430 || Loss: 0.7915 || timer: 0.0903 sec.
iter 104440 || Loss: 1.0560 || timer: 0.0909 sec.
iter 104450 || Loss: 1.1961 || timer: 0.0881 sec.
iter 104460 || Loss: 1.0279 || timer: 0.0879 sec.
iter 104470 || Loss: 0.9204 || timer: 0.0883 sec.
iter 104480 || Loss: 1.1254 || timer: 0.0890 sec.
iter 104490 || Loss: 0.6493 || timer: 0.0904 sec.
iter 104500 || Loss: 0.9000 || timer: 0.0226 sec.
iter 104510 || Loss: 0.9707 || timer: 0.0915 sec.
iter 104520 || Loss: 1.0097 || timer: 0.0917 sec.
iter 104530 || Loss: 0.9187 || timer: 0.0884 sec.
iter 104540 || Loss: 0.9068 || timer: 0.0883 sec.
iter 104550 || Loss: 0.8946 || timer: 0.0870 sec.
iter 104560 || Loss: 0.7458 || timer: 0.0947 sec.
iter 104570 || Loss: 0.9590 || timer: 0.0830 sec.
iter 104580 || Loss: 0.9708 || timer: 0.1016 sec.
iter 104590 || Loss: 0.8813 || timer: 0.0889 sec.
iter 104600 || Loss: 0.9703 || timer: 0.0933 sec.
iter 104610 || Loss: 1.1744 || timer: 0.0886 sec.
iter 104620 || Loss: 0.9411 || timer: 0.0884 sec.
iter 104630 || Loss: 0.8513 || timer: 0.0813 sec.
iter 104640 || Loss: 1.0735 || timer: 0.0846 sec.
iter 104650 || Loss: 0.9857 || timer: 0.1029 sec.
iter 104660 || Loss: 1.0974 || timer: 0.0891 sec.
iter 104670 || Loss: 1.0669 || timer: 0.0887 sec.
iter 104680 || Loss: 1.1866 || timer: 0.1289 sec.
iter 104690 || Loss: 0.7868 || timer: 0.0901 sec.
iter 104700 || Loss: 1.1929 || timer: 0.1089 sec.
iter 104710 || Loss: 1.1367 || timer: 0.0925 sec.
iter 104720 || Loss: 1.3512 || timer: 0.1054 sec.
iter 104730 || Loss: 0.8174 || timer: 0.0810 sec.
iter 104740 || Loss: 0.9366 || timer: 0.0878 sec.
iter 104750 || Loss: 0.8261 || timer: 0.0892 sec.
iter 104760 || Loss: 0.9560 || timer: 0.0830 sec.
iter 104770 || Loss: 1.0521 || timer: 0.0823 sec.
iter 104780 || Loss: 1.0948 || timer: 0.0919 sec.
iter 104790 || Loss: 1.3010 || timer: 0.0904 sec.
iter 104800 || Loss: 0.9289 || timer: 0.0863 sec.
iter 104810 || Loss: 1.3063 || timer: 0.0899 sec.
iter 104820 || Loss: 0.7713 || timer: 0.0800 sec.
iter 104830 || Loss: 1.1181 || timer: 0.0229 sec.
iter 104840 || Loss: 0.4101 || timer: 0.0840 sec.
iter 104850 || Loss: 1.1637 || timer: 0.0900 sec.
iter 104860 || Loss: 1.1223 || timer: 0.0878 sec.
iter 104870 || Loss: 0.7760 || timer: 0.0890 sec.
iter 104880 || Loss: 0.9966 || timer: 0.0821 sec.
iter 104890 || Loss: 1.0782 || timer: 0.1004 sec.
iter 104900 || Loss: 1.0083 || timer: 0.0908 sec.
iter 104910 || Loss: 1.1602 || timer: 0.1074 sec.
iter 104920 || Loss: 0.9551 || timer: 0.0876 sec.
iter 104930 || Loss: 1.2122 || timer: 0.1453 sec.
iter 104940 || Loss: 0.8633 || timer: 0.0975 sec.
iter 104950 || Loss: 1.1747 || timer: 0.1053 sec.
iter 104960 || Loss: 0.9271 || timer: 0.0822 sec.
iter 104970 || Loss: 1.0692 || timer: 0.1028 sec.
iter 104980 || Loss: 1.1622 || timer: 0.0898 sec.
iter 104990 || Loss: 1.1667 || timer: 0.1059 sec.
iter 105000 || Loss: 0.7680 || Saving state, iter: 105000
timer: 0.1037 sec.
iter 105010 || Loss: 0.9686 || timer: 0.0816 sec.
iter 105020 || Loss: 0.9503 || timer: 0.0952 sec.
iter 105030 || Loss: 1.0463 || timer: 0.0913 sec.
iter 105040 || Loss: 1.1054 || timer: 0.0894 sec.
iter 105050 || Loss: 1.3993 || timer: 0.0869 sec.
iter 105060 || Loss: 1.5770 || timer: 0.0886 sec.
iter 105070 || Loss: 0.8313 || timer: 0.0894 sec.
iter 105080 || Loss: 0.7597 || timer: 0.0832 sec.
iter 105090 || Loss: 0.7941 || timer: 0.0830 sec.
iter 105100 || Loss: 1.0694 || timer: 0.0811 sec.
iter 105110 || Loss: 0.8415 || timer: 0.0910 sec.
iter 105120 || Loss: 0.9734 || timer: 0.0819 sec.
iter 105130 || Loss: 1.0050 || timer: 0.0927 sec.
iter 105140 || Loss: 1.2213 || timer: 0.0906 sec.
iter 105150 || Loss: 0.9875 || timer: 0.0820 sec.
iter 105160 || Loss: 0.8332 || timer: 0.0208 sec.
iter 105170 || Loss: 0.3299 || timer: 0.0856 sec.
iter 105180 || Loss: 1.1711 || timer: 0.1071 sec.
iter 105190 || Loss: 0.9650 || timer: 0.0965 sec.
iter 105200 || Loss: 1.1655 || timer: 0.0813 sec.
iter 105210 || Loss: 1.0043 || timer: 0.0862 sec.
iter 105220 || Loss: 1.1855 || timer: 0.0971 sec.
iter 105230 || Loss: 0.8311 || timer: 0.0916 sec.
iter 105240 || Loss: 0.9356 || timer: 0.0814 sec.
iter 105250 || Loss: 1.0094 || timer: 0.0809 sec.
iter 105260 || Loss: 0.9838 || timer: 0.0984 sec.
iter 105270 || Loss: 0.9916 || timer: 0.0848 sec.
iter 105280 || Loss: 1.6073 || timer: 0.0871 sec.
iter 105290 || Loss: 0.9406 || timer: 0.0826 sec.
iter 105300 || Loss: 1.1804 || timer: 0.0907 sec.
iter 105310 || Loss: 1.4712 || timer: 0.0831 sec.
iter 105320 || Loss: 0.9385 || timer: 0.1046 sec.
iter 105330 || Loss: 1.1589 || timer: 0.0927 sec.
iter 105340 || Loss: 1.1269 || timer: 0.1023 sec.
iter 105350 || Loss: 0.9859 || timer: 0.0887 sec.
iter 105360 || Loss: 1.1314 || timer: 0.0902 sec.
iter 105370 || Loss: 0.9328 || timer: 0.1262 sec.
iter 105380 || Loss: 1.1147 || timer: 0.1122 sec.
iter 105390 || Loss: 0.8210 || timer: 0.0914 sec.
iter 105400 || Loss: 1.0441 || timer: 0.0878 sec.
iter 105410 || Loss: 0.8503 || timer: 0.0834 sec.
iter 105420 || Loss: 0.9330 || timer: 0.0928 sec.
iter 105430 || Loss: 1.2044 || timer: 0.1383 sec.
iter 105440 || Loss: 1.2124 || timer: 0.0860 sec.
iter 105450 || Loss: 0.9913 || timer: 0.1100 sec.
iter 105460 || Loss: 0.9086 || timer: 0.0840 sec.
iter 105470 || Loss: 1.2035 || timer: 0.0944 sec.
iter 105480 || Loss: 1.2274 || timer: 0.0863 sec.
iter 105490 || Loss: 0.9155 || timer: 0.0234 sec.
iter 105500 || Loss: 0.4329 || timer: 0.0907 sec.
iter 105510 || Loss: 0.8898 || timer: 0.0886 sec.
iter 105520 || Loss: 1.0063 || timer: 0.0859 sec.
iter 105530 || Loss: 0.8315 || timer: 0.0902 sec.
iter 105540 || Loss: 0.6145 || timer: 0.0816 sec.
iter 105550 || Loss: 0.9828 || timer: 0.0800 sec.
iter 105560 || Loss: 1.1561 || timer: 0.0907 sec.
iter 105570 || Loss: 1.1902 || timer: 0.0887 sec.
iter 105580 || Loss: 0.7030 || timer: 0.0873 sec.
iter 105590 || Loss: 1.1064 || timer: 0.0954 sec.
iter 105600 || Loss: 1.0637 || timer: 0.0881 sec.
iter 105610 || Loss: 0.9948 || timer: 0.0885 sec.
iter 105620 || Loss: 0.7954 || timer: 0.0884 sec.
iter 105630 || Loss: 0.9528 || timer: 0.0866 sec.
iter 105640 || Loss: 0.9875 || timer: 0.0833 sec.
iter 105650 || Loss: 0.7917 || timer: 0.1145 sec.
iter 105660 || Loss: 1.3261 || timer: 0.1046 sec.
iter 105670 || Loss: 0.9793 || timer: 0.1089 sec.
iter 105680 || Loss: 0.7831 || timer: 0.0811 sec.
iter 105690 || Loss: 1.0455 || timer: 0.1093 sec.
iter 105700 || Loss: 0.9606 || timer: 0.0921 sec.
iter 105710 || Loss: 1.0726 || timer: 0.0826 sec.
iter 105720 || Loss: 1.0831 || timer: 0.0891 sec.
iter 105730 || Loss: 0.8144 || timer: 0.0890 sec.
iter 105740 || Loss: 0.9438 || timer: 0.0824 sec.
iter 105750 || Loss: 0.9085 || timer: 0.0832 sec.
iter 105760 || Loss: 0.8135 || timer: 0.1048 sec.
iter 105770 || Loss: 1.2050 || timer: 0.0884 sec.
iter 105780 || Loss: 1.0293 || timer: 0.0882 sec.
iter 105790 || Loss: 0.9260 || timer: 0.0817 sec.
iter 105800 || Loss: 1.2358 || timer: 0.0853 sec.
iter 105810 || Loss: 1.1076 || timer: 0.0914 sec.
iter 105820 || Loss: 1.3031 || timer: 0.0160 sec.
iter 105830 || Loss: 0.7830 || timer: 0.0814 sec.
iter 105840 || Loss: 0.7698 || timer: 0.0891 sec.
iter 105850 || Loss: 1.6188 || timer: 0.0902 sec.
iter 105860 || Loss: 1.1468 || timer: 0.1103 sec.
iter 105870 || Loss: 0.8383 || timer: 0.0895 sec.
iter 105880 || Loss: 1.2209 || timer: 0.0866 sec.
iter 105890 || Loss: 1.3156 || timer: 0.0898 sec.
iter 105900 || Loss: 0.9308 || timer: 0.0993 sec.
iter 105910 || Loss: 0.7482 || timer: 0.0905 sec.
iter 105920 || Loss: 0.7932 || timer: 0.1119 sec.
iter 105930 || Loss: 1.0421 || timer: 0.0825 sec.
iter 105940 || Loss: 0.9568 || timer: 0.0912 sec.
iter 105950 || Loss: 1.0611 || timer: 0.0895 sec.
iter 105960 || Loss: 0.8342 || timer: 0.0955 sec.
iter 105970 || Loss: 1.0831 || timer: 0.0902 sec.
iter 105980 || Loss: 1.0493 || timer: 0.0901 sec.
iter 105990 || Loss: 0.9336 || timer: 0.0897 sec.
iter 106000 || Loss: 1.0940 || timer: 0.0970 sec.
iter 106010 || Loss: 0.8592 || timer: 0.0892 sec.
iter 106020 || Loss: 0.7816 || timer: 0.0887 sec.
iter 106030 || Loss: 0.7884 || timer: 0.0895 sec.
iter 106040 || Loss: 0.7656 || timer: 0.1008 sec.
iter 106050 || Loss: 0.7756 || timer: 0.0877 sec.
iter 106060 || Loss: 0.9757 || timer: 0.0915 sec.
iter 106070 || Loss: 1.2135 || timer: 0.0884 sec.
iter 106080 || Loss: 0.9789 || timer: 0.0811 sec.
iter 106090 || Loss: 0.7576 || timer: 0.1077 sec.
iter 106100 || Loss: 1.7571 || timer: 0.0865 sec.
iter 106110 || Loss: 0.8259 || timer: 0.0862 sec.
iter 106120 || Loss: 0.9954 || timer: 0.0920 sec.
iter 106130 || Loss: 0.6871 || timer: 0.0814 sec.
iter 106140 || Loss: 1.0244 || timer: 0.0964 sec.
iter 106150 || Loss: 0.9954 || timer: 0.0268 sec.
iter 106160 || Loss: 0.3053 || timer: 0.0819 sec.
iter 106170 || Loss: 1.0810 || timer: 0.0827 sec.
iter 106180 || Loss: 0.9218 || timer: 0.0894 sec.
iter 106190 || Loss: 0.8859 || timer: 0.0862 sec.
iter 106200 || Loss: 0.9475 || timer: 0.0896 sec.
iter 106210 || Loss: 1.0332 || timer: 0.0883 sec.
iter 106220 || Loss: 1.0621 || timer: 0.0819 sec.
iter 106230 || Loss: 0.9240 || timer: 0.0817 sec.
iter 106240 || Loss: 0.9004 || timer: 0.0892 sec.
iter 106250 || Loss: 1.3234 || timer: 0.0947 sec.
iter 106260 || Loss: 1.0280 || timer: 0.0810 sec.
iter 106270 || Loss: 0.7504 || timer: 0.1008 sec.
iter 106280 || Loss: 0.7776 || timer: 0.0909 sec.
iter 106290 || Loss: 1.1463 || timer: 0.0808 sec.
iter 106300 || Loss: 0.9440 || timer: 0.0825 sec.
iter 106310 || Loss: 0.7799 || timer: 0.1037 sec.
iter 106320 || Loss: 1.0263 || timer: 0.0787 sec.
iter 106330 || Loss: 0.9829 || timer: 0.0897 sec.
iter 106340 || Loss: 1.2515 || timer: 0.1091 sec.
iter 106350 || Loss: 0.9928 || timer: 0.0821 sec.
iter 106360 || Loss: 1.0681 || timer: 0.0829 sec.
iter 106370 || Loss: 0.9301 || timer: 0.0894 sec.
iter 106380 || Loss: 0.8399 || timer: 0.0819 sec.
iter 106390 || Loss: 1.0351 || timer: 0.1066 sec.
iter 106400 || Loss: 1.8605 || timer: 0.0902 sec.
iter 106410 || Loss: 1.1897 || timer: 0.1045 sec.
iter 106420 || Loss: 1.1825 || timer: 0.1086 sec.
iter 106430 || Loss: 0.8825 || timer: 0.0966 sec.
iter 106440 || Loss: 0.8192 || timer: 0.0815 sec.
iter 106450 || Loss: 1.1473 || timer: 0.0893 sec.
iter 106460 || Loss: 1.1232 || timer: 0.0797 sec.
iter 106470 || Loss: 0.8269 || timer: 0.0823 sec.
iter 106480 || Loss: 0.6909 || timer: 0.0159 sec.
iter 106490 || Loss: 1.2896 || timer: 0.0919 sec.
iter 106500 || Loss: 1.2174 || timer: 0.0813 sec.
iter 106510 || Loss: 0.9488 || timer: 0.0894 sec.
iter 106520 || Loss: 0.8047 || timer: 0.1141 sec.
iter 106530 || Loss: 1.0615 || timer: 0.0898 sec.
iter 106540 || Loss: 1.1211 || timer: 0.0895 sec.
iter 106550 || Loss: 1.2007 || timer: 0.0805 sec.
iter 106560 || Loss: 1.0804 || timer: 0.0895 sec.
iter 106570 || Loss: 1.4352 || timer: 0.1076 sec.
iter 106580 || Loss: 0.9226 || timer: 0.1159 sec.
iter 106590 || Loss: 1.2094 || timer: 0.1060 sec.
iter 106600 || Loss: 1.0010 || timer: 0.0947 sec.
iter 106610 || Loss: 1.3350 || timer: 0.0796 sec.
iter 106620 || Loss: 1.1038 || timer: 0.0890 sec.
iter 106630 || Loss: 0.9816 || timer: 0.0875 sec.
iter 106640 || Loss: 0.9772 || timer: 0.0832 sec.
iter 106650 || Loss: 0.9303 || timer: 0.0887 sec.
iter 106660 || Loss: 0.8546 || timer: 0.0903 sec.
iter 106670 || Loss: 1.2765 || timer: 0.0856 sec.
iter 106680 || Loss: 0.8914 || timer: 0.0830 sec.
iter 106690 || Loss: 1.0641 || timer: 0.0835 sec.
iter 106700 || Loss: 1.2509 || timer: 0.0971 sec.
iter 106710 || Loss: 0.7434 || timer: 0.0866 sec.
iter 106720 || Loss: 0.8652 || timer: 0.0975 sec.
iter 106730 || Loss: 1.2079 || timer: 0.0814 sec.
iter 106740 || Loss: 0.9458 || timer: 0.0876 sec.
iter 106750 || Loss: 0.9341 || timer: 0.1002 sec.
iter 106760 || Loss: 0.9765 || timer: 0.0890 sec.
iter 106770 || Loss: 1.0391 || timer: 0.0814 sec.
iter 106780 || Loss: 0.9833 || timer: 0.0906 sec.
iter 106790 || Loss: 1.1791 || timer: 0.0949 sec.
iter 106800 || Loss: 0.7788 || timer: 0.1003 sec.
iter 106810 || Loss: 0.7052 || timer: 0.0295 sec.
iter 106820 || Loss: 0.5764 || timer: 0.0836 sec.
iter 106830 || Loss: 1.0256 || timer: 0.0910 sec.
iter 106840 || Loss: 1.3417 || timer: 0.0898 sec.
iter 106850 || Loss: 0.8981 || timer: 0.0970 sec.
iter 106860 || Loss: 0.8857 || timer: 0.0886 sec.
iter 106870 || Loss: 0.9486 || timer: 0.0829 sec.
iter 106880 || Loss: 1.2367 || timer: 0.0971 sec.
iter 106890 || Loss: 1.3567 || timer: 0.0845 sec.
iter 106900 || Loss: 0.9059 || timer: 0.0901 sec.
iter 106910 || Loss: 0.8269 || timer: 0.1124 sec.
iter 106920 || Loss: 1.0524 || timer: 0.0888 sec.
iter 106930 || Loss: 0.8734 || timer: 0.0982 sec.
iter 106940 || Loss: 0.8734 || timer: 0.0834 sec.
iter 106950 || Loss: 0.9707 || timer: 0.1001 sec.
iter 106960 || Loss: 1.0089 || timer: 0.0909 sec.
iter 106970 || Loss: 0.9903 || timer: 0.0869 sec.
iter 106980 || Loss: 1.1484 || timer: 0.0803 sec.
iter 106990 || Loss: 0.8671 || timer: 0.0813 sec.
iter 107000 || Loss: 1.0224 || timer: 0.1015 sec.
iter 107010 || Loss: 0.7550 || timer: 0.1097 sec.
iter 107020 || Loss: 1.2837 || timer: 0.1216 sec.
iter 107030 || Loss: 0.8496 || timer: 0.1013 sec.
iter 107040 || Loss: 1.2392 || timer: 0.0895 sec.
iter 107050 || Loss: 1.0733 || timer: 0.0888 sec.
iter 107060 || Loss: 0.9702 || timer: 0.0889 sec.
iter 107070 || Loss: 0.9835 || timer: 0.0877 sec.
iter 107080 || Loss: 1.1665 || timer: 0.0838 sec.
iter 107090 || Loss: 0.8668 || timer: 0.0876 sec.
iter 107100 || Loss: 0.8960 || timer: 0.1131 sec.
iter 107110 || Loss: 0.5684 || timer: 0.0941 sec.
iter 107120 || Loss: 1.3788 || timer: 0.1021 sec.
iter 107130 || Loss: 1.0410 || timer: 0.0832 sec.
iter 107140 || Loss: 1.0451 || timer: 0.0253 sec.
iter 107150 || Loss: 1.4619 || timer: 0.0813 sec.
iter 107160 || Loss: 0.8028 || timer: 0.1171 sec.
iter 107170 || Loss: 1.1936 || timer: 0.0879 sec.
iter 107180 || Loss: 0.9324 || timer: 0.1153 sec.
iter 107190 || Loss: 1.3967 || timer: 0.0871 sec.
iter 107200 || Loss: 0.9243 || timer: 0.0912 sec.
iter 107210 || Loss: 0.6374 || timer: 0.0817 sec.
iter 107220 || Loss: 1.5082 || timer: 0.0885 sec.
iter 107230 || Loss: 1.2370 || timer: 0.0872 sec.
iter 107240 || Loss: 0.9664 || timer: 0.0942 sec.
iter 107250 || Loss: 0.9026 || timer: 0.0822 sec.
iter 107260 || Loss: 1.0261 || timer: 0.0879 sec.
iter 107270 || Loss: 1.0306 || timer: 0.0813 sec.
iter 107280 || Loss: 0.8380 || timer: 0.1081 sec.
iter 107290 || Loss: 0.8973 || timer: 0.1086 sec.
iter 107300 || Loss: 0.9707 || timer: 0.1333 sec.
iter 107310 || Loss: 0.8667 || timer: 0.0863 sec.
iter 107320 || Loss: 1.0871 || timer: 0.0895 sec.
iter 107330 || Loss: 0.9959 || timer: 0.0896 sec.
iter 107340 || Loss: 0.9021 || timer: 0.0924 sec.
iter 107350 || Loss: 1.1135 || timer: 0.1014 sec.
iter 107360 || Loss: 0.8456 || timer: 0.0846 sec.
iter 107370 || Loss: 1.3523 || timer: 0.0884 sec.
iter 107380 || Loss: 1.0953 || timer: 0.0857 sec.
iter 107390 || Loss: 0.9581 || timer: 0.0884 sec.
iter 107400 || Loss: 1.1354 || timer: 0.0871 sec.
iter 107410 || Loss: 1.3618 || timer: 0.0808 sec.
iter 107420 || Loss: 0.9530 || timer: 0.0881 sec.
iter 107430 || Loss: 1.1849 || timer: 0.0869 sec.
iter 107440 || Loss: 1.0939 || timer: 0.1006 sec.
iter 107450 || Loss: 1.7798 || timer: 0.0808 sec.
iter 107460 || Loss: 1.1208 || timer: 0.0888 sec.
iter 107470 || Loss: 1.0361 || timer: 0.0221 sec.
iter 107480 || Loss: 0.2683 || timer: 0.1114 sec.
iter 107490 || Loss: 0.7817 || timer: 0.0859 sec.
iter 107500 || Loss: 0.9617 || timer: 0.0819 sec.
iter 107510 || Loss: 1.0033 || timer: 0.0866 sec.
iter 107520 || Loss: 1.6601 || timer: 0.1092 sec.
iter 107530 || Loss: 1.0191 || timer: 0.1050 sec.
iter 107540 || Loss: 1.4032 || timer: 0.0893 sec.
iter 107550 || Loss: 1.4879 || timer: 0.0817 sec.
iter 107560 || Loss: 1.3489 || timer: 0.1200 sec.
iter 107570 || Loss: 1.0554 || timer: 0.0945 sec.
iter 107580 || Loss: 0.9021 || timer: 0.1159 sec.
iter 107590 || Loss: 1.1165 || timer: 0.0908 sec.
iter 107600 || Loss: 1.0963 || timer: 0.1128 sec.
iter 107610 || Loss: 0.9622 || timer: 0.0810 sec.
iter 107620 || Loss: 1.0650 || timer: 0.0820 sec.
iter 107630 || Loss: 1.0329 || timer: 0.0826 sec.
iter 107640 || Loss: 0.8551 || timer: 0.0882 sec.
iter 107650 || Loss: 1.1763 || timer: 0.0992 sec.
iter 107660 || Loss: 0.9835 || timer: 0.0902 sec.
iter 107670 || Loss: 0.7813 || timer: 0.0824 sec.
iter 107680 || Loss: 0.8305 || timer: 0.0873 sec.
iter 107690 || Loss: 1.0727 || timer: 0.0912 sec.
iter 107700 || Loss: 1.1864 || timer: 0.0823 sec.
iter 107710 || Loss: 0.9288 || timer: 0.0822 sec.
iter 107720 || Loss: 1.0245 || timer: 0.0849 sec.
iter 107730 || Loss: 0.9223 || timer: 0.0842 sec.
iter 107740 || Loss: 1.1625 || timer: 0.0806 sec.
iter 107750 || Loss: 0.9809 || timer: 0.0805 sec.
iter 107760 || Loss: 0.7722 || timer: 0.0849 sec.
iter 107770 || Loss: 0.8945 || timer: 0.0842 sec.
iter 107780 || Loss: 1.3050 || timer: 0.0971 sec.
iter 107790 || Loss: 1.0085 || timer: 0.0938 sec.
iter 107800 || Loss: 1.4933 || timer: 0.0273 sec.
iter 107810 || Loss: 1.1793 || timer: 0.0814 sec.
iter 107820 || Loss: 1.1714 || timer: 0.1153 sec.
iter 107830 || Loss: 0.9336 || timer: 0.0886 sec.
iter 107840 || Loss: 0.9805 || timer: 0.0819 sec.
iter 107850 || Loss: 1.3718 || timer: 0.0876 sec.
iter 107860 || Loss: 1.0538 || timer: 0.0876 sec.
iter 107870 || Loss: 0.9948 || timer: 0.0807 sec.
iter 107880 || Loss: 0.8405 || timer: 0.0804 sec.
iter 107890 || Loss: 0.9661 || timer: 0.0902 sec.
iter 107900 || Loss: 0.8547 || timer: 0.0991 sec.
iter 107910 || Loss: 0.9142 || timer: 0.0799 sec.
iter 107920 || Loss: 0.8550 || timer: 0.0888 sec.
iter 107930 || Loss: 1.2798 || timer: 0.0852 sec.
iter 107940 || Loss: 1.0507 || timer: 0.1263 sec.
iter 107950 || Loss: 0.9237 || timer: 0.0818 sec.
iter 107960 || Loss: 1.1124 || timer: 0.0828 sec.
iter 107970 || Loss: 1.2860 || timer: 0.0891 sec.
iter 107980 || Loss: 1.3371 || timer: 0.0861 sec.
iter 107990 || Loss: 0.8745 || timer: 0.1039 sec.
iter 108000 || Loss: 1.3221 || timer: 0.0880 sec.
iter 108010 || Loss: 1.0234 || timer: 0.1043 sec.
iter 108020 || Loss: 1.0484 || timer: 0.0880 sec.
iter 108030 || Loss: 1.1959 || timer: 0.0935 sec.
iter 108040 || Loss: 0.8474 || timer: 0.0816 sec.
iter 108050 || Loss: 1.3376 || timer: 0.0917 sec.
iter 108060 || Loss: 1.1245 || timer: 0.1189 sec.
iter 108070 || Loss: 1.3969 || timer: 0.0824 sec.
iter 108080 || Loss: 1.0200 || timer: 0.0899 sec.
iter 108090 || Loss: 1.2723 || timer: 0.0885 sec.
iter 108100 || Loss: 0.7708 || timer: 0.0887 sec.
iter 108110 || Loss: 1.1836 || timer: 0.0903 sec.
iter 108120 || Loss: 0.7758 || timer: 0.0883 sec.
iter 108130 || Loss: 1.0988 || timer: 0.0319 sec.
iter 108140 || Loss: 0.3641 || timer: 0.0887 sec.
iter 108150 || Loss: 1.0209 || timer: 0.0909 sec.
iter 108160 || Loss: 0.9630 || timer: 0.0828 sec.
iter 108170 || Loss: 1.0285 || timer: 0.0920 sec.
iter 108180 || Loss: 2.3845 || timer: 0.0842 sec.
iter 108190 || Loss: 2.1644 || timer: 0.0935 sec.
iter 108200 || Loss: 1.6320 || timer: 0.1018 sec.
iter 108210 || Loss: 1.7454 || timer: 0.0949 sec.
iter 108220 || Loss: 1.2908 || timer: 0.0952 sec.
iter 108230 || Loss: 1.3451 || timer: 0.0957 sec.
iter 108240 || Loss: 1.3541 || timer: 0.0795 sec.
iter 108250 || Loss: 1.6858 || timer: 0.1042 sec.
iter 108260 || Loss: 0.9861 || timer: 0.0834 sec.
iter 108270 || Loss: 1.1972 || timer: 0.0925 sec.
iter 108280 || Loss: 1.5030 || timer: 0.0843 sec.
iter 108290 || Loss: 1.1038 || timer: 0.0847 sec.
iter 108300 || Loss: 1.3033 || timer: 0.0817 sec.
iter 108310 || Loss: 1.1780 || timer: 0.1025 sec.
iter 108320 || Loss: 1.2955 || timer: 0.0909 sec.
iter 108330 || Loss: 1.0890 || timer: 0.0975 sec.
iter 108340 || Loss: 1.0978 || timer: 0.0896 sec.
iter 108350 || Loss: 1.1189 || timer: 0.0880 sec.
iter 108360 || Loss: 1.0534 || timer: 0.0910 sec.
iter 108370 || Loss: 1.5149 || timer: 0.0909 sec.
iter 108380 || Loss: 1.1147 || timer: 0.0860 sec.
iter 108390 || Loss: 1.3413 || timer: 0.0894 sec.
iter 108400 || Loss: 1.3641 || timer: 0.1026 sec.
iter 108410 || Loss: 1.3856 || timer: 0.0849 sec.
iter 108420 || Loss: 0.9259 || timer: 0.1052 sec.
iter 108430 || Loss: 1.1965 || timer: 0.0833 sec.
iter 108440 || Loss: 1.0577 || timer: 0.0885 sec.
iter 108450 || Loss: 1.3451 || timer: 0.0854 sec.
iter 108460 || Loss: 1.4607 || timer: 0.0220 sec.
iter 108470 || Loss: 0.7573 || timer: 0.0775 sec.
iter 108480 || Loss: 1.2597 || timer: 0.0834 sec.
iter 108490 || Loss: 1.2501 || timer: 0.0828 sec.
iter 108500 || Loss: 1.1112 || timer: 0.0982 sec.
iter 108510 || Loss: 1.0244 || timer: 0.0941 sec.
iter 108520 || Loss: 0.8135 || timer: 0.0826 sec.
iter 108530 || Loss: 0.7094 || timer: 0.1079 sec.
iter 108540 || Loss: 1.0187 || timer: 0.1003 sec.
iter 108550 || Loss: 1.1108 || timer: 0.0825 sec.
iter 108560 || Loss: 0.8428 || timer: 0.0933 sec.
iter 108570 || Loss: 0.9411 || timer: 0.1003 sec.
iter 108580 || Loss: 1.3011 || timer: 0.1258 sec.
iter 108590 || Loss: 0.9528 || timer: 0.0868 sec.
iter 108600 || Loss: 1.0264 || timer: 0.1131 sec.
iter 108610 || Loss: 1.2539 || timer: 0.0873 sec.
iter 108620 || Loss: 0.9817 || timer: 0.0814 sec.
iter 108630 || Loss: 0.6863 || timer: 0.0942 sec.
iter 108640 || Loss: 0.9011 || timer: 0.0878 sec.
iter 108650 || Loss: 1.5183 || timer: 0.0986 sec.
iter 108660 || Loss: 1.4749 || timer: 0.0826 sec.
iter 108670 || Loss: 1.1875 || timer: 0.0897 sec.
iter 108680 || Loss: 1.0922 || timer: 0.0858 sec.
iter 108690 || Loss: 0.8255 || timer: 0.0878 sec.
iter 108700 || Loss: 1.0786 || timer: 0.1002 sec.
iter 108710 || Loss: 0.9222 || timer: 0.0893 sec.
iter 108720 || Loss: 0.9049 || timer: 0.0950 sec.
iter 108730 || Loss: 1.5728 || timer: 0.0919 sec.
iter 108740 || Loss: 0.9519 || timer: 0.0824 sec.
iter 108750 || Loss: 0.8550 || timer: 0.1077 sec.
iter 108760 || Loss: 1.2232 || timer: 0.0888 sec.
iter 108770 || Loss: 1.0948 || timer: 0.0898 sec.
iter 108780 || Loss: 0.9237 || timer: 0.0872 sec.
iter 108790 || Loss: 1.1063 || timer: 0.0240 sec.
iter 108800 || Loss: 1.5501 || timer: 0.0816 sec.
iter 108810 || Loss: 1.0542 || timer: 0.0825 sec.
iter 108820 || Loss: 1.3212 || timer: 0.0822 sec.
iter 108830 || Loss: 0.8734 || timer: 0.0826 sec.
iter 108840 || Loss: 1.0149 || timer: 0.0886 sec.
iter 108850 || Loss: 1.1973 || timer: 0.1095 sec.
iter 108860 || Loss: 0.9712 || timer: 0.0806 sec.
iter 108870 || Loss: 1.2698 || timer: 0.0893 sec.
iter 108880 || Loss: 1.2580 || timer: 0.0900 sec.
iter 108890 || Loss: 0.9197 || timer: 0.0959 sec.
iter 108900 || Loss: 1.0479 || timer: 0.0806 sec.
iter 108910 || Loss: 0.9809 || timer: 0.0816 sec.
iter 108920 || Loss: 0.9397 || timer: 0.0813 sec.
iter 108930 || Loss: 1.5279 || timer: 0.0818 sec.
iter 108940 || Loss: 1.3253 || timer: 0.0878 sec.
iter 108950 || Loss: 0.9877 || timer: 0.0908 sec.
iter 108960 || Loss: 0.9098 || timer: 0.0923 sec.
iter 108970 || Loss: 0.7134 || timer: 0.0889 sec.
iter 108980 || Loss: 1.2134 || timer: 0.0893 sec.
iter 108990 || Loss: 1.6014 || timer: 0.0891 sec.
iter 109000 || Loss: 1.2698 || timer: 0.1023 sec.
iter 109010 || Loss: 0.9597 || timer: 0.0890 sec.
iter 109020 || Loss: 0.8979 || timer: 0.0868 sec.
iter 109030 || Loss: 1.0992 || timer: 0.0961 sec.
iter 109040 || Loss: 1.3061 || timer: 0.0811 sec.
iter 109050 || Loss: 1.2579 || timer: 0.1043 sec.
iter 109060 || Loss: 0.8748 || timer: 0.0911 sec.
iter 109070 || Loss: 1.3639 || timer: 0.1078 sec.
iter 109080 || Loss: 1.4871 || timer: 0.0810 sec.
iter 109090 || Loss: 0.8423 || timer: 0.0833 sec.
iter 109100 || Loss: 1.3698 || timer: 0.0875 sec.
iter 109110 || Loss: 1.1418 || timer: 0.0890 sec.
iter 109120 || Loss: 1.1204 || timer: 0.0227 sec.
iter 109130 || Loss: 0.2976 || timer: 0.0811 sec.
iter 109140 || Loss: 0.8077 || timer: 0.1253 sec.
iter 109150 || Loss: 1.0925 || timer: 0.1257 sec.
iter 109160 || Loss: 0.9484 || timer: 0.0859 sec.
iter 109170 || Loss: 0.8062 || timer: 0.0814 sec.
iter 109180 || Loss: 0.9118 || timer: 0.0977 sec.
iter 109190 || Loss: 1.0736 || timer: 0.0959 sec.
iter 109200 || Loss: 1.1660 || timer: 0.0805 sec.
iter 109210 || Loss: 1.2845 || timer: 0.0889 sec.
iter 109220 || Loss: 0.8941 || timer: 0.0939 sec.
iter 109230 || Loss: 0.8148 || timer: 0.0897 sec.
iter 109240 || Loss: 0.6986 || timer: 0.1164 sec.
iter 109250 || Loss: 0.9065 || timer: 0.0863 sec.
iter 109260 || Loss: 1.1332 || timer: 0.0877 sec.
iter 109270 || Loss: 1.1792 || timer: 0.0855 sec.
iter 109280 || Loss: 1.1287 || timer: 0.1056 sec.
iter 109290 || Loss: 0.9274 || timer: 0.1057 sec.
iter 109300 || Loss: 1.3807 || timer: 0.1020 sec.
iter 109310 || Loss: 1.4498 || timer: 0.0813 sec.
iter 109320 || Loss: 1.0488 || timer: 0.0820 sec.
iter 109330 || Loss: 1.0869 || timer: 0.0905 sec.
iter 109340 || Loss: 0.9411 || timer: 0.0822 sec.
iter 109350 || Loss: 1.2158 || timer: 0.0929 sec.
iter 109360 || Loss: 1.0872 || timer: 0.0814 sec.
iter 109370 || Loss: 1.0035 || timer: 0.0853 sec.
iter 109380 || Loss: 0.8408 || timer: 0.1055 sec.
iter 109390 || Loss: 1.1473 || timer: 0.0885 sec.
iter 109400 || Loss: 1.3464 || timer: 0.0825 sec.
iter 109410 || Loss: 0.8175 || timer: 0.0826 sec.
iter 109420 || Loss: 0.8900 || timer: 0.0813 sec.
iter 109430 || Loss: 0.8590 || timer: 0.0876 sec.
iter 109440 || Loss: 0.8562 || timer: 0.0884 sec.
iter 109450 || Loss: 1.0272 || timer: 0.0211 sec.
iter 109460 || Loss: 1.5486 || timer: 0.0835 sec.
iter 109470 || Loss: 0.9440 || timer: 0.0910 sec.
iter 109480 || Loss: 1.4326 || timer: 0.0870 sec.
iter 109490 || Loss: 0.9897 || timer: 0.0931 sec.
iter 109500 || Loss: 1.2884 || timer: 0.0890 sec.
iter 109510 || Loss: 1.2447 || timer: 0.0833 sec.
iter 109520 || Loss: 1.2402 || timer: 0.0900 sec.
iter 109530 || Loss: 1.0099 || timer: 0.0820 sec.
iter 109540 || Loss: 1.1877 || timer: 0.0972 sec.
iter 109550 || Loss: 0.8417 || timer: 0.1273 sec.
iter 109560 || Loss: 1.6181 || timer: 0.1118 sec.
iter 109570 || Loss: 1.2113 || timer: 0.0888 sec.
iter 109580 || Loss: 1.0097 || timer: 0.0815 sec.
iter 109590 || Loss: 1.0905 || timer: 0.0877 sec.
iter 109600 || Loss: 1.0577 || timer: 0.0812 sec.
iter 109610 || Loss: 1.0827 || timer: 0.0909 sec.
iter 109620 || Loss: 1.3676 || timer: 0.0884 sec.
iter 109630 || Loss: 0.8032 || timer: 0.0862 sec.
iter 109640 || Loss: 0.8924 || timer: 0.1088 sec.
iter 109650 || Loss: 0.9135 || timer: 0.0815 sec.
iter 109660 || Loss: 1.1115 || timer: 0.0860 sec.
iter 109670 || Loss: 0.6315 || timer: 0.0875 sec.
iter 109680 || Loss: 0.7454 || timer: 0.0890 sec.
iter 109690 || Loss: 0.8320 || timer: 0.0807 sec.
iter 109700 || Loss: 1.0010 || timer: 0.0823 sec.
iter 109710 || Loss: 0.8824 || timer: 0.0905 sec.
iter 109720 || Loss: 1.0180 || timer: 0.0887 sec.
iter 109730 || Loss: 0.9544 || timer: 0.0819 sec.
iter 109740 || Loss: 1.0065 || timer: 0.1070 sec.
iter 109750 || Loss: 1.0245 || timer: 0.0910 sec.
iter 109760 || Loss: 0.9440 || timer: 0.0870 sec.
iter 109770 || Loss: 0.9000 || timer: 0.0887 sec.
iter 109780 || Loss: 0.9992 || timer: 0.0198 sec.
iter 109790 || Loss: 2.0791 || timer: 0.1038 sec.
iter 109800 || Loss: 1.0544 || timer: 0.0889 sec.
iter 109810 || Loss: 0.8987 || timer: 0.0793 sec.
iter 109820 || Loss: 1.0638 || timer: 0.0911 sec.
iter 109830 || Loss: 1.0031 || timer: 0.1045 sec.
iter 109840 || Loss: 1.1899 || timer: 0.0898 sec.
iter 109850 || Loss: 0.9173 || timer: 0.0818 sec.
iter 109860 || Loss: 0.9682 || timer: 0.0889 sec.
iter 109870 || Loss: 0.9275 || timer: 0.0876 sec.
iter 109880 || Loss: 0.9433 || timer: 0.1019 sec.
iter 109890 || Loss: 1.3262 || timer: 0.0799 sec.
iter 109900 || Loss: 1.0642 || timer: 0.0920 sec.
iter 109910 || Loss: 0.7251 || timer: 0.0901 sec.
iter 109920 || Loss: 1.1463 || timer: 0.1035 sec.
iter 109930 || Loss: 1.4145 || timer: 0.0885 sec.
iter 109940 || Loss: 1.0732 || timer: 0.0880 sec.
iter 109950 || Loss: 0.9829 || timer: 0.0909 sec.
iter 109960 || Loss: 1.1467 || timer: 0.0903 sec.
iter 109970 || Loss: 1.1930 || timer: 0.0965 sec.
iter 109980 || Loss: 1.0564 || timer: 0.0825 sec.
iter 109990 || Loss: 1.5367 || timer: 0.0901 sec.
iter 110000 || Loss: 1.0988 || Saving state, iter: 110000
timer: 0.1038 sec.
iter 110010 || Loss: 1.0532 || timer: 0.0758 sec.
iter 110020 || Loss: 1.3554 || timer: 0.0838 sec.
iter 110030 || Loss: 0.9585 || timer: 0.0925 sec.
iter 110040 || Loss: 0.9814 || timer: 0.0874 sec.
iter 110050 || Loss: 0.9489 || timer: 0.0844 sec.
iter 110060 || Loss: 0.8853 || timer: 0.0908 sec.
iter 110070 || Loss: 0.9537 || timer: 0.1119 sec.
iter 110080 || Loss: 0.8596 || timer: 0.0909 sec.
iter 110090 || Loss: 0.8447 || timer: 0.0928 sec.
iter 110100 || Loss: 1.0352 || timer: 0.0919 sec.
iter 110110 || Loss: 0.9199 || timer: 0.0253 sec.
iter 110120 || Loss: 0.4082 || timer: 0.0844 sec.
iter 110130 || Loss: 1.1112 || timer: 0.0889 sec.
iter 110140 || Loss: 1.2002 || timer: 0.1000 sec.
iter 110150 || Loss: 1.2901 || timer: 0.1096 sec.
iter 110160 || Loss: 0.9705 || timer: 0.0994 sec.
iter 110170 || Loss: 1.3090 || timer: 0.0812 sec.
iter 110180 || Loss: 1.1368 || timer: 0.0914 sec.
iter 110190 || Loss: 1.3001 || timer: 0.1011 sec.
iter 110200 || Loss: 0.7178 || timer: 0.0908 sec.
iter 110210 || Loss: 0.8311 || timer: 0.0985 sec.
iter 110220 || Loss: 0.8198 || timer: 0.1080 sec.
iter 110230 || Loss: 0.9202 || timer: 0.0884 sec.
iter 110240 || Loss: 0.8670 || timer: 0.0904 sec.
iter 110250 || Loss: 0.9197 || timer: 0.0992 sec.
iter 110260 || Loss: 0.8432 || timer: 0.0808 sec.
iter 110270 || Loss: 0.9236 || timer: 0.0897 sec.
iter 110280 || Loss: 0.7841 || timer: 0.0874 sec.
iter 110290 || Loss: 0.9832 || timer: 0.0903 sec.
iter 110300 || Loss: 1.0831 || timer: 0.0899 sec.
iter 110310 || Loss: 1.2637 || timer: 0.0902 sec.
iter 110320 || Loss: 1.0620 || timer: 0.0895 sec.
iter 110330 || Loss: 1.0669 || timer: 0.0823 sec.
iter 110340 || Loss: 0.7838 || timer: 0.0887 sec.
iter 110350 || Loss: 0.8686 || timer: 0.0824 sec.
iter 110360 || Loss: 1.1292 || timer: 0.0984 sec.
iter 110370 || Loss: 1.0932 || timer: 0.0909 sec.
iter 110380 || Loss: 0.8330 || timer: 0.0821 sec.
iter 110390 || Loss: 1.0044 || timer: 0.0872 sec.
iter 110400 || Loss: 1.2936 || timer: 0.0917 sec.
iter 110410 || Loss: 1.0181 || timer: 0.0894 sec.
iter 110420 || Loss: 1.0996 || timer: 0.0909 sec.
iter 110430 || Loss: 0.8854 || timer: 0.1072 sec.
iter 110440 || Loss: 0.8894 || timer: 0.0158 sec.
iter 110450 || Loss: 0.5901 || timer: 0.0887 sec.
iter 110460 || Loss: 0.9174 || timer: 0.0881 sec.
iter 110470 || Loss: 0.9014 || timer: 0.0977 sec.
iter 110480 || Loss: 1.2006 || timer: 0.0828 sec.
iter 110490 || Loss: 0.7331 || timer: 0.0854 sec.
iter 110500 || Loss: 1.1957 || timer: 0.0867 sec.
iter 110510 || Loss: 0.9709 || timer: 0.0973 sec.
iter 110520 || Loss: 1.3634 || timer: 0.0879 sec.
iter 110530 || Loss: 0.8298 || timer: 0.0888 sec.
iter 110540 || Loss: 1.3667 || timer: 0.0940 sec.
iter 110550 || Loss: 1.4919 || timer: 0.0816 sec.
iter 110560 || Loss: 0.8486 || timer: 0.0945 sec.
iter 110570 || Loss: 0.9787 || timer: 0.0899 sec.
iter 110580 || Loss: 1.0479 || timer: 0.0865 sec.
iter 110590 || Loss: 1.0542 || timer: 0.0877 sec.
iter 110600 || Loss: 1.2567 || timer: 0.0918 sec.
iter 110610 || Loss: 1.0957 || timer: 0.0907 sec.
iter 110620 || Loss: 0.9101 || timer: 0.0878 sec.
iter 110630 || Loss: 0.9345 || timer: 0.1064 sec.
iter 110640 || Loss: 0.8099 || timer: 0.1055 sec.
iter 110650 || Loss: 0.7152 || timer: 0.0889 sec.
iter 110660 || Loss: 0.9819 || timer: 0.0864 sec.
iter 110670 || Loss: 1.1448 || timer: 0.0920 sec.
iter 110680 || Loss: 1.0984 || timer: 0.0909 sec.
iter 110690 || Loss: 1.0318 || timer: 0.0942 sec.
iter 110700 || Loss: 0.7405 || timer: 0.1183 sec.
iter 110710 || Loss: 1.0040 || timer: 0.0912 sec.
iter 110720 || Loss: 0.7943 || timer: 0.0914 sec.
iter 110730 || Loss: 1.0665 || timer: 0.0811 sec.
iter 110740 || Loss: 0.9550 || timer: 0.0826 sec.
iter 110750 || Loss: 0.7587 || timer: 0.0881 sec.
iter 110760 || Loss: 0.9201 || timer: 0.1003 sec.
iter 110770 || Loss: 0.9328 || timer: 0.0266 sec.
iter 110780 || Loss: 0.5051 || timer: 0.0821 sec.
iter 110790 || Loss: 1.0438 || timer: 0.1101 sec.
iter 110800 || Loss: 0.8028 || timer: 0.0888 sec.
iter 110810 || Loss: 1.0864 || timer: 0.0805 sec.
iter 110820 || Loss: 0.9651 || timer: 0.0808 sec.
iter 110830 || Loss: 1.0180 || timer: 0.0990 sec.
iter 110840 || Loss: 0.8770 || timer: 0.0821 sec.
iter 110850 || Loss: 1.3519 || timer: 0.0807 sec.
iter 110860 || Loss: 1.2596 || timer: 0.0888 sec.
iter 110870 || Loss: 1.1700 || timer: 0.1024 sec.
iter 110880 || Loss: 1.3051 || timer: 0.0859 sec.
iter 110890 || Loss: 0.8770 || timer: 0.0818 sec.
iter 110900 || Loss: 0.8666 || timer: 0.0800 sec.
iter 110910 || Loss: 0.9463 || timer: 0.0979 sec.
iter 110920 || Loss: 0.8717 || timer: 0.0871 sec.
iter 110930 || Loss: 1.3746 || timer: 0.0908 sec.
iter 110940 || Loss: 1.0866 || timer: 0.1039 sec.
iter 110950 || Loss: 1.1075 || timer: 0.0881 sec.
iter 110960 || Loss: 0.9578 || timer: 0.0906 sec.
iter 110970 || Loss: 1.0478 || timer: 0.0818 sec.
iter 110980 || Loss: 0.7330 || timer: 0.0818 sec.
iter 110990 || Loss: 1.3175 || timer: 0.1095 sec.
iter 111000 || Loss: 0.8169 || timer: 0.0881 sec.
iter 111010 || Loss: 1.0674 || timer: 0.0857 sec.
iter 111020 || Loss: 0.7930 || timer: 0.1099 sec.
iter 111030 || Loss: 1.2300 || timer: 0.1100 sec.
iter 111040 || Loss: 0.8626 || timer: 0.0817 sec.
iter 111050 || Loss: 0.7941 || timer: 0.0864 sec.
iter 111060 || Loss: 1.1351 || timer: 0.0899 sec.
iter 111070 || Loss: 1.0584 || timer: 0.0830 sec.
iter 111080 || Loss: 0.8854 || timer: 0.0897 sec.
iter 111090 || Loss: 0.9765 || timer: 0.0904 sec.
iter 111100 || Loss: 0.9723 || timer: 0.0199 sec.
iter 111110 || Loss: 1.6357 || timer: 0.0885 sec.
iter 111120 || Loss: 1.0647 || timer: 0.0913 sec.
iter 111130 || Loss: 1.1845 || timer: 0.0822 sec.
iter 111140 || Loss: 0.8329 || timer: 0.0871 sec.
iter 111150 || Loss: 1.5322 || timer: 0.0890 sec.
iter 111160 || Loss: 0.6569 || timer: 0.0900 sec.
iter 111170 || Loss: 1.0883 || timer: 0.0939 sec.
iter 111180 || Loss: 1.0723 || timer: 0.0877 sec.
iter 111190 || Loss: 1.2138 || timer: 0.0836 sec.
iter 111200 || Loss: 0.9582 || timer: 0.1169 sec.
iter 111210 || Loss: 0.8213 || timer: 0.0908 sec.
iter 111220 || Loss: 0.8457 || timer: 0.0875 sec.
iter 111230 || Loss: 1.1704 || timer: 0.0978 sec.
iter 111240 || Loss: 1.3628 || timer: 0.0901 sec.
iter 111250 || Loss: 0.8414 || timer: 0.1053 sec.
iter 111260 || Loss: 0.8944 || timer: 0.0891 sec.
iter 111270 || Loss: 0.7141 || timer: 0.0898 sec.
iter 111280 || Loss: 1.1560 || timer: 0.0894 sec.
iter 111290 || Loss: 1.0276 || timer: 0.0832 sec.
iter 111300 || Loss: 0.7686 || timer: 0.0912 sec.
iter 111310 || Loss: 1.2008 || timer: 0.0818 sec.
iter 111320 || Loss: 0.8585 || timer: 0.0814 sec.
iter 111330 || Loss: 0.7471 || timer: 0.0813 sec.
iter 111340 || Loss: 1.1720 || timer: 0.1050 sec.
iter 111350 || Loss: 0.9486 || timer: 0.0901 sec.
iter 111360 || Loss: 1.1128 || timer: 0.0828 sec.
iter 111370 || Loss: 1.2006 || timer: 0.0958 sec.
iter 111380 || Loss: 0.6629 || timer: 0.0930 sec.
iter 111390 || Loss: 0.7763 || timer: 0.0884 sec.
iter 111400 || Loss: 0.6946 || timer: 0.0889 sec.
iter 111410 || Loss: 1.0147 || timer: 0.0872 sec.
iter 111420 || Loss: 1.1286 || timer: 0.0912 sec.
iter 111430 || Loss: 1.2030 || timer: 0.0170 sec.
iter 111440 || Loss: 0.5840 || timer: 0.0849 sec.
iter 111450 || Loss: 0.8414 || timer: 0.0900 sec.
iter 111460 || Loss: 0.7780 || timer: 0.1155 sec.
iter 111470 || Loss: 1.1192 || timer: 0.0908 sec.
iter 111480 || Loss: 0.8514 || timer: 0.0828 sec.
iter 111490 || Loss: 0.7826 || timer: 0.1077 sec.
iter 111500 || Loss: 1.1246 || timer: 0.1147 sec.
iter 111510 || Loss: 1.0604 || timer: 0.1043 sec.
iter 111520 || Loss: 1.1730 || timer: 0.0833 sec.
iter 111530 || Loss: 0.8782 || timer: 0.0955 sec.
iter 111540 || Loss: 0.8594 || timer: 0.1059 sec.
iter 111550 || Loss: 1.2444 || timer: 0.0986 sec.
iter 111560 || Loss: 0.7060 || timer: 0.1079 sec.
iter 111570 || Loss: 0.8348 || timer: 0.0912 sec.
iter 111580 || Loss: 0.8578 || timer: 0.1036 sec.
iter 111590 || Loss: 1.2494 || timer: 0.0906 sec.
iter 111600 || Loss: 1.0513 || timer: 0.0909 sec.
iter 111610 || Loss: 0.7664 || timer: 0.0887 sec.
iter 111620 || Loss: 1.1509 || timer: 0.0879 sec.
iter 111630 || Loss: 1.1506 || timer: 0.0894 sec.
iter 111640 || Loss: 0.9449 || timer: 0.0830 sec.
iter 111650 || Loss: 1.0853 || timer: 0.1077 sec.
iter 111660 || Loss: 1.1886 || timer: 0.0916 sec.
iter 111670 || Loss: 0.8218 || timer: 0.0825 sec.
iter 111680 || Loss: 1.5986 || timer: 0.1395 sec.
iter 111690 || Loss: 0.9706 || timer: 0.0883 sec.
iter 111700 || Loss: 0.9314 || timer: 0.0893 sec.
iter 111710 || Loss: 1.1503 || timer: 0.0882 sec.
iter 111720 || Loss: 0.8508 || timer: 0.0834 sec.
iter 111730 || Loss: 1.2802 || timer: 0.0906 sec.
iter 111740 || Loss: 1.1252 || timer: 0.0946 sec.
iter 111750 || Loss: 1.0003 || timer: 0.0910 sec.
iter 111760 || Loss: 1.0946 || timer: 0.0241 sec.
iter 111770 || Loss: 0.3028 || timer: 0.0758 sec.
iter 111780 || Loss: 1.0425 || timer: 0.0812 sec.
iter 111790 || Loss: 0.7175 || timer: 0.0897 sec.
iter 111800 || Loss: 1.3481 || timer: 0.0879 sec.
iter 111810 || Loss: 0.7843 || timer: 0.0821 sec.
iter 111820 || Loss: 0.8073 || timer: 0.0908 sec.
iter 111830 || Loss: 1.3822 || timer: 0.0895 sec.
iter 111840 || Loss: 1.2275 || timer: 0.0903 sec.
iter 111850 || Loss: 0.9884 || timer: 0.0904 sec.
iter 111860 || Loss: 1.0418 || timer: 0.0956 sec.
iter 111870 || Loss: 1.2803 || timer: 0.1031 sec.
iter 111880 || Loss: 0.9503 || timer: 0.0887 sec.
iter 111890 || Loss: 1.2152 || timer: 0.0826 sec.
iter 111900 || Loss: 0.9456 || timer: 0.0823 sec.
iter 111910 || Loss: 1.0060 || timer: 0.0927 sec.
iter 111920 || Loss: 0.6149 || timer: 0.0891 sec.
iter 111930 || Loss: 0.8812 || timer: 0.0809 sec.
iter 111940 || Loss: 1.4515 || timer: 0.0851 sec.
iter 111950 || Loss: 0.7314 || timer: 0.1070 sec.
iter 111960 || Loss: 1.1503 || timer: 0.0899 sec.
iter 111970 || Loss: 0.6541 || timer: 0.0824 sec.
iter 111980 || Loss: 0.9719 || timer: 0.0984 sec.
iter 111990 || Loss: 0.7489 || timer: 0.0819 sec.
iter 112000 || Loss: 1.1888 || timer: 0.0904 sec.
iter 112010 || Loss: 1.2844 || timer: 0.0831 sec.
iter 112020 || Loss: 1.1083 || timer: 0.0948 sec.
iter 112030 || Loss: 1.0996 || timer: 0.0933 sec.
iter 112040 || Loss: 0.9136 || timer: 0.0902 sec.
iter 112050 || Loss: 1.0342 || timer: 0.0919 sec.
iter 112060 || Loss: 1.3762 || timer: 0.1286 sec.
iter 112070 || Loss: 1.0468 || timer: 0.0830 sec.
iter 112080 || Loss: 1.3226 || timer: 0.0831 sec.
iter 112090 || Loss: 0.9657 || timer: 0.0174 sec.
iter 112100 || Loss: 0.9220 || timer: 0.1004 sec.
iter 112110 || Loss: 1.1194 || timer: 0.1063 sec.
iter 112120 || Loss: 0.7924 || timer: 0.1034 sec.
iter 112130 || Loss: 1.1016 || timer: 0.0912 sec.
iter 112140 || Loss: 1.4292 || timer: 0.0882 sec.
iter 112150 || Loss: 1.0848 || timer: 0.0825 sec.
iter 112160 || Loss: 1.2922 || timer: 0.1110 sec.
iter 112170 || Loss: 2.0165 || timer: 0.0823 sec.
iter 112180 || Loss: 1.6144 || timer: 0.0894 sec.
iter 112190 || Loss: 1.8113 || timer: 0.0967 sec.
iter 112200 || Loss: 1.0615 || timer: 0.1112 sec.
iter 112210 || Loss: 1.0815 || timer: 0.0908 sec.
iter 112220 || Loss: 1.2123 || timer: 0.0846 sec.
iter 112230 || Loss: 0.7683 || timer: 0.0903 sec.
iter 112240 || Loss: 1.2591 || timer: 0.0892 sec.
iter 112250 || Loss: 0.9749 || timer: 0.0898 sec.
iter 112260 || Loss: 0.9542 || timer: 0.0906 sec.
iter 112270 || Loss: 0.7740 || timer: 0.0825 sec.
iter 112280 || Loss: 0.8937 || timer: 0.0832 sec.
iter 112290 || Loss: 0.9620 || timer: 0.0897 sec.
iter 112300 || Loss: 0.7036 || timer: 0.0898 sec.
iter 112310 || Loss: 1.0098 || timer: 0.0888 sec.
iter 112320 || Loss: 0.9431 || timer: 0.0930 sec.
iter 112330 || Loss: 0.9685 || timer: 0.0814 sec.
iter 112340 || Loss: 1.0074 || timer: 0.0825 sec.
iter 112350 || Loss: 0.8764 || timer: 0.0809 sec.
iter 112360 || Loss: 1.1134 || timer: 0.0861 sec.
iter 112370 || Loss: 1.1299 || timer: 0.0825 sec.
iter 112380 || Loss: 1.1967 || timer: 0.0827 sec.
iter 112390 || Loss: 0.9582 || timer: 0.0955 sec.
iter 112400 || Loss: 1.1097 || timer: 0.0913 sec.
iter 112410 || Loss: 0.8647 || timer: 0.0891 sec.
iter 112420 || Loss: 1.2037 || timer: 0.0235 sec.
iter 112430 || Loss: 0.6777 || timer: 0.0902 sec.
iter 112440 || Loss: 1.2721 || timer: 0.0974 sec.
iter 112450 || Loss: 1.2216 || timer: 0.0865 sec.
iter 112460 || Loss: 1.0986 || timer: 0.1034 sec.
iter 112470 || Loss: 0.8658 || timer: 0.0871 sec.
iter 112480 || Loss: 0.8035 || timer: 0.0821 sec.
iter 112490 || Loss: 0.9668 || timer: 0.0910 sec.
iter 112500 || Loss: 0.9983 || timer: 0.0983 sec.
iter 112510 || Loss: 1.0367 || timer: 0.0885 sec.
iter 112520 || Loss: 0.9763 || timer: 0.1156 sec.
iter 112530 || Loss: 1.0969 || timer: 0.0816 sec.
iter 112540 || Loss: 1.3204 || timer: 0.0825 sec.
iter 112550 || Loss: 0.9015 || timer: 0.0911 sec.
iter 112560 || Loss: 0.9957 || timer: 0.0821 sec.
iter 112570 || Loss: 1.0990 || timer: 0.0882 sec.
iter 112580 || Loss: 1.1375 || timer: 0.0820 sec.
iter 112590 || Loss: 1.0372 || timer: 0.0818 sec.
iter 112600 || Loss: 1.0809 || timer: 0.0903 sec.
iter 112610 || Loss: 0.9733 || timer: 0.0962 sec.
iter 112620 || Loss: 1.4463 || timer: 0.0830 sec.
iter 112630 || Loss: 1.1876 || timer: 0.0885 sec.
iter 112640 || Loss: 1.2242 || timer: 0.0887 sec.
iter 112650 || Loss: 0.8461 || timer: 0.0825 sec.
iter 112660 || Loss: 0.9157 || timer: 0.0966 sec.
iter 112670 || Loss: 0.8882 || timer: 0.1039 sec.
iter 112680 || Loss: 0.9977 || timer: 0.0914 sec.
iter 112690 || Loss: 1.1660 || timer: 0.1215 sec.
iter 112700 || Loss: 0.9667 || timer: 0.0828 sec.
iter 112710 || Loss: 1.1288 || timer: 0.0827 sec.
iter 112720 || Loss: 0.9496 || timer: 0.0876 sec.
iter 112730 || Loss: 0.8542 || timer: 0.1039 sec.
iter 112740 || Loss: 0.8025 || timer: 0.0876 sec.
iter 112750 || Loss: 0.7850 || timer: 0.0207 sec.
iter 112760 || Loss: 0.8418 || timer: 0.0900 sec.
iter 112770 || Loss: 0.9670 || timer: 0.0900 sec.
iter 112780 || Loss: 0.7975 || timer: 0.0840 sec.
iter 112790 || Loss: 1.1223 || timer: 0.0838 sec.
iter 112800 || Loss: 0.9924 || timer: 0.1049 sec.
iter 112810 || Loss: 1.1369 || timer: 0.0884 sec.
iter 112820 || Loss: 0.8820 || timer: 0.1056 sec.
iter 112830 || Loss: 1.3272 || timer: 0.0892 sec.
iter 112840 || Loss: 1.1355 || timer: 0.0892 sec.
iter 112850 || Loss: 1.0726 || timer: 0.0967 sec.
iter 112860 || Loss: 0.9212 || timer: 0.0895 sec.
iter 112870 || Loss: 1.0295 || timer: 0.0902 sec.
iter 112880 || Loss: 0.9352 || timer: 0.0833 sec.
iter 112890 || Loss: 0.7987 || timer: 0.0824 sec.
iter 112900 || Loss: 1.0545 || timer: 0.1046 sec.
iter 112910 || Loss: 0.8278 || timer: 0.0904 sec.
iter 112920 || Loss: 1.1560 || timer: 0.0826 sec.
iter 112930 || Loss: 1.1226 || timer: 0.0899 sec.
iter 112940 || Loss: 1.0153 || timer: 0.0907 sec.
iter 112950 || Loss: 0.9225 || timer: 0.0835 sec.
iter 112960 || Loss: 1.1392 || timer: 0.0898 sec.
iter 112970 || Loss: 1.2764 || timer: 0.0884 sec.
iter 112980 || Loss: 1.3513 || timer: 0.0915 sec.
iter 112990 || Loss: 0.7145 || timer: 0.0950 sec.
iter 113000 || Loss: 0.9858 || timer: 0.0878 sec.
iter 113010 || Loss: 1.0310 || timer: 0.1079 sec.
iter 113020 || Loss: 1.2029 || timer: 0.1012 sec.
iter 113030 || Loss: 0.9441 || timer: 0.0830 sec.
iter 113040 || Loss: 0.7964 || timer: 0.0824 sec.
iter 113050 || Loss: 0.8910 || timer: 0.0885 sec.
iter 113060 || Loss: 0.9705 || timer: 0.0870 sec.
iter 113070 || Loss: 1.2566 || timer: 0.0897 sec.
iter 113080 || Loss: 1.1248 || timer: 0.0252 sec.
iter 113090 || Loss: 1.0924 || timer: 0.0868 sec.
iter 113100 || Loss: 0.9687 || timer: 0.1043 sec.
iter 113110 || Loss: 0.9673 || timer: 0.0906 sec.
iter 113120 || Loss: 1.0666 || timer: 0.0882 sec.
iter 113130 || Loss: 1.1889 || timer: 0.0947 sec.
iter 113140 || Loss: 0.7759 || timer: 0.1016 sec.
iter 113150 || Loss: 0.9466 || timer: 0.0890 sec.
iter 113160 || Loss: 1.0427 || timer: 0.0893 sec.
iter 113170 || Loss: 1.1105 || timer: 0.0887 sec.
iter 113180 || Loss: 0.9539 || timer: 0.0917 sec.
iter 113190 || Loss: 0.9399 || timer: 0.0820 sec.
iter 113200 || Loss: 1.2980 || timer: 0.0890 sec.
iter 113210 || Loss: 1.1590 || timer: 0.0814 sec.
iter 113220 || Loss: 0.9481 || timer: 0.0984 sec.
iter 113230 || Loss: 0.9884 || timer: 0.0917 sec.
iter 113240 || Loss: 1.0922 || timer: 0.0989 sec.
iter 113250 || Loss: 0.8857 || timer: 0.1050 sec.
iter 113260 || Loss: 1.0290 || timer: 0.0988 sec.
iter 113270 || Loss: 1.0008 || timer: 0.1000 sec.
iter 113280 || Loss: 0.6956 || timer: 0.0877 sec.
iter 113290 || Loss: 0.9476 || timer: 0.1117 sec.
iter 113300 || Loss: 0.9137 || timer: 0.0910 sec.
iter 113310 || Loss: 0.9645 || timer: 0.0837 sec.
iter 113320 || Loss: 1.0420 || timer: 0.0893 sec.
iter 113330 || Loss: 1.0618 || timer: 0.0875 sec.
iter 113340 || Loss: 1.0552 || timer: 0.0818 sec.
iter 113350 || Loss: 1.2601 || timer: 0.1046 sec.
iter 113360 || Loss: 0.9669 || timer: 0.0977 sec.
iter 113370 || Loss: 0.9634 || timer: 0.1083 sec.
iter 113380 || Loss: 1.3405 || timer: 0.1122 sec.
iter 113390 || Loss: 0.9560 || timer: 0.0828 sec.
iter 113400 || Loss: 1.1256 || timer: 0.1139 sec.
iter 113410 || Loss: 0.9749 || timer: 0.0232 sec.
iter 113420 || Loss: 0.5971 || timer: 0.0924 sec.
iter 113430 || Loss: 0.8190 || timer: 0.1040 sec.
iter 113440 || Loss: 0.9523 || timer: 0.0897 sec.
iter 113450 || Loss: 1.0023 || timer: 0.0845 sec.
iter 113460 || Loss: 1.2954 || timer: 0.0973 sec.
iter 113470 || Loss: 1.0366 || timer: 0.1070 sec.
iter 113480 || Loss: 1.0596 || timer: 0.0821 sec.
iter 113490 || Loss: 1.1745 || timer: 0.0907 sec.
iter 113500 || Loss: 1.0066 || timer: 0.0891 sec.
iter 113510 || Loss: 0.8094 || timer: 0.1190 sec.
iter 113520 || Loss: 0.9769 || timer: 0.0812 sec.
iter 113530 || Loss: 0.8058 || timer: 0.0890 sec.
iter 113540 || Loss: 0.9776 || timer: 0.0892 sec.
iter 113550 || Loss: 1.1764 || timer: 0.0829 sec.
iter 113560 || Loss: 0.9016 || timer: 0.1311 sec.
iter 113570 || Loss: 0.9316 || timer: 0.0887 sec.
iter 113580 || Loss: 1.0104 || timer: 0.0892 sec.
iter 113590 || Loss: 1.4434 || timer: 0.0816 sec.
iter 113600 || Loss: 1.8600 || timer: 0.0874 sec.
iter 113610 || Loss: 1.5228 || timer: 0.0809 sec.
iter 113620 || Loss: 1.1056 || timer: 0.0916 sec.
iter 113630 || Loss: 1.0472 || timer: 0.0807 sec.
iter 113640 || Loss: 0.9610 || timer: 0.0814 sec.
iter 113650 || Loss: 0.9089 || timer: 0.0976 sec.
iter 113660 || Loss: 1.0983 || timer: 0.1122 sec.
iter 113670 || Loss: 0.9718 || timer: 0.0824 sec.
iter 113680 || Loss: 0.9158 || timer: 0.0887 sec.
iter 113690 || Loss: 0.9106 || timer: 0.0899 sec.
iter 113700 || Loss: 0.7360 || timer: 0.0899 sec.
iter 113710 || Loss: 0.8940 || timer: 0.0755 sec.
iter 113720 || Loss: 0.6494 || timer: 0.0819 sec.
iter 113730 || Loss: 1.1127 || timer: 0.0873 sec.
iter 113740 || Loss: 1.3017 || timer: 0.0258 sec.
iter 113750 || Loss: 1.1358 || timer: 0.0912 sec.
iter 113760 || Loss: 1.4430 || timer: 0.0973 sec.
iter 113770 || Loss: 0.8388 || timer: 0.0834 sec.
iter 113780 || Loss: 0.9832 || timer: 0.0856 sec.
iter 113790 || Loss: 1.0619 || timer: 0.0949 sec.
iter 113800 || Loss: 1.0694 || timer: 0.0843 sec.
iter 113810 || Loss: 0.9733 || timer: 0.0887 sec.
iter 113820 || Loss: 0.9095 || timer: 0.0820 sec.
iter 113830 || Loss: 0.8241 || timer: 0.0896 sec.
iter 113840 || Loss: 1.0126 || timer: 0.0966 sec.
iter 113850 || Loss: 0.9219 || timer: 0.1016 sec.
iter 113860 || Loss: 0.8643 || timer: 0.0901 sec.
iter 113870 || Loss: 1.0154 || timer: 0.0850 sec.
iter 113880 || Loss: 0.8279 || timer: 0.0894 sec.
iter 113890 || Loss: 1.0839 || timer: 0.0837 sec.
iter 113900 || Loss: 1.2005 || timer: 0.1027 sec.
iter 113910 || Loss: 0.8800 || timer: 0.0882 sec.
iter 113920 || Loss: 0.8101 || timer: 0.0833 sec.
iter 113930 || Loss: 0.9254 || timer: 0.0919 sec.
iter 113940 || Loss: 0.7807 || timer: 0.0842 sec.
iter 113950 || Loss: 0.9274 || timer: 0.0945 sec.
iter 113960 || Loss: 1.0594 || timer: 0.0828 sec.
iter 113970 || Loss: 1.0872 || timer: 0.0894 sec.
iter 113980 || Loss: 1.1428 || timer: 0.0877 sec.
iter 113990 || Loss: 0.6758 || timer: 0.0842 sec.
iter 114000 || Loss: 0.8257 || timer: 0.0887 sec.
iter 114010 || Loss: 0.7860 || timer: 0.0818 sec.
iter 114020 || Loss: 1.1603 || timer: 0.1167 sec.
iter 114030 || Loss: 1.1404 || timer: 0.0847 sec.
iter 114040 || Loss: 1.3565 || timer: 0.0888 sec.
iter 114050 || Loss: 0.9157 || timer: 0.0875 sec.
iter 114060 || Loss: 0.8825 || timer: 0.0909 sec.
iter 114070 || Loss: 1.0727 || timer: 0.0249 sec.
iter 114080 || Loss: 1.5195 || timer: 0.0830 sec.
iter 114090 || Loss: 1.2825 || timer: 0.1056 sec.
iter 114100 || Loss: 0.8925 || timer: 0.0883 sec.
iter 114110 || Loss: 0.9056 || timer: 0.0955 sec.
iter 114120 || Loss: 0.8234 || timer: 0.0905 sec.
iter 114130 || Loss: 1.2233 || timer: 0.1150 sec.
iter 114140 || Loss: 0.8771 || timer: 0.1002 sec.
iter 114150 || Loss: 0.8335 || timer: 0.1023 sec.
iter 114160 || Loss: 0.8903 || timer: 0.1056 sec.
iter 114170 || Loss: 0.8497 || timer: 0.1145 sec.
iter 114180 || Loss: 0.8770 || timer: 0.0895 sec.
iter 114190 || Loss: 0.8894 || timer: 0.0914 sec.
iter 114200 || Loss: 0.8588 || timer: 0.0866 sec.
iter 114210 || Loss: 0.7596 || timer: 0.0921 sec.
iter 114220 || Loss: 1.1132 || timer: 0.0870 sec.
iter 114230 || Loss: 0.9303 || timer: 0.1100 sec.
iter 114240 || Loss: 1.2701 || timer: 0.0918 sec.
iter 114250 || Loss: 0.8779 || timer: 0.0826 sec.
iter 114260 || Loss: 1.2347 || timer: 0.0885 sec.
iter 114270 || Loss: 1.2925 || timer: 0.0887 sec.
iter 114280 || Loss: 1.0186 || timer: 0.0904 sec.
iter 114290 || Loss: 0.8704 || timer: 0.0960 sec.
iter 114300 || Loss: 0.7544 || timer: 0.0823 sec.
iter 114310 || Loss: 1.0036 || timer: 0.0884 sec.
iter 114320 || Loss: 1.4451 || timer: 0.0897 sec.
iter 114330 || Loss: 1.1281 || timer: 0.0896 sec.
iter 114340 || Loss: 1.1045 || timer: 0.0895 sec.
iter 114350 || Loss: 1.3938 || timer: 0.0824 sec.
iter 114360 || Loss: 0.8662 || timer: 0.0850 sec.
iter 114370 || Loss: 1.1313 || timer: 0.0885 sec.
iter 114380 || Loss: 1.3113 || timer: 0.0832 sec.
iter 114390 || Loss: 1.0562 || timer: 0.0900 sec.
iter 114400 || Loss: 0.9292 || timer: 0.0218 sec.
iter 114410 || Loss: 2.3708 || timer: 0.1052 sec.
iter 114420 || Loss: 1.0423 || timer: 0.0816 sec.
iter 114430 || Loss: 1.1965 || timer: 0.0913 sec.
iter 114440 || Loss: 0.8956 || timer: 0.0823 sec.
iter 114450 || Loss: 1.2027 || timer: 0.0893 sec.
iter 114460 || Loss: 0.9119 || timer: 0.0976 sec.
iter 114470 || Loss: 0.7700 || timer: 0.0822 sec.
iter 114480 || Loss: 0.7707 || timer: 0.0839 sec.
iter 114490 || Loss: 0.8422 || timer: 0.0901 sec.
iter 114500 || Loss: 1.0425 || timer: 0.1536 sec.
iter 114510 || Loss: 0.8597 || timer: 0.0877 sec.
iter 114520 || Loss: 1.0712 || timer: 0.0900 sec.
iter 114530 || Loss: 0.9349 || timer: 0.0888 sec.
iter 114540 || Loss: 0.9997 || timer: 0.0887 sec.
iter 114550 || Loss: 1.2936 || timer: 0.0828 sec.
iter 114560 || Loss: 0.9858 || timer: 0.0915 sec.
iter 114570 || Loss: 1.0888 || timer: 0.0885 sec.
iter 114580 || Loss: 1.1470 || timer: 0.0870 sec.
iter 114590 || Loss: 0.6585 || timer: 0.0837 sec.
iter 114600 || Loss: 1.0178 || timer: 0.0825 sec.
iter 114610 || Loss: 1.0381 || timer: 0.1042 sec.
iter 114620 || Loss: 0.9746 || timer: 0.0902 sec.
iter 114630 || Loss: 1.0085 || timer: 0.0833 sec.
iter 114640 || Loss: 0.9429 || timer: 0.0766 sec.
iter 114650 || Loss: 1.0775 || timer: 0.1068 sec.
iter 114660 || Loss: 1.0058 || timer: 0.0836 sec.
iter 114670 || Loss: 0.9996 || timer: 0.0833 sec.
iter 114680 || Loss: 1.2654 || timer: 0.0871 sec.
iter 114690 || Loss: 1.5607 || timer: 0.0941 sec.
iter 114700 || Loss: 0.8407 || timer: 0.0887 sec.
iter 114710 || Loss: 1.1555 || timer: 0.0941 sec.
iter 114720 || Loss: 1.7377 || timer: 0.0834 sec.
iter 114730 || Loss: 1.0170 || timer: 0.0269 sec.
iter 114740 || Loss: 0.7205 || timer: 0.1085 sec.
iter 114750 || Loss: 1.0576 || timer: 0.1085 sec.
iter 114760 || Loss: 1.6999 || timer: 0.0900 sec.
iter 114770 || Loss: 0.9430 || timer: 0.0824 sec.
iter 114780 || Loss: 1.0592 || timer: 0.0826 sec.
iter 114790 || Loss: 0.9960 || timer: 0.0892 sec.
iter 114800 || Loss: 1.2505 || timer: 0.0896 sec.
iter 114810 || Loss: 1.0565 || timer: 0.0976 sec.
iter 114820 || Loss: 1.2338 || timer: 0.1053 sec.
iter 114830 || Loss: 0.6993 || timer: 0.0943 sec.
iter 114840 || Loss: 0.9282 || timer: 0.0907 sec.
iter 114850 || Loss: 0.9637 || timer: 0.1080 sec.
iter 114860 || Loss: 1.6051 || timer: 0.0814 sec.
iter 114870 || Loss: 1.3953 || timer: 0.0988 sec.
iter 114880 || Loss: 1.1332 || timer: 0.0895 sec.
iter 114890 || Loss: 1.2289 || timer: 0.0835 sec.
iter 114900 || Loss: 1.0913 || timer: 0.0891 sec.
iter 114910 || Loss: 1.1992 || timer: 0.0815 sec.
iter 114920 || Loss: 0.9619 || timer: 0.0834 sec.
iter 114930 || Loss: 0.9640 || timer: 0.0891 sec.
iter 114940 || Loss: 0.8363 || timer: 0.0887 sec.
iter 114950 || Loss: 1.1122 || timer: 0.0902 sec.
iter 114960 || Loss: 1.4808 || timer: 0.0819 sec.
iter 114970 || Loss: 1.0492 || timer: 0.0880 sec.
iter 114980 || Loss: 0.9564 || timer: 0.1059 sec.
iter 114990 || Loss: 1.0472 || timer: 0.0933 sec.
iter 115000 || Loss: 1.0468 || Saving state, iter: 115000
timer: 0.0904 sec.
iter 115010 || Loss: 1.5831 || timer: 0.1085 sec.
iter 115020 || Loss: 0.9328 || timer: 0.0842 sec.
iter 115030 || Loss: 0.8096 || timer: 0.0869 sec.
iter 115040 || Loss: 0.8243 || timer: 0.0896 sec.
iter 115050 || Loss: 0.9203 || timer: 0.0997 sec.
iter 115060 || Loss: 0.6735 || timer: 0.0237 sec.
iter 115070 || Loss: 0.6872 || timer: 0.1032 sec.
iter 115080 || Loss: 0.8972 || timer: 0.0889 sec.
iter 115090 || Loss: 0.8968 || timer: 0.0893 sec.
iter 115100 || Loss: 0.8278 || timer: 0.0851 sec.
iter 115110 || Loss: 1.0722 || timer: 0.0811 sec.
iter 115120 || Loss: 1.3168 || timer: 0.1170 sec.
iter 115130 || Loss: 1.1135 || timer: 0.0838 sec.
iter 115140 || Loss: 0.9448 || timer: 0.0818 sec.
iter 115150 || Loss: 1.3288 || timer: 0.0912 sec.
iter 115160 || Loss: 1.2762 || timer: 0.1048 sec.
iter 115170 || Loss: 0.9566 || timer: 0.0842 sec.
iter 115180 || Loss: 0.9671 || timer: 0.0881 sec.
iter 115190 || Loss: 1.0439 || timer: 0.0865 sec.
iter 115200 || Loss: 1.0935 || timer: 0.0883 sec.
iter 115210 || Loss: 1.1226 || timer: 0.0915 sec.
iter 115220 || Loss: 1.0631 || timer: 0.1092 sec.
iter 115230 || Loss: 0.8871 || timer: 0.0875 sec.
iter 115240 || Loss: 1.0368 || timer: 0.0817 sec.
iter 115250 || Loss: 0.7853 || timer: 0.0878 sec.
iter 115260 || Loss: 1.1378 || timer: 0.0823 sec.
iter 115270 || Loss: 0.6973 || timer: 0.0896 sec.
iter 115280 || Loss: 1.1981 || timer: 0.0898 sec.
iter 115290 || Loss: 0.9674 || timer: 0.0893 sec.
iter 115300 || Loss: 1.3639 || timer: 0.1278 sec.
iter 115310 || Loss: 1.1680 || timer: 0.0825 sec.
iter 115320 || Loss: 1.3909 || timer: 0.0924 sec.
iter 115330 || Loss: 0.7745 || timer: 0.0880 sec.
iter 115340 || Loss: 1.6663 || timer: 0.0879 sec.
iter 115350 || Loss: 1.3482 || timer: 0.0876 sec.
iter 115360 || Loss: 1.5069 || timer: 0.0817 sec.
iter 115370 || Loss: 1.3122 || timer: 0.0824 sec.
iter 115380 || Loss: 1.3868 || timer: 0.1032 sec.
iter 115390 || Loss: 0.8459 || timer: 0.0278 sec.
iter 115400 || Loss: 1.3264 || timer: 0.0752 sec.
iter 115410 || Loss: 0.8856 || timer: 0.0852 sec.
iter 115420 || Loss: 1.3064 || timer: 0.0807 sec.
iter 115430 || Loss: 1.1035 || timer: 0.0912 sec.
iter 115440 || Loss: 1.3127 || timer: 0.0832 sec.
iter 115450 || Loss: 1.1537 || timer: 0.0827 sec.
iter 115460 || Loss: 0.8941 || timer: 0.0917 sec.
iter 115470 || Loss: 1.2574 || timer: 0.0923 sec.
iter 115480 || Loss: 1.1057 || timer: 0.1015 sec.
iter 115490 || Loss: 0.8688 || timer: 0.0959 sec.
iter 115500 || Loss: 0.8149 || timer: 0.1115 sec.
iter 115510 || Loss: 1.4859 || timer: 0.0922 sec.
iter 115520 || Loss: 1.0393 || timer: 0.0919 sec.
iter 115530 || Loss: 0.7067 || timer: 0.0879 sec.
iter 115540 || Loss: 1.6791 || timer: 0.0929 sec.
iter 115550 || Loss: 1.1550 || timer: 0.0842 sec.
iter 115560 || Loss: 0.9707 || timer: 0.0883 sec.
iter 115570 || Loss: 1.3574 || timer: 0.0912 sec.
iter 115580 || Loss: 1.1353 || timer: 0.0914 sec.
iter 115590 || Loss: 1.5192 || timer: 0.0992 sec.
iter 115600 || Loss: 0.9370 || timer: 0.0916 sec.
iter 115610 || Loss: 0.9050 || timer: 0.0935 sec.
iter 115620 || Loss: 0.7757 || timer: 0.0902 sec.
iter 115630 || Loss: 0.7908 || timer: 0.0828 sec.
iter 115640 || Loss: 1.5047 || timer: 0.1056 sec.
iter 115650 || Loss: 0.7866 || timer: 0.0885 sec.
iter 115660 || Loss: 1.2824 || timer: 0.0841 sec.
iter 115670 || Loss: 0.9061 || timer: 0.0895 sec.
iter 115680 || Loss: 1.0890 || timer: 0.0904 sec.
iter 115690 || Loss: 0.8836 || timer: 0.0865 sec.
iter 115700 || Loss: 0.9090 || timer: 0.0841 sec.
iter 115710 || Loss: 0.9019 || timer: 0.0906 sec.
iter 115720 || Loss: 0.8297 || timer: 0.0271 sec.
iter 115730 || Loss: 0.2949 || timer: 0.0921 sec.
iter 115740 || Loss: 0.8860 || timer: 0.0919 sec.
iter 115750 || Loss: 0.6931 || timer: 0.0848 sec.
iter 115760 || Loss: 0.8223 || timer: 0.0886 sec.
iter 115770 || Loss: 1.2549 || timer: 0.0900 sec.
iter 115780 || Loss: 1.0492 || timer: 0.1069 sec.
iter 115790 || Loss: 0.9981 || timer: 0.0914 sec.
iter 115800 || Loss: 1.0172 || timer: 0.0840 sec.
iter 115810 || Loss: 1.2260 || timer: 0.0920 sec.
iter 115820 || Loss: 0.7578 || timer: 0.1009 sec.
iter 115830 || Loss: 1.0896 || timer: 0.0912 sec.
iter 115840 || Loss: 0.8559 || timer: 0.0875 sec.
iter 115850 || Loss: 0.8918 || timer: 0.0919 sec.
iter 115860 || Loss: 0.9362 || timer: 0.0916 sec.
iter 115870 || Loss: 0.9039 || timer: 0.0841 sec.
iter 115880 || Loss: 1.0375 || timer: 0.0935 sec.
iter 115890 || Loss: 1.2840 || timer: 0.0973 sec.
iter 115900 || Loss: 0.8000 || timer: 0.0836 sec.
iter 115910 || Loss: 1.1536 || timer: 0.0834 sec.
iter 115920 || Loss: 0.9916 || timer: 0.0842 sec.
iter 115930 || Loss: 0.9237 || timer: 0.0826 sec.
iter 115940 || Loss: 0.7982 || timer: 0.0855 sec.
iter 115950 || Loss: 0.9056 || timer: 0.0823 sec.
iter 115960 || Loss: 0.9326 || timer: 0.0828 sec.
iter 115970 || Loss: 1.2618 || timer: 0.0902 sec.
iter 115980 || Loss: 0.7858 || timer: 0.0907 sec.
iter 115990 || Loss: 0.6888 || timer: 0.1070 sec.
iter 116000 || Loss: 0.9761 || timer: 0.0924 sec.
iter 116010 || Loss: 0.7739 || timer: 0.1077 sec.
iter 116020 || Loss: 1.4705 || timer: 0.0843 sec.
iter 116030 || Loss: 0.8282 || timer: 0.0906 sec.
iter 116040 || Loss: 1.0301 || timer: 0.1088 sec.
iter 116050 || Loss: 0.9382 || timer: 0.0209 sec.
iter 116060 || Loss: 1.2939 || timer: 0.0840 sec.
iter 116070 || Loss: 1.1617 || timer: 0.0907 sec.
iter 116080 || Loss: 0.9562 || timer: 0.0927 sec.
iter 116090 || Loss: 0.9674 || timer: 0.0840 sec.
iter 116100 || Loss: 1.0628 || timer: 0.0850 sec.
iter 116110 || Loss: 0.9197 || timer: 0.0930 sec.
iter 116120 || Loss: 1.1855 || timer: 0.0922 sec.
iter 116130 || Loss: 1.1685 || timer: 0.1034 sec.
iter 116140 || Loss: 0.9621 || timer: 0.0943 sec.
iter 116150 || Loss: 1.0820 || timer: 0.0974 sec.
iter 116160 || Loss: 1.0863 || timer: 0.0944 sec.
iter 116170 || Loss: 1.0566 || timer: 0.0832 sec.
iter 116180 || Loss: 0.8278 || timer: 0.0909 sec.
iter 116190 || Loss: 0.9668 || timer: 0.0824 sec.
iter 116200 || Loss: 0.8782 || timer: 0.0941 sec.
iter 116210 || Loss: 1.1697 || timer: 0.1019 sec.
iter 116220 || Loss: 0.8083 || timer: 0.1299 sec.
iter 116230 || Loss: 0.8477 || timer: 0.0972 sec.
iter 116240 || Loss: 1.0984 || timer: 0.0847 sec.
iter 116250 || Loss: 0.7453 || timer: 0.0848 sec.
iter 116260 || Loss: 1.1181 || timer: 0.0865 sec.
iter 116270 || Loss: 0.9651 || timer: 0.0917 sec.
iter 116280 || Loss: 0.9777 || timer: 0.0913 sec.
iter 116290 || Loss: 1.0073 || timer: 0.0885 sec.
iter 116300 || Loss: 0.8620 || timer: 0.0872 sec.
iter 116310 || Loss: 1.6858 || timer: 0.0826 sec.
iter 116320 || Loss: 1.2457 || timer: 0.0864 sec.
iter 116330 || Loss: 0.8405 || timer: 0.0938 sec.
iter 116340 || Loss: 0.8054 || timer: 0.1028 sec.
iter 116350 || Loss: 0.9791 || timer: 0.0895 sec.
iter 116360 || Loss: 0.7986 || timer: 0.1151 sec.
iter 116370 || Loss: 0.9295 || timer: 0.0905 sec.
iter 116380 || Loss: 0.7156 || timer: 0.0212 sec.
iter 116390 || Loss: 0.1160 || timer: 0.0853 sec.
iter 116400 || Loss: 1.1324 || timer: 0.0819 sec.
iter 116410 || Loss: 1.0965 || timer: 0.0924 sec.
iter 116420 || Loss: 0.8167 || timer: 0.0823 sec.
iter 116430 || Loss: 0.7868 || timer: 0.0878 sec.
iter 116440 || Loss: 1.4637 || timer: 0.1016 sec.
iter 116450 || Loss: 0.9645 || timer: 0.0826 sec.
iter 116460 || Loss: 1.4960 || timer: 0.0894 sec.
iter 116470 || Loss: 1.1092 || timer: 0.0887 sec.
iter 116480 || Loss: 1.3932 || timer: 0.0976 sec.
iter 116490 || Loss: 0.8716 || timer: 0.0901 sec.
iter 116500 || Loss: 1.0763 || timer: 0.1013 sec.
iter 116510 || Loss: 1.0171 || timer: 0.0839 sec.
iter 116520 || Loss: 0.9234 || timer: 0.0891 sec.
iter 116530 || Loss: 0.9523 || timer: 0.0838 sec.
iter 116540 || Loss: 0.8744 || timer: 0.1106 sec.
iter 116550 || Loss: 1.1415 || timer: 0.0990 sec.
iter 116560 || Loss: 0.9119 || timer: 0.0895 sec.
iter 116570 || Loss: 1.3038 || timer: 0.0886 sec.
iter 116580 || Loss: 1.1871 || timer: 0.0822 sec.
iter 116590 || Loss: 0.9934 || timer: 0.1028 sec.
iter 116600 || Loss: 0.9802 || timer: 0.0831 sec.
iter 116610 || Loss: 0.8966 || timer: 0.0841 sec.
iter 116620 || Loss: 1.1224 || timer: 0.0872 sec.
iter 116630 || Loss: 0.7589 || timer: 0.0843 sec.
iter 116640 || Loss: 1.1366 || timer: 0.0903 sec.
iter 116650 || Loss: 0.9258 || timer: 0.1027 sec.
iter 116660 || Loss: 0.9840 || timer: 0.0953 sec.
iter 116670 || Loss: 0.9029 || timer: 0.0919 sec.
iter 116680 || Loss: 0.7621 || timer: 0.0912 sec.
iter 116690 || Loss: 0.9575 || timer: 0.0943 sec.
iter 116700 || Loss: 0.9611 || timer: 0.1067 sec.
iter 116710 || Loss: 1.0573 || timer: 0.0215 sec.
iter 116720 || Loss: 0.4974 || timer: 0.0824 sec.
iter 116730 || Loss: 0.7367 || timer: 0.0824 sec.
iter 116740 || Loss: 0.7479 || timer: 0.0923 sec.
iter 116750 || Loss: 0.8449 || timer: 0.0967 sec.
iter 116760 || Loss: 0.9364 || timer: 0.0899 sec.
iter 116770 || Loss: 1.2269 || timer: 0.0927 sec.
iter 116780 || Loss: 1.1183 || timer: 0.0826 sec.
iter 116790 || Loss: 1.1010 || timer: 0.0821 sec.
iter 116800 || Loss: 1.1587 || timer: 0.0841 sec.
iter 116810 || Loss: 0.6824 || timer: 0.1011 sec.
iter 116820 || Loss: 0.8126 || timer: 0.0899 sec.
iter 116830 || Loss: 0.8231 || timer: 0.0840 sec.
iter 116840 || Loss: 1.1337 || timer: 0.0834 sec.
iter 116850 || Loss: 0.7989 || timer: 0.0897 sec.
iter 116860 || Loss: 0.9263 || timer: 0.1034 sec.
iter 116870 || Loss: 0.7765 || timer: 0.0825 sec.
iter 116880 || Loss: 0.8773 || timer: 0.0888 sec.
iter 116890 || Loss: 0.9366 || timer: 0.0918 sec.
iter 116900 || Loss: 1.1556 || timer: 0.0904 sec.
iter 116910 || Loss: 1.2623 || timer: 0.0926 sec.
iter 116920 || Loss: 0.6896 || timer: 0.0962 sec.
iter 116930 || Loss: 0.7066 || timer: 0.1101 sec.
iter 116940 || Loss: 1.0104 || timer: 0.0981 sec.
iter 116950 || Loss: 0.9207 || timer: 0.0979 sec.
iter 116960 || Loss: 1.1488 || timer: 0.0909 sec.
iter 116970 || Loss: 0.9531 || timer: 0.0927 sec.
iter 116980 || Loss: 1.8256 || timer: 0.0911 sec.
iter 116990 || Loss: 1.1673 || timer: 0.0918 sec.
iter 117000 || Loss: 0.9997 || timer: 0.0876 sec.
iter 117010 || Loss: 0.9485 || timer: 0.0930 sec.
iter 117020 || Loss: 0.8743 || timer: 0.0886 sec.
iter 117030 || Loss: 1.0332 || timer: 0.0885 sec.
iter 117040 || Loss: 1.2358 || timer: 0.0268 sec.
iter 117050 || Loss: 0.5942 || timer: 0.0937 sec.
iter 117060 || Loss: 1.2566 || timer: 0.0927 sec.
iter 117070 || Loss: 0.9134 || timer: 0.0922 sec.
iter 117080 || Loss: 0.7886 || timer: 0.0888 sec.
iter 117090 || Loss: 1.1111 || timer: 0.0927 sec.
iter 117100 || Loss: 1.0317 || timer: 0.0835 sec.
iter 117110 || Loss: 0.9408 || timer: 0.0844 sec.
iter 117120 || Loss: 0.8461 || timer: 0.1010 sec.
iter 117130 || Loss: 1.0368 || timer: 0.0976 sec.
iter 117140 || Loss: 1.1185 || timer: 0.0957 sec.
iter 117150 || Loss: 0.9705 || timer: 0.0861 sec.
iter 117160 || Loss: 0.8265 || timer: 0.0917 sec.
iter 117170 || Loss: 1.1771 || timer: 0.0906 sec.
iter 117180 || Loss: 1.1530 || timer: 0.1004 sec.
iter 117190 || Loss: 0.8122 || timer: 0.0903 sec.
iter 117200 || Loss: 1.1664 || timer: 0.0874 sec.
iter 117210 || Loss: 0.7851 || timer: 0.0902 sec.
iter 117220 || Loss: 0.9723 || timer: 0.1035 sec.
iter 117230 || Loss: 0.9852 || timer: 0.1039 sec.
iter 117240 || Loss: 0.9730 || timer: 0.0847 sec.
iter 117250 || Loss: 0.7339 || timer: 0.0835 sec.
iter 117260 || Loss: 1.2663 || timer: 0.0836 sec.
iter 117270 || Loss: 1.1579 || timer: 0.0881 sec.
iter 117280 || Loss: 1.0624 || timer: 0.0820 sec.
iter 117290 || Loss: 1.3727 || timer: 0.0894 sec.
iter 117300 || Loss: 1.0597 || timer: 0.0888 sec.
iter 117310 || Loss: 0.8295 || timer: 0.0822 sec.
iter 117320 || Loss: 0.9549 || timer: 0.0915 sec.
iter 117330 || Loss: 1.0830 || timer: 0.0834 sec.
iter 117340 || Loss: 0.8003 || timer: 0.0895 sec.
iter 117350 || Loss: 0.9353 || timer: 0.1105 sec.
iter 117360 || Loss: 1.1934 || timer: 0.0837 sec.
iter 117370 || Loss: 0.9985 || timer: 0.0175 sec.
iter 117380 || Loss: 0.7456 || timer: 0.1026 sec.
iter 117390 || Loss: 1.0749 || timer: 0.0976 sec.
iter 117400 || Loss: 0.9198 || timer: 0.0902 sec.
iter 117410 || Loss: 1.1928 || timer: 0.0829 sec.
iter 117420 || Loss: 1.1697 || timer: 0.1115 sec.
iter 117430 || Loss: 1.4111 || timer: 0.0900 sec.
iter 117440 || Loss: 1.3941 || timer: 0.0891 sec.
iter 117450 || Loss: 0.8737 || timer: 0.0888 sec.
iter 117460 || Loss: 1.2624 || timer: 0.0896 sec.
iter 117470 || Loss: 0.9071 || timer: 0.1250 sec.
iter 117480 || Loss: 0.7392 || timer: 0.0884 sec.
iter 117490 || Loss: 0.7413 || timer: 0.1005 sec.
iter 117500 || Loss: 0.5954 || timer: 0.0916 sec.
iter 117510 || Loss: 1.0596 || timer: 0.0833 sec.
iter 117520 || Loss: 0.6377 || timer: 0.0908 sec.
iter 117530 || Loss: 1.3532 || timer: 0.0877 sec.
iter 117540 || Loss: 1.0601 || timer: 0.0826 sec.
iter 117550 || Loss: 0.7851 || timer: 0.0885 sec.
iter 117560 || Loss: 0.7104 || timer: 0.0900 sec.
iter 117570 || Loss: 1.4526 || timer: 0.0822 sec.
iter 117580 || Loss: 0.9861 || timer: 0.0912 sec.
iter 117590 || Loss: 0.9957 || timer: 0.0920 sec.
iter 117600 || Loss: 1.0836 || timer: 0.0833 sec.
iter 117610 || Loss: 0.9290 || timer: 0.0916 sec.
iter 117620 || Loss: 1.0566 || timer: 0.0836 sec.
iter 117630 || Loss: 0.9240 || timer: 0.0837 sec.
iter 117640 || Loss: 1.2012 || timer: 0.1100 sec.
iter 117650 || Loss: 1.6941 || timer: 0.0897 sec.
iter 117660 || Loss: 0.9854 || timer: 0.0838 sec.
iter 117670 || Loss: 0.8920 || timer: 0.0964 sec.
iter 117680 || Loss: 0.8557 || timer: 0.0920 sec.
iter 117690 || Loss: 0.9008 || timer: 0.0938 sec.
iter 117700 || Loss: 1.2934 || timer: 0.0281 sec.
iter 117710 || Loss: 0.4688 || timer: 0.0871 sec.
iter 117720 || Loss: 0.9737 || timer: 0.0831 sec.
iter 117730 || Loss: 1.1017 || timer: 0.1055 sec.
iter 117740 || Loss: 1.1783 || timer: 0.0836 sec.
iter 117750 || Loss: 1.2126 || timer: 0.0821 sec.
iter 117760 || Loss: 0.8507 || timer: 0.0860 sec.
iter 117770 || Loss: 1.5351 || timer: 0.0847 sec.
iter 117780 || Loss: 1.3754 || timer: 0.0827 sec.
iter 117790 || Loss: 1.6912 || timer: 0.0907 sec.
iter 117800 || Loss: 1.2719 || timer: 0.0980 sec.
iter 117810 || Loss: 0.9952 || timer: 0.0915 sec.
iter 117820 || Loss: 1.0846 || timer: 0.0956 sec.
iter 117830 || Loss: 0.7336 || timer: 0.0909 sec.
iter 117840 || Loss: 1.4183 || timer: 0.0931 sec.
iter 117850 || Loss: 1.0875 || timer: 0.0876 sec.
iter 117860 || Loss: 1.1272 || timer: 0.0897 sec.
iter 117870 || Loss: 1.3166 || timer: 0.0926 sec.
iter 117880 || Loss: 1.0871 || timer: 0.0997 sec.
iter 117890 || Loss: 1.0363 || timer: 0.1041 sec.
iter 117900 || Loss: 0.9225 || timer: 0.0819 sec.
iter 117910 || Loss: 0.9347 || timer: 0.1085 sec.
iter 117920 || Loss: 1.2032 || timer: 0.0885 sec.
iter 117930 || Loss: 1.0173 || timer: 0.0827 sec.
iter 117940 || Loss: 1.2226 || timer: 0.0978 sec.
iter 117950 || Loss: 1.0967 || timer: 0.0838 sec.
iter 117960 || Loss: 1.4046 || timer: 0.0927 sec.
iter 117970 || Loss: 0.7565 || timer: 0.0826 sec.
iter 117980 || Loss: 0.6882 || timer: 0.1038 sec.
iter 117990 || Loss: 1.0128 || timer: 0.0928 sec.
iter 118000 || Loss: 0.8203 || timer: 0.0903 sec.
iter 118010 || Loss: 1.0034 || timer: 0.1088 sec.
iter 118020 || Loss: 1.4730 || timer: 0.0893 sec.
iter 118030 || Loss: 1.1308 || timer: 0.0265 sec.
iter 118040 || Loss: 0.6642 || timer: 0.0930 sec.
iter 118050 || Loss: 1.1074 || timer: 0.0910 sec.
iter 118060 || Loss: 1.0763 || timer: 0.0897 sec.
iter 118070 || Loss: 0.9602 || timer: 0.0982 sec.
iter 118080 || Loss: 1.1057 || timer: 0.0903 sec.
iter 118090 || Loss: 0.8860 || timer: 0.1074 sec.
iter 118100 || Loss: 1.0729 || timer: 0.0907 sec.
iter 118110 || Loss: 1.2669 || timer: 0.0836 sec.
iter 118120 || Loss: 1.2620 || timer: 0.0978 sec.
iter 118130 || Loss: 1.2344 || timer: 0.1326 sec.
iter 118140 || Loss: 0.8225 || timer: 0.0870 sec.
iter 118150 || Loss: 0.9351 || timer: 0.0917 sec.
iter 118160 || Loss: 1.4225 || timer: 0.0920 sec.
iter 118170 || Loss: 0.9467 || timer: 0.0831 sec.
iter 118180 || Loss: 1.2917 || timer: 0.1038 sec.
iter 118190 || Loss: 1.4752 || timer: 0.1014 sec.
iter 118200 || Loss: 1.2997 || timer: 0.1060 sec.
iter 118210 || Loss: 1.1316 || timer: 0.0946 sec.
iter 118220 || Loss: 0.9781 || timer: 0.0899 sec.
iter 118230 || Loss: 0.7942 || timer: 0.0888 sec.
iter 118240 || Loss: 1.0764 || timer: 0.0873 sec.
iter 118250 || Loss: 1.0733 || timer: 0.0972 sec.
iter 118260 || Loss: 0.8766 || timer: 0.0824 sec.
iter 118270 || Loss: 1.0620 || timer: 0.1011 sec.
iter 118280 || Loss: 1.0524 || timer: 0.0904 sec.
iter 118290 || Loss: 0.7554 || timer: 0.0905 sec.
iter 118300 || Loss: 1.2067 || timer: 0.0862 sec.
iter 118310 || Loss: 0.7803 || timer: 0.1088 sec.
iter 118320 || Loss: 1.0151 || timer: 0.1054 sec.
iter 118330 || Loss: 1.0249 || timer: 0.0886 sec.
iter 118340 || Loss: 1.2356 || timer: 0.0892 sec.
iter 118350 || Loss: 1.0798 || timer: 0.1073 sec.
iter 118360 || Loss: 0.8015 || timer: 0.0184 sec.
iter 118370 || Loss: 0.9732 || timer: 0.0834 sec.
iter 118380 || Loss: 0.8764 || timer: 0.0898 sec.
iter 118390 || Loss: 0.8961 || timer: 0.0884 sec.
iter 118400 || Loss: 0.9694 || timer: 0.0831 sec.
iter 118410 || Loss: 0.9552 || timer: 0.0883 sec.
iter 118420 || Loss: 0.9073 || timer: 0.0826 sec.
iter 118430 || Loss: 0.7781 || timer: 0.0925 sec.
iter 118440 || Loss: 0.6968 || timer: 0.0894 sec.
iter 118450 || Loss: 0.9139 || timer: 0.0917 sec.
iter 118460 || Loss: 0.8813 || timer: 0.0966 sec.
iter 118470 || Loss: 0.9564 || timer: 0.0836 sec.
iter 118480 || Loss: 0.8481 || timer: 0.0820 sec.
iter 118490 || Loss: 1.0767 || timer: 0.0907 sec.
iter 118500 || Loss: 0.8530 || timer: 0.0842 sec.
iter 118510 || Loss: 0.9492 || timer: 0.0912 sec.
iter 118520 || Loss: 1.0848 || timer: 0.0920 sec.
iter 118530 || Loss: 0.7979 || timer: 0.1057 sec.
iter 118540 || Loss: 1.1718 || timer: 0.0917 sec.
iter 118550 || Loss: 0.9940 || timer: 0.0856 sec.
iter 118560 || Loss: 0.8813 || timer: 0.0958 sec.
iter 118570 || Loss: 0.6885 || timer: 0.0845 sec.
iter 118580 || Loss: 0.8736 || timer: 0.0809 sec.
iter 118590 || Loss: 0.7875 || timer: 0.0832 sec.
iter 118600 || Loss: 1.0630 || timer: 0.0906 sec.
iter 118610 || Loss: 1.0621 || timer: 0.0912 sec.
iter 118620 || Loss: 1.0542 || timer: 0.0893 sec.
iter 118630 || Loss: 0.7847 || timer: 0.0866 sec.
iter 118640 || Loss: 1.2057 || timer: 0.0987 sec.
iter 118650 || Loss: 0.9652 || timer: 0.1008 sec.
iter 118660 || Loss: 1.0338 || timer: 0.0898 sec.
iter 118670 || Loss: 0.7881 || timer: 0.0990 sec.
iter 118680 || Loss: 0.6525 || timer: 0.0894 sec.
iter 118690 || Loss: 0.6741 || timer: 0.0170 sec.
iter 118700 || Loss: 0.4980 || timer: 0.1033 sec.
iter 118710 || Loss: 0.8333 || timer: 0.0998 sec.
iter 118720 || Loss: 0.8975 || timer: 0.0905 sec.
iter 118730 || Loss: 1.0334 || timer: 0.0883 sec.
iter 118740 || Loss: 1.2499 || timer: 0.0888 sec.
iter 118750 || Loss: 1.4735 || timer: 0.0871 sec.
iter 118760 || Loss: 1.1554 || timer: 0.0832 sec.
iter 118770 || Loss: 1.4221 || timer: 0.0940 sec.
iter 118780 || Loss: 1.1385 || timer: 0.0985 sec.
iter 118790 || Loss: 1.1092 || timer: 0.0974 sec.
iter 118800 || Loss: 1.0350 || timer: 0.0906 sec.
iter 118810 || Loss: 1.0769 || timer: 0.0896 sec.
iter 118820 || Loss: 0.8641 || timer: 0.0873 sec.
iter 118830 || Loss: 0.8802 || timer: 0.0825 sec.
iter 118840 || Loss: 1.1668 || timer: 0.0896 sec.
iter 118850 || Loss: 0.6999 || timer: 0.0884 sec.
iter 118860 || Loss: 1.1693 || timer: 0.1045 sec.
iter 118870 || Loss: 0.8123 || timer: 0.0926 sec.
iter 118880 || Loss: 0.9890 || timer: 0.0845 sec.
iter 118890 || Loss: 1.1221 || timer: 0.1066 sec.
iter 118900 || Loss: 0.9795 || timer: 0.1254 sec.
iter 118910 || Loss: 0.8220 || timer: 0.0883 sec.
iter 118920 || Loss: 1.1053 || timer: 0.0908 sec.
iter 118930 || Loss: 1.0294 || timer: 0.0822 sec.
iter 118940 || Loss: 1.1222 || timer: 0.0919 sec.
iter 118950 || Loss: 0.8578 || timer: 0.0823 sec.
iter 118960 || Loss: 0.7637 || timer: 0.0870 sec.
iter 118970 || Loss: 0.8236 || timer: 0.0930 sec.
iter 118980 || Loss: 1.0068 || timer: 0.0913 sec.
iter 118990 || Loss: 0.8980 || timer: 0.0886 sec.
iter 119000 || Loss: 1.4802 || timer: 0.1018 sec.
iter 119010 || Loss: 0.9907 || timer: 0.0906 sec.
iter 119020 || Loss: 1.1178 || timer: 0.0207 sec.
iter 119030 || Loss: 1.1860 || timer: 0.1025 sec.
iter 119040 || Loss: 1.2997 || timer: 0.0919 sec.
iter 119050 || Loss: 0.8570 || timer: 0.0921 sec.
iter 119060 || Loss: 1.2334 || timer: 0.0833 sec.
iter 119070 || Loss: 1.3635 || timer: 0.0904 sec.
iter 119080 || Loss: 0.6817 || timer: 0.0912 sec.
iter 119090 || Loss: 1.3467 || timer: 0.0920 sec.
iter 119100 || Loss: 0.8857 || timer: 0.0985 sec.
iter 119110 || Loss: 0.8207 || timer: 0.1113 sec.
iter 119120 || Loss: 1.0652 || timer: 0.1132 sec.
iter 119130 || Loss: 1.0398 || timer: 0.1159 sec.
iter 119140 || Loss: 0.9510 || timer: 0.1098 sec.
iter 119150 || Loss: 0.7649 || timer: 0.0913 sec.
iter 119160 || Loss: 0.9385 || timer: 0.0931 sec.
iter 119170 || Loss: 0.9827 || timer: 0.0896 sec.
iter 119180 || Loss: 1.0166 || timer: 0.0843 sec.
iter 119190 || Loss: 1.2114 || timer: 0.1105 sec.
iter 119200 || Loss: 1.0097 || timer: 0.0893 sec.
iter 119210 || Loss: 0.9213 || timer: 0.0896 sec.
iter 119220 || Loss: 0.9542 || timer: 0.0921 sec.
iter 119230 || Loss: 0.8594 || timer: 0.1032 sec.
iter 119240 || Loss: 1.0938 || timer: 0.1135 sec.
iter 119250 || Loss: 0.9626 || timer: 0.0833 sec.
iter 119260 || Loss: 1.0529 || timer: 0.0925 sec.
iter 119270 || Loss: 1.1063 || timer: 0.0998 sec.
iter 119280 || Loss: 0.8567 || timer: 0.0853 sec.
iter 119290 || Loss: 0.9952 || timer: 0.0868 sec.
iter 119300 || Loss: 1.2812 || timer: 0.0840 sec.
iter 119310 || Loss: 1.1703 || timer: 0.0829 sec.
iter 119320 || Loss: 1.3973 || timer: 0.0894 sec.
iter 119330 || Loss: 1.3063 || timer: 0.0913 sec.
iter 119340 || Loss: 1.1394 || timer: 0.1076 sec.
iter 119350 || Loss: 1.0102 || timer: 0.0260 sec.
iter 119360 || Loss: 0.2243 || timer: 0.0842 sec.
iter 119370 || Loss: 1.2517 || timer: 0.0927 sec.
iter 119380 || Loss: 1.2808 || timer: 0.1087 sec.
iter 119390 || Loss: 1.0960 || timer: 0.0889 sec.
iter 119400 || Loss: 0.8259 || timer: 0.0970 sec.
iter 119410 || Loss: 1.2455 || timer: 0.0829 sec.
iter 119420 || Loss: 1.2650 || timer: 0.1032 sec.
iter 119430 || Loss: 0.8462 || timer: 0.0902 sec.
iter 119440 || Loss: 1.6362 || timer: 0.1085 sec.
iter 119450 || Loss: 1.3406 || timer: 0.0948 sec.
iter 119460 || Loss: 1.1129 || timer: 0.0905 sec.
iter 119470 || Loss: 1.4048 || timer: 0.0827 sec.
iter 119480 || Loss: 0.8445 || timer: 0.0886 sec.
iter 119490 || Loss: 1.2627 || timer: 0.0825 sec.
iter 119500 || Loss: 0.9879 || timer: 0.0905 sec.
iter 119510 || Loss: 0.8054 || timer: 0.0916 sec.
iter 119520 || Loss: 0.9356 || timer: 0.0897 sec.
iter 119530 || Loss: 1.1064 || timer: 0.0823 sec.
iter 119540 || Loss: 0.9457 || timer: 0.0885 sec.
iter 119550 || Loss: 0.9254 || timer: 0.0908 sec.
iter 119560 || Loss: 1.0847 || timer: 0.0884 sec.
iter 119570 || Loss: 0.7281 || timer: 0.0899 sec.
iter 119580 || Loss: 0.9115 || timer: 0.0906 sec.
iter 119590 || Loss: 1.2303 || timer: 0.1099 sec.
iter 119600 || Loss: 1.1652 || timer: 0.0894 sec.
iter 119610 || Loss: 0.8535 || timer: 0.0904 sec.
iter 119620 || Loss: 0.8108 || timer: 0.0989 sec.
iter 119630 || Loss: 1.2619 || timer: 0.0904 sec.
iter 119640 || Loss: 0.8631 || timer: 0.0992 sec.
iter 119650 || Loss: 0.8781 || timer: 0.0874 sec.
iter 119660 || Loss: 0.9533 || timer: 0.1054 sec.
iter 119670 || Loss: 1.0120 || timer: 0.0815 sec.
iter 119680 || Loss: 0.9295 || timer: 0.0278 sec.
iter 119690 || Loss: 2.5889 || timer: 0.0823 sec.
iter 119700 || Loss: 1.2775 || timer: 0.0900 sec.
iter 119710 || Loss: 1.0257 || timer: 0.0823 sec.
iter 119720 || Loss: 1.0692 || timer: 0.0906 sec.
iter 119730 || Loss: 0.8702 || timer: 0.0891 sec.
iter 119740 || Loss: 1.0318 || timer: 0.1259 sec.
iter 119750 || Loss: 1.0360 || timer: 0.0879 sec.
iter 119760 || Loss: 1.1337 || timer: 0.0980 sec.
iter 119770 || Loss: 1.1606 || timer: 0.0872 sec.
iter 119780 || Loss: 1.2089 || timer: 0.0986 sec.
iter 119790 || Loss: 0.7761 || timer: 0.0926 sec.
iter 119800 || Loss: 0.9422 || timer: 0.0834 sec.
iter 119810 || Loss: 1.1735 || timer: 0.0916 sec.
iter 119820 || Loss: 0.9457 || timer: 0.0827 sec.
iter 119830 || Loss: 0.8031 || timer: 0.0836 sec.
iter 119840 || Loss: 1.0300 || timer: 0.0828 sec.
iter 119850 || Loss: 0.9983 || timer: 0.1143 sec.
iter 119860 || Loss: 1.0163 || timer: 0.0935 sec.
iter 119870 || Loss: 0.8670 || timer: 0.0831 sec.
iter 119880 || Loss: 0.8160 || timer: 0.0903 sec.
iter 119890 || Loss: 0.6016 || timer: 0.0977 sec.
iter 119900 || Loss: 1.0749 || timer: 0.0891 sec.
iter 119910 || Loss: 0.8981 || timer: 0.0842 sec.
iter 119920 || Loss: 1.3176 || timer: 0.0834 sec.
iter 119930 || Loss: 1.1018 || timer: 0.0893 sec.
iter 119940 || Loss: 0.8895 || timer: 0.0922 sec.
iter 119950 || Loss: 1.1131 || timer: 0.0835 sec.
iter 119960 || Loss: 1.1506 || timer: 0.0910 sec.
iter 119970 || Loss: 1.2640 || timer: 0.0760 sec.
iter 119980 || Loss: 0.8314 || timer: 0.0754 sec.
iter 119990 || Loss: 0.8566 || timer: 0.0987 sec.
iter 120000 || Loss: 0.7519 || Saving state, iter: 120000
timer: 0.0814 sec.
iter 120010 || Loss: 0.8118 || timer: 0.0250 sec.
iter 120020 || Loss: 2.1854 || timer: 0.0829 sec.
iter 120030 || Loss: 1.3253 || timer: 0.0890 sec.
iter 120040 || Loss: 1.3933 || timer: 0.0875 sec.
iter 120050 || Loss: 1.0950 || timer: 0.0830 sec.
iter 120060 || Loss: 1.1303 || timer: 0.1086 sec.
iter 120070 || Loss: 0.9639 || timer: 0.0827 sec.
iter 120080 || Loss: 1.0273 || timer: 0.0919 sec.
iter 120090 || Loss: 1.0974 || timer: 0.1121 sec.
iter 120100 || Loss: 1.0953 || timer: 0.0849 sec.
iter 120110 || Loss: 1.3153 || timer: 0.1040 sec.
iter 120120 || Loss: 0.8913 || timer: 0.0853 sec.
iter 120130 || Loss: 0.9480 || timer: 0.0908 sec.
iter 120140 || Loss: 1.0509 || timer: 0.0860 sec.
iter 120150 || Loss: 0.7767 || timer: 0.0897 sec.
iter 120160 || Loss: 1.6422 || timer: 0.0904 sec.
iter 120170 || Loss: 0.9423 || timer: 0.1031 sec.
iter 120180 || Loss: 0.7571 || timer: 0.0914 sec.
iter 120190 || Loss: 1.0716 || timer: 0.0900 sec.
iter 120200 || Loss: 1.0168 || timer: 0.0890 sec.
iter 120210 || Loss: 1.1043 || timer: 0.0876 sec.
iter 120220 || Loss: 0.8715 || timer: 0.0874 sec.
iter 120230 || Loss: 1.0710 || timer: 0.0865 sec.
iter 120240 || Loss: 0.9553 || timer: 0.0925 sec.
iter 120250 || Loss: 1.1832 || timer: 0.1136 sec.
iter 120260 || Loss: 1.4916 || timer: 0.0867 sec.
iter 120270 || Loss: 1.5074 || timer: 0.1090 sec.
iter 120280 || Loss: 1.1817 || timer: 0.1128 sec.
iter 120290 || Loss: 1.1363 || timer: 0.0974 sec.
iter 120300 || Loss: 1.1401 || timer: 0.0830 sec.
iter 120310 || Loss: 1.0611 || timer: 0.0899 sec.
iter 120320 || Loss: 1.2376 || timer: 0.0892 sec.
iter 120330 || Loss: 0.9695 || timer: 0.0910 sec.
iter 120340 || Loss: 1.0030 || timer: 0.0190 sec.
iter 120350 || Loss: 0.5423 || timer: 0.0823 sec.
iter 120360 || Loss: 0.7957 || timer: 0.0873 sec.
iter 120370 || Loss: 1.0758 || timer: 0.0907 sec.
iter 120380 || Loss: 0.8780 || timer: 0.0837 sec.
iter 120390 || Loss: 1.0134 || timer: 0.1030 sec.
iter 120400 || Loss: 1.2107 || timer: 0.0904 sec.
iter 120410 || Loss: 1.0713 || timer: 0.1091 sec.
iter 120420 || Loss: 1.0064 || timer: 0.0828 sec.
iter 120430 || Loss: 1.0747 || timer: 0.1042 sec.
iter 120440 || Loss: 1.1245 || timer: 0.0968 sec.
iter 120450 || Loss: 1.2039 || timer: 0.1043 sec.
iter 120460 || Loss: 1.3766 || timer: 0.0911 sec.
iter 120470 || Loss: 1.1322 || timer: 0.0830 sec.
iter 120480 || Loss: 1.0927 || timer: 0.1030 sec.
iter 120490 || Loss: 1.1921 || timer: 0.0888 sec.
iter 120500 || Loss: 1.0129 || timer: 0.0923 sec.
iter 120510 || Loss: 1.1978 || timer: 0.1154 sec.
iter 120520 || Loss: 0.9670 || timer: 0.0917 sec.
iter 120530 || Loss: 1.0223 || timer: 0.0900 sec.
iter 120540 || Loss: 1.1224 || timer: 0.0889 sec.
iter 120550 || Loss: 1.0891 || timer: 0.0923 sec.
iter 120560 || Loss: 1.1914 || timer: 0.0990 sec.
iter 120570 || Loss: 0.9112 || timer: 0.0831 sec.
iter 120580 || Loss: 0.6415 || timer: 0.1061 sec.
iter 120590 || Loss: 1.5555 || timer: 0.0906 sec.
iter 120600 || Loss: 1.2831 || timer: 0.0856 sec.
iter 120610 || Loss: 1.5023 || timer: 0.0889 sec.
iter 120620 || Loss: 1.3510 || timer: 0.0837 sec.
iter 120630 || Loss: 1.1063 || timer: 0.0823 sec.
iter 120640 || Loss: 0.8645 || timer: 0.0832 sec.
iter 120650 || Loss: 1.2192 || timer: 0.0856 sec.
iter 120660 || Loss: 0.8532 || timer: 0.0910 sec.
iter 120670 || Loss: 0.9409 || timer: 0.0200 sec.
iter 120680 || Loss: 0.5939 || timer: 0.0902 sec.
iter 120690 || Loss: 1.1812 || timer: 0.0908 sec.
iter 120700 || Loss: 1.3637 || timer: 0.0836 sec.
iter 120710 || Loss: 0.7836 || timer: 0.0901 sec.
iter 120720 || Loss: 1.1776 || timer: 0.0913 sec.
iter 120730 || Loss: 1.0751 || timer: 0.1091 sec.
iter 120740 || Loss: 0.9450 || timer: 0.0898 sec.
iter 120750 || Loss: 1.0042 || timer: 0.0899 sec.
iter 120760 || Loss: 0.9392 || timer: 0.0986 sec.
iter 120770 || Loss: 1.0501 || timer: 0.1334 sec.
iter 120780 || Loss: 0.9596 || timer: 0.0886 sec.
iter 120790 || Loss: 1.0496 || timer: 0.0845 sec.
iter 120800 || Loss: 1.2383 || timer: 0.0840 sec.
iter 120810 || Loss: 0.9475 || timer: 0.0938 sec.
iter 120820 || Loss: 1.0065 || timer: 0.0897 sec.
iter 120830 || Loss: 1.0926 || timer: 0.0852 sec.
iter 120840 || Loss: 0.8892 || timer: 0.0916 sec.
iter 120850 || Loss: 0.8630 || timer: 0.0886 sec.
iter 120860 || Loss: 0.9801 || timer: 0.0910 sec.
iter 120870 || Loss: 1.0015 || timer: 0.0914 sec.
iter 120880 || Loss: 1.2021 || timer: 0.0942 sec.
iter 120890 || Loss: 0.7980 || timer: 0.0918 sec.
iter 120900 || Loss: 1.3480 || timer: 0.0834 sec.
iter 120910 || Loss: 0.8365 || timer: 0.0850 sec.
iter 120920 || Loss: 0.8972 || timer: 0.0822 sec.
iter 120930 || Loss: 1.3019 || timer: 0.0903 sec.
iter 120940 || Loss: 0.9214 || timer: 0.0896 sec.
iter 120950 || Loss: 0.8014 || timer: 0.0823 sec.
iter 120960 || Loss: 0.8312 || timer: 0.0861 sec.
iter 120970 || Loss: 1.2250 || timer: 0.0840 sec.
iter 120980 || Loss: 1.1414 || timer: 0.0904 sec.
iter 120990 || Loss: 0.8861 || timer: 0.0833 sec.
iter 121000 || Loss: 0.8814 || timer: 0.0241 sec.
iter 121010 || Loss: 1.3053 || timer: 0.0824 sec.
iter 121020 || Loss: 1.0577 || timer: 0.0907 sec.
iter 121030 || Loss: 0.9406 || timer: 0.0874 sec.
iter 121040 || Loss: 1.1160 || timer: 0.0917 sec.
iter 121050 || Loss: 1.0284 || timer: 0.0858 sec.
iter 121060 || Loss: 1.3961 || timer: 0.0839 sec.
iter 121070 || Loss: 1.0353 || timer: 0.0928 sec.
iter 121080 || Loss: 1.2340 || timer: 0.0906 sec.
iter 121090 || Loss: 0.9592 || timer: 0.0921 sec.
iter 121100 || Loss: 0.9157 || timer: 0.0955 sec.
iter 121110 || Loss: 0.7252 || timer: 0.0904 sec.
iter 121120 || Loss: 1.2492 || timer: 0.0894 sec.
iter 121130 || Loss: 1.1988 || timer: 0.0910 sec.
iter 121140 || Loss: 1.2364 || timer: 0.0829 sec.
iter 121150 || Loss: 0.9423 || timer: 0.0870 sec.
iter 121160 || Loss: 0.9945 || timer: 0.0919 sec.
iter 121170 || Loss: 0.8983 || timer: 0.0903 sec.
iter 121180 || Loss: 1.0821 || timer: 0.1020 sec.
iter 121190 || Loss: 1.2452 || timer: 0.1036 sec.
iter 121200 || Loss: 1.0952 || timer: 0.0902 sec.
iter 121210 || Loss: 1.0084 || timer: 0.0900 sec.
iter 121220 || Loss: 0.8652 || timer: 0.1088 sec.
iter 121230 || Loss: 0.7922 || timer: 0.0909 sec.
iter 121240 || Loss: 0.7849 || timer: 0.0908 sec.
iter 121250 || Loss: 1.2581 || timer: 0.0848 sec.
iter 121260 || Loss: 0.9367 || timer: 0.0930 sec.
iter 121270 || Loss: 1.0677 || timer: 0.1228 sec.
iter 121280 || Loss: 1.0354 || timer: 0.0890 sec.
iter 121290 || Loss: 1.0879 || timer: 0.0863 sec.
iter 121300 || Loss: 0.8401 || timer: 0.0916 sec.
iter 121310 || Loss: 1.2655 || timer: 0.1003 sec.
iter 121320 || Loss: 1.3465 || timer: 0.0946 sec.
iter 121330 || Loss: 0.9105 || timer: 0.0273 sec.
iter 121340 || Loss: 1.2405 || timer: 0.0901 sec.
iter 121350 || Loss: 1.0804 || timer: 0.0836 sec.
iter 121360 || Loss: 1.1306 || timer: 0.0909 sec.
iter 121370 || Loss: 1.2007 || timer: 0.0920 sec.
iter 121380 || Loss: 1.2539 || timer: 0.1049 sec.
iter 121390 || Loss: 1.1544 || timer: 0.0899 sec.
iter 121400 || Loss: 1.4789 || timer: 0.0904 sec.
iter 121410 || Loss: 0.8796 || timer: 0.0837 sec.
iter 121420 || Loss: 0.9433 || timer: 0.1053 sec.
iter 121430 || Loss: 1.0187 || timer: 0.1190 sec.
iter 121440 || Loss: 1.0324 || timer: 0.0926 sec.
iter 121450 || Loss: 0.9086 || timer: 0.0871 sec.
iter 121460 || Loss: 1.0247 || timer: 0.0898 sec.
iter 121470 || Loss: 1.1529 || timer: 0.0907 sec.
iter 121480 || Loss: 1.0284 || timer: 0.0936 sec.
iter 121490 || Loss: 0.9804 || timer: 0.0825 sec.
iter 121500 || Loss: 0.8846 || timer: 0.0916 sec.
iter 121510 || Loss: 0.8879 || timer: 0.0833 sec.
iter 121520 || Loss: 1.0612 || timer: 0.0866 sec.
iter 121530 || Loss: 0.6087 || timer: 0.0928 sec.
iter 121540 || Loss: 0.8520 || timer: 0.0831 sec.
iter 121550 || Loss: 0.9400 || timer: 0.0838 sec.
iter 121560 || Loss: 1.1651 || timer: 0.0910 sec.
iter 121570 || Loss: 0.7434 || timer: 0.0922 sec.
iter 121580 || Loss: 1.7429 || timer: 0.1305 sec.
iter 121590 || Loss: 0.9865 || timer: 0.1079 sec.
iter 121600 || Loss: 1.1891 || timer: 0.1005 sec.
iter 121610 || Loss: 0.8134 || timer: 0.1100 sec.
iter 121620 || Loss: 1.0313 || timer: 0.0933 sec.
iter 121630 || Loss: 0.8183 || timer: 0.0935 sec.
iter 121640 || Loss: 1.3240 || timer: 0.0921 sec.
iter 121650 || Loss: 0.9026 || timer: 0.1183 sec.
iter 121660 || Loss: 1.2142 || timer: 0.0264 sec.
iter 121670 || Loss: 3.1096 || timer: 0.0893 sec.
iter 121680 || Loss: 0.9272 || timer: 0.0895 sec.
iter 121690 || Loss: 0.9957 || timer: 0.0902 sec.
iter 121700 || Loss: 0.9882 || timer: 0.0909 sec.
iter 121710 || Loss: 0.7607 || timer: 0.0841 sec.
iter 121720 || Loss: 1.3313 || timer: 0.0888 sec.
iter 121730 || Loss: 0.8218 || timer: 0.0903 sec.
iter 121740 || Loss: 1.0449 || timer: 0.0891 sec.
iter 121750 || Loss: 1.0590 || timer: 0.1047 sec.
iter 121760 || Loss: 0.9443 || timer: 0.1021 sec.
iter 121770 || Loss: 1.1076 || timer: 0.0910 sec.
iter 121780 || Loss: 1.1024 || timer: 0.0907 sec.
iter 121790 || Loss: 1.0840 || timer: 0.0886 sec.
iter 121800 || Loss: 1.0464 || timer: 0.0879 sec.
iter 121810 || Loss: 1.2451 || timer: 0.1007 sec.
iter 121820 || Loss: 0.8472 || timer: 0.1031 sec.
iter 121830 || Loss: 1.0046 || timer: 0.1054 sec.
iter 121840 || Loss: 0.9606 || timer: 0.0837 sec.
iter 121850 || Loss: 1.0220 || timer: 0.0915 sec.
iter 121860 || Loss: 0.9314 || timer: 0.0978 sec.
iter 121870 || Loss: 1.3167 || timer: 0.1080 sec.
iter 121880 || Loss: 0.7786 || timer: 0.0833 sec.
iter 121890 || Loss: 0.7478 || timer: 0.0906 sec.
iter 121900 || Loss: 1.0424 || timer: 0.0908 sec.
iter 121910 || Loss: 1.4247 || timer: 0.0929 sec.
iter 121920 || Loss: 1.1555 || timer: 0.1003 sec.
iter 121930 || Loss: 0.8690 || timer: 0.0907 sec.
iter 121940 || Loss: 1.3919 || timer: 0.0888 sec.
iter 121950 || Loss: 1.0996 || timer: 0.0877 sec.
iter 121960 || Loss: 1.2814 || timer: 0.0882 sec.
iter 121970 || Loss: 0.8532 || timer: 0.0893 sec.
iter 121980 || Loss: 1.3346 || timer: 0.1024 sec.
iter 121990 || Loss: 1.2507 || timer: 0.0270 sec.
iter 122000 || Loss: 0.4670 || timer: 0.0830 sec.
iter 122010 || Loss: 0.8395 || timer: 0.1011 sec.
iter 122020 || Loss: 1.0772 || timer: 0.0896 sec.
iter 122030 || Loss: 1.0665 || timer: 0.0913 sec.
iter 122040 || Loss: 0.9545 || timer: 0.0971 sec.
iter 122050 || Loss: 0.9403 || timer: 0.0837 sec.
iter 122060 || Loss: 1.0269 || timer: 0.0833 sec.
iter 122070 || Loss: 1.1888 || timer: 0.0898 sec.
iter 122080 || Loss: 1.0222 || timer: 0.1053 sec.
iter 122090 || Loss: 1.0986 || timer: 0.0864 sec.
iter 122100 || Loss: 1.1048 || timer: 0.0810 sec.
iter 122110 || Loss: 1.6600 || timer: 0.0934 sec.
iter 122120 || Loss: 0.8136 || timer: 0.0887 sec.
iter 122130 || Loss: 0.9493 || timer: 0.0872 sec.
iter 122140 || Loss: 1.5043 || timer: 0.0873 sec.
iter 122150 || Loss: 1.2053 || timer: 0.0886 sec.
iter 122160 || Loss: 0.9566 || timer: 0.1089 sec.
iter 122170 || Loss: 0.9853 || timer: 0.0906 sec.
iter 122180 || Loss: 1.1754 || timer: 0.1024 sec.
iter 122190 || Loss: 0.8764 || timer: 0.0909 sec.
iter 122200 || Loss: 0.9209 || timer: 0.0830 sec.
iter 122210 || Loss: 0.8829 || timer: 0.0894 sec.
iter 122220 || Loss: 1.2562 || timer: 0.0892 sec.
iter 122230 || Loss: 1.0636 || timer: 0.1002 sec.
iter 122240 || Loss: 1.2305 || timer: 0.0990 sec.
iter 122250 || Loss: 1.0401 || timer: 0.1033 sec.
iter 122260 || Loss: 0.9514 || timer: 0.0862 sec.
iter 122270 || Loss: 0.9723 || timer: 0.1102 sec.
iter 122280 || Loss: 1.0874 || timer: 0.0896 sec.
iter 122290 || Loss: 1.1890 || timer: 0.0895 sec.
iter 122300 || Loss: 0.9456 || timer: 0.0900 sec.
iter 122310 || Loss: 0.6843 || timer: 0.0874 sec.
iter 122320 || Loss: 1.0460 || timer: 0.0244 sec.
iter 122330 || Loss: 2.0450 || timer: 0.0906 sec.
iter 122340 || Loss: 1.5020 || timer: 0.0917 sec.
iter 122350 || Loss: 0.9337 || timer: 0.0908 sec.
iter 122360 || Loss: 1.1693 || timer: 0.0830 sec.
iter 122370 || Loss: 0.9429 || timer: 0.0843 sec.
iter 122380 || Loss: 1.1437 || timer: 0.1087 sec.
iter 122390 || Loss: 0.6963 || timer: 0.0818 sec.
iter 122400 || Loss: 1.4034 || timer: 0.0885 sec.
iter 122410 || Loss: 1.2704 || timer: 0.0911 sec.
iter 122420 || Loss: 0.9260 || timer: 0.0960 sec.
iter 122430 || Loss: 1.2928 || timer: 0.0898 sec.
iter 122440 || Loss: 1.1676 || timer: 0.0914 sec.
iter 122450 || Loss: 1.2464 || timer: 0.0837 sec.
iter 122460 || Loss: 1.0023 || timer: 0.0824 sec.
iter 122470 || Loss: 0.8401 || timer: 0.0824 sec.
iter 122480 || Loss: 1.0294 || timer: 0.0895 sec.
iter 122490 || Loss: 0.8114 || timer: 0.1059 sec.
iter 122500 || Loss: 0.8730 || timer: 0.0855 sec.
iter 122510 || Loss: 0.7849 || timer: 0.0912 sec.
iter 122520 || Loss: 1.1052 || timer: 0.0897 sec.
iter 122530 || Loss: 0.9807 || timer: 0.0823 sec.
iter 122540 || Loss: 1.1552 || timer: 0.0905 sec.
iter 122550 || Loss: 1.0320 || timer: 0.0799 sec.
iter 122560 || Loss: 0.8165 || timer: 0.0915 sec.
iter 122570 || Loss: 1.7946 || timer: 0.0823 sec.
iter 122580 || Loss: 1.1358 || timer: 0.0803 sec.
iter 122590 || Loss: 1.0817 || timer: 0.0753 sec.
iter 122600 || Loss: 1.1126 || timer: 0.1185 sec.
iter 122610 || Loss: 1.4791 || timer: 0.0930 sec.
iter 122620 || Loss: 0.8289 || timer: 0.0951 sec.
iter 122630 || Loss: 0.9343 || timer: 0.0859 sec.
iter 122640 || Loss: 1.3187 || timer: 0.0835 sec.
iter 122650 || Loss: 0.9533 || timer: 0.0234 sec.
iter 122660 || Loss: 2.0822 || timer: 0.0913 sec.
iter 122670 || Loss: 1.2530 || timer: 0.0927 sec.
iter 122680 || Loss: 0.9787 || timer: 0.0923 sec.
iter 122690 || Loss: 1.5846 || timer: 0.0908 sec.
iter 122700 || Loss: 1.5266 || timer: 0.0924 sec.
iter 122710 || Loss: 1.0326 || timer: 0.0904 sec.
iter 122720 || Loss: 1.3273 || timer: 0.0831 sec.
iter 122730 || Loss: 1.4502 || timer: 0.0970 sec.
iter 122740 || Loss: 1.3690 || timer: 0.0998 sec.
iter 122750 || Loss: 1.0317 || timer: 0.0982 sec.
iter 122760 || Loss: 0.9973 || timer: 0.0835 sec.
iter 122770 || Loss: 1.2788 || timer: 0.0900 sec.
iter 122780 || Loss: 0.9969 || timer: 0.1113 sec.
iter 122790 || Loss: 1.0167 || timer: 0.0866 sec.
iter 122800 || Loss: 0.6483 || timer: 0.0919 sec.
iter 122810 || Loss: 1.0555 || timer: 0.0910 sec.
iter 122820 || Loss: 0.8158 || timer: 0.0826 sec.
iter 122830 || Loss: 1.1040 || timer: 0.0823 sec.
iter 122840 || Loss: 0.6861 || timer: 0.0878 sec.
iter 122850 || Loss: 0.9322 || timer: 0.0872 sec.
iter 122860 || Loss: 1.1117 || timer: 0.0875 sec.
iter 122870 || Loss: 0.7563 || timer: 0.0843 sec.
iter 122880 || Loss: 1.2042 || timer: 0.0926 sec.
iter 122890 || Loss: 1.2757 || timer: 0.1152 sec.
iter 122900 || Loss: 1.3182 || timer: 0.0901 sec.
iter 122910 || Loss: 0.8296 || timer: 0.0946 sec.
iter 122920 || Loss: 0.6733 || timer: 0.0853 sec.
iter 122930 || Loss: 0.9402 || timer: 0.1060 sec.
iter 122940 || Loss: 1.2376 || timer: 0.0916 sec.
iter 122950 || Loss: 0.6684 || timer: 0.1003 sec.
iter 122960 || Loss: 0.7889 || timer: 0.0922 sec.
iter 122970 || Loss: 1.0001 || timer: 0.0925 sec.
iter 122980 || Loss: 1.0929 || timer: 0.0182 sec.
iter 122990 || Loss: 2.1991 || timer: 0.0834 sec.
iter 123000 || Loss: 1.1753 || timer: 0.0829 sec.
iter 123010 || Loss: 1.3535 || timer: 0.0915 sec.
iter 123020 || Loss: 0.9475 || timer: 0.0823 sec.
iter 123030 || Loss: 1.1028 || timer: 0.1055 sec.
iter 123040 || Loss: 0.9113 || timer: 0.0867 sec.
iter 123050 || Loss: 1.0413 || timer: 0.0859 sec.
iter 123060 || Loss: 0.7802 || timer: 0.0835 sec.
iter 123070 || Loss: 1.4921 || timer: 0.0905 sec.
iter 123080 || Loss: 1.5308 || timer: 0.1221 sec.
iter 123090 || Loss: 1.1448 || timer: 0.0919 sec.
iter 123100 || Loss: 0.9389 || timer: 0.0900 sec.
iter 123110 || Loss: 0.9667 || timer: 0.0836 sec.
iter 123120 || Loss: 1.0102 || timer: 0.0900 sec.
iter 123130 || Loss: 1.1221 || timer: 0.0893 sec.
iter 123140 || Loss: 0.9303 || timer: 0.0905 sec.
iter 123150 || Loss: 1.4290 || timer: 0.1124 sec.
iter 123160 || Loss: 1.0309 || timer: 0.0830 sec.
iter 123170 || Loss: 1.0394 || timer: 0.0917 sec.
iter 123180 || Loss: 1.1000 || timer: 0.0844 sec.
iter 123190 || Loss: 1.0293 || timer: 0.0913 sec.
iter 123200 || Loss: 1.5106 || timer: 0.0914 sec.
iter 123210 || Loss: 1.0077 || timer: 0.0914 sec.
iter 123220 || Loss: 1.1020 || timer: 0.0904 sec.
iter 123230 || Loss: 0.9167 || timer: 0.1068 sec.
iter 123240 || Loss: 0.8163 || timer: 0.0912 sec.
iter 123250 || Loss: 0.8283 || timer: 0.0928 sec.
iter 123260 || Loss: 1.0785 || timer: 0.0875 sec.
iter 123270 || Loss: 1.1307 || timer: 0.0927 sec.
iter 123280 || Loss: 0.8809 || timer: 0.0856 sec.
iter 123290 || Loss: 1.0966 || timer: 0.0909 sec.
iter 123300 || Loss: 0.7828 || timer: 0.1023 sec.
iter 123310 || Loss: 0.8434 || timer: 0.0183 sec.
iter 123320 || Loss: 1.0389 || timer: 0.0828 sec.
iter 123330 || Loss: 1.2008 || timer: 0.0909 sec.
iter 123340 || Loss: 0.8797 || timer: 0.1034 sec.
iter 123350 || Loss: 1.1116 || timer: 0.0909 sec.
iter 123360 || Loss: 1.0468 || timer: 0.0901 sec.
iter 123370 || Loss: 0.8234 || timer: 0.0891 sec.
iter 123380 || Loss: 0.9860 || timer: 0.0867 sec.
iter 123390 || Loss: 0.8170 || timer: 0.0841 sec.
iter 123400 || Loss: 1.1222 || timer: 0.0820 sec.
iter 123410 || Loss: 0.8567 || timer: 0.1188 sec.
iter 123420 || Loss: 0.7467 || timer: 0.0823 sec.
iter 123430 || Loss: 0.8456 || timer: 0.0903 sec.
iter 123440 || Loss: 0.8480 || timer: 0.0895 sec.
iter 123450 || Loss: 1.0267 || timer: 0.0825 sec.
iter 123460 || Loss: 1.2217 || timer: 0.0912 sec.
iter 123470 || Loss: 0.9358 || timer: 0.0830 sec.
iter 123480 || Loss: 1.0378 || timer: 0.0866 sec.
iter 123490 || Loss: 0.8515 || timer: 0.1033 sec.
iter 123500 || Loss: 1.0407 || timer: 0.0922 sec.
iter 123510 || Loss: 0.8621 || timer: 0.0950 sec.
iter 123520 || Loss: 0.9908 || timer: 0.0903 sec.
iter 123530 || Loss: 0.8600 || timer: 0.0905 sec.
iter 123540 || Loss: 1.0625 || timer: 0.0842 sec.
iter 123550 || Loss: 0.8055 || timer: 0.0914 sec.
iter 123560 || Loss: 1.1283 || timer: 0.0901 sec.
iter 123570 || Loss: 0.9520 || timer: 0.0887 sec.
iter 123580 || Loss: 1.2689 || timer: 0.0823 sec.
iter 123590 || Loss: 0.9755 || timer: 0.0819 sec.
iter 123600 || Loss: 0.7935 || timer: 0.0832 sec.
iter 123610 || Loss: 0.8496 || timer: 0.0925 sec.
iter 123620 || Loss: 0.8802 || timer: 0.0915 sec.
iter 123630 || Loss: 0.9719 || timer: 0.0915 sec.
iter 123640 || Loss: 0.8287 || timer: 0.0281 sec.
iter 123650 || Loss: 0.8996 || timer: 0.0936 sec.
iter 123660 || Loss: 1.0542 || timer: 0.0906 sec.
iter 123670 || Loss: 1.2118 || timer: 0.0820 sec.
iter 123680 || Loss: 1.2006 || timer: 0.0901 sec.
iter 123690 || Loss: 0.8094 || timer: 0.0931 sec.
iter 123700 || Loss: 0.8751 || timer: 0.1039 sec.
iter 123710 || Loss: 0.9312 || timer: 0.0928 sec.
iter 123720 || Loss: 1.4044 || timer: 0.0839 sec.
iter 123730 || Loss: 1.1031 || timer: 0.1015 sec.
iter 123740 || Loss: 0.9003 || timer: 0.0987 sec.
iter 123750 || Loss: 1.1833 || timer: 0.0896 sec.
iter 123760 || Loss: 1.0128 || timer: 0.0899 sec.
iter 123770 || Loss: 1.1981 || timer: 0.0827 sec.
iter 123780 || Loss: 0.7457 || timer: 0.0930 sec.
iter 123790 || Loss: 0.9261 || timer: 0.0851 sec.
iter 123800 || Loss: 1.0273 || timer: 0.0918 sec.
iter 123810 || Loss: 0.9592 || timer: 0.0911 sec.
iter 123820 || Loss: 1.0276 || timer: 0.0928 sec.
iter 123830 || Loss: 0.8312 || timer: 0.0922 sec.
iter 123840 || Loss: 0.8673 || timer: 0.0890 sec.
iter 123850 || Loss: 1.3315 || timer: 0.0887 sec.
iter 123860 || Loss: 1.0959 || timer: 0.0901 sec.
iter 123870 || Loss: 1.2584 || timer: 0.0836 sec.
iter 123880 || Loss: 0.9315 || timer: 0.0827 sec.
iter 123890 || Loss: 1.4041 || timer: 0.0822 sec.
iter 123900 || Loss: 1.4297 || timer: 0.0917 sec.
iter 123910 || Loss: 1.1747 || timer: 0.0825 sec.
iter 123920 || Loss: 0.9044 || timer: 0.0874 sec.
iter 123930 || Loss: 1.2420 || timer: 0.0906 sec.
iter 123940 || Loss: 1.1296 || timer: 0.0893 sec.
iter 123950 || Loss: 1.1421 || timer: 0.0838 sec.
iter 123960 || Loss: 0.9298 || timer: 0.0906 sec.
iter 123970 || Loss: 0.9669 || timer: 0.0173 sec.
iter 123980 || Loss: 1.4596 || timer: 0.0889 sec.
iter 123990 || Loss: 1.1065 || timer: 0.0929 sec.
iter 124000 || Loss: 1.1131 || timer: 0.0928 sec.
iter 124010 || Loss: 1.1072 || timer: 0.0823 sec.
iter 124020 || Loss: 0.7750 || timer: 0.1136 sec.
iter 124030 || Loss: 1.0164 || timer: 0.0823 sec.
iter 124040 || Loss: 1.0748 || timer: 0.0840 sec.
iter 124050 || Loss: 1.3562 || timer: 0.1171 sec.
iter 124060 || Loss: 1.3669 || timer: 0.0932 sec.
iter 124070 || Loss: 1.3802 || timer: 0.1118 sec.
iter 124080 || Loss: 1.0450 || timer: 0.0853 sec.
iter 124090 || Loss: 1.0372 || timer: 0.0904 sec.
iter 124100 || Loss: 0.8946 || timer: 0.0902 sec.
iter 124110 || Loss: 1.3348 || timer: 0.0915 sec.
iter 124120 || Loss: 1.0004 || timer: 0.1078 sec.
iter 124130 || Loss: 0.9831 || timer: 0.1075 sec.
iter 124140 || Loss: 0.8547 || timer: 0.1163 sec.
iter 124150 || Loss: 0.8529 || timer: 0.0901 sec.
iter 124160 || Loss: 1.1811 || timer: 0.0915 sec.
iter 124170 || Loss: 1.1428 || timer: 0.0980 sec.
iter 124180 || Loss: 1.1301 || timer: 0.0878 sec.
iter 124190 || Loss: 0.8831 || timer: 0.0908 sec.
iter 124200 || Loss: 0.8342 || timer: 0.0904 sec.
iter 124210 || Loss: 1.2201 || timer: 0.0890 sec.
iter 124220 || Loss: 0.9217 || timer: 0.0918 sec.
iter 124230 || Loss: 1.0176 || timer: 0.0903 sec.
iter 124240 || Loss: 0.7674 || timer: 0.0882 sec.
iter 124250 || Loss: 1.0584 || timer: 0.0885 sec.
iter 124260 || Loss: 1.2055 || timer: 0.0916 sec.
iter 124270 || Loss: 0.8675 || timer: 0.1069 sec.
iter 124280 || Loss: 1.0818 || timer: 0.0897 sec.
iter 124290 || Loss: 0.9926 || timer: 0.1050 sec.
iter 124300 || Loss: 0.9754 || timer: 0.0271 sec.
iter 124310 || Loss: 0.2130 || timer: 0.0910 sec.
iter 124320 || Loss: 0.7480 || timer: 0.0906 sec.
iter 124330 || Loss: 1.3131 || timer: 0.0904 sec.
iter 124340 || Loss: 0.8006 || timer: 0.0867 sec.
iter 124350 || Loss: 1.1620 || timer: 0.0855 sec.
iter 124360 || Loss: 1.0191 || timer: 0.0906 sec.
iter 124370 || Loss: 1.1396 || timer: 0.0890 sec.
iter 124380 || Loss: 0.7414 || timer: 0.0901 sec.
iter 124390 || Loss: 0.9416 || timer: 0.0868 sec.
iter 124400 || Loss: 1.3130 || timer: 0.0980 sec.
iter 124410 || Loss: 1.0128 || timer: 0.0881 sec.
iter 124420 || Loss: 0.9063 || timer: 0.1051 sec.
iter 124430 || Loss: 1.0974 || timer: 0.0911 sec.
iter 124440 || Loss: 0.8599 || timer: 0.0839 sec.
iter 124450 || Loss: 0.9362 || timer: 0.1112 sec.
iter 124460 || Loss: 0.8658 || timer: 0.0912 sec.
iter 124470 || Loss: 0.8935 || timer: 0.1041 sec.
iter 124480 || Loss: 0.8826 || timer: 0.1051 sec.
iter 124490 || Loss: 1.1486 || timer: 0.0900 sec.
iter 124500 || Loss: 1.0622 || timer: 0.0889 sec.
iter 124510 || Loss: 1.3983 || timer: 0.0880 sec.
iter 124520 || Loss: 1.1195 || timer: 0.0894 sec.
iter 124530 || Loss: 0.6418 || timer: 0.0963 sec.
iter 124540 || Loss: 0.9770 || timer: 0.0889 sec.
iter 124550 || Loss: 0.8369 || timer: 0.0826 sec.
iter 124560 || Loss: 0.7444 || timer: 0.0913 sec.
iter 124570 || Loss: 1.2570 || timer: 0.0898 sec.
iter 124580 || Loss: 0.8900 || timer: 0.0923 sec.
iter 124590 || Loss: 1.3503 || timer: 0.0885 sec.
iter 124600 || Loss: 1.1232 || timer: 0.0900 sec.
iter 124610 || Loss: 0.8294 || timer: 0.0886 sec.
iter 124620 || Loss: 0.5733 || timer: 0.0911 sec.
iter 124630 || Loss: 1.0753 || timer: 0.0266 sec.
iter 124640 || Loss: 1.4683 || timer: 0.0901 sec.
iter 124650 || Loss: 0.9926 || timer: 0.0824 sec.
iter 124660 || Loss: 1.2425 || timer: 0.1073 sec.
iter 124670 || Loss: 0.9971 || timer: 0.0964 sec.
iter 124680 || Loss: 1.1417 || timer: 0.0835 sec.
iter 124690 || Loss: 0.8339 || timer: 0.1044 sec.
iter 124700 || Loss: 1.3073 || timer: 0.0921 sec.
iter 124710 || Loss: 0.7239 || timer: 0.0918 sec.
iter 124720 || Loss: 1.0074 || timer: 0.0833 sec.
iter 124730 || Loss: 0.7327 || timer: 0.1094 sec.
iter 124740 || Loss: 1.1009 || timer: 0.0828 sec.
iter 124750 || Loss: 1.2013 || timer: 0.1047 sec.
iter 124760 || Loss: 0.8886 || timer: 0.0925 sec.
iter 124770 || Loss: 1.0070 || timer: 0.0823 sec.
iter 124780 || Loss: 0.7788 || timer: 0.0968 sec.
iter 124790 || Loss: 1.0003 || timer: 0.0924 sec.
iter 124800 || Loss: 1.1112 || timer: 0.0892 sec.
iter 124810 || Loss: 2.7235 || timer: 0.0891 sec.
iter 124820 || Loss: 1.9256 || timer: 0.0886 sec.
iter 124830 || Loss: 1.3787 || timer: 0.1118 sec.
iter 124840 || Loss: 1.3249 || timer: 0.1227 sec.
iter 124850 || Loss: 1.3517 || timer: 0.0914 sec.
iter 124860 || Loss: 1.1779 || timer: 0.0922 sec.
iter 124870 || Loss: 1.4442 || timer: 0.0903 sec.
iter 124880 || Loss: 1.3221 || timer: 0.0822 sec.
iter 124890 || Loss: 1.2819 || timer: 0.0848 sec.
iter 124900 || Loss: 0.8488 || timer: 0.0901 sec.
iter 124910 || Loss: 0.8985 || timer: 0.1026 sec.
iter 124920 || Loss: 1.0074 || timer: 0.0888 sec.
iter 124930 || Loss: 1.2228 || timer: 0.0870 sec.
iter 124940 || Loss: 1.2433 || timer: 0.0881 sec.
iter 124950 || Loss: 1.0428 || timer: 0.0894 sec.
iter 124960 || Loss: 1.0491 || timer: 0.0203 sec.
iter 124970 || Loss: 1.6009 || timer: 0.1036 sec.
iter 124980 || Loss: 1.1816 || timer: 0.0894 sec.
iter 124990 || Loss: 1.1147 || timer: 0.0896 sec.
iter 125000 || Loss: 1.1236 || Saving state, iter: 125000
timer: 0.0924 sec.
iter 125010 || Loss: 1.0207 || timer: 0.0916 sec.
iter 125020 || Loss: 1.2162 || timer: 0.0826 sec.
iter 125030 || Loss: 1.0921 || timer: 0.0889 sec.
iter 125040 || Loss: 1.0018 || timer: 0.0830 sec.
iter 125050 || Loss: 1.0915 || timer: 0.1277 sec.
iter 125060 || Loss: 0.8344 || timer: 0.0941 sec.
iter 125070 || Loss: 1.1021 || timer: 0.0828 sec.
iter 125080 || Loss: 0.7792 || timer: 0.0861 sec.
iter 125090 || Loss: 1.2040 || timer: 0.0817 sec.
iter 125100 || Loss: 0.7942 || timer: 0.0912 sec.
iter 125110 || Loss: 1.1303 || timer: 0.0922 sec.
iter 125120 || Loss: 0.8861 || timer: 0.0824 sec.
iter 125130 || Loss: 1.1564 || timer: 0.0899 sec.
iter 125140 || Loss: 1.0478 || timer: 0.1085 sec.
iter 125150 || Loss: 1.0096 || timer: 0.0890 sec.
iter 125160 || Loss: 1.0598 || timer: 0.0845 sec.
iter 125170 || Loss: 0.7825 || timer: 0.0921 sec.
iter 125180 || Loss: 1.3670 || timer: 0.0825 sec.
iter 125190 || Loss: 0.9516 || timer: 0.0917 sec.
iter 125200 || Loss: 1.0053 || timer: 0.0915 sec.
iter 125210 || Loss: 1.4482 || timer: 0.0821 sec.
iter 125220 || Loss: 1.0539 || timer: 0.0948 sec.
iter 125230 || Loss: 1.0252 || timer: 0.0970 sec.
iter 125240 || Loss: 0.9064 || timer: 0.0964 sec.
iter 125250 || Loss: 1.3321 || timer: 0.0902 sec.
iter 125260 || Loss: 1.2905 || timer: 0.0834 sec.
iter 125270 || Loss: 0.9422 || timer: 0.1034 sec.
iter 125280 || Loss: 0.9859 || timer: 0.0911 sec.
iter 125290 || Loss: 1.1221 || timer: 0.0189 sec.
iter 125300 || Loss: 1.1415 || timer: 0.0850 sec.
iter 125310 || Loss: 1.4880 || timer: 0.0884 sec.
iter 125320 || Loss: 1.2820 || timer: 0.0914 sec.
iter 125330 || Loss: 1.0829 || timer: 0.0943 sec.
iter 125340 || Loss: 1.0331 || timer: 0.1000 sec.
iter 125350 || Loss: 0.9112 || timer: 0.0902 sec.
iter 125360 || Loss: 0.8998 || timer: 0.0841 sec.
iter 125370 || Loss: 0.8763 || timer: 0.0911 sec.
iter 125380 || Loss: 1.0930 || timer: 0.0902 sec.
iter 125390 || Loss: 0.9273 || timer: 0.1013 sec.
iter 125400 || Loss: 0.9674 || timer: 0.0919 sec.
iter 125410 || Loss: 1.2598 || timer: 0.0887 sec.
iter 125420 || Loss: 0.7225 || timer: 0.0913 sec.
iter 125430 || Loss: 1.1478 || timer: 0.1100 sec.
iter 125440 || Loss: 0.7973 || timer: 0.0830 sec.
iter 125450 || Loss: 1.0701 || timer: 0.0956 sec.
iter 125460 || Loss: 1.1543 || timer: 0.0826 sec.
iter 125470 || Loss: 0.7689 || timer: 0.0896 sec.
iter 125480 || Loss: 0.9879 || timer: 0.1152 sec.
iter 125490 || Loss: 0.9760 || timer: 0.0826 sec.
iter 125500 || Loss: 0.9788 || timer: 0.0895 sec.
iter 125510 || Loss: 0.9539 || timer: 0.0906 sec.
iter 125520 || Loss: 1.2653 || timer: 0.0920 sec.
iter 125530 || Loss: 0.9205 || timer: 0.0889 sec.
iter 125540 || Loss: 0.9224 || timer: 0.1094 sec.
iter 125550 || Loss: 1.0522 || timer: 0.0925 sec.
iter 125560 || Loss: 0.8230 || timer: 0.0875 sec.
iter 125570 || Loss: 0.9879 || timer: 0.0931 sec.
iter 125580 || Loss: 0.8444 || timer: 0.0908 sec.
iter 125590 || Loss: 0.6665 || timer: 0.0899 sec.
iter 125600 || Loss: 0.8151 || timer: 0.0904 sec.
iter 125610 || Loss: 0.6495 || timer: 0.0862 sec.
iter 125620 || Loss: 0.7555 || timer: 0.0183 sec.
iter 125630 || Loss: 1.1076 || timer: 0.0912 sec.
iter 125640 || Loss: 1.1990 || timer: 0.0972 sec.
iter 125650 || Loss: 0.9501 || timer: 0.0917 sec.
iter 125660 || Loss: 1.0557 || timer: 0.0833 sec.
iter 125670 || Loss: 0.8168 || timer: 0.0906 sec.
iter 125680 || Loss: 0.9236 || timer: 0.0881 sec.
iter 125690 || Loss: 1.0587 || timer: 0.0971 sec.
iter 125700 || Loss: 0.8895 || timer: 0.0826 sec.
iter 125710 || Loss: 0.9405 || timer: 0.0893 sec.
iter 125720 || Loss: 1.4778 || timer: 0.1183 sec.
iter 125730 || Loss: 0.9108 || timer: 0.0880 sec.
iter 125740 || Loss: 1.0248 || timer: 0.0827 sec.
iter 125750 || Loss: 0.9188 || timer: 0.0924 sec.
iter 125760 || Loss: 0.8305 || timer: 0.0922 sec.
iter 125770 || Loss: 0.7676 || timer: 0.0886 sec.
iter 125780 || Loss: 0.9740 || timer: 0.0912 sec.
iter 125790 || Loss: 0.8377 || timer: 0.1037 sec.
iter 125800 || Loss: 1.1958 || timer: 0.0913 sec.
iter 125810 || Loss: 0.9702 || timer: 0.0886 sec.
iter 125820 || Loss: 0.9662 || timer: 0.1143 sec.
iter 125830 || Loss: 0.9963 || timer: 0.0908 sec.
iter 125840 || Loss: 1.0185 || timer: 0.0832 sec.
iter 125850 || Loss: 0.7462 || timer: 0.0903 sec.
iter 125860 || Loss: 1.0608 || timer: 0.1037 sec.
iter 125870 || Loss: 0.8815 || timer: 0.0907 sec.
iter 125880 || Loss: 0.8499 || timer: 0.0902 sec.
iter 125890 || Loss: 1.3377 || timer: 0.0912 sec.
iter 125900 || Loss: 1.2783 || timer: 0.0900 sec.
iter 125910 || Loss: 1.2571 || timer: 0.0818 sec.
iter 125920 || Loss: 0.7262 || timer: 0.0825 sec.
iter 125930 || Loss: 0.8456 || timer: 0.1012 sec.
iter 125940 || Loss: 1.1657 || timer: 0.0906 sec.
iter 125950 || Loss: 0.7401 || timer: 0.0257 sec.
iter 125960 || Loss: 0.2674 || timer: 0.0829 sec.
iter 125970 || Loss: 1.0568 || timer: 0.0997 sec.
iter 125980 || Loss: 0.7560 || timer: 0.0933 sec.
iter 125990 || Loss: 0.8487 || timer: 0.0916 sec.
iter 126000 || Loss: 1.0463 || timer: 0.0876 sec.
iter 126010 || Loss: 1.1434 || timer: 0.1071 sec.
iter 126020 || Loss: 1.1724 || timer: 0.0823 sec.
iter 126030 || Loss: 0.9767 || timer: 0.0834 sec.
iter 126040 || Loss: 0.8218 || timer: 0.0920 sec.
iter 126050 || Loss: 1.0338 || timer: 0.1099 sec.
iter 126060 || Loss: 1.1949 || timer: 0.0896 sec.
iter 126070 || Loss: 0.9783 || timer: 0.0835 sec.
iter 126080 || Loss: 0.8895 || timer: 0.1078 sec.
iter 126090 || Loss: 1.1138 || timer: 0.0888 sec.
iter 126100 || Loss: 0.9658 || timer: 0.0897 sec.
iter 126110 || Loss: 0.9613 || timer: 0.0916 sec.
iter 126120 || Loss: 0.9834 || timer: 0.0923 sec.
iter 126130 || Loss: 1.0965 || timer: 0.0917 sec.
iter 126140 || Loss: 1.5721 || timer: 0.0896 sec.
iter 126150 || Loss: 1.3288 || timer: 0.0897 sec.
iter 126160 || Loss: 1.2347 || timer: 0.0907 sec.
iter 126170 || Loss: 1.1167 || timer: 0.0984 sec.
iter 126180 || Loss: 1.1889 || timer: 0.0949 sec.
iter 126190 || Loss: 0.9482 || timer: 0.0945 sec.
iter 126200 || Loss: 0.9732 || timer: 0.0924 sec.
iter 126210 || Loss: 1.0024 || timer: 0.0902 sec.
iter 126220 || Loss: 1.1001 || timer: 0.1089 sec.
iter 126230 || Loss: 1.3016 || timer: 0.0841 sec.
iter 126240 || Loss: 0.9819 || timer: 0.0975 sec.
iter 126250 || Loss: 1.2534 || timer: 0.0930 sec.
iter 126260 || Loss: 1.0906 || timer: 0.0922 sec.
iter 126270 || Loss: 0.7878 || timer: 0.1024 sec.
iter 126280 || Loss: 1.0333 || timer: 0.0208 sec.
iter 126290 || Loss: 1.2664 || timer: 0.0914 sec.
iter 126300 || Loss: 1.3239 || timer: 0.0924 sec.
iter 126310 || Loss: 0.5940 || timer: 0.0889 sec.
iter 126320 || Loss: 0.9927 || timer: 0.0910 sec.
iter 126330 || Loss: 0.8671 || timer: 0.0885 sec.
iter 126340 || Loss: 1.1367 || timer: 0.0829 sec.
iter 126350 || Loss: 0.9211 || timer: 0.0839 sec.
iter 126360 || Loss: 0.9064 || timer: 0.0908 sec.
iter 126370 || Loss: 0.8906 || timer: 0.0973 sec.
iter 126380 || Loss: 1.1132 || timer: 0.1073 sec.
iter 126390 || Loss: 1.0189 || timer: 0.0885 sec.
iter 126400 || Loss: 1.0582 || timer: 0.0922 sec.
iter 126410 || Loss: 0.9298 || timer: 0.0979 sec.
iter 126420 || Loss: 1.0563 || timer: 0.0922 sec.
iter 126430 || Loss: 1.0672 || timer: 0.0842 sec.
iter 126440 || Loss: 1.7986 || timer: 0.0935 sec.
iter 126450 || Loss: 0.9387 || timer: 0.0934 sec.
iter 126460 || Loss: 1.0353 || timer: 0.0891 sec.
iter 126470 || Loss: 1.1374 || timer: 0.0880 sec.
iter 126480 || Loss: 1.0869 || timer: 0.1292 sec.
iter 126490 || Loss: 1.2508 || timer: 0.0916 sec.
iter 126500 || Loss: 1.0426 || timer: 0.0838 sec.
iter 126510 || Loss: 1.0621 || timer: 0.0926 sec.
iter 126520 || Loss: 1.0737 || timer: 0.1070 sec.
iter 126530 || Loss: 1.1345 || timer: 0.0838 sec.
iter 126540 || Loss: 1.0633 || timer: 0.0858 sec.
iter 126550 || Loss: 1.1694 || timer: 0.1080 sec.
iter 126560 || Loss: 0.7307 || timer: 0.0896 sec.
iter 126570 || Loss: 1.1169 || timer: 0.0840 sec.
iter 126580 || Loss: 0.7812 || timer: 0.0914 sec.
iter 126590 || Loss: 0.8638 || timer: 0.0877 sec.
iter 126600 || Loss: 0.9546 || timer: 0.0907 sec.
iter 126610 || Loss: 1.1482 || timer: 0.0217 sec.
iter 126620 || Loss: 1.2245 || timer: 0.0901 sec.
iter 126630 || Loss: 0.8533 || timer: 0.0940 sec.
iter 126640 || Loss: 1.1996 || timer: 0.0856 sec.
iter 126650 || Loss: 1.0194 || timer: 0.0835 sec.
iter 126660 || Loss: 0.9314 || timer: 0.0931 sec.
iter 126670 || Loss: 0.9954 || timer: 0.1180 sec.
iter 126680 || Loss: 1.0139 || timer: 0.0974 sec.
iter 126690 || Loss: 2.0209 || timer: 0.0765 sec.
iter 126700 || Loss: 1.1980 || timer: 0.0837 sec.
iter 126710 || Loss: 1.0444 || timer: 0.1001 sec.
iter 126720 || Loss: 0.8877 || timer: 0.0833 sec.
iter 126730 || Loss: 1.3176 || timer: 0.0923 sec.
iter 126740 || Loss: 1.3710 || timer: 0.0895 sec.
iter 126750 || Loss: 1.3727 || timer: 0.0898 sec.
iter 126760 || Loss: 1.1673 || timer: 0.0832 sec.
iter 126770 || Loss: 1.0177 || timer: 0.1076 sec.
iter 126780 || Loss: 0.9691 || timer: 0.0981 sec.
iter 126790 || Loss: 0.8708 || timer: 0.1040 sec.
iter 126800 || Loss: 0.7504 || timer: 0.0874 sec.
iter 126810 || Loss: 1.2404 || timer: 0.0928 sec.
iter 126820 || Loss: 1.2408 || timer: 0.1108 sec.
iter 126830 || Loss: 0.9430 || timer: 0.0890 sec.
iter 126840 || Loss: 1.2755 || timer: 0.0836 sec.
iter 126850 || Loss: 1.2633 || timer: 0.0895 sec.
iter 126860 || Loss: 1.1689 || timer: 0.0895 sec.
iter 126870 || Loss: 1.0064 || timer: 0.0931 sec.
iter 126880 || Loss: 0.7751 || timer: 0.0842 sec.
iter 126890 || Loss: 0.7136 || timer: 0.1242 sec.
iter 126900 || Loss: 0.8867 || timer: 0.1050 sec.
iter 126910 || Loss: 0.9928 || timer: 0.0893 sec.
iter 126920 || Loss: 1.1861 || timer: 0.0962 sec.
iter 126930 || Loss: 0.8907 || timer: 0.0934 sec.
iter 126940 || Loss: 0.9157 || timer: 0.0242 sec.
iter 126950 || Loss: 2.8536 || timer: 0.0960 sec.
iter 126960 || Loss: 1.5017 || timer: 0.1193 sec.
iter 126970 || Loss: 1.0326 || timer: 0.0868 sec.
iter 126980 || Loss: 1.8068 || timer: 0.0894 sec.
iter 126990 || Loss: 1.3233 || timer: 0.0908 sec.
iter 127000 || Loss: 1.1684 || timer: 0.0998 sec.
iter 127010 || Loss: 0.9767 || timer: 0.0842 sec.
iter 127020 || Loss: 1.0257 || timer: 0.0904 sec.
iter 127030 || Loss: 1.0427 || timer: 0.1054 sec.
iter 127040 || Loss: 1.3403 || timer: 0.1094 sec.
iter 127050 || Loss: 0.9504 || timer: 0.0834 sec.
iter 127060 || Loss: 0.9145 || timer: 0.0941 sec.
iter 127070 || Loss: 1.1841 || timer: 0.0842 sec.
iter 127080 || Loss: 0.7890 || timer: 0.0912 sec.
iter 127090 || Loss: 0.8961 || timer: 0.0892 sec.
iter 127100 || Loss: 1.0142 || timer: 0.0838 sec.
iter 127110 || Loss: 1.1148 || timer: 0.0915 sec.
iter 127120 || Loss: 0.8514 || timer: 0.0906 sec.
iter 127130 || Loss: 0.6606 || timer: 0.0879 sec.
iter 127140 || Loss: 1.1761 || timer: 0.0911 sec.
iter 127150 || Loss: 0.9723 || timer: 0.0909 sec.
iter 127160 || Loss: 0.8118 || timer: 0.0838 sec.
iter 127170 || Loss: 0.9575 || timer: 0.0844 sec.
iter 127180 || Loss: 1.1077 || timer: 0.0932 sec.
iter 127190 || Loss: 1.0468 || timer: 0.0922 sec.
iter 127200 || Loss: 1.1417 || timer: 0.0859 sec.
iter 127210 || Loss: 1.1713 || timer: 0.0843 sec.
iter 127220 || Loss: 1.0961 || timer: 0.0918 sec.
iter 127230 || Loss: 1.1739 || timer: 0.0987 sec.
iter 127240 || Loss: 0.7292 || timer: 0.0860 sec.
iter 127250 || Loss: 0.9158 || timer: 0.0977 sec.
iter 127260 || Loss: 0.7811 || timer: 0.0839 sec.
iter 127270 || Loss: 0.8932 || timer: 0.0217 sec.
iter 127280 || Loss: 0.3092 || timer: 0.0842 sec.
iter 127290 || Loss: 0.9869 || timer: 0.0949 sec.
iter 127300 || Loss: 0.8671 || timer: 0.1059 sec.
iter 127310 || Loss: 0.8340 || timer: 0.0914 sec.
iter 127320 || Loss: 1.3905 || timer: 0.0902 sec.
iter 127330 || Loss: 0.9021 || timer: 0.0913 sec.
iter 127340 || Loss: 0.7913 || timer: 0.0930 sec.
iter 127350 || Loss: 0.9901 || timer: 0.0848 sec.
iter 127360 || Loss: 0.9127 || timer: 0.0840 sec.
iter 127370 || Loss: 1.1789 || timer: 0.0881 sec.
iter 127380 || Loss: 1.2052 || timer: 0.1069 sec.
iter 127390 || Loss: 0.9692 || timer: 0.1008 sec.
iter 127400 || Loss: 1.0304 || timer: 0.0821 sec.
iter 127410 || Loss: 1.2339 || timer: 0.1041 sec.
iter 127420 || Loss: 0.8724 || timer: 0.0991 sec.
iter 127430 || Loss: 0.9124 || timer: 0.0927 sec.
iter 127440 || Loss: 1.0887 || timer: 0.0964 sec.
iter 127450 || Loss: 0.7139 || timer: 0.0934 sec.
iter 127460 || Loss: 1.0515 || timer: 0.0899 sec.
iter 127470 || Loss: 0.7444 || timer: 0.1012 sec.
iter 127480 || Loss: 0.8461 || timer: 0.0912 sec.
iter 127490 || Loss: 0.8641 || timer: 0.0969 sec.
iter 127500 || Loss: 0.9736 || timer: 0.0895 sec.
iter 127510 || Loss: 1.1700 || timer: 0.1068 sec.
iter 127520 || Loss: 1.7968 || timer: 0.0898 sec.
iter 127530 || Loss: 1.4916 || timer: 0.0969 sec.
iter 127540 || Loss: 1.0003 || timer: 0.1071 sec.
iter 127550 || Loss: 1.2680 || timer: 0.0840 sec.
iter 127560 || Loss: 1.1025 || timer: 0.0843 sec.
iter 127570 || Loss: 0.9032 || timer: 0.0917 sec.
iter 127580 || Loss: 1.0022 || timer: 0.1033 sec.
iter 127590 || Loss: 0.9602 || timer: 0.0901 sec.
iter 127600 || Loss: 0.8518 || timer: 0.0211 sec.
iter 127610 || Loss: 1.1195 || timer: 0.0903 sec.
iter 127620 || Loss: 0.9287 || timer: 0.0855 sec.
iter 127630 || Loss: 1.3399 || timer: 0.0926 sec.
iter 127640 || Loss: 0.9528 || timer: 0.0909 sec.
iter 127650 || Loss: 0.9984 || timer: 0.0965 sec.
iter 127660 || Loss: 0.9697 || timer: 0.0841 sec.
iter 127670 || Loss: 1.1291 || timer: 0.1036 sec.
iter 127680 || Loss: 0.9999 || timer: 0.1052 sec.
iter 127690 || Loss: 1.0548 || timer: 0.0912 sec.
iter 127700 || Loss: 1.3049 || timer: 0.0991 sec.
iter 127710 || Loss: 1.2351 || timer: 0.0913 sec.
iter 127720 || Loss: 0.8908 || timer: 0.0957 sec.
iter 127730 || Loss: 1.0802 || timer: 0.0896 sec.
iter 127740 || Loss: 0.8417 || timer: 0.1087 sec.
iter 127750 || Loss: 0.8493 || timer: 0.0905 sec.
iter 127760 || Loss: 0.7868 || timer: 0.0818 sec.
iter 127770 || Loss: 0.9692 || timer: 0.1128 sec.
iter 127780 || Loss: 1.0586 || timer: 0.0920 sec.
iter 127790 || Loss: 0.9477 || timer: 0.0859 sec.
iter 127800 || Loss: 0.9603 || timer: 0.0878 sec.
iter 127810 || Loss: 0.7959 || timer: 0.0832 sec.
iter 127820 || Loss: 0.9829 || timer: 0.0849 sec.
iter 127830 || Loss: 1.2453 || timer: 0.0821 sec.
iter 127840 || Loss: 0.8262 || timer: 0.0851 sec.
iter 127850 || Loss: 0.9744 || timer: 0.0851 sec.
iter 127860 || Loss: 1.2052 || timer: 0.0773 sec.
iter 127870 || Loss: 0.7325 || timer: 0.0841 sec.
iter 127880 || Loss: 1.1384 || timer: 0.0816 sec.
iter 127890 || Loss: 1.1815 || timer: 0.0911 sec.
iter 127900 || Loss: 1.0163 || timer: 0.0855 sec.
iter 127910 || Loss: 1.2900 || timer: 0.0956 sec.
iter 127920 || Loss: 1.1958 || timer: 0.0844 sec.
iter 127930 || Loss: 1.2502 || timer: 0.0208 sec.
iter 127940 || Loss: 2.2181 || timer: 0.1072 sec.
iter 127950 || Loss: 0.8871 || timer: 0.0863 sec.
iter 127960 || Loss: 0.9202 || timer: 0.0998 sec.
iter 127970 || Loss: 1.2136 || timer: 0.0846 sec.
iter 127980 || Loss: 0.8462 || timer: 0.1027 sec.
iter 127990 || Loss: 0.7500 || timer: 0.0810 sec.
iter 128000 || Loss: 0.7264 || timer: 0.1060 sec.
iter 128010 || Loss: 1.3281 || timer: 0.0910 sec.
iter 128020 || Loss: 0.8554 || timer: 0.0901 sec.
iter 128030 || Loss: 1.2785 || timer: 0.0960 sec.
iter 128040 || Loss: 0.9646 || timer: 0.0912 sec.
iter 128050 || Loss: 1.2197 || timer: 0.0916 sec.
iter 128060 || Loss: 0.8864 || timer: 0.0894 sec.
iter 128070 || Loss: 1.0636 || timer: 0.0827 sec.
iter 128080 || Loss: 0.8789 || timer: 0.0918 sec.
iter 128090 || Loss: 0.9323 || timer: 0.0907 sec.
iter 128100 || Loss: 1.1474 || timer: 0.0917 sec.
iter 128110 || Loss: 0.9791 || timer: 0.0922 sec.
iter 128120 || Loss: 1.0177 || timer: 0.1090 sec.
iter 128130 || Loss: 0.8940 || timer: 0.0966 sec.
iter 128140 || Loss: 0.7765 || timer: 0.0914 sec.
iter 128150 || Loss: 0.8678 || timer: 0.0843 sec.
iter 128160 || Loss: 1.0275 || timer: 0.0924 sec.
iter 128170 || Loss: 1.3219 || timer: 0.0907 sec.
iter 128180 || Loss: 1.0387 || timer: 0.0889 sec.
iter 128190 || Loss: 1.2599 || timer: 0.0860 sec.
iter 128200 || Loss: 0.9369 || timer: 0.0835 sec.
iter 128210 || Loss: 0.8226 || timer: 0.0829 sec.
iter 128220 || Loss: 1.0448 || timer: 0.0881 sec.
iter 128230 || Loss: 0.8838 || timer: 0.0879 sec.
iter 128240 || Loss: 1.3004 || timer: 0.0901 sec.
iter 128250 || Loss: 0.8073 || timer: 0.0865 sec.
iter 128260 || Loss: 0.7596 || timer: 0.0210 sec.
iter 128270 || Loss: 0.6407 || timer: 0.0870 sec.
iter 128280 || Loss: 1.2396 || timer: 0.0828 sec.
iter 128290 || Loss: 0.9959 || timer: 0.0893 sec.
iter 128300 || Loss: 0.9524 || timer: 0.0820 sec.
iter 128310 || Loss: 1.0640 || timer: 0.0903 sec.
iter 128320 || Loss: 1.1458 || timer: 0.0881 sec.
iter 128330 || Loss: 0.8839 || timer: 0.0919 sec.
iter 128340 || Loss: 0.9392 || timer: 0.0884 sec.
iter 128350 || Loss: 1.2107 || timer: 0.1020 sec.
iter 128360 || Loss: 0.8897 || timer: 0.0924 sec.
iter 128370 || Loss: 0.9271 || timer: 0.0883 sec.
iter 128380 || Loss: 1.2238 || timer: 0.0873 sec.
iter 128390 || Loss: 0.8441 || timer: 0.0883 sec.
iter 128400 || Loss: 0.8706 || timer: 0.1277 sec.
iter 128410 || Loss: 0.8599 || timer: 0.0836 sec.
iter 128420 || Loss: 1.0936 || timer: 0.0830 sec.
iter 128430 || Loss: 1.0166 || timer: 0.0891 sec.
iter 128440 || Loss: 1.0098 || timer: 0.1076 sec.
iter 128450 || Loss: 0.8775 || timer: 0.0827 sec.
iter 128460 || Loss: 1.1125 || timer: 0.0894 sec.
iter 128470 || Loss: 1.3334 || timer: 0.0899 sec.
iter 128480 || Loss: 1.3244 || timer: 0.1003 sec.
iter 128490 || Loss: 1.2459 || timer: 0.0811 sec.
iter 128500 || Loss: 1.2061 || timer: 0.0875 sec.
iter 128510 || Loss: 0.7456 || timer: 0.0916 sec.
iter 128520 || Loss: 0.6908 || timer: 0.1032 sec.
iter 128530 || Loss: 0.8129 || timer: 0.0845 sec.
iter 128540 || Loss: 0.8620 || timer: 0.0913 sec.
iter 128550 || Loss: 1.2098 || timer: 0.0892 sec.
iter 128560 || Loss: 1.0434 || timer: 0.0999 sec.
iter 128570 || Loss: 1.3385 || timer: 0.0822 sec.
iter 128580 || Loss: 0.6998 || timer: 0.0823 sec.
iter 128590 || Loss: 0.9871 || timer: 0.0231 sec.
iter 128600 || Loss: 1.4078 || timer: 0.0960 sec.
iter 128610 || Loss: 0.9820 || timer: 0.0815 sec.
iter 128620 || Loss: 0.7958 || timer: 0.0813 sec.
iter 128630 || Loss: 0.8687 || timer: 0.0882 sec.
iter 128640 || Loss: 1.1834 || timer: 0.0840 sec.
iter 128650 || Loss: 0.8279 || timer: 0.0815 sec.
iter 128660 || Loss: 1.0230 || timer: 0.1105 sec.
iter 128670 || Loss: 1.0224 || timer: 0.0916 sec.
iter 128680 || Loss: 1.0775 || timer: 0.0884 sec.
iter 128690 || Loss: 1.0767 || timer: 0.1146 sec.
iter 128700 || Loss: 1.3261 || timer: 0.1048 sec.
iter 128710 || Loss: 1.0925 || timer: 0.1052 sec.
iter 128720 || Loss: 0.9831 || timer: 0.0900 sec.
iter 128730 || Loss: 0.7598 || timer: 0.0870 sec.
iter 128740 || Loss: 1.0475 || timer: 0.0829 sec.
iter 128750 || Loss: 1.1151 || timer: 0.1068 sec.
iter 128760 || Loss: 0.9132 || timer: 0.0905 sec.
iter 128770 || Loss: 1.1803 || timer: 0.1017 sec.
iter 128780 || Loss: 0.7308 || timer: 0.0904 sec.
iter 128790 || Loss: 1.0756 || timer: 0.0900 sec.
iter 128800 || Loss: 1.1074 || timer: 0.0991 sec.
iter 128810 || Loss: 0.9900 || timer: 0.1008 sec.
iter 128820 || Loss: 0.9193 || timer: 0.0814 sec.
iter 128830 || Loss: 1.0968 || timer: 0.0822 sec.
iter 128840 || Loss: 0.9202 || timer: 0.0901 sec.
iter 128850 || Loss: 0.8118 || timer: 0.0903 sec.
iter 128860 || Loss: 0.8768 || timer: 0.0893 sec.
iter 128870 || Loss: 1.4146 || timer: 0.0865 sec.
iter 128880 || Loss: 0.6818 || timer: 0.0880 sec.
iter 128890 || Loss: 0.7270 || timer: 0.0882 sec.
iter 128900 || Loss: 0.9956 || timer: 0.0893 sec.
iter 128910 || Loss: 1.1227 || timer: 0.0880 sec.
iter 128920 || Loss: 1.0299 || timer: 0.0226 sec.
iter 128930 || Loss: 1.7248 || timer: 0.0818 sec.
iter 128940 || Loss: 0.9410 || timer: 0.1116 sec.
iter 128950 || Loss: 1.1136 || timer: 0.0829 sec.
iter 128960 || Loss: 0.6520 || timer: 0.0844 sec.
iter 128970 || Loss: 0.7664 || timer: 0.0904 sec.
iter 128980 || Loss: 1.4126 || timer: 0.0830 sec.
iter 128990 || Loss: 0.9415 || timer: 0.0766 sec.
iter 129000 || Loss: 0.8124 || timer: 0.0874 sec.
iter 129010 || Loss: 0.8919 || timer: 0.1014 sec.
iter 129020 || Loss: 0.9488 || timer: 0.0950 sec.
iter 129030 || Loss: 1.0800 || timer: 0.0918 sec.
iter 129040 || Loss: 1.0630 || timer: 0.1054 sec.
iter 129050 || Loss: 0.9776 || timer: 0.0959 sec.
iter 129060 || Loss: 0.8102 || timer: 0.0972 sec.
iter 129070 || Loss: 0.8073 || timer: 0.0984 sec.
iter 129080 || Loss: 1.0802 || timer: 0.0877 sec.
iter 129090 || Loss: 0.8332 || timer: 0.0901 sec.
iter 129100 || Loss: 0.8449 || timer: 0.1028 sec.
iter 129110 || Loss: 0.9517 || timer: 0.1086 sec.
iter 129120 || Loss: 0.8990 || timer: 0.0822 sec.
iter 129130 || Loss: 1.1850 || timer: 0.0835 sec.
iter 129140 || Loss: 1.1964 || timer: 0.0838 sec.
iter 129150 || Loss: 1.4063 || timer: 0.0918 sec.
iter 129160 || Loss: 1.0953 || timer: 0.0829 sec.
iter 129170 || Loss: 0.7470 || timer: 0.0908 sec.
iter 129180 || Loss: 0.7266 || timer: 0.0872 sec.
iter 129190 || Loss: 0.7433 || timer: 0.0854 sec.
iter 129200 || Loss: 0.9249 || timer: 0.0985 sec.
iter 129210 || Loss: 1.2353 || timer: 0.0897 sec.
iter 129220 || Loss: 1.1229 || timer: 0.0875 sec.
iter 129230 || Loss: 0.8908 || timer: 0.0805 sec.
iter 129240 || Loss: 0.7785 || timer: 0.0921 sec.
iter 129250 || Loss: 0.9392 || timer: 0.0234 sec.
iter 129260 || Loss: 4.9352 || timer: 0.0915 sec.
iter 129270 || Loss: 1.6755 || timer: 0.0885 sec.
iter 129280 || Loss: 1.7267 || timer: 0.0822 sec.
iter 129290 || Loss: 1.3745 || timer: 0.1094 sec.
iter 129300 || Loss: 0.8637 || timer: 0.0916 sec.
iter 129310 || Loss: 0.8400 || timer: 0.0913 sec.
iter 129320 || Loss: 1.1493 || timer: 0.0892 sec.
iter 129330 || Loss: 0.7346 || timer: 0.0816 sec.
iter 129340 || Loss: 0.8897 || timer: 0.0821 sec.
iter 129350 || Loss: 0.6704 || timer: 0.1069 sec.
iter 129360 || Loss: 0.8361 || timer: 0.0848 sec.
iter 129370 || Loss: 1.0358 || timer: 0.0900 sec.
iter 129380 || Loss: 1.0322 || timer: 0.0824 sec.
iter 129390 || Loss: 0.8374 || timer: 0.0963 sec.
iter 129400 || Loss: 0.8333 || timer: 0.1018 sec.
iter 129410 || Loss: 1.4381 || timer: 0.1309 sec.
iter 129420 || Loss: 1.0490 || timer: 0.1108 sec.
iter 129430 || Loss: 1.5217 || timer: 0.0992 sec.
iter 129440 || Loss: 1.0090 || timer: 0.0896 sec.
iter 129450 || Loss: 1.1894 || timer: 0.0847 sec.
iter 129460 || Loss: 0.9454 || timer: 0.0811 sec.
iter 129470 || Loss: 1.2695 || timer: 0.1155 sec.
iter 129480 || Loss: 1.1096 || timer: 0.0908 sec.
iter 129490 || Loss: 0.7725 || timer: 0.0893 sec.
iter 129500 || Loss: 0.7020 || timer: 0.0883 sec.
iter 129510 || Loss: 1.0174 || timer: 0.0907 sec.
iter 129520 || Loss: 1.1571 || timer: 0.0896 sec.
iter 129530 || Loss: 0.9393 || timer: 0.0938 sec.
iter 129540 || Loss: 1.0205 || timer: 0.1090 sec.
iter 129550 || Loss: 1.0289 || timer: 0.0898 sec.
iter 129560 || Loss: 0.9218 || timer: 0.0863 sec.
iter 129570 || Loss: 1.0715 || timer: 0.1082 sec.
iter 129580 || Loss: 1.0558 || timer: 0.0170 sec.
iter 129590 || Loss: 0.6148 || timer: 0.0927 sec.
iter 129600 || Loss: 1.1491 || timer: 0.0862 sec.
iter 129610 || Loss: 0.8900 || timer: 0.0835 sec.
iter 129620 || Loss: 1.0226 || timer: 0.0811 sec.
iter 129630 || Loss: 0.8628 || timer: 0.0884 sec.
iter 129640 || Loss: 1.1698 || timer: 0.0870 sec.
iter 129650 || Loss: 1.5376 || timer: 0.0893 sec.
iter 129660 || Loss: 0.9884 || timer: 0.0837 sec.
iter 129670 || Loss: 1.2433 || timer: 0.0808 sec.
iter 129680 || Loss: 1.2709 || timer: 0.0960 sec.
iter 129690 || Loss: 0.9749 || timer: 0.0860 sec.
iter 129700 || Loss: 0.9051 || timer: 0.0847 sec.
iter 129710 || Loss: 0.9137 || timer: 0.0866 sec.
iter 129720 || Loss: 0.7980 || timer: 0.0897 sec.
iter 129730 || Loss: 1.1364 || timer: 0.0815 sec.
iter 129740 || Loss: 0.9624 || timer: 0.0925 sec.
iter 129750 || Loss: 1.0877 || timer: 0.0895 sec.
iter 129760 || Loss: 0.8703 || timer: 0.0884 sec.
iter 129770 || Loss: 0.8723 || timer: 0.0886 sec.
iter 129780 || Loss: 1.2034 || timer: 0.1156 sec.
iter 129790 || Loss: 0.7209 || timer: 0.0911 sec.
iter 129800 || Loss: 1.2975 || timer: 0.1012 sec.
iter 129810 || Loss: 0.9816 || timer: 0.0810 sec.
iter 129820 || Loss: 0.7716 || timer: 0.0912 sec.
iter 129830 || Loss: 0.9417 || timer: 0.0807 sec.
iter 129840 || Loss: 0.8655 || timer: 0.0964 sec.
iter 129850 || Loss: 1.2237 || timer: 0.0818 sec.
iter 129860 || Loss: 0.7675 || timer: 0.0930 sec.
iter 129870 || Loss: 0.6885 || timer: 0.0917 sec.
iter 129880 || Loss: 0.7708 || timer: 0.0872 sec.
iter 129890 || Loss: 0.7849 || timer: 0.1000 sec.
iter 129900 || Loss: 0.8485 || timer: 0.0906 sec.
iter 129910 || Loss: 0.9756 || timer: 0.0172 sec.
iter 129920 || Loss: 3.2147 || timer: 0.0825 sec.
iter 129930 || Loss: 1.4247 || timer: 0.0863 sec.
iter 129940 || Loss: 1.1521 || timer: 0.0809 sec.
iter 129950 || Loss: 1.0644 || timer: 0.0919 sec.
iter 129960 || Loss: 1.1337 || timer: 0.0889 sec.
iter 129970 || Loss: 0.9857 || timer: 0.1112 sec.
iter 129980 || Loss: 1.0802 || timer: 0.0918 sec.
iter 129990 || Loss: 1.0821 || timer: 0.1069 sec.
iter 130000 || Loss: 1.3380 || Saving state, iter: 130000
timer: 0.0953 sec.
iter 130010 || Loss: 0.8074 || timer: 0.1023 sec.
iter 130020 || Loss: 0.9880 || timer: 0.1073 sec.
iter 130030 || Loss: 1.0146 || timer: 0.0836 sec.
iter 130040 || Loss: 1.0982 || timer: 0.0904 sec.
iter 130050 || Loss: 0.7868 || timer: 0.0823 sec.
iter 130060 || Loss: 1.3602 || timer: 0.1165 sec.
iter 130070 || Loss: 0.9258 || timer: 0.0878 sec.
iter 130080 || Loss: 1.0251 || timer: 0.0945 sec.
iter 130090 || Loss: 0.7528 || timer: 0.0892 sec.
iter 130100 || Loss: 0.9218 || timer: 0.0766 sec.
iter 130110 || Loss: 1.0164 || timer: 0.0965 sec.
iter 130120 || Loss: 0.8444 || timer: 0.1077 sec.
iter 130130 || Loss: 1.0950 || timer: 0.0808 sec.
iter 130140 || Loss: 1.0330 || timer: 0.0836 sec.
iter 130150 || Loss: 0.9944 || timer: 0.0804 sec.
iter 130160 || Loss: 0.6045 || timer: 0.0900 sec.
iter 130170 || Loss: 1.0038 || timer: 0.0930 sec.
iter 130180 || Loss: 0.8158 || timer: 0.0863 sec.
iter 130190 || Loss: 1.4264 || timer: 0.0811 sec.
iter 130200 || Loss: 0.7065 || timer: 0.1047 sec.
iter 130210 || Loss: 1.0139 || timer: 0.0840 sec.
iter 130220 || Loss: 0.9668 || timer: 0.1124 sec.
iter 130230 || Loss: 1.0328 || timer: 0.0814 sec.
iter 130240 || Loss: 0.7203 || timer: 0.0273 sec.
iter 130250 || Loss: 1.4850 || timer: 0.0808 sec.
iter 130260 || Loss: 0.9136 || timer: 0.0818 sec.
iter 130270 || Loss: 1.0539 || timer: 0.0877 sec.
iter 130280 || Loss: 0.9323 || timer: 0.0902 sec.
iter 130290 || Loss: 1.0565 || timer: 0.1044 sec.
iter 130300 || Loss: 1.1088 || timer: 0.0963 sec.
iter 130310 || Loss: 0.8430 || timer: 0.1102 sec.
iter 130320 || Loss: 0.6135 || timer: 0.0831 sec.
iter 130330 || Loss: 0.8675 || timer: 0.0824 sec.
iter 130340 || Loss: 0.8840 || timer: 0.1071 sec.
iter 130350 || Loss: 0.9255 || timer: 0.0918 sec.
iter 130360 || Loss: 0.7403 || timer: 0.0814 sec.
iter 130370 || Loss: 1.2261 || timer: 0.0813 sec.
iter 130380 || Loss: 0.9183 || timer: 0.0840 sec.
iter 130390 || Loss: 0.6113 || timer: 0.0919 sec.
iter 130400 || Loss: 0.9258 || timer: 0.0809 sec.
iter 130410 || Loss: 1.1662 || timer: 0.0881 sec.
iter 130420 || Loss: 0.9050 || timer: 0.0913 sec.
iter 130430 || Loss: 1.0707 || timer: 0.0967 sec.
iter 130440 || Loss: 0.9132 || timer: 0.0879 sec.
iter 130450 || Loss: 1.0197 || timer: 0.0918 sec.
iter 130460 || Loss: 0.9533 || timer: 0.0899 sec.
iter 130470 || Loss: 0.8283 || timer: 0.0837 sec.
iter 130480 || Loss: 1.0409 || timer: 0.0823 sec.
iter 130490 || Loss: 0.8424 || timer: 0.0906 sec.
iter 130500 || Loss: 0.7772 || timer: 0.0783 sec.
iter 130510 || Loss: 0.8371 || timer: 0.0896 sec.
iter 130520 || Loss: 1.1031 || timer: 0.0888 sec.
iter 130530 || Loss: 0.8351 || timer: 0.0922 sec.
iter 130540 || Loss: 1.1447 || timer: 0.0887 sec.
iter 130550 || Loss: 1.0609 || timer: 0.0875 sec.
iter 130560 || Loss: 1.3384 || timer: 0.0835 sec.
iter 130570 || Loss: 1.3178 || timer: 0.0223 sec.
iter 130580 || Loss: 1.0485 || timer: 0.0941 sec.
iter 130590 || Loss: 0.9434 || timer: 0.0919 sec.
iter 130600 || Loss: 0.8320 || timer: 0.1152 sec.
iter 130610 || Loss: 0.9712 || timer: 0.0904 sec.
iter 130620 || Loss: 1.3609 || timer: 0.1020 sec.
iter 130630 || Loss: 1.1095 || timer: 0.0898 sec.
iter 130640 || Loss: 1.1850 || timer: 0.0894 sec.
iter 130650 || Loss: 1.3494 || timer: 0.0830 sec.
iter 130660 || Loss: 1.1507 || timer: 0.0858 sec.
iter 130670 || Loss: 0.9547 || timer: 0.0985 sec.
iter 130680 || Loss: 1.0980 || timer: 0.1043 sec.
iter 130690 || Loss: 1.3547 || timer: 0.0833 sec.
iter 130700 || Loss: 0.9100 || timer: 0.0730 sec.
iter 130710 || Loss: 0.9149 || timer: 0.0808 sec.
iter 130720 || Loss: 0.9480 || timer: 0.0825 sec.
iter 130730 || Loss: 0.6839 || timer: 0.1071 sec.
iter 130740 || Loss: 1.0766 || timer: 0.1031 sec.
iter 130750 || Loss: 0.8563 || timer: 0.0901 sec.
iter 130760 || Loss: 0.8671 || timer: 0.1043 sec.
iter 130770 || Loss: 1.1414 || timer: 0.0916 sec.
iter 130780 || Loss: 1.0959 || timer: 0.0948 sec.
iter 130790 || Loss: 1.0543 || timer: 0.0895 sec.
iter 130800 || Loss: 0.7638 || timer: 0.0810 sec.
iter 130810 || Loss: 0.9207 || timer: 0.0900 sec.
iter 130820 || Loss: 0.8421 || timer: 0.0900 sec.
iter 130830 || Loss: 1.0290 || timer: 0.0989 sec.
iter 130840 || Loss: 0.7696 || timer: 0.0915 sec.
iter 130850 || Loss: 1.4821 || timer: 0.0878 sec.
iter 130860 || Loss: 1.1005 || timer: 0.0839 sec.
iter 130870 || Loss: 0.7744 || timer: 0.0858 sec.
iter 130880 || Loss: 0.8575 || timer: 0.0808 sec.
iter 130890 || Loss: 1.1477 || timer: 0.0829 sec.
iter 130900 || Loss: 0.9694 || timer: 0.0258 sec.
iter 130910 || Loss: 0.9424 || timer: 0.0921 sec.
iter 130920 || Loss: 1.0035 || timer: 0.0916 sec.
iter 130930 || Loss: 1.1714 || timer: 0.0871 sec.
iter 130940 || Loss: 0.8357 || timer: 0.0891 sec.
iter 130950 || Loss: 0.7329 || timer: 0.1121 sec.
iter 130960 || Loss: 0.9688 || timer: 0.0925 sec.
iter 130970 || Loss: 1.0918 || timer: 0.0912 sec.
iter 130980 || Loss: 0.9411 || timer: 0.0856 sec.
iter 130990 || Loss: 0.9859 || timer: 0.0919 sec.
iter 131000 || Loss: 0.9722 || timer: 0.1012 sec.
iter 131010 || Loss: 1.3939 || timer: 0.0871 sec.
iter 131020 || Loss: 0.8226 || timer: 0.0992 sec.
iter 131030 || Loss: 0.8642 || timer: 0.0910 sec.
iter 131040 || Loss: 1.1807 || timer: 0.0901 sec.
iter 131050 || Loss: 0.9876 || timer: 0.1148 sec.
iter 131060 || Loss: 0.8254 || timer: 0.0834 sec.
iter 131070 || Loss: 0.9567 || timer: 0.0859 sec.
iter 131080 || Loss: 0.8078 || timer: 0.0828 sec.
iter 131090 || Loss: 0.9007 || timer: 0.0794 sec.
iter 131100 || Loss: 1.3558 || timer: 0.0860 sec.
iter 131110 || Loss: 0.7120 || timer: 0.0924 sec.
iter 131120 || Loss: 1.1178 || timer: 0.0842 sec.
iter 131130 || Loss: 0.9926 || timer: 0.0839 sec.
iter 131140 || Loss: 0.9673 || timer: 0.0936 sec.
iter 131150 || Loss: 0.9214 || timer: 0.0832 sec.
iter 131160 || Loss: 1.0374 || timer: 0.0941 sec.
iter 131170 || Loss: 0.8456 || timer: 0.0917 sec.
iter 131180 || Loss: 0.8998 || timer: 0.1071 sec.
iter 131190 || Loss: 0.7239 || timer: 0.1029 sec.
iter 131200 || Loss: 0.8741 || timer: 0.0833 sec.
iter 131210 || Loss: 1.1734 || timer: 0.0953 sec.
iter 131220 || Loss: 1.1229 || timer: 0.1221 sec.
iter 131230 || Loss: 0.9388 || timer: 0.0261 sec.
iter 131240 || Loss: 1.3759 || timer: 0.0886 sec.
iter 131250 || Loss: 0.8036 || timer: 0.0895 sec.
iter 131260 || Loss: 0.9150 || timer: 0.0925 sec.
iter 131270 || Loss: 0.9983 || timer: 0.0883 sec.
iter 131280 || Loss: 0.8470 || timer: 0.0900 sec.
iter 131290 || Loss: 0.7273 || timer: 0.0871 sec.
iter 131300 || Loss: 0.8998 || timer: 0.0920 sec.
iter 131310 || Loss: 1.1767 || timer: 0.0931 sec.
iter 131320 || Loss: 0.9353 || timer: 0.1133 sec.
iter 131330 || Loss: 1.0703 || timer: 0.1047 sec.
iter 131340 || Loss: 0.7939 || timer: 0.0917 sec.
iter 131350 || Loss: 1.2848 || timer: 0.0944 sec.
iter 131360 || Loss: 0.8193 || timer: 0.0885 sec.
iter 131370 || Loss: 1.1795 || timer: 0.0845 sec.
iter 131380 || Loss: 1.1883 || timer: 0.1087 sec.
iter 131390 || Loss: 0.6653 || timer: 0.0903 sec.
iter 131400 || Loss: 1.0786 || timer: 0.1027 sec.
iter 131410 || Loss: 0.8255 || timer: 0.0881 sec.
iter 131420 || Loss: 1.0010 || timer: 0.0896 sec.
iter 131430 || Loss: 0.8819 || timer: 0.1131 sec.
iter 131440 || Loss: 0.7481 || timer: 0.1033 sec.
iter 131450 || Loss: 0.7705 || timer: 0.0822 sec.
iter 131460 || Loss: 0.6378 || timer: 0.0814 sec.
iter 131470 || Loss: 0.9624 || timer: 0.0856 sec.
iter 131480 || Loss: 1.3163 || timer: 0.0838 sec.
iter 131490 || Loss: 0.9994 || timer: 0.0955 sec.
iter 131500 || Loss: 0.7893 || timer: 0.1033 sec.
iter 131510 || Loss: 1.5734 || timer: 0.0765 sec.
iter 131520 || Loss: 0.8413 || timer: 0.1038 sec.
iter 131530 || Loss: 1.2375 || timer: 0.0911 sec.
iter 131540 || Loss: 1.0584 || timer: 0.0892 sec.
iter 131550 || Loss: 1.1303 || timer: 0.0828 sec.
iter 131560 || Loss: 0.7369 || timer: 0.0224 sec.
iter 131570 || Loss: 1.3575 || timer: 0.0864 sec.
iter 131580 || Loss: 1.0063 || timer: 0.0824 sec.
iter 131590 || Loss: 0.7554 || timer: 0.0890 sec.
iter 131600 || Loss: 0.5438 || timer: 0.0882 sec.
iter 131610 || Loss: 1.0540 || timer: 0.0910 sec.
iter 131620 || Loss: 0.7036 || timer: 0.0824 sec.
iter 131630 || Loss: 1.2538 || timer: 0.0819 sec.
iter 131640 || Loss: 1.0076 || timer: 0.0866 sec.
iter 131650 || Loss: 1.0582 || timer: 0.0841 sec.
iter 131660 || Loss: 1.1623 || timer: 0.1166 sec.
iter 131670 || Loss: 0.9837 || timer: 0.0820 sec.
iter 131680 || Loss: 0.8919 || timer: 0.0842 sec.
iter 131690 || Loss: 0.8675 || timer: 0.0886 sec.
iter 131700 || Loss: 0.9921 || timer: 0.0958 sec.
iter 131710 || Loss: 0.7434 || timer: 0.0875 sec.
iter 131720 || Loss: 0.8623 || timer: 0.0818 sec.
iter 131730 || Loss: 0.8898 || timer: 0.0818 sec.
iter 131740 || Loss: 0.9877 || timer: 0.0830 sec.
iter 131750 || Loss: 0.9147 || timer: 0.0863 sec.
iter 131760 || Loss: 1.1820 || timer: 0.0824 sec.
iter 131770 || Loss: 1.3890 || timer: 0.0969 sec.
iter 131780 || Loss: 1.1609 || timer: 0.0919 sec.
iter 131790 || Loss: 0.9103 || timer: 0.0817 sec.
iter 131800 || Loss: 1.0845 || timer: 0.0735 sec.
iter 131810 || Loss: 1.1644 || timer: 0.0812 sec.
iter 131820 || Loss: 0.7861 || timer: 0.0887 sec.
iter 131830 || Loss: 0.8094 || timer: 0.0834 sec.
iter 131840 || Loss: 0.6705 || timer: 0.0832 sec.
iter 131850 || Loss: 0.8507 || timer: 0.0937 sec.
iter 131860 || Loss: 0.9287 || timer: 0.0820 sec.
iter 131870 || Loss: 0.9597 || timer: 0.0968 sec.
iter 131880 || Loss: 0.9986 || timer: 0.0878 sec.
iter 131890 || Loss: 0.8939 || timer: 0.0274 sec.
iter 131900 || Loss: 0.4983 || timer: 0.1068 sec.
iter 131910 || Loss: 1.2051 || timer: 0.1099 sec.
iter 131920 || Loss: 0.9939 || timer: 0.0892 sec.
iter 131930 || Loss: 1.0796 || timer: 0.0810 sec.
iter 131940 || Loss: 0.9088 || timer: 0.1035 sec.
iter 131950 || Loss: 0.8829 || timer: 0.0915 sec.
iter 131960 || Loss: 1.0036 || timer: 0.0826 sec.
iter 131970 || Loss: 1.2521 || timer: 0.0891 sec.
iter 131980 || Loss: 1.2193 || timer: 0.1118 sec.
iter 131990 || Loss: 0.8155 || timer: 0.1355 sec.
iter 132000 || Loss: 1.0044 || timer: 0.0913 sec.
iter 132010 || Loss: 0.8012 || timer: 0.0895 sec.
iter 132020 || Loss: 1.0781 || timer: 0.0885 sec.
iter 132030 || Loss: 1.2216 || timer: 0.0855 sec.
iter 132040 || Loss: 1.1203 || timer: 0.1023 sec.
iter 132050 || Loss: 0.9542 || timer: 0.1073 sec.
iter 132060 || Loss: 0.8736 || timer: 0.0860 sec.
iter 132070 || Loss: 1.2281 || timer: 0.1129 sec.
iter 132080 || Loss: 1.0916 || timer: 0.0900 sec.
iter 132090 || Loss: 0.7633 || timer: 0.0840 sec.
iter 132100 || Loss: 1.2939 || timer: 0.0881 sec.
iter 132110 || Loss: 1.3102 || timer: 0.0823 sec.
iter 132120 || Loss: 0.8077 || timer: 0.0818 sec.
iter 132130 || Loss: 0.7842 || timer: 0.0916 sec.
iter 132140 || Loss: 0.6619 || timer: 0.1120 sec.
iter 132150 || Loss: 1.2328 || timer: 0.0896 sec.
iter 132160 || Loss: 1.0444 || timer: 0.0917 sec.
iter 132170 || Loss: 0.8490 || timer: 0.0908 sec.
iter 132180 || Loss: 1.2725 || timer: 0.0891 sec.
iter 132190 || Loss: 0.9618 || timer: 0.0843 sec.
iter 132200 || Loss: 0.9462 || timer: 0.0883 sec.
iter 132210 || Loss: 0.9468 || timer: 0.1076 sec.
iter 132220 || Loss: 1.0039 || timer: 0.0247 sec.
iter 132230 || Loss: 0.8623 || timer: 0.0750 sec.
iter 132240 || Loss: 0.7956 || timer: 0.0771 sec.
iter 132250 || Loss: 1.1269 || timer: 0.0827 sec.
iter 132260 || Loss: 1.2150 || timer: 0.0892 sec.
iter 132270 || Loss: 1.1739 || timer: 0.0903 sec.
iter 132280 || Loss: 0.7728 || timer: 0.0911 sec.
iter 132290 || Loss: 1.0915 || timer: 0.1090 sec.
iter 132300 || Loss: 0.8147 || timer: 0.1050 sec.
iter 132310 || Loss: 0.9353 || timer: 0.0899 sec.
iter 132320 || Loss: 1.0511 || timer: 0.1218 sec.
iter 132330 || Loss: 0.9337 || timer: 0.0832 sec.
iter 132340 || Loss: 1.0010 || timer: 0.1153 sec.
iter 132350 || Loss: 0.7875 || timer: 0.0821 sec.
iter 132360 || Loss: 1.3180 || timer: 0.0852 sec.
iter 132370 || Loss: 0.7207 || timer: 0.0821 sec.
iter 132380 || Loss: 1.0849 || timer: 0.0826 sec.
iter 132390 || Loss: 1.0047 || timer: 0.0875 sec.
iter 132400 || Loss: 0.7911 || timer: 0.0839 sec.
iter 132410 || Loss: 0.9470 || timer: 0.0739 sec.
iter 132420 || Loss: 0.8955 || timer: 0.1049 sec.
iter 132430 || Loss: 1.0142 || timer: 0.0888 sec.
iter 132440 || Loss: 0.5555 || timer: 0.0778 sec.
iter 132450 || Loss: 1.1355 || timer: 0.0742 sec.
iter 132460 || Loss: 0.9719 || timer: 0.0768 sec.
iter 132470 || Loss: 1.0078 || timer: 0.0829 sec.
iter 132480 || Loss: 1.0607 || timer: 0.0824 sec.
iter 132490 || Loss: 0.9292 || timer: 0.0811 sec.
iter 132500 || Loss: 0.8635 || timer: 0.1044 sec.
iter 132510 || Loss: 1.0433 || timer: 0.0808 sec.
iter 132520 || Loss: 1.0732 || timer: 0.0819 sec.
iter 132530 || Loss: 0.9746 || timer: 0.0879 sec.
iter 132540 || Loss: 0.8025 || timer: 0.1025 sec.
iter 132550 || Loss: 1.0297 || timer: 0.0296 sec.
iter 132560 || Loss: 0.7317 || timer: 0.0890 sec.
iter 132570 || Loss: 0.5990 || timer: 0.0897 sec.
iter 132580 || Loss: 1.1106 || timer: 0.0883 sec.
iter 132590 || Loss: 0.9937 || timer: 0.1093 sec.
iter 132600 || Loss: 1.1280 || timer: 0.0987 sec.
iter 132610 || Loss: 1.0505 || timer: 0.0823 sec.
iter 132620 || Loss: 0.7617 || timer: 0.0821 sec.
iter 132630 || Loss: 0.9764 || timer: 0.0834 sec.
iter 132640 || Loss: 0.9567 || timer: 0.0908 sec.
iter 132650 || Loss: 1.0175 || timer: 0.0978 sec.
iter 132660 || Loss: 0.9956 || timer: 0.0938 sec.
iter 132670 || Loss: 0.9638 || timer: 0.0865 sec.
iter 132680 || Loss: 0.8501 || timer: 0.0731 sec.
iter 132690 || Loss: 0.8516 || timer: 0.0833 sec.
iter 132700 || Loss: 0.9652 || timer: 0.0866 sec.
iter 132710 || Loss: 1.2650 || timer: 0.0829 sec.
iter 132720 || Loss: 1.0680 || timer: 0.0789 sec.
iter 132730 || Loss: 0.9530 || timer: 0.0839 sec.
iter 132740 || Loss: 1.0170 || timer: 0.0864 sec.
iter 132750 || Loss: 0.6974 || timer: 0.0824 sec.
iter 132760 || Loss: 0.8425 || timer: 0.0906 sec.
iter 132770 || Loss: 0.9113 || timer: 0.0819 sec.
iter 132780 || Loss: 0.9479 || timer: 0.0867 sec.
iter 132790 || Loss: 0.8160 || timer: 0.0838 sec.
iter 132800 || Loss: 1.3065 || timer: 0.0886 sec.
iter 132810 || Loss: 0.8771 || timer: 0.0884 sec.
iter 132820 || Loss: 1.1031 || timer: 0.0816 sec.
iter 132830 || Loss: 1.3665 || timer: 0.0776 sec.
iter 132840 || Loss: 1.4562 || timer: 0.0831 sec.
iter 132850 || Loss: 0.8813 || timer: 0.0916 sec.
iter 132860 || Loss: 0.7877 || timer: 0.0818 sec.
iter 132870 || Loss: 1.1288 || timer: 0.0872 sec.
iter 132880 || Loss: 0.8903 || timer: 0.0191 sec.
iter 132890 || Loss: 0.4985 || timer: 0.0966 sec.
iter 132900 || Loss: 0.7703 || timer: 0.0879 sec.
iter 132910 || Loss: 1.0797 || timer: 0.0936 sec.
iter 132920 || Loss: 0.7998 || timer: 0.0812 sec.
iter 132930 || Loss: 0.7190 || timer: 0.0812 sec.
iter 132940 || Loss: 0.8113 || timer: 0.0887 sec.
iter 132950 || Loss: 0.9738 || timer: 0.0886 sec.
iter 132960 || Loss: 0.7033 || timer: 0.0997 sec.
iter 132970 || Loss: 1.0949 || timer: 0.0822 sec.
iter 132980 || Loss: 0.8841 || timer: 0.0954 sec.
iter 132990 || Loss: 0.9446 || timer: 0.1203 sec.
iter 133000 || Loss: 1.0241 || timer: 0.0959 sec.
iter 133010 || Loss: 0.6201 || timer: 0.0837 sec.
iter 133020 || Loss: 0.9179 || timer: 0.1086 sec.
iter 133030 || Loss: 0.8384 || timer: 0.0907 sec.
iter 133040 || Loss: 0.9522 || timer: 0.0817 sec.
iter 133050 || Loss: 0.5672 || timer: 0.0853 sec.
iter 133060 || Loss: 0.8933 || timer: 0.0917 sec.
iter 133070 || Loss: 1.0159 || timer: 0.0821 sec.
iter 133080 || Loss: 0.9262 || timer: 0.0925 sec.
iter 133090 || Loss: 0.9883 || timer: 0.0823 sec.
iter 133100 || Loss: 1.1724 || timer: 0.1042 sec.
iter 133110 || Loss: 1.3757 || timer: 0.0878 sec.
iter 133120 || Loss: 1.2263 || timer: 0.0816 sec.
iter 133130 || Loss: 1.0207 || timer: 0.0815 sec.
iter 133140 || Loss: 0.8503 || timer: 0.0912 sec.
iter 133150 || Loss: 1.2228 || timer: 0.0809 sec.
iter 133160 || Loss: 0.9247 || timer: 0.0830 sec.
iter 133170 || Loss: 1.2212 || timer: 0.1002 sec.
iter 133180 || Loss: 1.0522 || timer: 0.0820 sec.
iter 133190 || Loss: 0.8477 || timer: 0.0890 sec.
iter 133200 || Loss: 1.0091 || timer: 0.0897 sec.
iter 133210 || Loss: 1.1113 || timer: 0.0259 sec.
iter 133220 || Loss: 2.6324 || timer: 0.1024 sec.
iter 133230 || Loss: 1.1591 || timer: 0.0754 sec.
iter 133240 || Loss: 1.6220 || timer: 0.0860 sec.
iter 133250 || Loss: 1.1357 || timer: 0.1219 sec.
iter 133260 || Loss: 0.9532 || timer: 0.0982 sec.
iter 133270 || Loss: 0.8448 || timer: 0.0828 sec.
iter 133280 || Loss: 0.9712 || timer: 0.1090 sec.
iter 133290 || Loss: 1.0312 || timer: 0.0852 sec.
iter 133300 || Loss: 1.0679 || timer: 0.0916 sec.
iter 133310 || Loss: 0.9993 || timer: 0.1196 sec.
iter 133320 || Loss: 0.8562 || timer: 0.0755 sec.
iter 133330 || Loss: 1.0080 || timer: 0.0824 sec.
iter 133340 || Loss: 0.7675 || timer: 0.0913 sec.
iter 133350 || Loss: 0.8036 || timer: 0.0816 sec.
iter 133360 || Loss: 0.8585 || timer: 0.0853 sec.
iter 133370 || Loss: 1.1515 || timer: 0.0844 sec.
iter 133380 || Loss: 0.9850 || timer: 0.1014 sec.
iter 133390 || Loss: 0.7303 || timer: 0.0902 sec.
iter 133400 || Loss: 0.9537 || timer: 0.0901 sec.
iter 133410 || Loss: 0.9759 || timer: 0.0974 sec.
iter 133420 || Loss: 0.9360 || timer: 0.0855 sec.
iter 133430 || Loss: 1.1645 || timer: 0.0843 sec.
iter 133440 || Loss: 1.3082 || timer: 0.0894 sec.
iter 133450 || Loss: 0.8710 || timer: 0.0852 sec.
iter 133460 || Loss: 1.0730 || timer: 0.1031 sec.
iter 133470 || Loss: 0.8866 || timer: 0.0911 sec.
iter 133480 || Loss: 0.8015 || timer: 0.0974 sec.
iter 133490 || Loss: 1.3694 || timer: 0.0919 sec.
iter 133500 || Loss: 1.0070 || timer: 0.0949 sec.
iter 133510 || Loss: 0.9974 || timer: 0.0950 sec.
iter 133520 || Loss: 0.9634 || timer: 0.1005 sec.
iter 133530 || Loss: 0.7450 || timer: 0.0822 sec.
iter 133540 || Loss: 1.0712 || timer: 0.0221 sec.
iter 133550 || Loss: 1.8885 || timer: 0.0930 sec.
iter 133560 || Loss: 0.9874 || timer: 0.0882 sec.
iter 133570 || Loss: 0.8168 || timer: 0.0904 sec.
iter 133580 || Loss: 0.5864 || timer: 0.0839 sec.
iter 133590 || Loss: 1.0579 || timer: 0.0898 sec.
iter 133600 || Loss: 1.0946 || timer: 0.0841 sec.
iter 133610 || Loss: 1.0537 || timer: 0.0849 sec.
iter 133620 || Loss: 1.1353 || timer: 0.0955 sec.
iter 133630 || Loss: 1.0556 || timer: 0.0772 sec.
iter 133640 || Loss: 0.7807 || timer: 0.1130 sec.
iter 133650 || Loss: 0.7918 || timer: 0.0860 sec.
iter 133660 || Loss: 0.8525 || timer: 0.1038 sec.
iter 133670 || Loss: 0.9147 || timer: 0.0934 sec.
iter 133680 || Loss: 0.9600 || timer: 0.1075 sec.
iter 133690 || Loss: 0.8705 || timer: 0.0927 sec.
iter 133700 || Loss: 1.0933 || timer: 0.1024 sec.
iter 133710 || Loss: 0.9573 || timer: 0.1067 sec.
iter 133720 || Loss: 0.9835 || timer: 0.0860 sec.
iter 133730 || Loss: 1.0890 || timer: 0.0833 sec.
iter 133740 || Loss: 1.1156 || timer: 0.0837 sec.
iter 133750 || Loss: 0.9134 || timer: 0.1073 sec.
iter 133760 || Loss: 0.9943 || timer: 0.0830 sec.
iter 133770 || Loss: 0.8505 || timer: 0.0849 sec.
iter 133780 || Loss: 0.8955 || timer: 0.0857 sec.
iter 133790 || Loss: 0.8173 || timer: 0.0843 sec.
iter 133800 || Loss: 0.9349 || timer: 0.0992 sec.
iter 133810 || Loss: 0.8687 || timer: 0.0918 sec.
iter 133820 || Loss: 1.3829 || timer: 0.0841 sec.
iter 133830 || Loss: 1.1580 || timer: 0.0842 sec.
iter 133840 || Loss: 1.0530 || timer: 0.0950 sec.
iter 133850 || Loss: 1.0549 || timer: 0.1043 sec.
iter 133860 || Loss: 0.9790 || timer: 0.0918 sec.
iter 133870 || Loss: 1.1676 || timer: 0.0178 sec.
iter 133880 || Loss: 0.6761 || timer: 0.0769 sec.
iter 133890 || Loss: 1.1975 || timer: 0.0842 sec.
iter 133900 || Loss: 0.9657 || timer: 0.0835 sec.
iter 133910 || Loss: 0.9503 || timer: 0.0849 sec.
iter 133920 || Loss: 1.3514 || timer: 0.0861 sec.
iter 133930 || Loss: 0.6046 || timer: 0.1172 sec.
iter 133940 || Loss: 0.9755 || timer: 0.1026 sec.
iter 133950 || Loss: 0.9250 || timer: 0.0863 sec.
iter 133960 || Loss: 1.1166 || timer: 0.0855 sec.
iter 133970 || Loss: 1.4742 || timer: 0.0897 sec.
iter 133980 || Loss: 0.7711 || timer: 0.0847 sec.
iter 133990 || Loss: 1.2065 || timer: 0.0773 sec.
iter 134000 || Loss: 0.9226 || timer: 0.0759 sec.
iter 134010 || Loss: 0.8443 || timer: 0.0910 sec.
iter 134020 || Loss: 0.9668 || timer: 0.0892 sec.
iter 134030 || Loss: 0.9202 || timer: 0.0895 sec.
iter 134040 || Loss: 0.7438 || timer: 0.1027 sec.
iter 134050 || Loss: 1.2717 || timer: 0.0963 sec.
iter 134060 || Loss: 0.9574 || timer: 0.0843 sec.
iter 134070 || Loss: 0.8722 || timer: 0.0953 sec.
iter 134080 || Loss: 0.9247 || timer: 0.0927 sec.
iter 134090 || Loss: 1.1011 || timer: 0.0853 sec.
iter 134100 || Loss: 0.9502 || timer: 0.0835 sec.
iter 134110 || Loss: 0.8972 || timer: 0.0844 sec.
iter 134120 || Loss: 0.7922 || timer: 0.0859 sec.
iter 134130 || Loss: 0.7129 || timer: 0.0923 sec.
iter 134140 || Loss: 1.1095 || timer: 0.0885 sec.
iter 134150 || Loss: 1.1708 || timer: 0.0923 sec.
iter 134160 || Loss: 1.2982 || timer: 0.0974 sec.
iter 134170 || Loss: 0.9276 || timer: 0.1106 sec.
iter 134180 || Loss: 1.0069 || timer: 0.0857 sec.
iter 134190 || Loss: 1.0682 || timer: 0.0974 sec.
iter 134200 || Loss: 0.9082 || timer: 0.0174 sec.
iter 134210 || Loss: 0.4679 || timer: 0.0881 sec.
iter 134220 || Loss: 0.8999 || timer: 0.0773 sec.
iter 134230 || Loss: 1.0879 || timer: 0.1057 sec.
iter 134240 || Loss: 0.8022 || timer: 0.1005 sec.
iter 134250 || Loss: 0.9651 || timer: 0.0920 sec.
iter 134260 || Loss: 0.6514 || timer: 0.0843 sec.
iter 134270 || Loss: 1.0109 || timer: 0.0850 sec.
iter 134280 || Loss: 0.9038 || timer: 0.1093 sec.
iter 134290 || Loss: 1.1126 || timer: 0.0904 sec.
iter 134300 || Loss: 0.9796 || timer: 0.1124 sec.
iter 134310 || Loss: 1.1281 || timer: 0.0980 sec.
iter 134320 || Loss: 0.6721 || timer: 0.0973 sec.
iter 134330 || Loss: 0.8856 || timer: 0.0907 sec.
iter 134340 || Loss: 0.8996 || timer: 0.1000 sec.
iter 134350 || Loss: 0.8876 || timer: 0.0859 sec.
iter 134360 || Loss: 1.2856 || timer: 0.0925 sec.
iter 134370 || Loss: 1.1099 || timer: 0.0862 sec.
iter 134380 || Loss: 1.0058 || timer: 0.0792 sec.
iter 134390 || Loss: 0.8438 || timer: 0.0847 sec.
iter 134400 || Loss: 1.0575 || timer: 0.0859 sec.
iter 134410 || Loss: 0.8742 || timer: 0.0878 sec.
iter 134420 || Loss: 0.9274 || timer: 0.0935 sec.
iter 134430 || Loss: 1.2011 || timer: 0.0841 sec.
iter 134440 || Loss: 1.3636 || timer: 0.1144 sec.
iter 134450 || Loss: 0.7487 || timer: 0.0842 sec.
iter 134460 || Loss: 1.0010 || timer: 0.0881 sec.
iter 134470 || Loss: 1.2059 || timer: 0.0916 sec.
iter 134480 || Loss: 0.9549 || timer: 0.0890 sec.
iter 134490 || Loss: 0.9437 || timer: 0.1141 sec.
iter 134500 || Loss: 0.7332 || timer: 0.0843 sec.
iter 134510 || Loss: 0.8688 || timer: 0.0844 sec.
iter 134520 || Loss: 0.8350 || timer: 0.1039 sec.
iter 134530 || Loss: 1.0602 || timer: 0.0304 sec.
iter 134540 || Loss: 0.4799 || timer: 0.0918 sec.
iter 134550 || Loss: 0.8742 || timer: 0.1058 sec.
iter 134560 || Loss: 1.1167 || timer: 0.0829 sec.
iter 134570 || Loss: 0.8947 || timer: 0.0843 sec.
iter 134580 || Loss: 0.8326 || timer: 0.0750 sec.
iter 134590 || Loss: 1.0805 || timer: 0.1037 sec.
iter 134600 || Loss: 0.8563 || timer: 0.0811 sec.
iter 134610 || Loss: 1.7508 || timer: 0.0925 sec.
iter 134620 || Loss: 1.2041 || timer: 0.1013 sec.
iter 134630 || Loss: 0.9627 || timer: 0.1018 sec.
iter 134640 || Loss: 0.9694 || timer: 0.1091 sec.
iter 134650 || Loss: 0.9796 || timer: 0.1114 sec.
iter 134660 || Loss: 0.9462 || timer: 0.0836 sec.
iter 134670 || Loss: 1.2912 || timer: 0.0773 sec.
iter 134680 || Loss: 1.0709 || timer: 0.0847 sec.
iter 134690 || Loss: 1.1202 || timer: 0.0775 sec.
iter 134700 || Loss: 1.1004 || timer: 0.0861 sec.
iter 134710 || Loss: 1.0817 || timer: 0.0839 sec.
iter 134720 || Loss: 0.7636 || timer: 0.0845 sec.
iter 134730 || Loss: 0.9205 || timer: 0.0850 sec.
iter 134740 || Loss: 0.9148 || timer: 0.0847 sec.
iter 134750 || Loss: 0.8677 || timer: 0.0927 sec.
iter 134760 || Loss: 0.7901 || timer: 0.0928 sec.
iter 134770 || Loss: 1.0378 || timer: 0.0855 sec.
iter 134780 || Loss: 1.0312 || timer: 0.0884 sec.
iter 134790 || Loss: 0.9472 || timer: 0.1170 sec.
iter 134800 || Loss: 0.8804 || timer: 0.0852 sec.
iter 134810 || Loss: 1.1213 || timer: 0.0936 sec.
iter 134820 || Loss: 1.3201 || timer: 0.0916 sec.
iter 134830 || Loss: 1.1330 || timer: 0.0858 sec.
iter 134840 || Loss: 0.8856 || timer: 0.0915 sec.
iter 134850 || Loss: 1.3000 || timer: 0.0929 sec.
iter 134860 || Loss: 0.9442 || timer: 0.0190 sec.
iter 134870 || Loss: 2.3889 || timer: 0.0845 sec.
iter 134880 || Loss: 1.3297 || timer: 0.1014 sec.
iter 134890 || Loss: 1.2884 || timer: 0.0852 sec.
iter 134900 || Loss: 1.2015 || timer: 0.1082 sec.
iter 134910 || Loss: 1.2645 || timer: 0.0895 sec.
iter 134920 || Loss: 1.1305 || timer: 0.0837 sec.
iter 134930 || Loss: 0.8275 || timer: 0.0940 sec.
iter 134940 || Loss: 0.9173 || timer: 0.0940 sec.
iter 134950 || Loss: 0.9557 || timer: 0.0898 sec.
iter 134960 || Loss: 0.8532 || timer: 0.1050 sec.
iter 134970 || Loss: 0.7910 || timer: 0.0855 sec.
iter 134980 || Loss: 0.8265 || timer: 0.0927 sec.
iter 134990 || Loss: 1.0001 || timer: 0.0926 sec.
iter 135000 || Loss: 1.1012 || Saving state, iter: 135000
timer: 0.0844 sec.
iter 135010 || Loss: 0.7977 || timer: 0.0944 sec.
iter 135020 || Loss: 1.1552 || timer: 0.1030 sec.
iter 135030 || Loss: 1.5954 || timer: 0.0915 sec.
iter 135040 || Loss: 0.6372 || timer: 0.0869 sec.
iter 135050 || Loss: 1.0207 || timer: 0.0967 sec.
iter 135060 || Loss: 0.9766 || timer: 0.0946 sec.
iter 135070 || Loss: 0.8966 || timer: 0.0850 sec.
iter 135080 || Loss: 0.7572 || timer: 0.0848 sec.
iter 135090 || Loss: 1.1228 || timer: 0.0937 sec.
iter 135100 || Loss: 0.8667 || timer: 0.0815 sec.
iter 135110 || Loss: 0.9608 || timer: 0.0910 sec.
iter 135120 || Loss: 0.7155 || timer: 0.0944 sec.
iter 135130 || Loss: 1.0260 || timer: 0.0879 sec.
iter 135140 || Loss: 1.0413 || timer: 0.0949 sec.
iter 135150 || Loss: 0.9579 || timer: 0.0991 sec.
iter 135160 || Loss: 1.0205 || timer: 0.0930 sec.
iter 135170 || Loss: 1.1794 || timer: 0.0917 sec.
iter 135180 || Loss: 1.0812 || timer: 0.1092 sec.
iter 135190 || Loss: 1.0772 || timer: 0.0235 sec.
iter 135200 || Loss: 2.1005 || timer: 0.0853 sec.
iter 135210 || Loss: 1.1819 || timer: 0.0842 sec.
iter 135220 || Loss: 1.0496 || timer: 0.0843 sec.
iter 135230 || Loss: 0.8243 || timer: 0.0927 sec.
iter 135240 || Loss: 0.8728 || timer: 0.0835 sec.
iter 135250 || Loss: 0.9358 || timer: 0.0880 sec.
iter 135260 || Loss: 1.5209 || timer: 0.0885 sec.
iter 135270 || Loss: 1.1150 || timer: 0.0869 sec.
iter 135280 || Loss: 1.0593 || timer: 0.0877 sec.
iter 135290 || Loss: 0.8386 || timer: 0.1325 sec.
iter 135300 || Loss: 0.8121 || timer: 0.0904 sec.
iter 135310 || Loss: 1.1392 || timer: 0.0992 sec.
iter 135320 || Loss: 1.0146 || timer: 0.0908 sec.
iter 135330 || Loss: 0.5862 || timer: 0.1139 sec.
iter 135340 || Loss: 1.3226 || timer: 0.1113 sec.
iter 135350 || Loss: 1.0664 || timer: 0.0829 sec.
iter 135360 || Loss: 0.8168 || timer: 0.0927 sec.
iter 135370 || Loss: 0.8697 || timer: 0.0966 sec.
iter 135380 || Loss: 1.0918 || timer: 0.0930 sec.
iter 135390 || Loss: 1.0438 || timer: 0.0851 sec.
iter 135400 || Loss: 1.1074 || timer: 0.0812 sec.
iter 135410 || Loss: 0.8400 || timer: 0.0902 sec.
iter 135420 || Loss: 0.9622 || timer: 0.0884 sec.
iter 135430 || Loss: 1.1480 || timer: 0.0890 sec.
iter 135440 || Loss: 1.0174 || timer: 0.0894 sec.
iter 135450 || Loss: 0.6192 || timer: 0.0928 sec.
iter 135460 || Loss: 0.7300 || timer: 0.0862 sec.
iter 135470 || Loss: 0.8416 || timer: 0.1007 sec.
iter 135480 || Loss: 0.8804 || timer: 0.0830 sec.
iter 135490 || Loss: 1.4451 || timer: 0.0908 sec.
iter 135500 || Loss: 1.0375 || timer: 0.0809 sec.
iter 135510 || Loss: 0.8291 || timer: 0.0897 sec.
iter 135520 || Loss: 0.8090 || timer: 0.0193 sec.
iter 135530 || Loss: 1.3749 || timer: 0.0825 sec.
iter 135540 || Loss: 1.0981 || timer: 0.0914 sec.
iter 135550 || Loss: 0.8249 || timer: 0.0978 sec.
iter 135560 || Loss: 1.1818 || timer: 0.1051 sec.
iter 135570 || Loss: 1.2281 || timer: 0.1041 sec.
iter 135580 || Loss: 0.8639 || timer: 0.1049 sec.
iter 135590 || Loss: 0.7746 || timer: 0.0954 sec.
iter 135600 || Loss: 0.8687 || timer: 0.0778 sec.
iter 135610 || Loss: 0.8719 || timer: 0.0830 sec.
iter 135620 || Loss: 0.8535 || timer: 0.1020 sec.
iter 135630 || Loss: 0.8136 || timer: 0.1037 sec.
iter 135640 || Loss: 0.9383 || timer: 0.1276 sec.
iter 135650 || Loss: 0.8906 || timer: 0.0801 sec.
iter 135660 || Loss: 0.9196 || timer: 0.0870 sec.
iter 135670 || Loss: 1.1211 || timer: 0.0776 sec.
iter 135680 || Loss: 0.9310 || timer: 0.1188 sec.
iter 135690 || Loss: 0.9261 || timer: 0.0895 sec.
iter 135700 || Loss: 1.0420 || timer: 0.0787 sec.
iter 135710 || Loss: 0.8075 || timer: 0.0821 sec.
iter 135720 || Loss: 0.8743 || timer: 0.0898 sec.
iter 135730 || Loss: 0.9953 || timer: 0.0822 sec.
iter 135740 || Loss: 0.8214 || timer: 0.1053 sec.
iter 135750 || Loss: 0.9624 || timer: 0.0818 sec.
iter 135760 || Loss: 0.9658 || timer: 0.0836 sec.
iter 135770 || Loss: 1.5633 || timer: 0.0815 sec.
iter 135780 || Loss: 1.2710 || timer: 0.0895 sec.
iter 135790 || Loss: 1.0151 || timer: 0.0909 sec.
iter 135800 || Loss: 0.8904 || timer: 0.0922 sec.
iter 135810 || Loss: 1.1431 || timer: 0.0890 sec.
iter 135820 || Loss: 0.7659 || timer: 0.0901 sec.
iter 135830 || Loss: 0.9680 || timer: 0.0825 sec.
iter 135840 || Loss: 1.0129 || timer: 0.0852 sec.
iter 135850 || Loss: 1.0060 || timer: 0.0165 sec.
iter 135860 || Loss: 1.1689 || timer: 0.1110 sec.
iter 135870 || Loss: 1.1752 || timer: 0.0886 sec.
iter 135880 || Loss: 1.0474 || timer: 0.1074 sec.
iter 135890 || Loss: 0.8624 || timer: 0.0772 sec.
iter 135900 || Loss: 0.7491 || timer: 0.1050 sec.
iter 135910 || Loss: 1.0102 || timer: 0.0844 sec.
iter 135920 || Loss: 0.7317 || timer: 0.0859 sec.
iter 135930 || Loss: 0.9900 || timer: 0.0826 sec.
iter 135940 || Loss: 1.1409 || timer: 0.0822 sec.
iter 135950 || Loss: 0.9794 || timer: 0.1075 sec.
iter 135960 || Loss: 0.7997 || timer: 0.0889 sec.
iter 135970 || Loss: 0.8009 || timer: 0.0913 sec.
iter 135980 || Loss: 0.7228 || timer: 0.0830 sec.
iter 135990 || Loss: 0.8214 || timer: 0.0822 sec.
iter 136000 || Loss: 1.0509 || timer: 0.0807 sec.
iter 136010 || Loss: 0.6510 || timer: 0.0882 sec.
iter 136020 || Loss: 0.9770 || timer: 0.0890 sec.
iter 136030 || Loss: 1.1562 || timer: 0.0997 sec.
iter 136040 || Loss: 0.8633 || timer: 0.1108 sec.
iter 136050 || Loss: 0.9427 || timer: 0.1062 sec.
iter 136060 || Loss: 1.0480 || timer: 0.1151 sec.
iter 136070 || Loss: 0.7776 || timer: 0.0912 sec.
iter 136080 || Loss: 1.1133 || timer: 0.0928 sec.
iter 136090 || Loss: 1.2344 || timer: 0.0922 sec.
iter 136100 || Loss: 0.9005 || timer: 0.1009 sec.
iter 136110 || Loss: 0.9403 || timer: 0.1105 sec.
iter 136120 || Loss: 1.0961 || timer: 0.0899 sec.
iter 136130 || Loss: 1.0247 || timer: 0.1012 sec.
iter 136140 || Loss: 0.9125 || timer: 0.0915 sec.
iter 136150 || Loss: 0.9234 || timer: 0.0837 sec.
iter 136160 || Loss: 1.1048 || timer: 0.0921 sec.
iter 136170 || Loss: 0.9996 || timer: 0.1012 sec.
iter 136180 || Loss: 1.1624 || timer: 0.0214 sec.
iter 136190 || Loss: 2.0190 || timer: 0.0809 sec.
iter 136200 || Loss: 1.1203 || timer: 0.0864 sec.
iter 136210 || Loss: 0.9330 || timer: 0.0820 sec.
iter 136220 || Loss: 0.7136 || timer: 0.0861 sec.
iter 136230 || Loss: 0.7739 || timer: 0.1099 sec.
iter 136240 || Loss: 1.3826 || timer: 0.0833 sec.
iter 136250 || Loss: 0.9227 || timer: 0.1300 sec.
iter 136260 || Loss: 1.4732 || timer: 0.0916 sec.
iter 136270 || Loss: 1.0807 || timer: 0.0906 sec.
iter 136280 || Loss: 0.8184 || timer: 0.0971 sec.
iter 136290 || Loss: 0.9680 || timer: 0.0907 sec.
iter 136300 || Loss: 0.7925 || timer: 0.0945 sec.
iter 136310 || Loss: 0.8292 || timer: 0.0809 sec.
iter 136320 || Loss: 1.1312 || timer: 0.0834 sec.
iter 136330 || Loss: 1.3111 || timer: 0.0902 sec.
iter 136340 || Loss: 0.7140 || timer: 0.0894 sec.
iter 136350 || Loss: 0.8335 || timer: 0.0897 sec.
iter 136360 || Loss: 0.8403 || timer: 0.0912 sec.
iter 136370 || Loss: 1.0942 || timer: 0.1253 sec.
iter 136380 || Loss: 0.7260 || timer: 0.0859 sec.
iter 136390 || Loss: 0.8162 || timer: 0.1053 sec.
iter 136400 || Loss: 1.1172 || timer: 0.1183 sec.
iter 136410 || Loss: 0.9470 || timer: 0.0801 sec.
iter 136420 || Loss: 0.8933 || timer: 0.0883 sec.
iter 136430 || Loss: 0.8164 || timer: 0.0899 sec.
iter 136440 || Loss: 1.1576 || timer: 0.0861 sec.
iter 136450 || Loss: 1.1398 || timer: 0.0955 sec.
iter 136460 || Loss: 0.9916 || timer: 0.1062 sec.
iter 136470 || Loss: 1.0119 || timer: 0.0929 sec.
iter 136480 || Loss: 0.8124 || timer: 0.0839 sec.
iter 136490 || Loss: 1.1147 || timer: 0.1160 sec.
iter 136500 || Loss: 0.9138 || timer: 0.0968 sec.
iter 136510 || Loss: 0.8008 || timer: 0.0175 sec.
iter 136520 || Loss: 0.6574 || timer: 0.1066 sec.
iter 136530 || Loss: 1.5490 || timer: 0.0927 sec.
iter 136540 || Loss: 1.1213 || timer: 0.0890 sec.
iter 136550 || Loss: 1.3048 || timer: 0.0924 sec.
iter 136560 || Loss: 1.0009 || timer: 0.0997 sec.
iter 136570 || Loss: 0.8642 || timer: 0.0827 sec.
iter 136580 || Loss: 0.7323 || timer: 0.0939 sec.
iter 136590 || Loss: 0.9610 || timer: 0.1147 sec.
iter 136600 || Loss: 1.0552 || timer: 0.1108 sec.
iter 136610 || Loss: 1.2914 || timer: 0.0992 sec.
iter 136620 || Loss: 1.0866 || timer: 0.0832 sec.
iter 136630 || Loss: 0.8575 || timer: 0.0918 sec.
iter 136640 || Loss: 0.7409 || timer: 0.0932 sec.
iter 136650 || Loss: 1.2064 || timer: 0.0901 sec.
iter 136660 || Loss: 0.9135 || timer: 0.1097 sec.
iter 136670 || Loss: 0.9989 || timer: 0.0762 sec.
iter 136680 || Loss: 0.9644 || timer: 0.0962 sec.
iter 136690 || Loss: 0.7819 || timer: 0.0988 sec.
iter 136700 || Loss: 0.9811 || timer: 0.1340 sec.
iter 136710 || Loss: 1.1350 || timer: 0.0827 sec.
iter 136720 || Loss: 0.9588 || timer: 0.0853 sec.
iter 136730 || Loss: 1.2612 || timer: 0.0886 sec.
iter 136740 || Loss: 0.8054 || timer: 0.0913 sec.
iter 136750 || Loss: 0.9123 || timer: 0.0915 sec.
iter 136760 || Loss: 1.5410 || timer: 0.0927 sec.
iter 136770 || Loss: 0.7594 || timer: 0.0842 sec.
iter 136780 || Loss: 0.9859 || timer: 0.0918 sec.
iter 136790 || Loss: 1.0526 || timer: 0.0940 sec.
iter 136800 || Loss: 0.9867 || timer: 0.1042 sec.
iter 136810 || Loss: 0.8860 || timer: 0.0917 sec.
iter 136820 || Loss: 1.0569 || timer: 0.0768 sec.
iter 136830 || Loss: 1.0070 || timer: 0.0863 sec.
iter 136840 || Loss: 1.2681 || timer: 0.0227 sec.
iter 136850 || Loss: 2.6729 || timer: 0.0840 sec.
iter 136860 || Loss: 0.9638 || timer: 0.1057 sec.
iter 136870 || Loss: 1.0523 || timer: 0.0809 sec.
iter 136880 || Loss: 1.2323 || timer: 0.0844 sec.
iter 136890 || Loss: 1.0598 || timer: 0.0881 sec.
iter 136900 || Loss: 1.2031 || timer: 0.0833 sec.
iter 136910 || Loss: 1.3506 || timer: 0.0902 sec.
iter 136920 || Loss: 0.8883 || timer: 0.0980 sec.
iter 136930 || Loss: 1.2884 || timer: 0.0924 sec.
iter 136940 || Loss: 1.1073 || timer: 0.0908 sec.
iter 136950 || Loss: 0.8185 || timer: 0.0765 sec.
iter 136960 || Loss: 0.8577 || timer: 0.0769 sec.
iter 136970 || Loss: 0.8635 || timer: 0.0906 sec.
iter 136980 || Loss: 0.8618 || timer: 0.0837 sec.
iter 136990 || Loss: 0.7691 || timer: 0.1002 sec.
iter 137000 || Loss: 0.9300 || timer: 0.0903 sec.
iter 137010 || Loss: 0.8876 || timer: 0.0909 sec.
iter 137020 || Loss: 0.9560 || timer: 0.0913 sec.
iter 137030 || Loss: 1.0071 || timer: 0.1072 sec.
iter 137040 || Loss: 1.6856 || timer: 0.0985 sec.
iter 137050 || Loss: 0.7885 || timer: 0.0841 sec.
iter 137060 || Loss: 1.3271 || timer: 0.0829 sec.
iter 137070 || Loss: 1.2003 || timer: 0.0844 sec.
iter 137080 || Loss: 0.8127 || timer: 0.0900 sec.
iter 137090 || Loss: 0.7012 || timer: 0.0845 sec.
iter 137100 || Loss: 1.0126 || timer: 0.0901 sec.
iter 137110 || Loss: 0.8452 || timer: 0.0821 sec.
iter 137120 || Loss: 0.8783 || timer: 0.0830 sec.
iter 137130 || Loss: 1.2302 || timer: 0.0822 sec.
iter 137140 || Loss: 0.9030 || timer: 0.0906 sec.
iter 137150 || Loss: 0.9132 || timer: 0.0914 sec.
iter 137160 || Loss: 1.0399 || timer: 0.0893 sec.
iter 137170 || Loss: 1.0975 || timer: 0.0191 sec.
iter 137180 || Loss: 0.3165 || timer: 0.0845 sec.
iter 137190 || Loss: 0.8757 || timer: 0.0906 sec.
iter 137200 || Loss: 0.9359 || timer: 0.1011 sec.
iter 137210 || Loss: 1.2308 || timer: 0.0905 sec.
iter 137220 || Loss: 0.7459 || timer: 0.0908 sec.
iter 137230 || Loss: 1.0260 || timer: 0.0940 sec.
iter 137240 || Loss: 0.8803 || timer: 0.0951 sec.
iter 137250 || Loss: 0.7526 || timer: 0.0850 sec.
iter 137260 || Loss: 0.7714 || timer: 0.0843 sec.
iter 137270 || Loss: 1.2760 || timer: 0.1025 sec.
iter 137280 || Loss: 0.9309 || timer: 0.0919 sec.
iter 137290 || Loss: 0.7453 || timer: 0.0904 sec.
iter 137300 || Loss: 1.0912 || timer: 0.0911 sec.
iter 137310 || Loss: 0.8299 || timer: 0.0871 sec.
iter 137320 || Loss: 1.0143 || timer: 0.0897 sec.
iter 137330 || Loss: 0.9564 || timer: 0.0908 sec.
iter 137340 || Loss: 0.8447 || timer: 0.0905 sec.
iter 137350 || Loss: 0.9358 || timer: 0.0913 sec.
iter 137360 || Loss: 0.8276 || timer: 0.1168 sec.
iter 137370 || Loss: 0.7914 || timer: 0.0761 sec.
iter 137380 || Loss: 0.9525 || timer: 0.0977 sec.
iter 137390 || Loss: 1.0204 || timer: 0.0914 sec.
iter 137400 || Loss: 0.5801 || timer: 0.0829 sec.
iter 137410 || Loss: 0.8303 || timer: 0.1139 sec.
iter 137420 || Loss: 0.8118 || timer: 0.0846 sec.
iter 137430 || Loss: 1.0481 || timer: 0.0822 sec.
iter 137440 || Loss: 1.3114 || timer: 0.0835 sec.
iter 137450 || Loss: 1.2845 || timer: 0.0985 sec.
iter 137460 || Loss: 0.9341 || timer: 0.0907 sec.
iter 137470 || Loss: 1.0076 || timer: 0.0852 sec.
iter 137480 || Loss: 0.9537 || timer: 0.0825 sec.
iter 137490 || Loss: 1.1535 || timer: 0.1169 sec.
iter 137500 || Loss: 1.3176 || timer: 0.0275 sec.
iter 137510 || Loss: 4.0969 || timer: 0.0849 sec.
iter 137520 || Loss: 1.3233 || timer: 0.0773 sec.
iter 137530 || Loss: 1.1097 || timer: 0.0840 sec.
iter 137540 || Loss: 0.9491 || timer: 0.1085 sec.
iter 137550 || Loss: 1.0275 || timer: 0.0839 sec.
iter 137560 || Loss: 0.9254 || timer: 0.0848 sec.
iter 137570 || Loss: 1.0410 || timer: 0.0860 sec.
iter 137580 || Loss: 0.8469 || timer: 0.0885 sec.
iter 137590 || Loss: 0.7649 || timer: 0.1023 sec.
iter 137600 || Loss: 0.8911 || timer: 0.0972 sec.
iter 137610 || Loss: 0.8528 || timer: 0.0922 sec.
iter 137620 || Loss: 1.2171 || timer: 0.0929 sec.
iter 137630 || Loss: 0.9554 || timer: 0.0833 sec.
iter 137640 || Loss: 0.8123 || timer: 0.0842 sec.
iter 137650 || Loss: 1.0679 || timer: 0.0901 sec.
iter 137660 || Loss: 0.8246 || timer: 0.0901 sec.
iter 137670 || Loss: 1.1147 || timer: 0.0979 sec.
iter 137680 || Loss: 0.7411 || timer: 0.1048 sec.
iter 137690 || Loss: 0.7136 || timer: 0.0881 sec.
iter 137700 || Loss: 1.2967 || timer: 0.0833 sec.
iter 137710 || Loss: 0.6758 || timer: 0.0899 sec.
iter 137720 || Loss: 1.1978 || timer: 0.0908 sec.
iter 137730 || Loss: 1.0611 || timer: 0.0894 sec.
iter 137740 || Loss: 1.2034 || timer: 0.0946 sec.
iter 137750 || Loss: 0.8480 || timer: 0.0969 sec.
iter 137760 || Loss: 0.9368 || timer: 0.0840 sec.
iter 137770 || Loss: 1.0478 || timer: 0.0910 sec.
iter 137780 || Loss: 0.7139 || timer: 0.0922 sec.
iter 137790 || Loss: 0.8249 || timer: 0.0779 sec.
iter 137800 || Loss: 0.9107 || timer: 0.0835 sec.
iter 137810 || Loss: 1.0666 || timer: 0.0907 sec.
iter 137820 || Loss: 0.7086 || timer: 0.0907 sec.
iter 137830 || Loss: 1.0170 || timer: 0.0203 sec.
iter 137840 || Loss: 0.7491 || timer: 0.0937 sec.
iter 137850 || Loss: 0.7713 || timer: 0.0904 sec.
iter 137860 || Loss: 0.7784 || timer: 0.0838 sec.
iter 137870 || Loss: 1.2175 || timer: 0.0985 sec.
iter 137880 || Loss: 0.8654 || timer: 0.0909 sec.
iter 137890 || Loss: 0.9215 || timer: 0.0822 sec.
iter 137900 || Loss: 1.3632 || timer: 0.0982 sec.
iter 137910 || Loss: 0.9828 || timer: 0.0897 sec.
iter 137920 || Loss: 1.3872 || timer: 0.0773 sec.
iter 137930 || Loss: 0.9252 || timer: 0.1075 sec.
iter 137940 || Loss: 0.9597 || timer: 0.1069 sec.
iter 137950 || Loss: 1.2392 || timer: 0.0900 sec.
iter 137960 || Loss: 0.9822 || timer: 0.0805 sec.
iter 137970 || Loss: 0.8987 || timer: 0.0773 sec.
iter 137980 || Loss: 1.0806 || timer: 0.0770 sec.
iter 137990 || Loss: 0.9269 || timer: 0.0758 sec.
iter 138000 || Loss: 0.6286 || timer: 0.0907 sec.
iter 138010 || Loss: 0.7248 || timer: 0.0890 sec.
iter 138020 || Loss: 0.8667 || timer: 0.0922 sec.
iter 138030 || Loss: 0.7834 || timer: 0.0843 sec.
iter 138040 || Loss: 0.8928 || timer: 0.0847 sec.
iter 138050 || Loss: 0.7239 || timer: 0.0909 sec.
iter 138060 || Loss: 0.9836 || timer: 0.0829 sec.
iter 138070 || Loss: 0.9168 || timer: 0.0912 sec.
iter 138080 || Loss: 1.0080 || timer: 0.0909 sec.
iter 138090 || Loss: 0.7310 || timer: 0.0862 sec.
iter 138100 || Loss: 0.9495 || timer: 0.0858 sec.
iter 138110 || Loss: 1.2368 || timer: 0.0965 sec.
iter 138120 || Loss: 0.8842 || timer: 0.0911 sec.
iter 138130 || Loss: 0.9062 || timer: 0.0914 sec.
iter 138140 || Loss: 0.9982 || timer: 0.0930 sec.
iter 138150 || Loss: 0.7120 || timer: 0.0912 sec.
iter 138160 || Loss: 1.1898 || timer: 0.0252 sec.
iter 138170 || Loss: 0.2777 || timer: 0.0841 sec.
iter 138180 || Loss: 0.8314 || timer: 0.1035 sec.
iter 138190 || Loss: 0.9508 || timer: 0.0918 sec.
iter 138200 || Loss: 0.9788 || timer: 0.0916 sec.
iter 138210 || Loss: 1.2872 || timer: 0.0849 sec.
iter 138220 || Loss: 0.8176 || timer: 0.0771 sec.
iter 138230 || Loss: 0.9503 || timer: 0.0843 sec.
iter 138240 || Loss: 0.9217 || timer: 0.0836 sec.
iter 138250 || Loss: 0.9968 || timer: 0.0880 sec.
iter 138260 || Loss: 0.8600 || timer: 0.0987 sec.
iter 138270 || Loss: 0.8454 || timer: 0.0884 sec.
iter 138280 || Loss: 0.9158 || timer: 0.1121 sec.
iter 138290 || Loss: 0.9813 || timer: 0.1052 sec.
iter 138300 || Loss: 1.0084 || timer: 0.0914 sec.
iter 138310 || Loss: 1.4804 || timer: 0.0843 sec.
iter 138320 || Loss: 0.7414 || timer: 0.0905 sec.
iter 138330 || Loss: 0.8529 || timer: 0.1166 sec.
iter 138340 || Loss: 0.7660 || timer: 0.1042 sec.
iter 138350 || Loss: 0.6332 || timer: 0.0878 sec.
iter 138360 || Loss: 0.6314 || timer: 0.0839 sec.
iter 138370 || Loss: 0.7337 || timer: 0.0836 sec.
iter 138380 || Loss: 0.7562 || timer: 0.0919 sec.
iter 138390 || Loss: 1.0323 || timer: 0.0836 sec.
iter 138400 || Loss: 1.2238 || timer: 0.1040 sec.
iter 138410 || Loss: 1.0631 || timer: 0.0826 sec.
iter 138420 || Loss: 1.2814 || timer: 0.0911 sec.
iter 138430 || Loss: 1.1067 || timer: 0.0838 sec.
iter 138440 || Loss: 1.0713 || timer: 0.0963 sec.
iter 138450 || Loss: 0.9774 || timer: 0.0831 sec.
iter 138460 || Loss: 1.0870 || timer: 0.0897 sec.
iter 138470 || Loss: 0.8521 || timer: 0.0900 sec.
iter 138480 || Loss: 0.9908 || timer: 0.0904 sec.
iter 138490 || Loss: 0.8598 || timer: 0.0183 sec.
iter 138500 || Loss: 0.3204 || timer: 0.0885 sec.
iter 138510 || Loss: 0.9797 || timer: 0.0837 sec.
iter 138520 || Loss: 1.0652 || timer: 0.0899 sec.
iter 138530 || Loss: 1.0404 || timer: 0.0884 sec.
iter 138540 || Loss: 1.1250 || timer: 0.0822 sec.
iter 138550 || Loss: 0.8301 || timer: 0.0991 sec.
iter 138560 || Loss: 1.1057 || timer: 0.0903 sec.
iter 138570 || Loss: 1.0472 || timer: 0.0935 sec.
iter 138580 || Loss: 1.3690 || timer: 0.1301 sec.
iter 138590 || Loss: 0.9780 || timer: 0.0962 sec.
iter 138600 || Loss: 0.8282 || timer: 0.0888 sec.
iter 138610 || Loss: 0.8781 || timer: 0.0895 sec.
iter 138620 || Loss: 1.1358 || timer: 0.0887 sec.
iter 138630 || Loss: 1.0341 || timer: 0.1009 sec.
iter 138640 || Loss: 1.2111 || timer: 0.0828 sec.
iter 138650 || Loss: 0.8269 || timer: 0.0901 sec.
iter 138660 || Loss: 1.3990 || timer: 0.0869 sec.
iter 138670 || Loss: 0.7173 || timer: 0.0911 sec.
iter 138680 || Loss: 1.1105 || timer: 0.0918 sec.
iter 138690 || Loss: 1.1332 || timer: 0.0817 sec.
iter 138700 || Loss: 0.8861 || timer: 0.0926 sec.
iter 138710 || Loss: 0.9688 || timer: 0.1032 sec.
iter 138720 || Loss: 0.8860 || timer: 0.0814 sec.
iter 138730 || Loss: 0.9239 || timer: 0.0897 sec.
iter 138740 || Loss: 0.8691 || timer: 0.0892 sec.
iter 138750 || Loss: 0.9838 || timer: 0.0876 sec.
iter 138760 || Loss: 0.9923 || timer: 0.0832 sec.
iter 138770 || Loss: 0.8164 || timer: 0.1173 sec.
iter 138780 || Loss: 1.2056 || timer: 0.0891 sec.
iter 138790 || Loss: 0.6545 || timer: 0.1161 sec.
iter 138800 || Loss: 0.7085 || timer: 0.0898 sec.
iter 138810 || Loss: 0.8469 || timer: 0.0773 sec.
iter 138820 || Loss: 1.0925 || timer: 0.0247 sec.
iter 138830 || Loss: 0.2646 || timer: 0.0898 sec.
iter 138840 || Loss: 0.7505 || timer: 0.0908 sec.
iter 138850 || Loss: 0.7411 || timer: 0.1086 sec.
iter 138860 || Loss: 0.7991 || timer: 0.1059 sec.
iter 138870 || Loss: 0.9483 || timer: 0.0916 sec.
iter 138880 || Loss: 0.4917 || timer: 0.0901 sec.
iter 138890 || Loss: 1.1160 || timer: 0.0834 sec.
iter 138900 || Loss: 0.7042 || timer: 0.0850 sec.
iter 138910 || Loss: 1.2947 || timer: 0.0839 sec.
iter 138920 || Loss: 1.1135 || timer: 0.0953 sec.
iter 138930 || Loss: 0.8730 || timer: 0.0883 sec.
iter 138940 || Loss: 0.7496 || timer: 0.0915 sec.
iter 138950 || Loss: 0.6642 || timer: 0.0842 sec.
iter 138960 || Loss: 0.9737 || timer: 0.1142 sec.
iter 138970 || Loss: 1.4006 || timer: 0.0891 sec.
iter 138980 || Loss: 1.0167 || timer: 0.0920 sec.
iter 138990 || Loss: 1.0485 || timer: 0.0827 sec.
iter 139000 || Loss: 1.1382 || timer: 0.0979 sec.
iter 139010 || Loss: 0.9620 || timer: 0.0921 sec.
iter 139020 || Loss: 0.6921 || timer: 0.0863 sec.
iter 139030 || Loss: 0.7727 || timer: 0.1308 sec.
iter 139040 || Loss: 0.8388 || timer: 0.0894 sec.
iter 139050 || Loss: 0.9469 || timer: 0.0882 sec.
iter 139060 || Loss: 0.8207 || timer: 0.0988 sec.
iter 139070 || Loss: 0.8051 || timer: 0.0838 sec.
iter 139080 || Loss: 1.1534 || timer: 0.0910 sec.
iter 139090 || Loss: 0.9333 || timer: 0.0903 sec.
iter 139100 || Loss: 0.8582 || timer: 0.0886 sec.
iter 139110 || Loss: 0.8657 || timer: 0.0887 sec.
iter 139120 || Loss: 0.8149 || timer: 0.0896 sec.
iter 139130 || Loss: 0.9497 || timer: 0.0894 sec.
iter 139140 || Loss: 0.9768 || timer: 0.0837 sec.
iter 139150 || Loss: 1.0210 || timer: 0.0217 sec.
iter 139160 || Loss: 0.5378 || timer: 0.0940 sec.
iter 139170 || Loss: 1.2066 || timer: 0.0836 sec.
iter 139180 || Loss: 1.2708 || timer: 0.0734 sec.
iter 139190 || Loss: 1.1181 || timer: 0.0892 sec.
iter 139200 || Loss: 1.0248 || timer: 0.0909 sec.
iter 139210 || Loss: 1.0779 || timer: 0.0828 sec.
iter 139220 || Loss: 1.0621 || timer: 0.0879 sec.
iter 139230 || Loss: 1.1320 || timer: 0.0840 sec.
iter 139240 || Loss: 1.3600 || timer: 0.0841 sec.
iter 139250 || Loss: 1.0002 || timer: 0.1200 sec.
iter 139260 || Loss: 0.9051 || timer: 0.0867 sec.
iter 139270 || Loss: 1.2981 || timer: 0.0900 sec.
iter 139280 || Loss: 0.9396 || timer: 0.0903 sec.
iter 139290 || Loss: 0.8162 || timer: 0.0882 sec.
iter 139300 || Loss: 0.9118 || timer: 0.0835 sec.
iter 139310 || Loss: 0.7366 || timer: 0.0844 sec.
iter 139320 || Loss: 0.8151 || timer: 0.1071 sec.
iter 139330 || Loss: 1.1829 || timer: 0.0837 sec.
iter 139340 || Loss: 0.9115 || timer: 0.0927 sec.
iter 139350 || Loss: 0.8197 || timer: 0.0835 sec.
iter 139360 || Loss: 1.1245 || timer: 0.0907 sec.
iter 139370 || Loss: 1.7180 || timer: 0.0863 sec.
iter 139380 || Loss: 0.8253 || timer: 0.0850 sec.
iter 139390 || Loss: 0.9576 || timer: 0.0856 sec.
iter 139400 || Loss: 1.5963 || timer: 0.0911 sec.
iter 139410 || Loss: 1.5680 || timer: 0.0926 sec.
iter 139420 || Loss: 1.0011 || timer: 0.1136 sec.
iter 139430 || Loss: 1.2106 || timer: 0.0829 sec.
iter 139440 || Loss: 0.8658 || timer: 0.0834 sec.
iter 139450 || Loss: 0.9032 || timer: 0.0827 sec.
iter 139460 || Loss: 1.1508 || timer: 0.1237 sec.
iter 139470 || Loss: 1.0763 || timer: 0.0910 sec.
iter 139480 || Loss: 0.9316 || timer: 0.0252 sec.
iter 139490 || Loss: 0.9343 || timer: 0.0832 sec.
iter 139500 || Loss: 0.9372 || timer: 0.1276 sec.
iter 139510 || Loss: 1.0721 || timer: 0.0812 sec.
iter 139520 || Loss: 1.1819 || timer: 0.0831 sec.
iter 139530 || Loss: 1.2436 || timer: 0.0992 sec.
iter 139540 || Loss: 1.1442 || timer: 0.0915 sec.
iter 139550 || Loss: 0.8315 || timer: 0.1005 sec.
iter 139560 || Loss: 1.1325 || timer: 0.1101 sec.
iter 139570 || Loss: 0.9686 || timer: 0.1089 sec.
iter 139580 || Loss: 1.0401 || timer: 0.0974 sec.
iter 139590 || Loss: 0.7358 || timer: 0.1120 sec.
iter 139600 || Loss: 0.7792 || timer: 0.0826 sec.
iter 139610 || Loss: 1.1783 || timer: 0.0831 sec.
iter 139620 || Loss: 1.0248 || timer: 0.0848 sec.
iter 139630 || Loss: 1.0219 || timer: 0.0839 sec.
iter 139640 || Loss: 0.8645 || timer: 0.0881 sec.
iter 139650 || Loss: 0.9196 || timer: 0.0830 sec.
iter 139660 || Loss: 0.9880 || timer: 0.1024 sec.
iter 139670 || Loss: 0.9402 || timer: 0.1006 sec.
iter 139680 || Loss: 0.9195 || timer: 0.0976 sec.
iter 139690 || Loss: 0.7864 || timer: 0.0870 sec.
iter 139700 || Loss: 0.8540 || timer: 0.1057 sec.
iter 139710 || Loss: 0.7012 || timer: 0.0884 sec.
iter 139720 || Loss: 0.8380 || timer: 0.0924 sec.
iter 139730 || Loss: 1.0657 || timer: 0.0925 sec.
iter 139740 || Loss: 1.1883 || timer: 0.0834 sec.
iter 139750 || Loss: 0.8040 || timer: 0.0908 sec.
iter 139760 || Loss: 0.9417 || timer: 0.0876 sec.
iter 139770 || Loss: 1.0317 || timer: 0.0807 sec.
iter 139780 || Loss: 1.1691 || timer: 0.0904 sec.
iter 139790 || Loss: 0.9841 || timer: 0.0902 sec.
iter 139800 || Loss: 1.0771 || timer: 0.0917 sec.
iter 139810 || Loss: 1.0734 || timer: 0.0272 sec.
iter 139820 || Loss: 0.6165 || timer: 0.0920 sec.
iter 139830 || Loss: 1.0132 || timer: 0.0918 sec.
iter 139840 || Loss: 0.8668 || timer: 0.0913 sec.
iter 139850 || Loss: 0.6244 || timer: 0.0896 sec.
iter 139860 || Loss: 0.9460 || timer: 0.0879 sec.
iter 139870 || Loss: 1.0049 || timer: 0.0915 sec.
iter 139880 || Loss: 0.6826 || timer: 0.0897 sec.
iter 139890 || Loss: 1.1422 || timer: 0.0825 sec.
iter 139900 || Loss: 0.7520 || timer: 0.1058 sec.
iter 139910 || Loss: 1.5059 || timer: 0.0958 sec.
iter 139920 || Loss: 1.0891 || timer: 0.0904 sec.
iter 139930 || Loss: 1.1812 || timer: 0.0882 sec.
iter 139940 || Loss: 1.3446 || timer: 0.0898 sec.
iter 139950 || Loss: 0.6639 || timer: 0.0901 sec.
iter 139960 || Loss: 1.3025 || timer: 0.1094 sec.
iter 139970 || Loss: 1.0188 || timer: 0.0832 sec.
iter 139980 || Loss: 1.3620 || timer: 0.0926 sec.
iter 139990 || Loss: 0.9094 || timer: 0.0880 sec.
iter 140000 || Loss: 0.6678 || Saving state, iter: 140000
timer: 0.0827 sec.
iter 140010 || Loss: 0.8576 || timer: 0.0940 sec.
iter 140020 || Loss: 0.9430 || timer: 0.0878 sec.
iter 140030 || Loss: 1.3073 || timer: 0.1134 sec.
iter 140040 || Loss: 1.0961 || timer: 0.0824 sec.
iter 140050 || Loss: 1.9330 || timer: 0.0839 sec.
iter 140060 || Loss: 1.8099 || timer: 0.0772 sec.
iter 140070 || Loss: 1.4898 || timer: 0.0889 sec.
iter 140080 || Loss: 2.4615 || timer: 0.0852 sec.
iter 140090 || Loss: 0.7676 || timer: 0.0940 sec.
iter 140100 || Loss: 1.1508 || timer: 0.1024 sec.
iter 140110 || Loss: 0.9391 || timer: 0.0965 sec.
iter 140120 || Loss: 1.2840 || timer: 0.0829 sec.
iter 140130 || Loss: 1.0851 || timer: 0.0908 sec.
iter 140140 || Loss: 0.9146 || timer: 0.0277 sec.
iter 140150 || Loss: 0.3375 || timer: 0.0829 sec.
iter 140160 || Loss: 1.1346 || timer: 0.0867 sec.
iter 140170 || Loss: 0.8306 || timer: 0.0889 sec.
iter 140180 || Loss: 1.0965 || timer: 0.0975 sec.
iter 140190 || Loss: 0.9347 || timer: 0.0914 sec.
iter 140200 || Loss: 0.8185 || timer: 0.0886 sec.
iter 140210 || Loss: 0.8085 || timer: 0.0939 sec.
iter 140220 || Loss: 1.0070 || timer: 0.0810 sec.
iter 140230 || Loss: 1.0129 || timer: 0.1055 sec.
iter 140240 || Loss: 1.1187 || timer: 0.1051 sec.
iter 140250 || Loss: 1.1508 || timer: 0.0837 sec.
iter 140260 || Loss: 0.8833 || timer: 0.0897 sec.
iter 140270 || Loss: 0.8699 || timer: 0.0904 sec.
iter 140280 || Loss: 0.6134 || timer: 0.0883 sec.
iter 140290 || Loss: 1.3067 || timer: 0.0914 sec.
iter 140300 || Loss: 1.1200 || timer: 0.0829 sec.
iter 140310 || Loss: 0.8201 || timer: 0.1031 sec.
iter 140320 || Loss: 0.9286 || timer: 0.0828 sec.
iter 140330 || Loss: 1.0683 || timer: 0.0907 sec.
iter 140340 || Loss: 1.3196 || timer: 0.0836 sec.
iter 140350 || Loss: 1.1249 || timer: 0.0838 sec.
iter 140360 || Loss: 1.0554 || timer: 0.0919 sec.
iter 140370 || Loss: 1.1529 || timer: 0.0917 sec.
iter 140380 || Loss: 0.9010 || timer: 0.0917 sec.
iter 140390 || Loss: 0.7757 || timer: 0.0841 sec.
iter 140400 || Loss: 0.7501 || timer: 0.0861 sec.
iter 140410 || Loss: 0.9947 || timer: 0.0842 sec.
iter 140420 || Loss: 0.9036 || timer: 0.0931 sec.
iter 140430 || Loss: 1.2823 || timer: 0.0891 sec.
iter 140440 || Loss: 1.0353 || timer: 0.0924 sec.
iter 140450 || Loss: 0.6970 || timer: 0.0894 sec.
iter 140460 || Loss: 0.8404 || timer: 0.0933 sec.
iter 140470 || Loss: 0.9375 || timer: 0.0190 sec.
iter 140480 || Loss: 0.6350 || timer: 0.0890 sec.
iter 140490 || Loss: 0.7748 || timer: 0.0969 sec.
iter 140500 || Loss: 0.9299 || timer: 0.0894 sec.
iter 140510 || Loss: 0.8476 || timer: 0.0990 sec.
iter 140520 || Loss: 0.9153 || timer: 0.0826 sec.
iter 140530 || Loss: 0.7420 || timer: 0.0830 sec.
iter 140540 || Loss: 0.9743 || timer: 0.0911 sec.
iter 140550 || Loss: 0.9266 || timer: 0.0971 sec.
iter 140560 || Loss: 1.2194 || timer: 0.0982 sec.
iter 140570 || Loss: 0.9498 || timer: 0.0981 sec.
iter 140580 || Loss: 1.0697 || timer: 0.1017 sec.
iter 140590 || Loss: 1.1099 || timer: 0.0903 sec.
iter 140600 || Loss: 0.7903 || timer: 0.0842 sec.
iter 140610 || Loss: 0.8696 || timer: 0.0850 sec.
iter 140620 || Loss: 1.0189 || timer: 0.0840 sec.
iter 140630 || Loss: 1.2375 || timer: 0.0840 sec.
iter 140640 || Loss: 0.8517 || timer: 0.0834 sec.
iter 140650 || Loss: 0.9950 || timer: 0.0835 sec.
iter 140660 || Loss: 0.8933 || timer: 0.0829 sec.
iter 140670 || Loss: 0.8870 || timer: 0.0834 sec.
iter 140680 || Loss: 1.1878 || timer: 0.0920 sec.
iter 140690 || Loss: 0.7608 || timer: 0.0827 sec.
iter 140700 || Loss: 1.3607 || timer: 0.0912 sec.
iter 140710 || Loss: 1.0901 || timer: 0.0894 sec.
iter 140720 || Loss: 1.2073 || timer: 0.0828 sec.
iter 140730 || Loss: 1.0027 || timer: 0.0910 sec.
iter 140740 || Loss: 0.8617 || timer: 0.0934 sec.
iter 140750 || Loss: 1.1185 || timer: 0.0911 sec.
iter 140760 || Loss: 0.9343 || timer: 0.0859 sec.
iter 140770 || Loss: 0.8423 || timer: 0.0834 sec.
iter 140780 || Loss: 1.0072 || timer: 0.1165 sec.
iter 140790 || Loss: 0.9050 || timer: 0.0911 sec.
iter 140800 || Loss: 0.9364 || timer: 0.0266 sec.
iter 140810 || Loss: 2.8705 || timer: 0.1289 sec.
iter 140820 || Loss: 1.3198 || timer: 0.0928 sec.
iter 140830 || Loss: 1.2384 || timer: 0.0865 sec.
iter 140840 || Loss: 1.4314 || timer: 0.1032 sec.
iter 140850 || Loss: 0.8909 || timer: 0.1059 sec.
iter 140860 || Loss: 0.9507 || timer: 0.0831 sec.
iter 140870 || Loss: 1.2843 || timer: 0.0905 sec.
iter 140880 || Loss: 1.0931 || timer: 0.0892 sec.
iter 140890 || Loss: 1.2263 || timer: 0.0915 sec.
iter 140900 || Loss: 1.0066 || timer: 0.1256 sec.
iter 140910 || Loss: 0.8418 || timer: 0.0823 sec.
iter 140920 || Loss: 1.2994 || timer: 0.0763 sec.
iter 140930 || Loss: 0.8640 || timer: 0.0930 sec.
iter 140940 || Loss: 1.2747 || timer: 0.0770 sec.
iter 140950 || Loss: 1.0562 || timer: 0.0917 sec.
iter 140960 || Loss: 1.7133 || timer: 0.0916 sec.
iter 140970 || Loss: 1.5869 || timer: 0.0941 sec.
iter 140980 || Loss: 0.9805 || timer: 0.0852 sec.
iter 140990 || Loss: 1.4675 || timer: 0.0949 sec.
iter 141000 || Loss: 1.0265 || timer: 0.0755 sec.
iter 141010 || Loss: 0.8592 || timer: 0.0886 sec.
iter 141020 || Loss: 1.1696 || timer: 0.1121 sec.
iter 141030 || Loss: 0.7612 || timer: 0.0842 sec.
iter 141040 || Loss: 0.8981 || timer: 0.0755 sec.
iter 141050 || Loss: 1.1851 || timer: 0.0883 sec.
iter 141060 || Loss: 1.0599 || timer: 0.0921 sec.
iter 141070 || Loss: 0.8469 || timer: 0.0944 sec.
iter 141080 || Loss: 1.0798 || timer: 0.0931 sec.
iter 141090 || Loss: 1.0203 || timer: 0.0893 sec.
iter 141100 || Loss: 0.8642 || timer: 0.0852 sec.
iter 141110 || Loss: 0.9700 || timer: 0.0772 sec.
iter 141120 || Loss: 1.1658 || timer: 0.0856 sec.
iter 141130 || Loss: 1.0206 || timer: 0.0275 sec.
iter 141140 || Loss: 3.9950 || timer: 0.0999 sec.
iter 141150 || Loss: 1.1313 || timer: 0.0942 sec.
iter 141160 || Loss: 1.0720 || timer: 0.0881 sec.
iter 141170 || Loss: 0.6614 || timer: 0.0932 sec.
iter 141180 || Loss: 0.9703 || timer: 0.0928 sec.
iter 141190 || Loss: 1.2400 || timer: 0.0914 sec.
iter 141200 || Loss: 1.0319 || timer: 0.0908 sec.
iter 141210 || Loss: 0.9894 || timer: 0.0905 sec.
iter 141220 || Loss: 1.0561 || timer: 0.0899 sec.
iter 141230 || Loss: 1.1771 || timer: 0.1251 sec.
iter 141240 || Loss: 1.1088 || timer: 0.0762 sec.
iter 141250 || Loss: 0.7615 || timer: 0.0785 sec.
iter 141260 || Loss: 0.9542 || timer: 0.0814 sec.
iter 141270 || Loss: 1.0835 || timer: 0.0828 sec.
iter 141280 || Loss: 0.8697 || timer: 0.1054 sec.
iter 141290 || Loss: 1.3769 || timer: 0.0835 sec.
iter 141300 || Loss: 0.8685 || timer: 0.0902 sec.
iter 141310 || Loss: 1.4951 || timer: 0.1055 sec.
iter 141320 || Loss: 0.8191 || timer: 0.0859 sec.
iter 141330 || Loss: 0.9387 || timer: 0.1131 sec.
iter 141340 || Loss: 0.7149 || timer: 0.0855 sec.
iter 141350 || Loss: 1.1350 || timer: 0.1044 sec.
iter 141360 || Loss: 0.7488 || timer: 0.0922 sec.
iter 141370 || Loss: 0.8640 || timer: 0.0981 sec.
iter 141380 || Loss: 0.9252 || timer: 0.0889 sec.
iter 141390 || Loss: 1.2335 || timer: 0.0812 sec.
iter 141400 || Loss: 0.8873 || timer: 0.0916 sec.
iter 141410 || Loss: 1.2636 || timer: 0.1128 sec.
iter 141420 || Loss: 1.0379 || timer: 0.0830 sec.
iter 141430 || Loss: 0.9511 || timer: 0.0843 sec.
iter 141440 || Loss: 0.8144 || timer: 0.0878 sec.
iter 141450 || Loss: 0.8652 || timer: 0.1065 sec.
iter 141460 || Loss: 0.8592 || timer: 0.0262 sec.
iter 141470 || Loss: 4.5917 || timer: 0.0830 sec.
iter 141480 || Loss: 0.9472 || timer: 0.0908 sec.
iter 141490 || Loss: 0.8439 || timer: 0.0889 sec.
iter 141500 || Loss: 1.0919 || timer: 0.0844 sec.
iter 141510 || Loss: 1.0028 || timer: 0.0927 sec.
iter 141520 || Loss: 1.0055 || timer: 0.0901 sec.
iter 141530 || Loss: 1.3863 || timer: 0.0765 sec.
iter 141540 || Loss: 0.8952 || timer: 0.0822 sec.
iter 141550 || Loss: 1.2075 || timer: 0.0813 sec.
iter 141560 || Loss: 0.9468 || timer: 0.0925 sec.
iter 141570 || Loss: 0.9008 || timer: 0.0834 sec.
iter 141580 || Loss: 1.4188 || timer: 0.0839 sec.
iter 141590 || Loss: 1.0929 || timer: 0.0890 sec.
iter 141600 || Loss: 1.3486 || timer: 0.0927 sec.
iter 141610 || Loss: 1.6193 || timer: 0.0818 sec.
iter 141620 || Loss: 1.0466 || timer: 0.0831 sec.
iter 141630 || Loss: 1.4006 || timer: 0.1081 sec.
iter 141640 || Loss: 1.3552 || timer: 0.0903 sec.
iter 141650 || Loss: 1.1603 || timer: 0.0893 sec.
iter 141660 || Loss: 1.2278 || timer: 0.1434 sec.
iter 141670 || Loss: 0.8148 || timer: 0.0872 sec.
iter 141680 || Loss: 1.4309 || timer: 0.0869 sec.
iter 141690 || Loss: 0.9002 || timer: 0.0749 sec.
iter 141700 || Loss: 1.2656 || timer: 0.0898 sec.
iter 141710 || Loss: 1.0223 || timer: 0.0828 sec.
iter 141720 || Loss: 0.8603 || timer: 0.0889 sec.
iter 141730 || Loss: 1.1340 || timer: 0.0825 sec.
iter 141740 || Loss: 1.4228 || timer: 0.0839 sec.
iter 141750 || Loss: 1.2728 || timer: 0.0921 sec.
iter 141760 || Loss: 1.1404 || timer: 0.0830 sec.
iter 141770 || Loss: 1.2353 || timer: 0.1410 sec.
iter 141780 || Loss: 1.2444 || timer: 0.0957 sec.
iter 141790 || Loss: 1.5210 || timer: 0.0263 sec.
iter 141800 || Loss: 0.9021 || timer: 0.0882 sec.
iter 141810 || Loss: 1.0986 || timer: 0.0903 sec.
iter 141820 || Loss: 0.9477 || timer: 0.0837 sec.
iter 141830 || Loss: 1.0718 || timer: 0.0830 sec.
iter 141840 || Loss: 0.9702 || timer: 0.0859 sec.
iter 141850 || Loss: 1.3209 || timer: 0.0892 sec.
iter 141860 || Loss: 1.0543 || timer: 0.0901 sec.
iter 141870 || Loss: 0.9340 || timer: 0.0831 sec.
iter 141880 || Loss: 0.7829 || timer: 0.0840 sec.
iter 141890 || Loss: 1.3891 || timer: 0.0949 sec.
iter 141900 || Loss: 1.0731 || timer: 0.0913 sec.
iter 141910 || Loss: 1.2600 || timer: 0.0895 sec.
iter 141920 || Loss: 0.9081 || timer: 0.0904 sec.
iter 141930 || Loss: 1.3442 || timer: 0.0789 sec.
iter 141940 || Loss: 1.3810 || timer: 0.0833 sec.
iter 141950 || Loss: 1.1154 || timer: 0.0841 sec.
iter 141960 || Loss: 0.7823 || timer: 0.0893 sec.
iter 141970 || Loss: 1.1058 || timer: 0.0838 sec.
iter 141980 || Loss: 0.8823 || timer: 0.0939 sec.
iter 141990 || Loss: 0.7636 || timer: 0.0844 sec.
iter 142000 || Loss: 1.2973 || timer: 0.0921 sec.
iter 142010 || Loss: 0.8722 || timer: 0.0901 sec.
iter 142020 || Loss: 0.8372 || timer: 0.0851 sec.
iter 142030 || Loss: 0.8135 || timer: 0.0908 sec.
iter 142040 || Loss: 1.3380 || timer: 0.0935 sec.
iter 142050 || Loss: 1.4927 || timer: 0.0839 sec.
iter 142060 || Loss: 0.8064 || timer: 0.0985 sec.
iter 142070 || Loss: 0.9903 || timer: 0.0838 sec.
iter 142080 || Loss: 1.1824 || timer: 0.0816 sec.
iter 142090 || Loss: 0.6548 || timer: 0.0774 sec.
iter 142100 || Loss: 0.8424 || timer: 0.0843 sec.
iter 142110 || Loss: 1.3847 || timer: 0.1066 sec.
iter 142120 || Loss: 0.8533 || timer: 0.0330 sec.
iter 142130 || Loss: 0.5481 || timer: 0.0899 sec.
iter 142140 || Loss: 0.9703 || timer: 0.0917 sec.
iter 142150 || Loss: 0.7974 || timer: 0.0917 sec.
iter 142160 || Loss: 1.2305 || timer: 0.0843 sec.
iter 142170 || Loss: 0.9486 || timer: 0.0934 sec.
iter 142180 || Loss: 0.7197 || timer: 0.0958 sec.
iter 142190 || Loss: 1.0676 || timer: 0.0826 sec.
iter 142200 || Loss: 0.7047 || timer: 0.0754 sec.
iter 142210 || Loss: 0.8237 || timer: 0.0887 sec.
iter 142220 || Loss: 0.9285 || timer: 0.1191 sec.
iter 142230 || Loss: 0.6190 || timer: 0.0986 sec.
iter 142240 || Loss: 1.0558 || timer: 0.0885 sec.
iter 142250 || Loss: 1.1196 || timer: 0.0917 sec.
iter 142260 || Loss: 0.9327 || timer: 0.0820 sec.
iter 142270 || Loss: 1.1290 || timer: 0.0897 sec.
iter 142280 || Loss: 1.1015 || timer: 0.0934 sec.
iter 142290 || Loss: 0.9930 || timer: 0.0922 sec.
iter 142300 || Loss: 0.8839 || timer: 0.0881 sec.
iter 142310 || Loss: 0.9901 || timer: 0.0900 sec.
iter 142320 || Loss: 0.7888 || timer: 0.0922 sec.
iter 142330 || Loss: 1.3751 || timer: 0.0966 sec.
iter 142340 || Loss: 0.9869 || timer: 0.0928 sec.
iter 142350 || Loss: 1.0410 || timer: 0.0929 sec.
iter 142360 || Loss: 0.6461 || timer: 0.0918 sec.
iter 142370 || Loss: 0.8688 || timer: 0.0884 sec.
iter 142380 || Loss: 0.8591 || timer: 0.0905 sec.
iter 142390 || Loss: 1.0436 || timer: 0.0860 sec.
iter 142400 || Loss: 0.8498 || timer: 0.0862 sec.
iter 142410 || Loss: 1.5188 || timer: 0.0829 sec.
iter 142420 || Loss: 1.2880 || timer: 0.0893 sec.
iter 142430 || Loss: 0.9485 || timer: 0.0809 sec.
iter 142440 || Loss: 0.7744 || timer: 0.0827 sec.
iter 142450 || Loss: 1.1586 || timer: 0.0157 sec.
iter 142460 || Loss: 2.0733 || timer: 0.0899 sec.
iter 142470 || Loss: 0.6844 || timer: 0.0893 sec.
iter 142480 || Loss: 0.8626 || timer: 0.0921 sec.
iter 142490 || Loss: 1.3379 || timer: 0.0831 sec.
iter 142500 || Loss: 1.0989 || timer: 0.0904 sec.
iter 142510 || Loss: 1.0356 || timer: 0.0961 sec.
iter 142520 || Loss: 0.9463 || timer: 0.0905 sec.
iter 142530 || Loss: 0.8288 || timer: 0.0755 sec.
iter 142540 || Loss: 1.2479 || timer: 0.0861 sec.
iter 142550 || Loss: 0.9007 || timer: 0.1491 sec.
iter 142560 || Loss: 1.2459 || timer: 0.1077 sec.
iter 142570 || Loss: 0.9031 || timer: 0.1010 sec.
iter 142580 || Loss: 0.8238 || timer: 0.0794 sec.
iter 142590 || Loss: 0.9269 || timer: 0.0834 sec.
iter 142600 || Loss: 1.0369 || timer: 0.1053 sec.
iter 142610 || Loss: 0.9129 || timer: 0.0952 sec.
iter 142620 || Loss: 0.9787 || timer: 0.0877 sec.
iter 142630 || Loss: 1.0570 || timer: 0.0927 sec.
iter 142640 || Loss: 0.7318 || timer: 0.0832 sec.
iter 142650 || Loss: 0.9203 || timer: 0.0824 sec.
iter 142660 || Loss: 1.3060 || timer: 0.0895 sec.
iter 142670 || Loss: 1.1396 || timer: 0.1014 sec.
iter 142680 || Loss: 0.7357 || timer: 0.0882 sec.
iter 142690 || Loss: 1.2543 || timer: 0.0830 sec.
iter 142700 || Loss: 1.0265 || timer: 0.0995 sec.
iter 142710 || Loss: 1.2426 || timer: 0.0925 sec.
iter 142720 || Loss: 0.8046 || timer: 0.0961 sec.
iter 142730 || Loss: 1.0123 || timer: 0.0817 sec.
iter 142740 || Loss: 0.7671 || timer: 0.0821 sec.
iter 142750 || Loss: 1.2791 || timer: 0.0852 sec.
iter 142760 || Loss: 0.9732 || timer: 0.0888 sec.
iter 142770 || Loss: 1.1525 || timer: 0.0898 sec.
iter 142780 || Loss: 1.1566 || timer: 0.0220 sec.
iter 142790 || Loss: 2.4769 || timer: 0.0899 sec.
iter 142800 || Loss: 1.0176 || timer: 0.0754 sec.
iter 142810 || Loss: 1.5111 || timer: 0.1007 sec.
iter 142820 || Loss: 0.7566 || timer: 0.0895 sec.
iter 142830 || Loss: 0.8778 || timer: 0.0826 sec.
iter 142840 || Loss: 1.2732 || timer: 0.0969 sec.
iter 142850 || Loss: 0.9353 || timer: 0.0789 sec.
iter 142860 || Loss: 0.9095 || timer: 0.0910 sec.
iter 142870 || Loss: 1.0371 || timer: 0.1142 sec.
iter 142880 || Loss: 0.9785 || timer: 0.1094 sec.
iter 142890 || Loss: 0.8511 || timer: 0.0904 sec.
iter 142900 || Loss: 0.8246 || timer: 0.0904 sec.
iter 142910 || Loss: 1.1109 || timer: 0.0916 sec.
iter 142920 || Loss: 0.9815 || timer: 0.0842 sec.
iter 142930 || Loss: 0.8048 || timer: 0.1105 sec.
iter 142940 || Loss: 0.6034 || timer: 0.0912 sec.
iter 142950 || Loss: 0.7807 || timer: 0.0890 sec.
iter 142960 || Loss: 0.9478 || timer: 0.0834 sec.
iter 142970 || Loss: 1.2644 || timer: 0.0888 sec.
iter 142980 || Loss: 1.2680 || timer: 0.1035 sec.
iter 142990 || Loss: 1.3968 || timer: 0.0974 sec.
iter 143000 || Loss: 1.0378 || timer: 0.0839 sec.
iter 143010 || Loss: 0.9686 || timer: 0.0838 sec.
iter 143020 || Loss: 0.8131 || timer: 0.0843 sec.
iter 143030 || Loss: 1.3952 || timer: 0.0846 sec.
iter 143040 || Loss: 0.9085 || timer: 0.1025 sec.
iter 143050 || Loss: 1.0812 || timer: 0.0761 sec.
iter 143060 || Loss: 1.2191 || timer: 0.1072 sec.
iter 143070 || Loss: 0.8870 || timer: 0.0886 sec.
iter 143080 || Loss: 1.0414 || timer: 0.0891 sec.
iter 143090 || Loss: 1.5246 || timer: 0.0883 sec.
iter 143100 || Loss: 0.6613 || timer: 0.1295 sec.
iter 143110 || Loss: 1.2278 || timer: 0.0195 sec.
iter 143120 || Loss: 0.8055 || timer: 0.0901 sec.
iter 143130 || Loss: 0.9073 || timer: 0.0884 sec.
iter 143140 || Loss: 0.7429 || timer: 0.0896 sec.
iter 143150 || Loss: 1.0374 || timer: 0.0838 sec.
iter 143160 || Loss: 1.2866 || timer: 0.0903 sec.
iter 143170 || Loss: 1.3350 || timer: 0.0918 sec.
iter 143180 || Loss: 0.8675 || timer: 0.0923 sec.
iter 143190 || Loss: 1.3608 || timer: 0.0880 sec.
iter 143200 || Loss: 0.7942 || timer: 0.0820 sec.
iter 143210 || Loss: 0.9327 || timer: 0.0915 sec.
iter 143220 || Loss: 1.4498 || timer: 0.0903 sec.
iter 143230 || Loss: 1.1955 || timer: 0.1093 sec.
iter 143240 || Loss: 0.7912 || timer: 0.0867 sec.
iter 143250 || Loss: 0.6998 || timer: 0.0878 sec.
iter 143260 || Loss: 1.0953 || timer: 0.0921 sec.
iter 143270 || Loss: 0.9251 || timer: 0.1111 sec.
iter 143280 || Loss: 0.9745 || timer: 0.0870 sec.
iter 143290 || Loss: 1.4865 || timer: 0.0892 sec.
iter 143300 || Loss: 0.9848 || timer: 0.0905 sec.
iter 143310 || Loss: 0.6976 || timer: 0.0766 sec.
iter 143320 || Loss: 0.6224 || timer: 0.0855 sec.
iter 143330 || Loss: 0.8562 || timer: 0.0836 sec.
iter 143340 || Loss: 0.7838 || timer: 0.0924 sec.
iter 143350 || Loss: 1.0500 || timer: 0.0887 sec.
iter 143360 || Loss: 1.0812 || timer: 0.1016 sec.
iter 143370 || Loss: 1.2750 || timer: 0.1017 sec.
iter 143380 || Loss: 0.9826 || timer: 0.0883 sec.
iter 143390 || Loss: 0.7674 || timer: 0.0955 sec.
iter 143400 || Loss: 1.0913 || timer: 0.0892 sec.
iter 143410 || Loss: 0.7769 || timer: 0.0863 sec.
iter 143420 || Loss: 1.0791 || timer: 0.0907 sec.
iter 143430 || Loss: 0.8087 || timer: 0.0905 sec.
iter 143440 || Loss: 1.0235 || timer: 0.0312 sec.
iter 143450 || Loss: 0.4260 || timer: 0.0825 sec.
iter 143460 || Loss: 0.8740 || timer: 0.0905 sec.
iter 143470 || Loss: 1.0425 || timer: 0.0839 sec.
iter 143480 || Loss: 1.0675 || timer: 0.0906 sec.
iter 143490 || Loss: 0.8150 || timer: 0.0871 sec.
iter 143500 || Loss: 0.9659 || timer: 0.1078 sec.
iter 143510 || Loss: 1.0774 || timer: 0.0919 sec.
iter 143520 || Loss: 1.0838 || timer: 0.0989 sec.
iter 143530 || Loss: 0.6021 || timer: 0.0933 sec.
iter 143540 || Loss: 0.8113 || timer: 0.1104 sec.
iter 143550 || Loss: 1.0996 || timer: 0.0899 sec.
iter 143560 || Loss: 0.8040 || timer: 0.0829 sec.
iter 143570 || Loss: 1.1876 || timer: 0.0903 sec.
iter 143580 || Loss: 1.0700 || timer: 0.0829 sec.
iter 143590 || Loss: 0.8946 || timer: 0.0820 sec.
iter 143600 || Loss: 0.8753 || timer: 0.0823 sec.
iter 143610 || Loss: 0.7541 || timer: 0.0886 sec.
iter 143620 || Loss: 0.9441 || timer: 0.0906 sec.
iter 143630 || Loss: 1.0387 || timer: 0.0908 sec.
iter 143640 || Loss: 1.6673 || timer: 0.1076 sec.
iter 143650 || Loss: 1.4262 || timer: 0.0896 sec.
iter 143660 || Loss: 0.8873 || timer: 0.0844 sec.
iter 143670 || Loss: 1.1004 || timer: 0.0908 sec.
iter 143680 || Loss: 1.2343 || timer: 0.0822 sec.
iter 143690 || Loss: 0.9224 || timer: 0.1044 sec.
iter 143700 || Loss: 1.0499 || timer: 0.0823 sec.
iter 143710 || Loss: 0.6930 || timer: 0.1028 sec.
iter 143720 || Loss: 0.8965 || timer: 0.1035 sec.
iter 143730 || Loss: 0.9534 || timer: 0.0999 sec.
iter 143740 || Loss: 0.7946 || timer: 0.0825 sec.
iter 143750 || Loss: 0.7705 || timer: 0.0795 sec.
iter 143760 || Loss: 0.9907 || timer: 0.0838 sec.
iter 143770 || Loss: 1.1965 || timer: 0.0241 sec.
iter 143780 || Loss: 2.9422 || timer: 0.0841 sec.
iter 143790 || Loss: 0.6969 || timer: 0.0839 sec.
iter 143800 || Loss: 1.0715 || timer: 0.0879 sec.
iter 143810 || Loss: 0.9334 || timer: 0.0903 sec.
iter 143820 || Loss: 0.9502 || timer: 0.0912 sec.
iter 143830 || Loss: 0.8704 || timer: 0.1056 sec.
iter 143840 || Loss: 1.1430 || timer: 0.0817 sec.
iter 143850 || Loss: 0.9232 || timer: 0.0904 sec.
iter 143860 || Loss: 1.3905 || timer: 0.0903 sec.
iter 143870 || Loss: 1.0975 || timer: 0.1150 sec.
iter 143880 || Loss: 0.9110 || timer: 0.0887 sec.
iter 143890 || Loss: 1.2565 || timer: 0.0818 sec.
iter 143900 || Loss: 1.3109 || timer: 0.0859 sec.
iter 143910 || Loss: 1.1230 || timer: 0.0863 sec.
iter 143920 || Loss: 0.8691 || timer: 0.0782 sec.
iter 143930 || Loss: 1.0758 || timer: 0.0742 sec.
iter 143940 || Loss: 1.1789 || timer: 0.0925 sec.
iter 143950 || Loss: 1.0432 || timer: 0.0841 sec.
iter 143960 || Loss: 1.0515 || timer: 0.0940 sec.
iter 143970 || Loss: 0.9682 || timer: 0.0954 sec.
iter 143980 || Loss: 0.9937 || timer: 0.1292 sec.
iter 143990 || Loss: 1.0271 || timer: 0.0837 sec.
iter 144000 || Loss: 1.3654 || timer: 0.0907 sec.
iter 144010 || Loss: 1.2382 || timer: 0.0900 sec.
iter 144020 || Loss: 1.0062 || timer: 0.1214 sec.
iter 144030 || Loss: 1.0152 || timer: 0.0912 sec.
iter 144040 || Loss: 1.1402 || timer: 0.0847 sec.
iter 144050 || Loss: 1.0988 || timer: 0.1057 sec.
iter 144060 || Loss: 0.8848 || timer: 0.0901 sec.
iter 144070 || Loss: 0.8627 || timer: 0.0881 sec.
iter 144080 || Loss: 0.8609 || timer: 0.0830 sec.
iter 144090 || Loss: 1.0186 || timer: 0.0823 sec.
iter 144100 || Loss: 0.9618 || timer: 0.0180 sec.
iter 144110 || Loss: 5.4058 || timer: 0.0917 sec.
iter 144120 || Loss: 1.5480 || timer: 0.0891 sec.
iter 144130 || Loss: 1.7273 || timer: 0.0900 sec.
iter 144140 || Loss: 1.1667 || timer: 0.0817 sec.
iter 144150 || Loss: 1.1781 || timer: 0.0901 sec.
iter 144160 || Loss: 1.0866 || timer: 0.0957 sec.
iter 144170 || Loss: 1.0912 || timer: 0.0881 sec.
iter 144180 || Loss: 1.0773 || timer: 0.0882 sec.
iter 144190 || Loss: 0.7987 || timer: 0.0827 sec.
iter 144200 || Loss: 0.9961 || timer: 0.0931 sec.
iter 144210 || Loss: 0.8694 || timer: 0.1033 sec.
iter 144220 || Loss: 0.9165 || timer: 0.0819 sec.
iter 144230 || Loss: 0.9623 || timer: 0.0907 sec.
iter 144240 || Loss: 0.9134 || timer: 0.0901 sec.
iter 144250 || Loss: 1.2864 || timer: 0.0875 sec.
iter 144260 || Loss: 0.5899 || timer: 0.0808 sec.
iter 144270 || Loss: 1.0656 || timer: 0.0967 sec.
iter 144280 || Loss: 0.9650 || timer: 0.0901 sec.
iter 144290 || Loss: 1.1869 || timer: 0.0896 sec.
iter 144300 || Loss: 0.9142 || timer: 0.0829 sec.
iter 144310 || Loss: 1.0810 || timer: 0.0824 sec.
iter 144320 || Loss: 1.0594 || timer: 0.0821 sec.
iter 144330 || Loss: 0.9407 || timer: 0.0819 sec.
iter 144340 || Loss: 0.7798 || timer: 0.0914 sec.
iter 144350 || Loss: 0.9692 || timer: 0.0969 sec.
iter 144360 || Loss: 1.3062 || timer: 0.0900 sec.
iter 144370 || Loss: 1.0559 || timer: 0.0813 sec.
iter 144380 || Loss: 0.9880 || timer: 0.0827 sec.
iter 144390 || Loss: 0.8660 || timer: 0.0853 sec.
iter 144400 || Loss: 0.7665 || timer: 0.0905 sec.
iter 144410 || Loss: 1.1176 || timer: 0.0904 sec.
iter 144420 || Loss: 1.0972 || timer: 0.0903 sec.
iter 144430 || Loss: 0.9236 || timer: 0.0188 sec.
iter 144440 || Loss: 2.3563 || timer: 0.0828 sec.
iter 144450 || Loss: 1.1724 || timer: 0.0892 sec.
iter 144460 || Loss: 1.0604 || timer: 0.1145 sec.
iter 144470 || Loss: 0.9440 || timer: 0.0882 sec.
iter 144480 || Loss: 0.9572 || timer: 0.0923 sec.
iter 144490 || Loss: 0.8516 || timer: 0.0827 sec.
iter 144500 || Loss: 1.2985 || timer: 0.0873 sec.
iter 144510 || Loss: 1.0145 || timer: 0.0901 sec.
iter 144520 || Loss: 0.8821 || timer: 0.0891 sec.
iter 144530 || Loss: 0.8995 || timer: 0.1261 sec.
iter 144540 || Loss: 1.0302 || timer: 0.0912 sec.
iter 144550 || Loss: 2.7634 || timer: 0.0934 sec.
iter 144560 || Loss: 1.8878 || timer: 0.1003 sec.
iter 144570 || Loss: 1.3543 || timer: 0.0870 sec.
iter 144580 || Loss: 1.3681 || timer: 0.0840 sec.
iter 144590 || Loss: 0.8501 || timer: 0.1118 sec.
iter 144600 || Loss: 0.9369 || timer: 0.0822 sec.
iter 144610 || Loss: 1.0392 || timer: 0.0907 sec.
iter 144620 || Loss: 1.0286 || timer: 0.0829 sec.
iter 144630 || Loss: 1.1528 || timer: 0.0825 sec.
iter 144640 || Loss: 0.8075 || timer: 0.1039 sec.
iter 144650 || Loss: 1.1773 || timer: 0.0866 sec.
iter 144660 || Loss: 0.9425 || timer: 0.0887 sec.
iter 144670 || Loss: 1.0907 || timer: 0.0911 sec.
iter 144680 || Loss: 1.0693 || timer: 0.0830 sec.
iter 144690 || Loss: 0.7785 || timer: 0.0825 sec.
iter 144700 || Loss: 0.9369 || timer: 0.0932 sec.
iter 144710 || Loss: 1.1127 || timer: 0.0825 sec.
iter 144720 || Loss: 0.7505 || timer: 0.0811 sec.
iter 144730 || Loss: 1.1165 || timer: 0.0897 sec.
iter 144740 || Loss: 1.4232 || timer: 0.0840 sec.
iter 144750 || Loss: 1.0930 || timer: 0.0926 sec.
iter 144760 || Loss: 1.1254 || timer: 0.0161 sec.
iter 144770 || Loss: 1.3079 || timer: 0.0819 sec.
iter 144780 || Loss: 0.9919 || timer: 0.0820 sec.
iter 144790 || Loss: 1.1704 || timer: 0.0881 sec.
iter 144800 || Loss: 0.7930 || timer: 0.0831 sec.
iter 144810 || Loss: 0.8584 || timer: 0.0874 sec.
iter 144820 || Loss: 0.9620 || timer: 0.0826 sec.
iter 144830 || Loss: 1.0178 || timer: 0.0922 sec.
iter 144840 || Loss: 1.0781 || timer: 0.0825 sec.
iter 144850 || Loss: 1.0429 || timer: 0.0925 sec.
iter 144860 || Loss: 0.9170 || timer: 0.0993 sec.
iter 144870 || Loss: 1.0081 || timer: 0.1019 sec.
iter 144880 || Loss: 1.0438 || timer: 0.0869 sec.
iter 144890 || Loss: 0.9622 || timer: 0.0826 sec.
iter 144900 || Loss: 1.0016 || timer: 0.0835 sec.
iter 144910 || Loss: 1.3817 || timer: 0.0823 sec.
iter 144920 || Loss: 0.8509 || timer: 0.0910 sec.
iter 144930 || Loss: 0.8266 || timer: 0.0884 sec.
iter 144940 || Loss: 0.8840 || timer: 0.0869 sec.
iter 144950 || Loss: 1.3307 || timer: 0.0882 sec.
iter 144960 || Loss: 0.9810 || timer: 0.0833 sec.
iter 144970 || Loss: 1.0925 || timer: 0.0824 sec.
iter 144980 || Loss: 0.5938 || timer: 0.0897 sec.
iter 144990 || Loss: 1.1290 || timer: 0.0797 sec.
iter 145000 || Loss: 1.1583 || Saving state, iter: 145000
timer: 0.0813 sec.
iter 145010 || Loss: 1.1483 || timer: 0.0892 sec.
iter 145020 || Loss: 0.6899 || timer: 0.0830 sec.
iter 145030 || Loss: 0.7236 || timer: 0.0890 sec.
iter 145040 || Loss: 1.1123 || timer: 0.0963 sec.
iter 145050 || Loss: 1.0166 || timer: 0.0816 sec.
iter 145060 || Loss: 0.9516 || timer: 0.1122 sec.
iter 145070 || Loss: 0.8464 || timer: 0.0961 sec.
iter 145080 || Loss: 1.0547 || timer: 0.0849 sec.
iter 145090 || Loss: 1.0593 || timer: 0.0196 sec.
iter 145100 || Loss: 2.9458 || timer: 0.0980 sec.
iter 145110 || Loss: 0.9363 || timer: 0.1037 sec.
iter 145120 || Loss: 1.1350 || timer: 0.0830 sec.
iter 145130 || Loss: 0.9508 || timer: 0.0816 sec.
iter 145140 || Loss: 1.2057 || timer: 0.1257 sec.
iter 145150 || Loss: 1.0303 || timer: 0.0799 sec.
iter 145160 || Loss: 0.9105 || timer: 0.0860 sec.
iter 145170 || Loss: 1.5993 || timer: 0.1049 sec.
iter 145180 || Loss: 1.0985 || timer: 0.0859 sec.
iter 145190 || Loss: 0.9258 || timer: 0.1113 sec.
iter 145200 || Loss: 0.9012 || timer: 0.1028 sec.
iter 145210 || Loss: 0.8684 || timer: 0.0817 sec.
iter 145220 || Loss: 0.7631 || timer: 0.0817 sec.
iter 145230 || Loss: 0.6447 || timer: 0.0919 sec.
iter 145240 || Loss: 1.1635 || timer: 0.0912 sec.
iter 145250 || Loss: 1.2693 || timer: 0.0820 sec.
iter 145260 || Loss: 0.9134 || timer: 0.0896 sec.
iter 145270 || Loss: 1.1027 || timer: 0.1124 sec.
iter 145280 || Loss: 0.7721 || timer: 0.0900 sec.
iter 145290 || Loss: 1.0010 || timer: 0.0956 sec.
iter 145300 || Loss: 1.0413 || timer: 0.1176 sec.
iter 145310 || Loss: 0.6694 || timer: 0.0909 sec.
iter 145320 || Loss: 0.7674 || timer: 0.0899 sec.
iter 145330 || Loss: 0.8606 || timer: 0.1026 sec.
iter 145340 || Loss: 0.8456 || timer: 0.0946 sec.
iter 145350 || Loss: 0.8565 || timer: 0.0837 sec.
iter 145360 || Loss: 1.1100 || timer: 0.1091 sec.
iter 145370 || Loss: 1.1696 || timer: 0.1000 sec.
iter 145380 || Loss: 1.0764 || timer: 0.0906 sec.
iter 145390 || Loss: 1.0700 || timer: 0.0889 sec.
iter 145400 || Loss: 0.7138 || timer: 0.1041 sec.
iter 145410 || Loss: 1.0084 || timer: 0.1281 sec.
iter 145420 || Loss: 1.0050 || timer: 0.0152 sec.
iter 145430 || Loss: 0.4559 || timer: 0.0906 sec.
iter 145440 || Loss: 0.9401 || timer: 0.0826 sec.
iter 145450 || Loss: 0.9234 || timer: 0.0821 sec.
iter 145460 || Loss: 0.8745 || timer: 0.1017 sec.
iter 145470 || Loss: 1.0760 || timer: 0.0959 sec.
iter 145480 || Loss: 0.8818 || timer: 0.0925 sec.
iter 145490 || Loss: 0.8927 || timer: 0.0915 sec.
iter 145500 || Loss: 1.0998 || timer: 0.1019 sec.
iter 145510 || Loss: 0.9519 || timer: 0.0841 sec.
iter 145520 || Loss: 0.9974 || timer: 0.1109 sec.
iter 145530 || Loss: 0.6775 || timer: 0.0926 sec.
iter 145540 || Loss: 1.0817 || timer: 0.0909 sec.
iter 145550 || Loss: 0.9856 || timer: 0.0888 sec.
iter 145560 || Loss: 0.9646 || timer: 0.0836 sec.
iter 145570 || Loss: 1.0110 || timer: 0.0912 sec.
iter 145580 || Loss: 1.0261 || timer: 0.1022 sec.
iter 145590 || Loss: 0.9760 || timer: 0.1393 sec.
iter 145600 || Loss: 0.9652 || timer: 0.0887 sec.
iter 145610 || Loss: 0.9383 || timer: 0.0892 sec.
iter 145620 || Loss: 1.1217 || timer: 0.0806 sec.
iter 145630 || Loss: 1.0596 || timer: 0.0839 sec.
iter 145640 || Loss: 1.1699 || timer: 0.0820 sec.
iter 145650 || Loss: 0.8978 || timer: 0.0923 sec.
iter 145660 || Loss: 1.1049 || timer: 0.1121 sec.
iter 145670 || Loss: 0.8824 || timer: 0.0863 sec.
iter 145680 || Loss: 0.9724 || timer: 0.0883 sec.
iter 145690 || Loss: 0.9549 || timer: 0.0849 sec.
iter 145700 || Loss: 1.0892 || timer: 0.1016 sec.
iter 145710 || Loss: 0.9056 || timer: 0.0924 sec.
iter 145720 || Loss: 0.9055 || timer: 0.1046 sec.
iter 145730 || Loss: 0.9199 || timer: 0.0814 sec.
iter 145740 || Loss: 1.1948 || timer: 0.0889 sec.
iter 145750 || Loss: 0.9144 || timer: 0.0238 sec.
iter 145760 || Loss: 1.3063 || timer: 0.0919 sec.
iter 145770 || Loss: 0.9532 || timer: 0.0913 sec.
iter 145780 || Loss: 0.7077 || timer: 0.0942 sec.
iter 145790 || Loss: 1.0188 || timer: 0.0940 sec.
iter 145800 || Loss: 1.0522 || timer: 0.0827 sec.
iter 145810 || Loss: 0.7640 || timer: 0.0838 sec.
iter 145820 || Loss: 0.8943 || timer: 0.1040 sec.
iter 145830 || Loss: 0.9562 || timer: 0.0794 sec.
iter 145840 || Loss: 1.1254 || timer: 0.0822 sec.
iter 145850 || Loss: 0.9773 || timer: 0.1010 sec.
iter 145860 || Loss: 0.9277 || timer: 0.1022 sec.
iter 145870 || Loss: 0.7718 || timer: 0.0963 sec.
iter 145880 || Loss: 1.2142 || timer: 0.0837 sec.
iter 145890 || Loss: 0.8615 || timer: 0.1024 sec.
iter 145900 || Loss: 0.7150 || timer: 0.1042 sec.
iter 145910 || Loss: 1.1812 || timer: 0.0914 sec.
iter 145920 || Loss: 0.8316 || timer: 0.1141 sec.
iter 145930 || Loss: 0.9820 || timer: 0.0770 sec.
iter 145940 || Loss: 1.4440 || timer: 0.1008 sec.
iter 145950 || Loss: 0.8003 || timer: 0.0817 sec.
iter 145960 || Loss: 0.9022 || timer: 0.0847 sec.
iter 145970 || Loss: 0.6528 || timer: 0.0839 sec.
iter 145980 || Loss: 0.9563 || timer: 0.0846 sec.
iter 145990 || Loss: 0.8906 || timer: 0.0927 sec.
iter 146000 || Loss: 1.0407 || timer: 0.0925 sec.
iter 146010 || Loss: 1.0037 || timer: 0.0861 sec.
iter 146020 || Loss: 0.9774 || timer: 0.0894 sec.
iter 146030 || Loss: 1.1621 || timer: 0.0937 sec.
iter 146040 || Loss: 0.9261 || timer: 0.1131 sec.
iter 146050 || Loss: 0.8271 || timer: 0.1063 sec.
iter 146060 || Loss: 1.1072 || timer: 0.1102 sec.
iter 146070 || Loss: 1.3254 || timer: 0.0945 sec.
iter 146080 || Loss: 0.9480 || timer: 0.0203 sec.
iter 146090 || Loss: 0.5115 || timer: 0.0923 sec.
iter 146100 || Loss: 1.3355 || timer: 0.0989 sec.
iter 146110 || Loss: 1.1292 || timer: 0.0911 sec.
iter 146120 || Loss: 1.0212 || timer: 0.1062 sec.
iter 146130 || Loss: 0.8606 || timer: 0.0839 sec.
iter 146140 || Loss: 1.0194 || timer: 0.0848 sec.
iter 146150 || Loss: 1.1038 || timer: 0.0928 sec.
iter 146160 || Loss: 1.1210 || timer: 0.0777 sec.
iter 146170 || Loss: 0.8930 || timer: 0.1020 sec.
iter 146180 || Loss: 0.8795 || timer: 0.0928 sec.
iter 146190 || Loss: 1.0077 || timer: 0.0900 sec.
iter 146200 || Loss: 0.7540 || timer: 0.0933 sec.
iter 146210 || Loss: 0.7548 || timer: 0.0851 sec.
iter 146220 || Loss: 0.8100 || timer: 0.0822 sec.
iter 146230 || Loss: 1.0731 || timer: 0.0847 sec.
iter 146240 || Loss: 0.8175 || timer: 0.1056 sec.
iter 146250 || Loss: 1.0866 || timer: 0.0941 sec.
iter 146260 || Loss: 1.1505 || timer: 0.0904 sec.
iter 146270 || Loss: 1.0482 || timer: 0.0868 sec.
iter 146280 || Loss: 0.9323 || timer: 0.0827 sec.
iter 146290 || Loss: 0.9790 || timer: 0.0754 sec.
iter 146300 || Loss: 1.3779 || timer: 0.1014 sec.
iter 146310 || Loss: 1.1454 || timer: 0.0818 sec.
iter 146320 || Loss: 0.9898 || timer: 0.0838 sec.
iter 146330 || Loss: 1.1626 || timer: 0.0846 sec.
iter 146340 || Loss: 1.4110 || timer: 0.1065 sec.
iter 146350 || Loss: 0.7975 || timer: 0.0877 sec.
iter 146360 || Loss: 1.2322 || timer: 0.0835 sec.
iter 146370 || Loss: 1.3600 || timer: 0.0902 sec.
iter 146380 || Loss: 0.9049 || timer: 0.0826 sec.
iter 146390 || Loss: 0.8528 || timer: 0.0923 sec.
iter 146400 || Loss: 1.2716 || timer: 0.0883 sec.
iter 146410 || Loss: 0.9166 || timer: 0.0192 sec.
iter 146420 || Loss: 0.1648 || timer: 0.0942 sec.
iter 146430 || Loss: 0.8960 || timer: 0.0909 sec.
iter 146440 || Loss: 1.0503 || timer: 0.1098 sec.
iter 146450 || Loss: 0.9658 || timer: 0.0884 sec.
iter 146460 || Loss: 1.0065 || timer: 0.0759 sec.
iter 146470 || Loss: 0.7980 || timer: 0.0749 sec.
iter 146480 || Loss: 0.8627 || timer: 0.1048 sec.
iter 146490 || Loss: 1.5523 || timer: 0.0903 sec.
iter 146500 || Loss: 1.1437 || timer: 0.0881 sec.
iter 146510 || Loss: 0.9249 || timer: 0.0948 sec.
iter 146520 || Loss: 1.0965 || timer: 0.0854 sec.
iter 146530 || Loss: 1.0649 || timer: 0.0895 sec.
iter 146540 || Loss: 1.0235 || timer: 0.0885 sec.
iter 146550 || Loss: 0.8646 || timer: 0.0828 sec.
iter 146560 || Loss: 0.7810 || timer: 0.0832 sec.
iter 146570 || Loss: 1.2135 || timer: 0.0839 sec.
iter 146580 || Loss: 1.0344 || timer: 0.0944 sec.
iter 146590 || Loss: 0.8209 || timer: 0.0965 sec.
iter 146600 || Loss: 0.6492 || timer: 0.0846 sec.
iter 146610 || Loss: 0.8247 || timer: 0.0841 sec.
iter 146620 || Loss: 1.2283 || timer: 0.0918 sec.
iter 146630 || Loss: 0.8734 || timer: 0.0905 sec.
iter 146640 || Loss: 0.9557 || timer: 0.0833 sec.
iter 146650 || Loss: 1.0375 || timer: 0.0906 sec.
iter 146660 || Loss: 1.1871 || timer: 0.0921 sec.
iter 146670 || Loss: 1.1037 || timer: 0.1030 sec.
iter 146680 || Loss: 0.8664 || timer: 0.0927 sec.
iter 146690 || Loss: 1.0364 || timer: 0.0846 sec.
iter 146700 || Loss: 0.7220 || timer: 0.0769 sec.
iter 146710 || Loss: 0.6866 || timer: 0.0909 sec.
iter 146720 || Loss: 0.9325 || timer: 0.0916 sec.
iter 146730 || Loss: 1.0560 || timer: 0.0848 sec.
iter 146740 || Loss: 0.7965 || timer: 0.0229 sec.
iter 146750 || Loss: 1.7985 || timer: 0.0917 sec.
iter 146760 || Loss: 1.0128 || timer: 0.0832 sec.
iter 146770 || Loss: 0.8268 || timer: 0.0822 sec.
iter 146780 || Loss: 0.9693 || timer: 0.1054 sec.
iter 146790 || Loss: 0.7754 || timer: 0.0878 sec.
iter 146800 || Loss: 0.9697 || timer: 0.0829 sec.
iter 146810 || Loss: 1.1721 || timer: 0.0859 sec.
iter 146820 || Loss: 0.8993 || timer: 0.1176 sec.
iter 146830 || Loss: 1.3816 || timer: 0.0831 sec.
iter 146840 || Loss: 1.1362 || timer: 0.0889 sec.
iter 146850 || Loss: 0.8207 || timer: 0.0904 sec.
iter 146860 || Loss: 1.3117 || timer: 0.0911 sec.
iter 146870 || Loss: 1.0286 || timer: 0.0895 sec.
iter 146880 || Loss: 1.1718 || timer: 0.0756 sec.
iter 146890 || Loss: 0.8999 || timer: 0.0863 sec.
iter 146900 || Loss: 0.8283 || timer: 0.0876 sec.
iter 146910 || Loss: 1.0757 || timer: 0.0841 sec.
iter 146920 || Loss: 0.7805 || timer: 0.0750 sec.
iter 146930 || Loss: 1.1747 || timer: 0.0994 sec.
iter 146940 || Loss: 0.8440 || timer: 0.0855 sec.
iter 146950 || Loss: 0.8020 || timer: 0.1134 sec.
iter 146960 || Loss: 0.7483 || timer: 0.1048 sec.
iter 146970 || Loss: 1.1003 || timer: 0.0848 sec.
iter 146980 || Loss: 0.8033 || timer: 0.0886 sec.
iter 146990 || Loss: 0.8557 || timer: 0.0895 sec.
iter 147000 || Loss: 0.9275 || timer: 0.0833 sec.
iter 147010 || Loss: 1.1045 || timer: 0.0822 sec.
iter 147020 || Loss: 0.7781 || timer: 0.0824 sec.
iter 147030 || Loss: 1.1209 || timer: 0.0888 sec.
iter 147040 || Loss: 1.1246 || timer: 0.0869 sec.
iter 147050 || Loss: 0.9262 || timer: 0.0880 sec.
iter 147060 || Loss: 0.8511 || timer: 0.1026 sec.
iter 147070 || Loss: 1.0448 || timer: 0.0169 sec.
iter 147080 || Loss: 1.2107 || timer: 0.0896 sec.
iter 147090 || Loss: 0.7101 || timer: 0.1028 sec.
iter 147100 || Loss: 0.8265 || timer: 0.0868 sec.
iter 147110 || Loss: 0.6867 || timer: 0.0825 sec.
iter 147120 || Loss: 0.7847 || timer: 0.0890 sec.
iter 147130 || Loss: 1.0637 || timer: 0.0877 sec.
iter 147140 || Loss: 1.0046 || timer: 0.0854 sec.
iter 147150 || Loss: 0.8739 || timer: 0.1144 sec.
iter 147160 || Loss: 1.1094 || timer: 0.0830 sec.
iter 147170 || Loss: 1.1318 || timer: 0.1131 sec.
iter 147180 || Loss: 0.8271 || timer: 0.1057 sec.
iter 147190 || Loss: 0.6834 || timer: 0.0851 sec.
iter 147200 || Loss: 0.9484 || timer: 0.0888 sec.
iter 147210 || Loss: 0.9409 || timer: 0.0829 sec.
iter 147220 || Loss: 1.0044 || timer: 0.0815 sec.
iter 147230 || Loss: 1.0593 || timer: 0.0953 sec.
iter 147240 || Loss: 0.9403 || timer: 0.0916 sec.
iter 147250 || Loss: 1.1473 || timer: 0.0915 sec.
iter 147260 || Loss: 0.9634 || timer: 0.0827 sec.
iter 147270 || Loss: 1.0002 || timer: 0.0826 sec.
iter 147280 || Loss: 0.7766 || timer: 0.1209 sec.
iter 147290 || Loss: 0.8375 || timer: 0.0832 sec.
iter 147300 || Loss: 1.2262 || timer: 0.0811 sec.
iter 147310 || Loss: 0.9129 || timer: 0.0824 sec.
iter 147320 || Loss: 0.9966 || timer: 0.0823 sec.
iter 147330 || Loss: 1.2718 || timer: 0.0859 sec.
iter 147340 || Loss: 0.8935 || timer: 0.0824 sec.
iter 147350 || Loss: 1.0279 || timer: 0.0822 sec.
iter 147360 || Loss: 1.0543 || timer: 0.1035 sec.
iter 147370 || Loss: 1.0522 || timer: 0.0856 sec.
iter 147380 || Loss: 0.9092 || timer: 0.1035 sec.
iter 147390 || Loss: 1.1045 || timer: 0.0866 sec.
iter 147400 || Loss: 1.1991 || timer: 0.0239 sec.
iter 147410 || Loss: 0.2159 || timer: 0.1087 sec.
iter 147420 || Loss: 1.2996 || timer: 0.0904 sec.
iter 147430 || Loss: 0.7505 || timer: 0.0894 sec.
iter 147440 || Loss: 1.0107 || timer: 0.1061 sec.
iter 147450 || Loss: 1.0070 || timer: 0.0897 sec.
iter 147460 || Loss: 0.9472 || timer: 0.0902 sec.
iter 147470 || Loss: 0.7223 || timer: 0.0921 sec.
iter 147480 || Loss: 0.8181 || timer: 0.1126 sec.
iter 147490 || Loss: 0.7760 || timer: 0.0913 sec.
iter 147500 || Loss: 0.7200 || timer: 0.0966 sec.
iter 147510 || Loss: 1.0170 || timer: 0.0838 sec.
iter 147520 || Loss: 1.2618 || timer: 0.0857 sec.
iter 147530 || Loss: 0.8487 || timer: 0.0827 sec.
iter 147540 || Loss: 0.5816 || timer: 0.0932 sec.
iter 147550 || Loss: 1.2328 || timer: 0.0841 sec.
iter 147560 || Loss: 1.2076 || timer: 0.0918 sec.
iter 147570 || Loss: 0.7763 || timer: 0.0915 sec.
iter 147580 || Loss: 0.7044 || timer: 0.0935 sec.
iter 147590 || Loss: 0.6956 || timer: 0.0937 sec.
iter 147600 || Loss: 0.8279 || timer: 0.0776 sec.
iter 147610 || Loss: 0.7713 || timer: 0.1004 sec.
iter 147620 || Loss: 0.7799 || timer: 0.1003 sec.
iter 147630 || Loss: 0.7637 || timer: 0.0909 sec.
iter 147640 || Loss: 1.0474 || timer: 0.0912 sec.
iter 147650 || Loss: 1.0799 || timer: 0.0954 sec.
iter 147660 || Loss: 0.8388 || timer: 0.0908 sec.
iter 147670 || Loss: 1.0379 || timer: 0.0872 sec.
iter 147680 || Loss: 0.6199 || timer: 0.0922 sec.
iter 147690 || Loss: 1.1781 || timer: 0.0905 sec.
iter 147700 || Loss: 1.0045 || timer: 0.0961 sec.
iter 147710 || Loss: 0.6620 || timer: 0.0901 sec.
iter 147720 || Loss: 1.3093 || timer: 0.1350 sec.
iter 147730 || Loss: 1.1557 || timer: 0.0240 sec.
iter 147740 || Loss: 0.3307 || timer: 0.0959 sec.
iter 147750 || Loss: 0.7306 || timer: 0.0857 sec.
iter 147760 || Loss: 1.0557 || timer: 0.0920 sec.
iter 147770 || Loss: 1.0086 || timer: 0.0841 sec.
iter 147780 || Loss: 0.9413 || timer: 0.0825 sec.
iter 147790 || Loss: 1.3460 || timer: 0.0848 sec.
iter 147800 || Loss: 1.1718 || timer: 0.1028 sec.
iter 147810 || Loss: 0.9741 || timer: 0.0898 sec.
iter 147820 || Loss: 0.9694 || timer: 0.1042 sec.
iter 147830 || Loss: 0.8030 || timer: 0.1000 sec.
iter 147840 || Loss: 1.1823 || timer: 0.0882 sec.
iter 147850 || Loss: 1.2341 || timer: 0.0833 sec.
iter 147860 || Loss: 0.9825 || timer: 0.0819 sec.
iter 147870 || Loss: 1.0082 || timer: 0.0929 sec.
iter 147880 || Loss: 1.2097 || timer: 0.0890 sec.
iter 147890 || Loss: 1.0477 || timer: 0.0965 sec.
iter 147900 || Loss: 1.0794 || timer: 0.0903 sec.
iter 147910 || Loss: 1.0359 || timer: 0.0886 sec.
iter 147920 || Loss: 0.7442 || timer: 0.0926 sec.
iter 147930 || Loss: 0.6980 || timer: 0.0933 sec.
iter 147940 || Loss: 0.7302 || timer: 0.0885 sec.
iter 147950 || Loss: 0.9462 || timer: 0.0859 sec.
iter 147960 || Loss: 1.2397 || timer: 0.0900 sec.
iter 147970 || Loss: 0.7530 || timer: 0.0843 sec.
iter 147980 || Loss: 1.1704 || timer: 0.0912 sec.
iter 147990 || Loss: 0.8048 || timer: 0.1060 sec.
iter 148000 || Loss: 0.9513 || timer: 0.0844 sec.
iter 148010 || Loss: 0.9934 || timer: 0.0888 sec.
iter 148020 || Loss: 1.2227 || timer: 0.0825 sec.
iter 148030 || Loss: 0.7037 || timer: 0.0875 sec.
iter 148040 || Loss: 0.9183 || timer: 0.0911 sec.
iter 148050 || Loss: 0.7171 || timer: 0.0915 sec.
iter 148060 || Loss: 0.9479 || timer: 0.0247 sec.
iter 148070 || Loss: 0.9589 || timer: 0.0915 sec.
iter 148080 || Loss: 0.8247 || timer: 0.0911 sec.
iter 148090 || Loss: 0.8259 || timer: 0.1107 sec.
iter 148100 || Loss: 0.8329 || timer: 0.1007 sec.
iter 148110 || Loss: 0.9343 || timer: 0.0904 sec.
iter 148120 || Loss: 0.7821 || timer: 0.0944 sec.
iter 148130 || Loss: 1.3133 || timer: 0.0875 sec.
iter 148140 || Loss: 0.8339 || timer: 0.0762 sec.
iter 148150 || Loss: 1.9149 || timer: 0.0843 sec.
iter 148160 || Loss: 1.0294 || timer: 0.0966 sec.
iter 148170 || Loss: 1.0151 || timer: 0.0940 sec.
iter 148180 || Loss: 0.9011 || timer: 0.0896 sec.
iter 148190 || Loss: 1.2526 || timer: 0.0895 sec.
iter 148200 || Loss: 0.8782 || timer: 0.0880 sec.
iter 148210 || Loss: 0.8037 || timer: 0.0899 sec.
iter 148220 || Loss: 0.9438 || timer: 0.1172 sec.
iter 148230 || Loss: 0.9559 || timer: 0.0865 sec.
iter 148240 || Loss: 1.0226 || timer: 0.0916 sec.
iter 148250 || Loss: 0.8085 || timer: 0.0916 sec.
iter 148260 || Loss: 1.0946 || timer: 0.1048 sec.
iter 148270 || Loss: 1.0401 || timer: 0.0838 sec.
iter 148280 || Loss: 1.4080 || timer: 0.0839 sec.
iter 148290 || Loss: 0.9376 || timer: 0.0827 sec.
iter 148300 || Loss: 0.8328 || timer: 0.0901 sec.
iter 148310 || Loss: 1.1363 || timer: 0.0924 sec.
iter 148320 || Loss: 1.1371 || timer: 0.0913 sec.
iter 148330 || Loss: 1.2156 || timer: 0.0910 sec.
iter 148340 || Loss: 1.2276 || timer: 0.0928 sec.
iter 148350 || Loss: 0.8353 || timer: 0.0767 sec.
iter 148360 || Loss: 0.9889 || timer: 0.0909 sec.
iter 148370 || Loss: 0.9602 || timer: 0.0841 sec.
iter 148380 || Loss: 1.0182 || timer: 0.0770 sec.
iter 148390 || Loss: 0.9719 || timer: 0.0178 sec.
iter 148400 || Loss: 0.3645 || timer: 0.0887 sec.
iter 148410 || Loss: 0.7882 || timer: 0.0890 sec.
iter 148420 || Loss: 0.9113 || timer: 0.0903 sec.
iter 148430 || Loss: 1.1240 || timer: 0.0804 sec.
iter 148440 || Loss: 1.1071 || timer: 0.0824 sec.
iter 148450 || Loss: 1.1174 || timer: 0.0857 sec.
iter 148460 || Loss: 1.0517 || timer: 0.0902 sec.
iter 148470 || Loss: 1.1908 || timer: 0.0862 sec.
iter 148480 || Loss: 1.0760 || timer: 0.1066 sec.
iter 148490 || Loss: 1.0151 || timer: 0.0963 sec.
iter 148500 || Loss: 1.0403 || timer: 0.0911 sec.
iter 148510 || Loss: 1.1074 || timer: 0.0836 sec.
iter 148520 || Loss: 1.0144 || timer: 0.0901 sec.
iter 148530 || Loss: 1.0028 || timer: 0.0917 sec.
iter 148540 || Loss: 1.1629 || timer: 0.0903 sec.
iter 148550 || Loss: 1.0854 || timer: 0.0864 sec.
iter 148560 || Loss: 0.8506 || timer: 0.0892 sec.
iter 148570 || Loss: 0.6351 || timer: 0.0869 sec.
iter 148580 || Loss: 0.8778 || timer: 0.0772 sec.
iter 148590 || Loss: 1.2412 || timer: 0.0821 sec.
iter 148600 || Loss: 1.2469 || timer: 0.0866 sec.
iter 148610 || Loss: 0.6656 || timer: 0.1068 sec.
iter 148620 || Loss: 0.9385 || timer: 0.0846 sec.
iter 148630 || Loss: 1.0488 || timer: 0.0892 sec.
iter 148640 || Loss: 1.2862 || timer: 0.0827 sec.
iter 148650 || Loss: 0.8897 || timer: 0.1025 sec.
iter 148660 || Loss: 0.8973 || timer: 0.1030 sec.
iter 148670 || Loss: 0.9237 || timer: 0.1105 sec.
iter 148680 || Loss: 1.0879 || timer: 0.0913 sec.
iter 148690 || Loss: 0.9768 || timer: 0.0851 sec.
iter 148700 || Loss: 0.9013 || timer: 0.0833 sec.
iter 148710 || Loss: 0.8166 || timer: 0.1090 sec.
iter 148720 || Loss: 0.7289 || timer: 0.0183 sec.
iter 148730 || Loss: 1.9596 || timer: 0.0889 sec.
iter 148740 || Loss: 1.2098 || timer: 0.0905 sec.
iter 148750 || Loss: 1.0367 || timer: 0.0892 sec.
iter 148760 || Loss: 0.6978 || timer: 0.0842 sec.
iter 148770 || Loss: 0.9192 || timer: 0.0957 sec.
iter 148780 || Loss: 0.6746 || timer: 0.0914 sec.
iter 148790 || Loss: 0.8993 || timer: 0.0844 sec.
iter 148800 || Loss: 1.0941 || timer: 0.0838 sec.
iter 148810 || Loss: 0.9812 || timer: 0.1110 sec.
iter 148820 || Loss: 0.9267 || timer: 0.0969 sec.
iter 148830 || Loss: 1.1073 || timer: 0.1212 sec.
iter 148840 || Loss: 0.9587 || timer: 0.0846 sec.
iter 148850 || Loss: 0.9543 || timer: 0.0876 sec.
iter 148860 || Loss: 0.9969 || timer: 0.0900 sec.
iter 148870 || Loss: 1.1626 || timer: 0.0979 sec.
iter 148880 || Loss: 1.2658 || timer: 0.0867 sec.
iter 148890 || Loss: 1.0931 || timer: 0.0919 sec.
iter 148900 || Loss: 1.2057 || timer: 0.1009 sec.
iter 148910 || Loss: 0.9929 || timer: 0.0879 sec.
iter 148920 || Loss: 0.9399 || timer: 0.0934 sec.
iter 148930 || Loss: 1.0942 || timer: 0.0820 sec.
iter 148940 || Loss: 1.1723 || timer: 0.0881 sec.
iter 148950 || Loss: 0.8777 || timer: 0.0820 sec.
iter 148960 || Loss: 1.6379 || timer: 0.1108 sec.
iter 148970 || Loss: 0.8699 || timer: 0.1047 sec.
iter 148980 || Loss: 0.8164 || timer: 0.0899 sec.
iter 148990 || Loss: 0.9767 || timer: 0.0767 sec.
iter 149000 || Loss: 0.7385 || timer: 0.0996 sec.
iter 149010 || Loss: 1.3020 || timer: 0.0926 sec.
iter 149020 || Loss: 0.8386 || timer: 0.0931 sec.
iter 149030 || Loss: 1.0436 || timer: 0.0847 sec.
iter 149040 || Loss: 0.7729 || timer: 0.0914 sec.
iter 149050 || Loss: 0.9658 || timer: 0.0225 sec.
iter 149060 || Loss: 0.5128 || timer: 0.0878 sec.
iter 149070 || Loss: 0.8328 || timer: 0.0980 sec.
iter 149080 || Loss: 0.9679 || timer: 0.0938 sec.
iter 149090 || Loss: 0.8283 || timer: 0.1413 sec.
iter 149100 || Loss: 1.1487 || timer: 0.0808 sec.
iter 149110 || Loss: 0.7430 || timer: 0.0940 sec.
iter 149120 || Loss: 1.1781 || timer: 0.0930 sec.
iter 149130 || Loss: 0.5850 || timer: 0.0837 sec.
iter 149140 || Loss: 0.9837 || timer: 0.0916 sec.
iter 149150 || Loss: 1.2631 || timer: 0.0898 sec.
iter 149160 || Loss: 0.7925 || timer: 0.0923 sec.
iter 149170 || Loss: 1.1200 || timer: 0.0953 sec.
iter 149180 || Loss: 0.8497 || timer: 0.1040 sec.
iter 149190 || Loss: 0.8214 || timer: 0.0913 sec.
iter 149200 || Loss: 1.1318 || timer: 0.0839 sec.
iter 149210 || Loss: 0.7649 || timer: 0.0920 sec.
iter 149220 || Loss: 0.9669 || timer: 0.0976 sec.
iter 149230 || Loss: 1.4910 || timer: 0.0888 sec.
iter 149240 || Loss: 1.0680 || timer: 0.0879 sec.
iter 149250 || Loss: 0.8671 || timer: 0.0826 sec.
iter 149260 || Loss: 1.0077 || timer: 0.0909 sec.
iter 149270 || Loss: 0.8433 || timer: 0.0810 sec.
iter 149280 || Loss: 1.3231 || timer: 0.0718 sec.
iter 149290 || Loss: 0.8911 || timer: 0.1152 sec.
iter 149300 || Loss: 1.7135 || timer: 0.0835 sec.
iter 149310 || Loss: 1.1307 || timer: 0.1032 sec.
iter 149320 || Loss: 0.8155 || timer: 0.0850 sec.
iter 149330 || Loss: 1.1682 || timer: 0.0930 sec.
iter 149340 || Loss: 0.9468 || timer: 0.1124 sec.
iter 149350 || Loss: 0.8755 || timer: 0.0853 sec.
iter 149360 || Loss: 0.8698 || timer: 0.0873 sec.
iter 149370 || Loss: 1.2293 || timer: 0.0816 sec.
iter 149380 || Loss: 1.0295 || timer: 0.0147 sec.
iter 149390 || Loss: 0.7181 || timer: 0.0885 sec.
iter 149400 || Loss: 1.0684 || timer: 0.0912 sec.
iter 149410 || Loss: 1.0857 || timer: 0.0848 sec.
iter 149420 || Loss: 1.0297 || timer: 0.0866 sec.
iter 149430 || Loss: 1.3019 || timer: 0.0867 sec.
iter 149440 || Loss: 0.9803 || timer: 0.1070 sec.
iter 149450 || Loss: 0.9579 || timer: 0.0834 sec.
iter 149460 || Loss: 1.3573 || timer: 0.0837 sec.
iter 149470 || Loss: 1.0351 || timer: 0.0915 sec.
iter 149480 || Loss: 1.2214 || timer: 0.1017 sec.
iter 149490 || Loss: 0.8818 || timer: 0.0910 sec.
iter 149500 || Loss: 0.9528 || timer: 0.0922 sec.
iter 149510 || Loss: 1.0618 || timer: 0.0893 sec.
iter 149520 || Loss: 1.2522 || timer: 0.0890 sec.
iter 149530 || Loss: 0.6506 || timer: 0.0877 sec.
iter 149540 || Loss: 0.7972 || timer: 0.1074 sec.
iter 149550 || Loss: 0.7855 || timer: 0.0832 sec.
iter 149560 || Loss: 1.1098 || timer: 0.0840 sec.
iter 149570 || Loss: 0.8617 || timer: 0.1052 sec.
iter 149580 || Loss: 0.8919 || timer: 0.0841 sec.
iter 149590 || Loss: 0.6989 || timer: 0.0830 sec.
iter 149600 || Loss: 1.0045 || timer: 0.0836 sec.
iter 149610 || Loss: 1.0963 || timer: 0.0901 sec.
iter 149620 || Loss: 0.8733 || timer: 0.0832 sec.
iter 149630 || Loss: 0.8797 || timer: 0.0768 sec.
iter 149640 || Loss: 0.9662 || timer: 0.0912 sec.
iter 149650 || Loss: 0.8304 || timer: 0.0840 sec.
iter 149660 || Loss: 1.4984 || timer: 0.1047 sec.
iter 149670 || Loss: 0.8374 || timer: 0.0936 sec.
iter 149680 || Loss: 0.7646 || timer: 0.0933 sec.
iter 149690 || Loss: 1.1778 || timer: 0.0934 sec.
iter 149700 || Loss: 1.1278 || timer: 0.0839 sec.
iter 149710 || Loss: 1.2737 || timer: 0.0255 sec.
iter 149720 || Loss: 0.8135 || timer: 0.0902 sec.
iter 149730 || Loss: 1.0887 || timer: 0.1043 sec.
iter 149740 || Loss: 0.9530 || timer: 0.0918 sec.
iter 149750 || Loss: 0.8091 || timer: 0.1048 sec.
iter 149760 || Loss: 0.9298 || timer: 0.1163 sec.
iter 149770 || Loss: 1.0691 || timer: 0.0907 sec.
iter 149780 || Loss: 1.3005 || timer: 0.1031 sec.
iter 149790 || Loss: 0.9694 || timer: 0.0979 sec.
iter 149800 || Loss: 1.2855 || timer: 0.0915 sec.
iter 149810 || Loss: 1.1058 || timer: 0.1188 sec.
iter 149820 || Loss: 0.9212 || timer: 0.0925 sec.
iter 149830 || Loss: 1.1985 || timer: 0.1084 sec.
iter 149840 || Loss: 1.0624 || timer: 0.1217 sec.
iter 149850 || Loss: 1.0175 || timer: 0.0773 sec.
iter 149860 || Loss: 0.9996 || timer: 0.0935 sec.
iter 149870 || Loss: 0.9458 || timer: 0.0844 sec.
iter 149880 || Loss: 0.9691 || timer: 0.0904 sec.
iter 149890 || Loss: 1.0240 || timer: 0.0839 sec.
iter 149900 || Loss: 0.9113 || timer: 0.0846 sec.
iter 149910 || Loss: 1.0482 || timer: 0.1056 sec.
iter 149920 || Loss: 0.7964 || timer: 0.0848 sec.
iter 149930 || Loss: 0.7917 || timer: 0.0980 sec.
iter 149940 || Loss: 0.8093 || timer: 0.0822 sec.
iter 149950 || Loss: 0.7116 || timer: 0.0853 sec.
iter 149960 || Loss: 1.0058 || timer: 0.0974 sec.
iter 149970 || Loss: 0.9479 || timer: 0.0823 sec.
iter 149980 || Loss: 1.0358 || timer: 0.1006 sec.
iter 149990 || Loss: 1.3187 || timer: 0.1119 sec.
iter 150000 || Loss: 1.4032 || Saving state, iter: 150000
timer: 0.1192 sec.
iter 150010 || Loss: 0.8056 || timer: 0.0913 sec.
iter 150020 || Loss: 1.0007 || timer: 0.1020 sec.
iter 150030 || Loss: 0.9987 || timer: 0.0888 sec.
iter 150040 || Loss: 0.8433 || timer: 0.0276 sec.
iter 150050 || Loss: 0.3401 || timer: 0.0968 sec.
iter 150060 || Loss: 0.7202 || timer: 0.0770 sec.
iter 150070 || Loss: 0.6640 || timer: 0.0822 sec.
iter 150080 || Loss: 0.9713 || timer: 0.1043 sec.
iter 150090 || Loss: 0.6452 || timer: 0.0956 sec.
iter 150100 || Loss: 1.0196 || timer: 0.0840 sec.
iter 150110 || Loss: 0.9803 || timer: 0.0848 sec.
iter 150120 || Loss: 1.1448 || timer: 0.0835 sec.
iter 150130 || Loss: 1.2201 || timer: 0.1029 sec.
iter 150140 || Loss: 1.2732 || timer: 0.0963 sec.
iter 150150 || Loss: 0.8279 || timer: 0.0839 sec.
iter 150160 || Loss: 0.9101 || timer: 0.0833 sec.
iter 150170 || Loss: 1.0294 || timer: 0.0910 sec.
iter 150180 || Loss: 1.1005 || timer: 0.0925 sec.
iter 150190 || Loss: 0.8515 || timer: 0.0851 sec.
iter 150200 || Loss: 1.0173 || timer: 0.0981 sec.
iter 150210 || Loss: 0.7490 || timer: 0.0965 sec.
iter 150220 || Loss: 1.0323 || timer: 0.0965 sec.
iter 150230 || Loss: 0.8981 || timer: 0.0836 sec.
iter 150240 || Loss: 0.7945 || timer: 0.0842 sec.
iter 150250 || Loss: 0.7518 || timer: 0.0929 sec.
iter 150260 || Loss: 0.6506 || timer: 0.0948 sec.
iter 150270 || Loss: 0.9540 || timer: 0.0794 sec.
iter 150280 || Loss: 0.7400 || timer: 0.0947 sec.
iter 150290 || Loss: 0.9048 || timer: 0.0885 sec.
iter 150300 || Loss: 0.8368 || timer: 0.0901 sec.
iter 150310 || Loss: 0.8733 || timer: 0.0946 sec.
iter 150320 || Loss: 0.8161 || timer: 0.1072 sec.
iter 150330 || Loss: 1.0105 || timer: 0.0827 sec.
iter 150340 || Loss: 1.1183 || timer: 0.0893 sec.
iter 150350 || Loss: 0.7582 || timer: 0.0888 sec.
iter 150360 || Loss: 0.8770 || timer: 0.0819 sec.
iter 150370 || Loss: 0.8992 || timer: 0.0149 sec.
iter 150380 || Loss: 1.2907 || timer: 0.0914 sec.
iter 150390 || Loss: 0.9011 || timer: 0.0900 sec.
iter 150400 || Loss: 0.8435 || timer: 0.0945 sec.
iter 150410 || Loss: 1.0698 || timer: 0.0941 sec.
iter 150420 || Loss: 0.8004 || timer: 0.0903 sec.
iter 150430 || Loss: 0.6822 || timer: 0.1051 sec.
iter 150440 || Loss: 1.1259 || timer: 0.0930 sec.
iter 150450 || Loss: 0.9751 || timer: 0.0934 sec.
iter 150460 || Loss: 1.1139 || timer: 0.0932 sec.
iter 150470 || Loss: 0.9057 || timer: 0.0999 sec.
iter 150480 || Loss: 1.1909 || timer: 0.0954 sec.
iter 150490 || Loss: 0.7881 || timer: 0.0860 sec.
iter 150500 || Loss: 0.9559 || timer: 0.0842 sec.
iter 150510 || Loss: 0.9597 || timer: 0.1321 sec.
iter 150520 || Loss: 0.9484 || timer: 0.0812 sec.
iter 150530 || Loss: 0.6925 || timer: 0.0841 sec.
iter 150540 || Loss: 0.6852 || timer: 0.0870 sec.
iter 150550 || Loss: 1.0822 || timer: 0.0898 sec.
iter 150560 || Loss: 0.8180 || timer: 0.0866 sec.
iter 150570 || Loss: 0.9150 || timer: 0.1047 sec.
iter 150580 || Loss: 1.1293 || timer: 0.0899 sec.
iter 150590 || Loss: 0.9279 || timer: 0.1062 sec.
iter 150600 || Loss: 0.8424 || timer: 0.0879 sec.
iter 150610 || Loss: 0.7942 || timer: 0.0868 sec.
iter 150620 || Loss: 0.8619 || timer: 0.0920 sec.
iter 150630 || Loss: 0.6426 || timer: 0.0957 sec.
iter 150640 || Loss: 0.8383 || timer: 0.0885 sec.
iter 150650 || Loss: 1.1748 || timer: 0.0981 sec.
iter 150660 || Loss: 1.2869 || timer: 0.0833 sec.
iter 150670 || Loss: 0.7634 || timer: 0.0915 sec.
iter 150680 || Loss: 1.2420 || timer: 0.0843 sec.
iter 150690 || Loss: 1.2120 || timer: 0.1024 sec.
iter 150700 || Loss: 1.0136 || timer: 0.0261 sec.
iter 150710 || Loss: 1.8429 || timer: 0.0888 sec.
iter 150720 || Loss: 1.1568 || timer: 0.0838 sec.
iter 150730 || Loss: 0.9057 || timer: 0.1101 sec.
iter 150740 || Loss: 1.0262 || timer: 0.0841 sec.
iter 150750 || Loss: 2.1943 || timer: 0.0982 sec.
iter 150760 || Loss: 1.9622 || timer: 0.0917 sec.
iter 150770 || Loss: 1.7508 || timer: 0.0840 sec.
iter 150780 || Loss: 1.9885 || timer: 0.1069 sec.
iter 150790 || Loss: 1.0690 || timer: 0.0910 sec.
iter 150800 || Loss: 0.9902 || timer: 0.0968 sec.
iter 150810 || Loss: 0.8146 || timer: 0.0910 sec.
iter 150820 || Loss: 1.3561 || timer: 0.1029 sec.
iter 150830 || Loss: 1.0330 || timer: 0.0889 sec.
iter 150840 || Loss: 1.7763 || timer: 0.0896 sec.
iter 150850 || Loss: 1.3143 || timer: 0.0907 sec.
iter 150860 || Loss: 1.2054 || timer: 0.0836 sec.
iter 150870 || Loss: 1.0869 || timer: 0.0834 sec.
iter 150880 || Loss: 1.0291 || timer: 0.0909 sec.
iter 150890 || Loss: 1.0445 || timer: 0.0902 sec.
iter 150900 || Loss: 1.0456 || timer: 0.0836 sec.
iter 150910 || Loss: 0.9116 || timer: 0.0842 sec.
iter 150920 || Loss: 1.2084 || timer: 0.0908 sec.
iter 150930 || Loss: 1.1916 || timer: 0.0890 sec.
iter 150940 || Loss: 0.8258 || timer: 0.0899 sec.
iter 150950 || Loss: 0.7975 || timer: 0.1008 sec.
iter 150960 || Loss: 1.0965 || timer: 0.0840 sec.
iter 150970 || Loss: 0.7291 || timer: 0.0776 sec.
iter 150980 || Loss: 0.7632 || timer: 0.0979 sec.
iter 150990 || Loss: 0.8874 || timer: 0.1206 sec.
iter 151000 || Loss: 0.9460 || timer: 0.0938 sec.
iter 151010 || Loss: 1.0632 || timer: 0.0929 sec.
iter 151020 || Loss: 0.7900 || timer: 0.0842 sec.
iter 151030 || Loss: 0.7505 || timer: 0.0189 sec.
iter 151040 || Loss: 0.3248 || timer: 0.0894 sec.
iter 151050 || Loss: 0.9908 || timer: 0.1083 sec.
iter 151060 || Loss: 0.7893 || timer: 0.0900 sec.
iter 151070 || Loss: 1.3956 || timer: 0.0842 sec.
iter 151080 || Loss: 1.5283 || timer: 0.0982 sec.
iter 151090 || Loss: 1.0519 || timer: 0.0838 sec.
iter 151100 || Loss: 0.8332 || timer: 0.0840 sec.
iter 151110 || Loss: 0.9274 || timer: 0.0844 sec.
iter 151120 || Loss: 0.9480 || timer: 0.0907 sec.
iter 151130 || Loss: 1.1674 || timer: 0.0897 sec.
iter 151140 || Loss: 0.7653 || timer: 0.0839 sec.
iter 151150 || Loss: 1.0717 || timer: 0.1026 sec.
iter 151160 || Loss: 1.1358 || timer: 0.0893 sec.
iter 151170 || Loss: 1.1003 || timer: 0.0891 sec.
iter 151180 || Loss: 0.7529 || timer: 0.0827 sec.
iter 151190 || Loss: 0.9343 || timer: 0.0895 sec.
iter 151200 || Loss: 0.7014 || timer: 0.1039 sec.
iter 151210 || Loss: 1.1214 || timer: 0.0913 sec.
iter 151220 || Loss: 0.8883 || timer: 0.0840 sec.
iter 151230 || Loss: 1.3713 || timer: 0.0851 sec.
iter 151240 || Loss: 0.8354 || timer: 0.0834 sec.
iter 151250 || Loss: 0.8738 || timer: 0.0909 sec.
iter 151260 || Loss: 0.7738 || timer: 0.0917 sec.
iter 151270 || Loss: 1.2536 || timer: 0.0835 sec.
iter 151280 || Loss: 0.7466 || timer: 0.0853 sec.
iter 151290 || Loss: 1.1700 || timer: 0.0923 sec.
iter 151300 || Loss: 1.0362 || timer: 0.0823 sec.
iter 151310 || Loss: 1.1508 || timer: 0.0750 sec.
iter 151320 || Loss: 0.9260 || timer: 0.1009 sec.
iter 151330 || Loss: 0.8683 || timer: 0.0861 sec.
iter 151340 || Loss: 0.9122 || timer: 0.0876 sec.
iter 151350 || Loss: 1.1032 || timer: 0.0837 sec.
iter 151360 || Loss: 1.0015 || timer: 0.0248 sec.
iter 151370 || Loss: 2.7116 || timer: 0.0904 sec.
iter 151380 || Loss: 0.6757 || timer: 0.0895 sec.
iter 151390 || Loss: 0.8891 || timer: 0.0845 sec.
iter 151400 || Loss: 0.7427 || timer: 0.0915 sec.
iter 151410 || Loss: 0.8863 || timer: 0.0902 sec.
iter 151420 || Loss: 0.8985 || timer: 0.1061 sec.
iter 151430 || Loss: 1.0014 || timer: 0.0844 sec.
iter 151440 || Loss: 0.8615 || timer: 0.0899 sec.
iter 151450 || Loss: 0.7108 || timer: 0.1121 sec.
iter 151460 || Loss: 0.8280 || timer: 0.0937 sec.
iter 151470 || Loss: 0.8776 || timer: 0.0933 sec.
iter 151480 || Loss: 0.7953 || timer: 0.0832 sec.
iter 151490 || Loss: 0.8389 || timer: 0.0850 sec.
iter 151500 || Loss: 0.8784 || timer: 0.0870 sec.
iter 151510 || Loss: 0.8548 || timer: 0.0961 sec.
iter 151520 || Loss: 1.1948 || timer: 0.0867 sec.
iter 151530 || Loss: 0.9734 || timer: 0.1019 sec.
iter 151540 || Loss: 1.1242 || timer: 0.0914 sec.
iter 151550 || Loss: 0.9702 || timer: 0.0828 sec.
iter 151560 || Loss: 0.9680 || timer: 0.0931 sec.
iter 151570 || Loss: 0.9835 || timer: 0.0888 sec.
iter 151580 || Loss: 0.8581 || timer: 0.0909 sec.
iter 151590 || Loss: 1.3586 || timer: 0.0868 sec.
iter 151600 || Loss: 0.7514 || timer: 0.0761 sec.
iter 151610 || Loss: 0.8704 || timer: 0.0947 sec.
iter 151620 || Loss: 0.6489 || timer: 0.0901 sec.
iter 151630 || Loss: 1.1610 || timer: 0.0897 sec.
iter 151640 || Loss: 0.8746 || timer: 0.0836 sec.
iter 151650 || Loss: 0.7300 || timer: 0.0910 sec.
iter 151660 || Loss: 1.0691 || timer: 0.0768 sec.
iter 151670 || Loss: 0.7008 || timer: 0.0912 sec.
iter 151680 || Loss: 1.2896 || timer: 0.0927 sec.
iter 151690 || Loss: 1.2172 || timer: 0.0291 sec.
iter 151700 || Loss: 0.6166 || timer: 0.0908 sec.
iter 151710 || Loss: 0.8808 || timer: 0.0910 sec.
iter 151720 || Loss: 0.7482 || timer: 0.0846 sec.
iter 151730 || Loss: 1.1899 || timer: 0.0930 sec.
iter 151740 || Loss: 0.9003 || timer: 0.0851 sec.
iter 151750 || Loss: 0.9717 || timer: 0.0929 sec.
iter 151760 || Loss: 0.9943 || timer: 0.0909 sec.
iter 151770 || Loss: 0.8794 || timer: 0.0916 sec.
iter 151780 || Loss: 0.9512 || timer: 0.0906 sec.
iter 151790 || Loss: 1.1709 || timer: 0.1197 sec.
iter 151800 || Loss: 1.5013 || timer: 0.1039 sec.
iter 151810 || Loss: 0.9170 || timer: 0.0838 sec.
iter 151820 || Loss: 0.5799 || timer: 0.0903 sec.
iter 151830 || Loss: 0.8675 || timer: 0.0909 sec.
iter 151840 || Loss: 0.7846 || timer: 0.0917 sec.
iter 151850 || Loss: 0.9594 || timer: 0.0839 sec.
iter 151860 || Loss: 1.0359 || timer: 0.0920 sec.
iter 151870 || Loss: 1.1707 || timer: 0.0833 sec.
iter 151880 || Loss: 1.2697 || timer: 0.1083 sec.
iter 151890 || Loss: 0.9617 || timer: 0.0913 sec.
iter 151900 || Loss: 0.8925 || timer: 0.0930 sec.
iter 151910 || Loss: 0.9230 || timer: 0.0752 sec.
iter 151920 || Loss: 0.9203 || timer: 0.0826 sec.
iter 151930 || Loss: 0.8732 || timer: 0.0892 sec.
iter 151940 || Loss: 1.5023 || timer: 0.1096 sec.
iter 151950 || Loss: 0.9227 || timer: 0.1098 sec.
iter 151960 || Loss: 0.7681 || timer: 0.0847 sec.
iter 151970 || Loss: 1.0623 || timer: 0.0840 sec.
iter 151980 || Loss: 0.7794 || timer: 0.0828 sec.
iter 151990 || Loss: 0.8852 || timer: 0.0915 sec.
iter 152000 || Loss: 0.9600 || timer: 0.0933 sec.
iter 152010 || Loss: 1.1006 || timer: 0.0839 sec.
iter 152020 || Loss: 0.9806 || timer: 0.0180 sec.
iter 152030 || Loss: 1.8193 || timer: 0.0886 sec.
iter 152040 || Loss: 1.1283 || timer: 0.0925 sec.
iter 152050 || Loss: 0.8118 || timer: 0.0994 sec.
iter 152060 || Loss: 1.1177 || timer: 0.0894 sec.
iter 152070 || Loss: 0.7663 || timer: 0.0993 sec.
iter 152080 || Loss: 0.9780 || timer: 0.0913 sec.
iter 152090 || Loss: 0.9155 || timer: 0.0921 sec.
iter 152100 || Loss: 0.7852 || timer: 0.1011 sec.
iter 152110 || Loss: 1.0658 || timer: 0.0848 sec.
iter 152120 || Loss: 0.9007 || timer: 0.1067 sec.
iter 152130 || Loss: 0.7998 || timer: 0.0843 sec.
iter 152140 || Loss: 0.8411 || timer: 0.0919 sec.
iter 152150 || Loss: 1.1075 || timer: 0.0899 sec.
iter 152160 || Loss: 1.2906 || timer: 0.0824 sec.
iter 152170 || Loss: 0.9641 || timer: 0.1061 sec.
iter 152180 || Loss: 0.7334 || timer: 0.1028 sec.
iter 152190 || Loss: 1.1170 || timer: 0.1073 sec.
iter 152200 || Loss: 0.8782 || timer: 0.0885 sec.
iter 152210 || Loss: 1.0603 || timer: 0.0907 sec.
iter 152220 || Loss: 1.0682 || timer: 0.0891 sec.
iter 152230 || Loss: 0.8374 || timer: 0.0966 sec.
iter 152240 || Loss: 1.2132 || timer: 0.0900 sec.
iter 152250 || Loss: 0.9074 || timer: 0.0833 sec.
iter 152260 || Loss: 0.8846 || timer: 0.0938 sec.
iter 152270 || Loss: 1.2265 || timer: 0.0918 sec.
iter 152280 || Loss: 0.9687 || timer: 0.0767 sec.
iter 152290 || Loss: 0.9960 || timer: 0.0847 sec.
iter 152300 || Loss: 0.9883 || timer: 0.1083 sec.
iter 152310 || Loss: 1.0863 || timer: 0.1058 sec.
iter 152320 || Loss: 0.8751 || timer: 0.0774 sec.
iter 152330 || Loss: 0.8659 || timer: 0.0938 sec.
iter 152340 || Loss: 0.9291 || timer: 0.0850 sec.
iter 152350 || Loss: 0.8429 || timer: 0.0236 sec.
iter 152360 || Loss: 0.9698 || timer: 0.0900 sec.
iter 152370 || Loss: 0.9471 || timer: 0.0937 sec.
iter 152380 || Loss: 0.9882 || timer: 0.0809 sec.
iter 152390 || Loss: 0.8171 || timer: 0.0838 sec.
iter 152400 || Loss: 0.9045 || timer: 0.0828 sec.
iter 152410 || Loss: 1.2447 || timer: 0.1004 sec.
iter 152420 || Loss: 0.7893 || timer: 0.0923 sec.
iter 152430 || Loss: 1.1534 || timer: 0.1067 sec.
iter 152440 || Loss: 0.9193 || timer: 0.0873 sec.
iter 152450 || Loss: 1.1031 || timer: 0.1050 sec.
iter 152460 || Loss: 0.7818 || timer: 0.0846 sec.
iter 152470 || Loss: 0.7408 || timer: 0.0851 sec.
iter 152480 || Loss: 1.2527 || timer: 0.1041 sec.
iter 152490 || Loss: 0.8247 || timer: 0.0930 sec.
iter 152500 || Loss: 1.0164 || timer: 0.0902 sec.
iter 152510 || Loss: 1.1819 || timer: 0.0825 sec.
iter 152520 || Loss: 0.8697 || timer: 0.0894 sec.
iter 152530 || Loss: 1.2278 || timer: 0.0774 sec.
iter 152540 || Loss: 0.8620 || timer: 0.0954 sec.
iter 152550 || Loss: 1.0046 || timer: 0.0953 sec.
iter 152560 || Loss: 1.0113 || timer: 0.0923 sec.
iter 152570 || Loss: 1.2870 || timer: 0.0914 sec.
iter 152580 || Loss: 0.9815 || timer: 0.0799 sec.
iter 152590 || Loss: 0.9014 || timer: 0.0846 sec.
iter 152600 || Loss: 1.0213 || timer: 0.0769 sec.
iter 152610 || Loss: 0.8089 || timer: 0.0891 sec.
iter 152620 || Loss: 1.0019 || timer: 0.0885 sec.
iter 152630 || Loss: 0.7527 || timer: 0.1006 sec.
iter 152640 || Loss: 1.0826 || timer: 0.1035 sec.
iter 152650 || Loss: 1.0033 || timer: 0.1131 sec.
iter 152660 || Loss: 0.5317 || timer: 0.0914 sec.
iter 152670 || Loss: 1.0732 || timer: 0.0896 sec.
iter 152680 || Loss: 0.8843 || timer: 0.0159 sec.
iter 152690 || Loss: 0.3698 || timer: 0.0902 sec.
iter 152700 || Loss: 0.8799 || timer: 0.0901 sec.
iter 152710 || Loss: 0.7936 || timer: 0.0874 sec.
iter 152720 || Loss: 0.9249 || timer: 0.0886 sec.
iter 152730 || Loss: 0.7240 || timer: 0.1109 sec.
iter 152740 || Loss: 1.0607 || timer: 0.0846 sec.
iter 152750 || Loss: 1.3621 || timer: 0.0867 sec.
iter 152760 || Loss: 1.4594 || timer: 0.0861 sec.
iter 152770 || Loss: 0.8662 || timer: 0.1079 sec.
iter 152780 || Loss: 1.0599 || timer: 0.0880 sec.
iter 152790 || Loss: 1.1206 || timer: 0.0833 sec.
iter 152800 || Loss: 0.8750 || timer: 0.1036 sec.
iter 152810 || Loss: 0.7370 || timer: 0.0890 sec.
iter 152820 || Loss: 1.0696 || timer: 0.0911 sec.
iter 152830 || Loss: 1.1217 || timer: 0.1068 sec.
iter 152840 || Loss: 0.7257 || timer: 0.0967 sec.
iter 152850 || Loss: 1.0001 || timer: 0.0911 sec.
iter 152860 || Loss: 1.0575 || timer: 0.0837 sec.
iter 152870 || Loss: 0.8951 || timer: 0.0837 sec.
iter 152880 || Loss: 0.7009 || timer: 0.0837 sec.
iter 152890 || Loss: 0.7639 || timer: 0.0919 sec.
iter 152900 || Loss: 0.9264 || timer: 0.0890 sec.
iter 152910 || Loss: 0.9321 || timer: 0.0900 sec.
iter 152920 || Loss: 0.8638 || timer: 0.0923 sec.
iter 152930 || Loss: 0.8549 || timer: 0.0895 sec.
iter 152940 || Loss: 1.8046 || timer: 0.0838 sec.
iter 152950 || Loss: 0.7438 || timer: 0.0917 sec.
iter 152960 || Loss: 1.0258 || timer: 0.0908 sec.
iter 152970 || Loss: 0.8916 || timer: 0.0839 sec.
iter 152980 || Loss: 0.6540 || timer: 0.0854 sec.
iter 152990 || Loss: 0.9910 || timer: 0.0840 sec.
iter 153000 || Loss: 0.7208 || timer: 0.0908 sec.
iter 153010 || Loss: 1.1322 || timer: 0.0263 sec.
iter 153020 || Loss: 1.9120 || timer: 0.0905 sec.
iter 153030 || Loss: 1.1869 || timer: 0.0932 sec.
iter 153040 || Loss: 0.7625 || timer: 0.0854 sec.
iter 153050 || Loss: 0.8972 || timer: 0.1002 sec.
iter 153060 || Loss: 1.2769 || timer: 0.0843 sec.
iter 153070 || Loss: 0.8112 || timer: 0.1108 sec.
iter 153080 || Loss: 1.2303 || timer: 0.0918 sec.
iter 153090 || Loss: 0.9784 || timer: 0.0884 sec.
iter 153100 || Loss: 0.6708 || timer: 0.0825 sec.
iter 153110 || Loss: 0.8170 || timer: 0.0955 sec.
iter 153120 || Loss: 0.7187 || timer: 0.0840 sec.
iter 153130 || Loss: 1.1964 || timer: 0.1095 sec.
iter 153140 || Loss: 1.5739 || timer: 0.0907 sec.
iter 153150 || Loss: 1.1701 || timer: 0.0829 sec.
iter 153160 || Loss: 1.6001 || timer: 0.0826 sec.
iter 153170 || Loss: 1.2183 || timer: 0.0931 sec.
iter 153180 || Loss: 0.9708 || timer: 0.0931 sec.
iter 153190 || Loss: 0.8705 || timer: 0.0838 sec.
iter 153200 || Loss: 1.0061 || timer: 0.1043 sec.
iter 153210 || Loss: 1.2826 || timer: 0.0860 sec.
iter 153220 || Loss: 0.8069 || timer: 0.0947 sec.
iter 153230 || Loss: 1.0286 || timer: 0.0842 sec.
iter 153240 || Loss: 0.9916 || timer: 0.0825 sec.
iter 153250 || Loss: 0.9038 || timer: 0.0767 sec.
iter 153260 || Loss: 0.8617 || timer: 0.1017 sec.
iter 153270 || Loss: 0.8376 || timer: 0.0916 sec.
iter 153280 || Loss: 0.8918 || timer: 0.0844 sec.
iter 153290 || Loss: 1.3002 || timer: 0.0919 sec.
iter 153300 || Loss: 0.8392 || timer: 0.0894 sec.
iter 153310 || Loss: 0.5827 || timer: 0.0854 sec.
iter 153320 || Loss: 0.6343 || timer: 0.0892 sec.
iter 153330 || Loss: 0.9680 || timer: 0.0910 sec.
iter 153340 || Loss: 1.2102 || timer: 0.0210 sec.
iter 153350 || Loss: 1.1327 || timer: 0.1028 sec.
iter 153360 || Loss: 1.3823 || timer: 0.0912 sec.
iter 153370 || Loss: 1.0736 || timer: 0.0857 sec.
iter 153380 || Loss: 1.2332 || timer: 0.0762 sec.
iter 153390 || Loss: 1.1017 || timer: 0.1065 sec.
iter 153400 || Loss: 0.8837 || timer: 0.0885 sec.
iter 153410 || Loss: 0.8207 || timer: 0.0940 sec.
iter 153420 || Loss: 1.3340 || timer: 0.0753 sec.
iter 153430 || Loss: 1.1112 || timer: 0.0917 sec.
iter 153440 || Loss: 0.9389 || timer: 0.0997 sec.
iter 153450 || Loss: 0.7729 || timer: 0.0839 sec.
iter 153460 || Loss: 1.2998 || timer: 0.0769 sec.
iter 153470 || Loss: 0.8964 || timer: 0.0885 sec.
iter 153480 || Loss: 0.8832 || timer: 0.0916 sec.
iter 153490 || Loss: 1.0215 || timer: 0.0924 sec.
iter 153500 || Loss: 0.8623 || timer: 0.0964 sec.
iter 153510 || Loss: 0.8778 || timer: 0.0808 sec.
iter 153520 || Loss: 1.1917 || timer: 0.0778 sec.
iter 153530 || Loss: 1.1504 || timer: 0.0758 sec.
iter 153540 || Loss: 0.8519 || timer: 0.0766 sec.
iter 153550 || Loss: 1.0223 || timer: 0.0829 sec.
iter 153560 || Loss: 2.0317 || timer: 0.1052 sec.
iter 153570 || Loss: 1.3139 || timer: 0.0817 sec.
iter 153580 || Loss: 0.9119 || timer: 0.0821 sec.
iter 153590 || Loss: 0.8046 || timer: 0.0904 sec.
iter 153600 || Loss: 1.2291 || timer: 0.0892 sec.
iter 153610 || Loss: 0.9159 || timer: 0.0838 sec.
iter 153620 || Loss: 1.0207 || timer: 0.1079 sec.
iter 153630 || Loss: 0.7621 || timer: 0.0771 sec.
iter 153640 || Loss: 0.9993 || timer: 0.0924 sec.
iter 153650 || Loss: 1.1539 || timer: 0.0844 sec.
iter 153660 || Loss: 0.8867 || timer: 0.0896 sec.
iter 153670 || Loss: 0.9979 || timer: 0.0242 sec.
iter 153680 || Loss: 0.9404 || timer: 0.0758 sec.
iter 153690 || Loss: 1.1337 || timer: 0.0845 sec.
iter 153700 || Loss: 0.6421 || timer: 0.0843 sec.
iter 153710 || Loss: 0.8419 || timer: 0.0837 sec.
iter 153720 || Loss: 1.1330 || timer: 0.0772 sec.
iter 153730 || Loss: 1.2201 || timer: 0.0842 sec.
iter 153740 || Loss: 1.0895 || timer: 0.0875 sec.
iter 153750 || Loss: 1.1199 || timer: 0.1114 sec.
iter 153760 || Loss: 0.8185 || timer: 0.0849 sec.
iter 153770 || Loss: 0.8090 || timer: 0.0980 sec.
iter 153780 || Loss: 0.7475 || timer: 0.0838 sec.
iter 153790 || Loss: 0.7630 || timer: 0.0920 sec.
iter 153800 || Loss: 0.8785 || timer: 0.0912 sec.
iter 153810 || Loss: 1.2063 || timer: 0.0953 sec.
iter 153820 || Loss: 0.9942 || timer: 0.0953 sec.
iter 153830 || Loss: 0.9920 || timer: 0.0842 sec.
iter 153840 || Loss: 0.7571 || timer: 0.0838 sec.
iter 153850 || Loss: 0.8687 || timer: 0.0932 sec.
iter 153860 || Loss: 0.9521 || timer: 0.0892 sec.
iter 153870 || Loss: 0.8094 || timer: 0.1051 sec.
iter 153880 || Loss: 1.0886 || timer: 0.0895 sec.
iter 153890 || Loss: 1.4459 || timer: 0.0904 sec.
iter 153900 || Loss: 1.0042 || timer: 0.0891 sec.
iter 153910 || Loss: 1.0159 || timer: 0.0927 sec.
iter 153920 || Loss: 1.2424 || timer: 0.0846 sec.
iter 153930 || Loss: 1.2279 || timer: 0.0913 sec.
iter 153940 || Loss: 1.6921 || timer: 0.1186 sec.
iter 153950 || Loss: 0.9475 || timer: 0.0841 sec.
iter 153960 || Loss: 0.9553 || timer: 0.0986 sec.
iter 153970 || Loss: 1.0804 || timer: 0.0897 sec.
iter 153980 || Loss: 0.7711 || timer: 0.0890 sec.
iter 153990 || Loss: 0.8572 || timer: 0.0978 sec.
iter 154000 || Loss: 0.7933 || timer: 0.0274 sec.
iter 154010 || Loss: 0.2741 || timer: 0.0911 sec.
iter 154020 || Loss: 1.1182 || timer: 0.0819 sec.
iter 154030 || Loss: 1.0041 || timer: 0.0907 sec.
iter 154040 || Loss: 1.3078 || timer: 0.0909 sec.
iter 154050 || Loss: 1.0287 || timer: 0.1048 sec.
iter 154060 || Loss: 0.7712 || timer: 0.0883 sec.
iter 154070 || Loss: 0.9017 || timer: 0.0919 sec.
iter 154080 || Loss: 0.5783 || timer: 0.0877 sec.
iter 154090 || Loss: 0.7765 || timer: 0.0764 sec.
iter 154100 || Loss: 1.0476 || timer: 0.0976 sec.
iter 154110 || Loss: 0.9371 || timer: 0.0841 sec.
iter 154120 || Loss: 1.0605 || timer: 0.0901 sec.
iter 154130 || Loss: 0.8019 || timer: 0.0903 sec.
iter 154140 || Loss: 0.9783 || timer: 0.0916 sec.
iter 154150 || Loss: 1.4000 || timer: 0.0994 sec.
iter 154160 || Loss: 0.9694 || timer: 0.0834 sec.
iter 154170 || Loss: 1.4305 || timer: 0.0861 sec.
iter 154180 || Loss: 1.3630 || timer: 0.1173 sec.
iter 154190 || Loss: 0.9978 || timer: 0.0836 sec.
iter 154200 || Loss: 0.7385 || timer: 0.1060 sec.
iter 154210 || Loss: 0.7808 || timer: 0.1009 sec.
iter 154220 || Loss: 0.9073 || timer: 0.0837 sec.
iter 154230 || Loss: 0.8134 || timer: 0.0786 sec.
iter 154240 || Loss: 0.7665 || timer: 0.0914 sec.
iter 154250 || Loss: 1.6632 || timer: 0.1037 sec.
iter 154260 || Loss: 1.4391 || timer: 0.0925 sec.
iter 154270 || Loss: 1.1870 || timer: 0.1052 sec.
iter 154280 || Loss: 1.4402 || timer: 0.0831 sec.
iter 154290 || Loss: 1.1251 || timer: 0.0834 sec.
iter 154300 || Loss: 1.1311 || timer: 0.0905 sec.
iter 154310 || Loss: 0.8887 || timer: 0.0892 sec.
iter 154320 || Loss: 0.8424 || timer: 0.0909 sec.
iter 154330 || Loss: 0.9013 || timer: 0.0251 sec.
iter 154340 || Loss: 1.0436 || timer: 0.0839 sec.
iter 154350 || Loss: 0.8713 || timer: 0.0861 sec.
iter 154360 || Loss: 0.7585 || timer: 0.0831 sec.
iter 154370 || Loss: 1.1146 || timer: 0.0847 sec.
iter 154380 || Loss: 1.0603 || timer: 0.0917 sec.
iter 154390 || Loss: 1.0972 || timer: 0.0918 sec.
iter 154400 || Loss: 0.9831 || timer: 0.1019 sec.
iter 154410 || Loss: 0.7784 || timer: 0.0842 sec.
iter 154420 || Loss: 1.1642 || timer: 0.0910 sec.
iter 154430 || Loss: 0.9241 || timer: 0.0969 sec.
iter 154440 || Loss: 1.0382 || timer: 0.0930 sec.
iter 154450 || Loss: 1.2528 || timer: 0.0787 sec.
iter 154460 || Loss: 0.9023 || timer: 0.0845 sec.
iter 154470 || Loss: 1.1413 || timer: 0.0836 sec.
iter 154480 || Loss: 0.8925 || timer: 0.1007 sec.
iter 154490 || Loss: 0.8578 || timer: 0.0853 sec.
iter 154500 || Loss: 1.3547 || timer: 0.0897 sec.
iter 154510 || Loss: 0.8103 || timer: 0.0840 sec.
iter 154520 || Loss: 0.8305 || timer: 0.0836 sec.
iter 154530 || Loss: 1.0633 || timer: 0.1013 sec.
iter 154540 || Loss: 1.0493 || timer: 0.1151 sec.
iter 154550 || Loss: 1.0672 || timer: 0.0912 sec.
iter 154560 || Loss: 1.1861 || timer: 0.0869 sec.
iter 154570 || Loss: 1.2064 || timer: 0.0830 sec.
iter 154580 || Loss: 1.0249 || timer: 0.1031 sec.
iter 154590 || Loss: 1.0865 || timer: 0.0881 sec.
iter 154600 || Loss: 0.9762 || timer: 0.0841 sec.
iter 154610 || Loss: 1.1120 || timer: 0.0883 sec.
iter 154620 || Loss: 0.7855 || timer: 0.0850 sec.
iter 154630 || Loss: 0.9973 || timer: 0.0849 sec.
iter 154640 || Loss: 0.8928 || timer: 0.0895 sec.
iter 154650 || Loss: 1.4309 || timer: 0.0997 sec.
iter 154660 || Loss: 0.8828 || timer: 0.0206 sec.
iter 154670 || Loss: 0.9046 || timer: 0.0825 sec.
iter 154680 || Loss: 0.9168 || timer: 0.0915 sec.
iter 154690 || Loss: 0.9334 || timer: 0.0840 sec.
iter 154700 || Loss: 0.6125 || timer: 0.0972 sec.
iter 154710 || Loss: 0.8775 || timer: 0.1022 sec.
iter 154720 || Loss: 0.8645 || timer: 0.1109 sec.
iter 154730 || Loss: 0.9576 || timer: 0.1022 sec.
iter 154740 || Loss: 1.0112 || timer: 0.1049 sec.
iter 154750 || Loss: 0.9926 || timer: 0.0835 sec.
iter 154760 || Loss: 0.9784 || timer: 0.0989 sec.
iter 154770 || Loss: 1.0430 || timer: 0.0977 sec.
iter 154780 || Loss: 0.7269 || timer: 0.1112 sec.
iter 154790 || Loss: 0.9308 || timer: 0.0790 sec.
iter 154800 || Loss: 1.1569 || timer: 0.1062 sec.
iter 154810 || Loss: 1.2136 || timer: 0.0942 sec.
iter 154820 || Loss: 1.2427 || timer: 0.0838 sec.
iter 154830 || Loss: 1.1121 || timer: 0.0913 sec.
iter 154840 || Loss: 1.0780 || timer: 0.0926 sec.
iter 154850 || Loss: 1.2205 || timer: 0.0823 sec.
iter 154860 || Loss: 1.0191 || timer: 0.0766 sec.
iter 154870 || Loss: 1.1616 || timer: 0.0844 sec.
iter 154880 || Loss: 0.9822 || timer: 0.0844 sec.
iter 154890 || Loss: 0.9995 || timer: 0.0825 sec.
iter 154900 || Loss: 0.9760 || timer: 0.1075 sec.
iter 154910 || Loss: 0.9012 || timer: 0.0837 sec.
iter 154920 || Loss: 0.9666 || timer: 0.0863 sec.
iter 154930 || Loss: 0.9566 || timer: 0.0829 sec.
iter 154940 || Loss: 1.0336 || timer: 0.0837 sec.
iter 154950 || Loss: 0.8604 || timer: 0.0979 sec.
iter 154960 || Loss: 1.1947 || timer: 0.1052 sec.
iter 154970 || Loss: 0.7707 || timer: 0.0931 sec.
iter 154980 || Loss: 1.1163 || timer: 0.0845 sec.
iter 154990 || Loss: 0.9950 || timer: 0.0178 sec.
iter 155000 || Loss: 0.6815 || Saving state, iter: 155000
timer: 0.1073 sec.
iter 155010 || Loss: 0.8707 || timer: 0.1003 sec.
iter 155020 || Loss: 1.0436 || timer: 0.0902 sec.
iter 155030 || Loss: 0.7279 || timer: 0.0776 sec.
iter 155040 || Loss: 0.7678 || timer: 0.0892 sec.
iter 155050 || Loss: 1.2173 || timer: 0.0885 sec.
iter 155060 || Loss: 0.9142 || timer: 0.0835 sec.
iter 155070 || Loss: 0.9627 || timer: 0.1125 sec.
iter 155080 || Loss: 0.9635 || timer: 0.1006 sec.
iter 155090 || Loss: 0.9734 || timer: 0.0971 sec.
iter 155100 || Loss: 1.2544 || timer: 0.1057 sec.
iter 155110 || Loss: 0.8978 || timer: 0.0826 sec.
iter 155120 || Loss: 1.1006 || timer: 0.0916 sec.
iter 155130 || Loss: 0.9688 || timer: 0.0918 sec.
iter 155140 || Loss: 1.2364 || timer: 0.0839 sec.
iter 155150 || Loss: 0.8148 || timer: 0.0880 sec.
iter 155160 || Loss: 0.9567 || timer: 0.0803 sec.
iter 155170 || Loss: 0.8249 || timer: 0.1118 sec.
iter 155180 || Loss: 0.9582 || timer: 0.0846 sec.
iter 155190 || Loss: 0.8816 || timer: 0.1000 sec.
iter 155200 || Loss: 0.7244 || timer: 0.0847 sec.
iter 155210 || Loss: 0.9805 || timer: 0.0841 sec.
iter 155220 || Loss: 0.9278 || timer: 0.0896 sec.
iter 155230 || Loss: 0.7297 || timer: 0.0884 sec.
iter 155240 || Loss: 0.9301 || timer: 0.0978 sec.
iter 155250 || Loss: 0.8745 || timer: 0.0895 sec.
iter 155260 || Loss: 0.6657 || timer: 0.0843 sec.
iter 155270 || Loss: 0.9214 || timer: 0.0841 sec.
iter 155280 || Loss: 1.2353 || timer: 0.0845 sec.
iter 155290 || Loss: 1.1406 || timer: 0.0928 sec.
iter 155300 || Loss: 0.8468 || timer: 0.0982 sec.
iter 155310 || Loss: 1.1742 || timer: 0.0817 sec.
iter 155320 || Loss: 1.0155 || timer: 0.0153 sec.
iter 155330 || Loss: 0.9203 || timer: 0.0837 sec.
iter 155340 || Loss: 0.9344 || timer: 0.1065 sec.
iter 155350 || Loss: 0.7753 || timer: 0.0914 sec.
iter 155360 || Loss: 1.3206 || timer: 0.0945 sec.
iter 155370 || Loss: 1.0342 || timer: 0.0917 sec.
iter 155380 || Loss: 0.7942 || timer: 0.0836 sec.
iter 155390 || Loss: 0.7789 || timer: 0.0849 sec.
iter 155400 || Loss: 0.9246 || timer: 0.0855 sec.
iter 155410 || Loss: 0.9617 || timer: 0.0924 sec.
iter 155420 || Loss: 1.2057 || timer: 0.1084 sec.
iter 155430 || Loss: 0.6184 || timer: 0.0757 sec.
iter 155440 || Loss: 1.0393 || timer: 0.0935 sec.
iter 155450 || Loss: 1.2489 || timer: 0.0917 sec.
iter 155460 || Loss: 1.0763 || timer: 0.1008 sec.
iter 155470 || Loss: 1.0318 || timer: 0.0821 sec.
iter 155480 || Loss: 0.6720 || timer: 0.0989 sec.
iter 155490 || Loss: 0.9886 || timer: 0.0911 sec.
iter 155500 || Loss: 0.8845 || timer: 0.0814 sec.
iter 155510 || Loss: 0.9804 || timer: 0.1170 sec.
iter 155520 || Loss: 0.7225 || timer: 0.0857 sec.
iter 155530 || Loss: 0.9816 || timer: 0.0914 sec.
iter 155540 || Loss: 0.9278 || timer: 0.0907 sec.
iter 155550 || Loss: 0.7180 || timer: 0.0909 sec.
iter 155560 || Loss: 0.9231 || timer: 0.0907 sec.
iter 155570 || Loss: 1.4176 || timer: 0.0920 sec.
iter 155580 || Loss: 1.2772 || timer: 0.0839 sec.
iter 155590 || Loss: 1.2621 || timer: 0.0841 sec.
iter 155600 || Loss: 0.7958 || timer: 0.0841 sec.
iter 155610 || Loss: 0.9536 || timer: 0.1124 sec.
iter 155620 || Loss: 0.9555 || timer: 0.0771 sec.
iter 155630 || Loss: 1.1848 || timer: 0.1172 sec.
iter 155640 || Loss: 0.7471 || timer: 0.0904 sec.
iter 155650 || Loss: 1.0547 || timer: 0.0261 sec.
iter 155660 || Loss: 0.7777 || timer: 0.1016 sec.
iter 155670 || Loss: 1.3593 || timer: 0.0921 sec.
iter 155680 || Loss: 1.0932 || timer: 0.0904 sec.
iter 155690 || Loss: 1.1122 || timer: 0.1055 sec.
iter 155700 || Loss: 0.8293 || timer: 0.0838 sec.
iter 155710 || Loss: 0.9904 || timer: 0.0814 sec.
iter 155720 || Loss: 0.7911 || timer: 0.0818 sec.
iter 155730 || Loss: 0.9789 || timer: 0.0825 sec.
iter 155740 || Loss: 1.0823 || timer: 0.0850 sec.
iter 155750 || Loss: 0.8861 || timer: 0.0968 sec.
iter 155760 || Loss: 0.8806 || timer: 0.0930 sec.
iter 155770 || Loss: 0.9483 || timer: 0.0924 sec.
iter 155780 || Loss: 0.7984 || timer: 0.1103 sec.
iter 155790 || Loss: 0.8102 || timer: 0.1304 sec.
iter 155800 || Loss: 1.5734 || timer: 0.0971 sec.
iter 155810 || Loss: 0.8593 || timer: 0.0922 sec.
iter 155820 || Loss: 0.7048 || timer: 0.0826 sec.
iter 155830 || Loss: 0.8423 || timer: 0.0840 sec.
iter 155840 || Loss: 1.2714 || timer: 0.0998 sec.
iter 155850 || Loss: 1.1352 || timer: 0.0838 sec.
iter 155860 || Loss: 0.9174 || timer: 0.0834 sec.
iter 155870 || Loss: 1.1315 || timer: 0.0887 sec.
iter 155880 || Loss: 0.9603 || timer: 0.0837 sec.
iter 155890 || Loss: 0.9489 || timer: 0.0832 sec.
iter 155900 || Loss: 1.3716 || timer: 0.0821 sec.
iter 155910 || Loss: 1.3005 || timer: 0.0899 sec.
iter 155920 || Loss: 0.9451 || timer: 0.1072 sec.
iter 155930 || Loss: 1.2624 || timer: 0.0919 sec.
iter 155940 || Loss: 0.9497 || timer: 0.0896 sec.
iter 155950 || Loss: 1.2934 || timer: 0.0925 sec.
iter 155960 || Loss: 0.8257 || timer: 0.0886 sec.
iter 155970 || Loss: 0.9514 || timer: 0.0900 sec.
iter 155980 || Loss: 0.8024 || timer: 0.0230 sec.
iter 155990 || Loss: 2.4152 || timer: 0.0907 sec.
iter 156000 || Loss: 0.8906 || timer: 0.1078 sec.
iter 156010 || Loss: 1.1472 || timer: 0.0911 sec.
iter 156020 || Loss: 0.9024 || timer: 0.1098 sec.
iter 156030 || Loss: 1.0569 || timer: 0.0979 sec.
iter 156040 || Loss: 1.4597 || timer: 0.0820 sec.
iter 156050 || Loss: 0.7200 || timer: 0.0829 sec.
iter 156060 || Loss: 0.9595 || timer: 0.1061 sec.
iter 156070 || Loss: 1.2526 || timer: 0.1022 sec.
iter 156080 || Loss: 0.8527 || timer: 0.1136 sec.
iter 156090 || Loss: 0.7012 || timer: 0.0907 sec.
iter 156100 || Loss: 0.7912 || timer: 0.0920 sec.
iter 156110 || Loss: 0.8789 || timer: 0.0916 sec.
iter 156120 || Loss: 1.0246 || timer: 0.0906 sec.
iter 156130 || Loss: 1.3432 || timer: 0.1094 sec.
iter 156140 || Loss: 0.8103 || timer: 0.0943 sec.
iter 156150 || Loss: 0.8401 || timer: 0.0905 sec.
iter 156160 || Loss: 0.9395 || timer: 0.1126 sec.
iter 156170 || Loss: 0.8100 || timer: 0.1146 sec.
iter 156180 || Loss: 0.9681 || timer: 0.0849 sec.
iter 156190 || Loss: 0.9929 || timer: 0.0964 sec.
iter 156200 || Loss: 0.8457 || timer: 0.0939 sec.
iter 156210 || Loss: 0.8752 || timer: 0.0904 sec.
iter 156220 || Loss: 1.0681 || timer: 0.1128 sec.
iter 156230 || Loss: 0.7332 || timer: 0.1129 sec.
iter 156240 || Loss: 0.7412 || timer: 0.1139 sec.
iter 156250 || Loss: 1.2476 || timer: 0.0909 sec.
iter 156260 || Loss: 0.8001 || timer: 0.1080 sec.
iter 156270 || Loss: 0.8784 || timer: 0.0840 sec.
iter 156280 || Loss: 1.0856 || timer: 0.0888 sec.
iter 156290 || Loss: 0.7753 || timer: 0.0844 sec.
iter 156300 || Loss: 0.9386 || timer: 0.0900 sec.
iter 156310 || Loss: 1.4020 || timer: 0.0152 sec.
iter 156320 || Loss: 1.9191 || timer: 0.0834 sec.
iter 156330 || Loss: 1.0244 || timer: 0.0773 sec.
iter 156340 || Loss: 0.7825 || timer: 0.0912 sec.
iter 156350 || Loss: 1.0149 || timer: 0.0869 sec.
iter 156360 || Loss: 1.1340 || timer: 0.0898 sec.
iter 156370 || Loss: 1.0026 || timer: 0.1136 sec.
iter 156380 || Loss: 1.0109 || timer: 0.0920 sec.
iter 156390 || Loss: 1.2042 || timer: 0.0916 sec.
iter 156400 || Loss: 0.6919 || timer: 0.0856 sec.
iter 156410 || Loss: 0.9487 || timer: 0.1205 sec.
iter 156420 || Loss: 0.9482 || timer: 0.0868 sec.
iter 156430 || Loss: 0.7037 || timer: 0.0908 sec.
iter 156440 || Loss: 0.8217 || timer: 0.0906 sec.
iter 156450 || Loss: 1.0281 || timer: 0.0833 sec.
iter 156460 || Loss: 0.9898 || timer: 0.0875 sec.
iter 156470 || Loss: 0.8419 || timer: 0.0974 sec.
iter 156480 || Loss: 0.8840 || timer: 0.0918 sec.
iter 156490 || Loss: 0.8834 || timer: 0.0844 sec.
iter 156500 || Loss: 1.1670 || timer: 0.0908 sec.
iter 156510 || Loss: 1.1730 || timer: 0.0846 sec.
iter 156520 || Loss: 0.5496 || timer: 0.0767 sec.
iter 156530 || Loss: 0.7561 || timer: 0.0872 sec.
iter 156540 || Loss: 0.8982 || timer: 0.0909 sec.
iter 156550 || Loss: 1.1589 || timer: 0.0912 sec.
iter 156560 || Loss: 0.8057 || timer: 0.0919 sec.
iter 156570 || Loss: 1.2403 || timer: 0.0940 sec.
iter 156580 || Loss: 0.8830 || timer: 0.0840 sec.
iter 156590 || Loss: 0.7175 || timer: 0.0915 sec.
iter 156600 || Loss: 0.9039 || timer: 0.0828 sec.
iter 156610 || Loss: 0.8101 || timer: 0.0974 sec.
iter 156620 || Loss: 0.9853 || timer: 0.0894 sec.
iter 156630 || Loss: 1.3483 || timer: 0.1132 sec.
iter 156640 || Loss: 0.9282 || timer: 0.0203 sec.
iter 156650 || Loss: 0.4511 || timer: 0.1020 sec.
iter 156660 || Loss: 1.1007 || timer: 0.1062 sec.
iter 156670 || Loss: 0.8825 || timer: 0.0917 sec.
iter 156680 || Loss: 1.0564 || timer: 0.0829 sec.
iter 156690 || Loss: 1.0400 || timer: 0.0768 sec.
iter 156700 || Loss: 1.0095 || timer: 0.0890 sec.
iter 156710 || Loss: 0.9855 || timer: 0.0914 sec.
iter 156720 || Loss: 1.4288 || timer: 0.1037 sec.
iter 156730 || Loss: 1.0005 || timer: 0.0861 sec.
iter 156740 || Loss: 0.5802 || timer: 0.1181 sec.
iter 156750 || Loss: 0.9118 || timer: 0.0837 sec.
iter 156760 || Loss: 1.3621 || timer: 0.0827 sec.
iter 156770 || Loss: 1.5010 || timer: 0.0887 sec.
iter 156780 || Loss: 0.8137 || timer: 0.0843 sec.
iter 156790 || Loss: 0.9946 || timer: 0.1037 sec.
iter 156800 || Loss: 0.9155 || timer: 0.0850 sec.
iter 156810 || Loss: 1.0249 || timer: 0.0852 sec.
iter 156820 || Loss: 1.0595 || timer: 0.1056 sec.
iter 156830 || Loss: 1.0751 || timer: 0.0768 sec.
iter 156840 || Loss: 0.8758 || timer: 0.0762 sec.
iter 156850 || Loss: 0.8848 || timer: 0.1006 sec.
iter 156860 || Loss: 0.9205 || timer: 0.0820 sec.
iter 156870 || Loss: 0.9138 || timer: 0.0817 sec.
iter 156880 || Loss: 0.8224 || timer: 0.1023 sec.
iter 156890 || Loss: 0.9965 || timer: 0.0772 sec.
iter 156900 || Loss: 0.9376 || timer: 0.0861 sec.
iter 156910 || Loss: 1.0160 || timer: 0.0842 sec.
iter 156920 || Loss: 1.0368 || timer: 0.0931 sec.
iter 156930 || Loss: 0.8788 || timer: 0.0902 sec.
iter 156940 || Loss: 1.1282 || timer: 0.0851 sec.
iter 156950 || Loss: 0.8011 || timer: 0.0834 sec.
iter 156960 || Loss: 0.8417 || timer: 0.0838 sec.
iter 156970 || Loss: 0.8321 || timer: 0.0152 sec.
iter 156980 || Loss: 0.6382 || timer: 0.1004 sec.
iter 156990 || Loss: 0.9968 || timer: 0.0891 sec.
iter 157000 || Loss: 1.0484 || timer: 0.0773 sec.
iter 157010 || Loss: 1.1954 || timer: 0.0901 sec.
iter 157020 || Loss: 0.8129 || timer: 0.0906 sec.
iter 157030 || Loss: 0.5631 || timer: 0.0791 sec.
iter 157040 || Loss: 1.1266 || timer: 0.0916 sec.
iter 157050 || Loss: 0.7146 || timer: 0.1107 sec.
iter 157060 || Loss: 1.2985 || timer: 0.0844 sec.
iter 157070 || Loss: 0.9585 || timer: 0.0895 sec.
iter 157080 || Loss: 1.3047 || timer: 0.0912 sec.
iter 157090 || Loss: 1.0084 || timer: 0.0917 sec.
iter 157100 || Loss: 0.8098 || timer: 0.0811 sec.
iter 157110 || Loss: 0.9214 || timer: 0.1046 sec.
iter 157120 || Loss: 1.0702 || timer: 0.0881 sec.
iter 157130 || Loss: 1.1346 || timer: 0.0950 sec.
iter 157140 || Loss: 0.7873 || timer: 0.0884 sec.
iter 157150 || Loss: 1.0839 || timer: 0.1045 sec.
iter 157160 || Loss: 0.9444 || timer: 0.0843 sec.
iter 157170 || Loss: 0.8901 || timer: 0.0890 sec.
iter 157180 || Loss: 1.1331 || timer: 0.0903 sec.
iter 157190 || Loss: 0.7904 || timer: 0.0878 sec.
iter 157200 || Loss: 1.2620 || timer: 0.0869 sec.
iter 157210 || Loss: 0.7778 || timer: 0.0848 sec.
iter 157220 || Loss: 0.7206 || timer: 0.0840 sec.
iter 157230 || Loss: 0.9798 || timer: 0.0948 sec.
iter 157240 || Loss: 1.0129 || timer: 0.0835 sec.
iter 157250 || Loss: 1.0249 || timer: 0.1157 sec.
iter 157260 || Loss: 1.1041 || timer: 0.0900 sec.
iter 157270 || Loss: 0.7799 || timer: 0.0925 sec.
iter 157280 || Loss: 1.0544 || timer: 0.0926 sec.
iter 157290 || Loss: 1.5576 || timer: 0.0834 sec.
iter 157300 || Loss: 0.7111 || timer: 0.0241 sec.
iter 157310 || Loss: 0.4556 || timer: 0.0841 sec.
iter 157320 || Loss: 0.6982 || timer: 0.0885 sec.
iter 157330 || Loss: 1.0160 || timer: 0.0860 sec.
iter 157340 || Loss: 0.8823 || timer: 0.0837 sec.
iter 157350 || Loss: 0.9131 || timer: 0.0842 sec.
iter 157360 || Loss: 0.7366 || timer: 0.0877 sec.
iter 157370 || Loss: 0.7455 || timer: 0.0931 sec.
iter 157380 || Loss: 1.0469 || timer: 0.0841 sec.
iter 157390 || Loss: 0.9960 || timer: 0.0847 sec.
iter 157400 || Loss: 0.7934 || timer: 0.0884 sec.
iter 157410 || Loss: 0.7216 || timer: 0.1071 sec.
iter 157420 || Loss: 1.0545 || timer: 0.0865 sec.
iter 157430 || Loss: 1.0963 || timer: 0.0828 sec.
iter 157440 || Loss: 0.8678 || timer: 0.0972 sec.
iter 157450 || Loss: 0.8077 || timer: 0.0851 sec.
iter 157460 || Loss: 1.0669 || timer: 0.1080 sec.
iter 157470 || Loss: 0.9176 || timer: 0.0907 sec.
iter 157480 || Loss: 0.8648 || timer: 0.1099 sec.
iter 157490 || Loss: 0.7617 || timer: 0.0833 sec.
iter 157500 || Loss: 1.4274 || timer: 0.1015 sec.
iter 157510 || Loss: 0.8178 || timer: 0.0862 sec.
iter 157520 || Loss: 0.9380 || timer: 0.0911 sec.
iter 157530 || Loss: 1.1136 || timer: 0.0903 sec.
iter 157540 || Loss: 0.9999 || timer: 0.0904 sec.
iter 157550 || Loss: 0.9454 || timer: 0.0915 sec.
iter 157560 || Loss: 1.2560 || timer: 0.0940 sec.
iter 157570 || Loss: 1.2930 || timer: 0.0840 sec.
iter 157580 || Loss: 1.1299 || timer: 0.0937 sec.
iter 157590 || Loss: 0.8232 || timer: 0.0836 sec.
iter 157600 || Loss: 0.7216 || timer: 0.0913 sec.
iter 157610 || Loss: 1.3435 || timer: 0.0885 sec.
iter 157620 || Loss: 0.9125 || timer: 0.0801 sec.
iter 157630 || Loss: 1.1079 || timer: 0.0158 sec.
iter 157640 || Loss: 0.8764 || timer: 0.1065 sec.
iter 157650 || Loss: 0.9069 || timer: 0.0910 sec.
iter 157660 || Loss: 0.9233 || timer: 0.0911 sec.
iter 157670 || Loss: 0.9168 || timer: 0.1171 sec.
iter 157680 || Loss: 0.8546 || timer: 0.0927 sec.
iter 157690 || Loss: 0.7602 || timer: 0.0928 sec.
iter 157700 || Loss: 0.8003 || timer: 0.1213 sec.
iter 157710 || Loss: 0.9993 || timer: 0.1123 sec.
iter 157720 || Loss: 0.9962 || timer: 0.0881 sec.
iter 157730 || Loss: 0.8554 || timer: 0.0953 sec.
iter 157740 || Loss: 1.2469 || timer: 0.0908 sec.
iter 157750 || Loss: 0.8649 || timer: 0.0951 sec.
iter 157760 || Loss: 1.1082 || timer: 0.0906 sec.
iter 157770 || Loss: 1.2653 || timer: 0.0903 sec.
iter 157780 || Loss: 0.9283 || timer: 0.1026 sec.
iter 157790 || Loss: 0.7604 || timer: 0.0827 sec.
iter 157800 || Loss: 1.0591 || timer: 0.0829 sec.
iter 157810 || Loss: 1.0348 || timer: 0.0840 sec.
iter 157820 || Loss: 0.8838 || timer: 0.0914 sec.
iter 157830 || Loss: 0.7467 || timer: 0.0860 sec.
iter 157840 || Loss: 0.9125 || timer: 0.1048 sec.
iter 157850 || Loss: 0.9994 || timer: 0.0913 sec.
iter 157860 || Loss: 0.7416 || timer: 0.0891 sec.
iter 157870 || Loss: 0.9678 || timer: 0.0905 sec.
iter 157880 || Loss: 1.1191 || timer: 0.1106 sec.
iter 157890 || Loss: 0.9732 || timer: 0.0827 sec.
iter 157900 || Loss: 1.0844 || timer: 0.0841 sec.
iter 157910 || Loss: 0.7060 || timer: 0.0784 sec.
iter 157920 || Loss: 0.6695 || timer: 0.0901 sec.
iter 157930 || Loss: 0.6367 || timer: 0.0905 sec.
iter 157940 || Loss: 0.6799 || timer: 0.0937 sec.
iter 157950 || Loss: 0.6731 || timer: 0.0837 sec.
iter 157960 || Loss: 1.2958 || timer: 0.0274 sec.
iter 157970 || Loss: 2.2871 || timer: 0.0914 sec.
iter 157980 || Loss: 0.8860 || timer: 0.0901 sec.
iter 157990 || Loss: 0.9989 || timer: 0.0929 sec.
iter 158000 || Loss: 0.9672 || timer: 0.0768 sec.
iter 158010 || Loss: 1.1029 || timer: 0.0974 sec.
iter 158020 || Loss: 0.9312 || timer: 0.0904 sec.
iter 158030 || Loss: 0.7662 || timer: 0.0858 sec.
iter 158040 || Loss: 0.8246 || timer: 0.0819 sec.
iter 158050 || Loss: 0.8089 || timer: 0.1033 sec.
iter 158060 || Loss: 1.0898 || timer: 0.0895 sec.
iter 158070 || Loss: 0.7468 || timer: 0.0782 sec.
iter 158080 || Loss: 0.8285 || timer: 0.0919 sec.
iter 158090 || Loss: 0.7654 || timer: 0.0883 sec.
iter 158100 || Loss: 0.8997 || timer: 0.0966 sec.
iter 158110 || Loss: 1.1619 || timer: 0.0767 sec.
iter 158120 || Loss: 0.8376 || timer: 0.0779 sec.
iter 158130 || Loss: 0.6942 || timer: 0.0781 sec.
iter 158140 || Loss: 0.8084 || timer: 0.0770 sec.
iter 158150 || Loss: 0.9214 || timer: 0.0934 sec.
iter 158160 || Loss: 0.8988 || timer: 0.0840 sec.
iter 158170 || Loss: 1.0319 || timer: 0.0939 sec.
iter 158180 || Loss: 0.7072 || timer: 0.0771 sec.
iter 158190 || Loss: 1.0846 || timer: 0.0823 sec.
iter 158200 || Loss: 0.9898 || timer: 0.0845 sec.
iter 158210 || Loss: 0.8336 || timer: 0.0855 sec.
iter 158220 || Loss: 1.1623 || timer: 0.0911 sec.
iter 158230 || Loss: 0.9742 || timer: 0.0898 sec.
iter 158240 || Loss: 1.0472 || timer: 0.0918 sec.
iter 158250 || Loss: 0.8042 || timer: 0.0876 sec.
iter 158260 || Loss: 0.7036 || timer: 0.0839 sec.
iter 158270 || Loss: 0.8137 || timer: 0.0904 sec.
iter 158280 || Loss: 0.9695 || timer: 0.0969 sec.
iter 158290 || Loss: 0.8869 || timer: 0.0176 sec.
iter 158300 || Loss: 3.7691 || timer: 0.0934 sec.
iter 158310 || Loss: 1.4202 || timer: 0.1096 sec.
iter 158320 || Loss: 1.2149 || timer: 0.0924 sec.
iter 158330 || Loss: 0.7365 || timer: 0.0900 sec.
iter 158340 || Loss: 0.7064 || timer: 0.0802 sec.
iter 158350 || Loss: 1.2072 || timer: 0.1000 sec.
iter 158360 || Loss: 0.9284 || timer: 0.1059 sec.
iter 158370 || Loss: 0.7743 || timer: 0.0815 sec.
iter 158380 || Loss: 0.9603 || timer: 0.0826 sec.
iter 158390 || Loss: 0.8794 || timer: 0.0944 sec.
iter 158400 || Loss: 0.9919 || timer: 0.0897 sec.
iter 158410 || Loss: 0.8487 || timer: 0.0896 sec.
iter 158420 || Loss: 0.8795 || timer: 0.0907 sec.
iter 158430 || Loss: 1.2543 || timer: 0.0897 sec.
iter 158440 || Loss: 1.1535 || timer: 0.0898 sec.
iter 158450 || Loss: 0.9128 || timer: 0.0885 sec.
iter 158460 || Loss: 1.2058 || timer: 0.0801 sec.
iter 158470 || Loss: 0.6574 || timer: 0.1032 sec.
iter 158480 || Loss: 0.7284 || timer: 0.0818 sec.
iter 158490 || Loss: 0.8348 || timer: 0.0883 sec.
iter 158500 || Loss: 1.0798 || timer: 0.1060 sec.
iter 158510 || Loss: 0.8835 || timer: 0.0848 sec.
iter 158520 || Loss: 0.9882 || timer: 0.0745 sec.
iter 158530 || Loss: 0.8741 || timer: 0.1042 sec.
iter 158540 || Loss: 0.8529 || timer: 0.0913 sec.
iter 158550 || Loss: 0.9648 || timer: 0.0897 sec.
iter 158560 || Loss: 0.9631 || timer: 0.0905 sec.
iter 158570 || Loss: 0.8091 || timer: 0.0919 sec.
iter 158580 || Loss: 0.7447 || timer: 0.1182 sec.
iter 158590 || Loss: 0.8689 || timer: 0.0918 sec.
iter 158600 || Loss: 0.9174 || timer: 0.0922 sec.
iter 158610 || Loss: 0.8787 || timer: 0.0845 sec.
iter 158620 || Loss: 1.1334 || timer: 0.0165 sec.
iter 158630 || Loss: 1.5581 || timer: 0.0768 sec.
iter 158640 || Loss: 0.9078 || timer: 0.0891 sec.
iter 158650 || Loss: 1.1073 || timer: 0.0833 sec.
iter 158660 || Loss: 0.9181 || timer: 0.0848 sec.
iter 158670 || Loss: 1.2148 || timer: 0.0895 sec.
iter 158680 || Loss: 0.9779 || timer: 0.0889 sec.
iter 158690 || Loss: 1.3232 || timer: 0.0949 sec.
iter 158700 || Loss: 0.8032 || timer: 0.0885 sec.
iter 158710 || Loss: 0.7221 || timer: 0.0949 sec.
iter 158720 || Loss: 0.8018 || timer: 0.0880 sec.
iter 158730 || Loss: 0.8771 || timer: 0.0893 sec.
iter 158740 || Loss: 1.0196 || timer: 0.0940 sec.
iter 158750 || Loss: 0.8775 || timer: 0.0885 sec.
iter 158760 || Loss: 0.9318 || timer: 0.0932 sec.
iter 158770 || Loss: 0.6957 || timer: 0.0886 sec.
iter 158780 || Loss: 0.9039 || timer: 0.0895 sec.
iter 158790 || Loss: 0.8547 || timer: 0.1009 sec.
iter 158800 || Loss: 0.9036 || timer: 0.0915 sec.
iter 158810 || Loss: 0.8525 || timer: 0.0899 sec.
iter 158820 || Loss: 1.3118 || timer: 0.0832 sec.
iter 158830 || Loss: 0.8135 || timer: 0.1064 sec.
iter 158840 || Loss: 0.9443 || timer: 0.0899 sec.
iter 158850 || Loss: 0.6219 || timer: 0.0922 sec.
iter 158860 || Loss: 0.8718 || timer: 0.0871 sec.
iter 158870 || Loss: 0.8888 || timer: 0.0929 sec.
iter 158880 || Loss: 1.0998 || timer: 0.0899 sec.
iter 158890 || Loss: 0.8190 || timer: 0.0839 sec.
iter 158900 || Loss: 1.1183 || timer: 0.0861 sec.
iter 158910 || Loss: 1.1737 || timer: 0.0910 sec.
iter 158920 || Loss: 1.0948 || timer: 0.0912 sec.
iter 158930 || Loss: 0.6805 || timer: 0.0878 sec.
iter 158940 || Loss: 0.9288 || timer: 0.0910 sec.
iter 158950 || Loss: 0.8238 || timer: 0.0244 sec.
iter 158960 || Loss: 0.6685 || timer: 0.0922 sec.
iter 158970 || Loss: 1.1358 || timer: 0.0930 sec.
iter 158980 || Loss: 0.8393 || timer: 0.0837 sec.
iter 158990 || Loss: 0.7202 || timer: 0.1012 sec.
iter 159000 || Loss: 1.0441 || timer: 0.0851 sec.
iter 159010 || Loss: 1.0185 || timer: 0.1023 sec.
iter 159020 || Loss: 1.2644 || timer: 0.1176 sec.
iter 159030 || Loss: 0.9996 || timer: 0.0842 sec.
iter 159040 || Loss: 0.9270 || timer: 0.0839 sec.
iter 159050 || Loss: 0.7725 || timer: 0.1010 sec.
iter 159060 || Loss: 1.1820 || timer: 0.0881 sec.
iter 159070 || Loss: 0.8571 || timer: 0.0828 sec.
iter 159080 || Loss: 0.9419 || timer: 0.1069 sec.
iter 159090 || Loss: 0.8660 || timer: 0.0818 sec.
iter 159100 || Loss: 0.7666 || timer: 0.0840 sec.
iter 159110 || Loss: 0.9318 || timer: 0.0932 sec.
iter 159120 || Loss: 1.1440 || timer: 0.0884 sec.
iter 159130 || Loss: 0.9099 || timer: 0.0848 sec.
iter 159140 || Loss: 0.9840 || timer: 0.0956 sec.
iter 159150 || Loss: 0.8702 || timer: 0.0917 sec.
iter 159160 || Loss: 0.7351 || timer: 0.0843 sec.
iter 159170 || Loss: 0.9010 || timer: 0.0893 sec.
iter 159180 || Loss: 0.8649 || timer: 0.0904 sec.
iter 159190 || Loss: 1.0010 || timer: 0.0901 sec.
iter 159200 || Loss: 0.9591 || timer: 0.0837 sec.
iter 159210 || Loss: 1.0098 || timer: 0.0828 sec.
iter 159220 || Loss: 0.9372 || timer: 0.0927 sec.
iter 159230 || Loss: 1.0239 || timer: 0.0898 sec.
iter 159240 || Loss: 0.9426 || timer: 0.1064 sec.
iter 159250 || Loss: 1.1094 || timer: 0.0924 sec.
iter 159260 || Loss: 1.1405 || timer: 0.0915 sec.
iter 159270 || Loss: 0.9597 || timer: 0.0840 sec.
iter 159280 || Loss: 1.2346 || timer: 0.0150 sec.
iter 159290 || Loss: 0.1714 || timer: 0.0894 sec.
iter 159300 || Loss: 0.8776 || timer: 0.1015 sec.
iter 159310 || Loss: 1.0826 || timer: 0.0961 sec.
iter 159320 || Loss: 0.9833 || timer: 0.0828 sec.
iter 159330 || Loss: 0.9911 || timer: 0.0856 sec.
iter 159340 || Loss: 1.0965 || timer: 0.0808 sec.
iter 159350 || Loss: 0.6197 || timer: 0.0840 sec.
iter 159360 || Loss: 0.8532 || timer: 0.0840 sec.
iter 159370 || Loss: 0.8186 || timer: 0.0956 sec.
iter 159380 || Loss: 1.3299 || timer: 0.0888 sec.
iter 159390 || Loss: 0.8420 || timer: 0.1019 sec.
iter 159400 || Loss: 0.8326 || timer: 0.0910 sec.
iter 159410 || Loss: 0.8866 || timer: 0.0839 sec.
iter 159420 || Loss: 1.0245 || timer: 0.0837 sec.
iter 159430 || Loss: 1.6656 || timer: 0.0838 sec.
iter 159440 || Loss: 1.0013 || timer: 0.0906 sec.
iter 159450 || Loss: 1.0037 || timer: 0.1043 sec.
iter 159460 || Loss: 0.9254 || timer: 0.0995 sec.
iter 159470 || Loss: 0.6671 || timer: 0.0755 sec.
iter 159480 || Loss: 1.0295 || timer: 0.0774 sec.
iter 159490 || Loss: 0.9401 || timer: 0.0840 sec.
iter 159500 || Loss: 0.8440 || timer: 0.0891 sec.
iter 159510 || Loss: 0.7802 || timer: 0.0838 sec.
iter 159520 || Loss: 0.8743 || timer: 0.1080 sec.
iter 159530 || Loss: 0.8781 || timer: 0.1029 sec.
iter 159540 || Loss: 1.0790 || timer: 0.0848 sec.
iter 159550 || Loss: 0.8902 || timer: 0.0860 sec.
iter 159560 || Loss: 0.9931 || timer: 0.1002 sec.
iter 159570 || Loss: 0.9098 || timer: 0.0902 sec.
iter 159580 || Loss: 0.9688 || timer: 0.0895 sec.
iter 159590 || Loss: 0.8779 || timer: 0.0910 sec.
iter 159600 || Loss: 0.9004 || timer: 0.0923 sec.
iter 159610 || Loss: 1.0368 || timer: 0.0184 sec.
iter 159620 || Loss: 0.9884 || timer: 0.0851 sec.
iter 159630 || Loss: 0.8399 || timer: 0.1071 sec.
iter 159640 || Loss: 0.9595 || timer: 0.0918 sec.
iter 159650 || Loss: 1.1429 || timer: 0.1015 sec.
iter 159660 || Loss: 1.0466 || timer: 0.0899 sec.
iter 159670 || Loss: 0.7428 || timer: 0.1107 sec.
iter 159680 || Loss: 0.9451 || timer: 0.1072 sec.
iter 159690 || Loss: 1.0404 || timer: 0.0892 sec.
iter 159700 || Loss: 1.3459 || timer: 0.0914 sec.
iter 159710 || Loss: 0.9008 || timer: 0.0950 sec.
iter 159720 || Loss: 0.6481 || timer: 0.0865 sec.
iter 159730 || Loss: 1.4658 || timer: 0.0927 sec.
iter 159740 || Loss: 0.8881 || timer: 0.0920 sec.
iter 159750 || Loss: 1.3150 || timer: 0.0832 sec.
iter 159760 || Loss: 1.0244 || timer: 0.1013 sec.
iter 159770 || Loss: 1.0421 || timer: 0.0917 sec.
iter 159780 || Loss: 0.8221 || timer: 0.1010 sec.
iter 159790 || Loss: 0.7155 || timer: 0.0851 sec.
iter 159800 || Loss: 0.8101 || timer: 0.1028 sec.
iter 159810 || Loss: 1.1159 || timer: 0.0918 sec.
iter 159820 || Loss: 0.7344 || timer: 0.0887 sec.
iter 159830 || Loss: 1.0332 || timer: 0.0945 sec.
iter 159840 || Loss: 1.1296 || timer: 0.0859 sec.
iter 159850 || Loss: 0.7690 || timer: 0.0910 sec.
iter 159860 || Loss: 0.9774 || timer: 0.0910 sec.
iter 159870 || Loss: 1.0676 || timer: 0.0804 sec.
iter 159880 || Loss: 1.1173 || timer: 0.0840 sec.
iter 159890 || Loss: 1.0204 || timer: 0.0833 sec.
iter 159900 || Loss: 1.1155 || timer: 0.0966 sec.
iter 159910 || Loss: 1.2228 || timer: 0.1058 sec.
iter 159920 || Loss: 0.8481 || timer: 0.0939 sec.
iter 159930 || Loss: 0.9248 || timer: 0.1256 sec.
iter 159940 || Loss: 0.6816 || timer: 0.0228 sec.
iter 159950 || Loss: 0.1658 || timer: 0.0933 sec.
iter 159960 || Loss: 1.1596 || timer: 0.0947 sec.
iter 159970 || Loss: 0.5886 || timer: 0.0839 sec.
iter 159980 || Loss: 1.2609 || timer: 0.0763 sec.
iter 159990 || Loss: 0.8440 || timer: 0.0827 sec.
iter 160000 || Loss: 0.9889 || Saving state, iter: 160000
timer: 0.0774 sec.
iter 160010 || Loss: 0.9533 || timer: 0.0990 sec.
iter 160020 || Loss: 0.9159 || timer: 0.0896 sec.
iter 160030 || Loss: 1.0795 || timer: 0.0924 sec.
iter 160040 || Loss: 0.9105 || timer: 0.1204 sec.
iter 160050 || Loss: 0.7719 || timer: 0.0826 sec.
iter 160060 || Loss: 0.8869 || timer: 0.0930 sec.
iter 160070 || Loss: 0.7208 || timer: 0.0922 sec.
iter 160080 || Loss: 0.9037 || timer: 0.0905 sec.
iter 160090 || Loss: 0.8154 || timer: 0.0846 sec.
iter 160100 || Loss: 0.9344 || timer: 0.0971 sec.
iter 160110 || Loss: 0.8785 || timer: 0.0905 sec.
iter 160120 || Loss: 0.7902 || timer: 0.0843 sec.
iter 160130 || Loss: 1.1915 || timer: 0.1021 sec.
iter 160140 || Loss: 1.1339 || timer: 0.0844 sec.
iter 160150 || Loss: 1.0170 || timer: 0.0838 sec.
iter 160160 || Loss: 1.1758 || timer: 0.0842 sec.
iter 160170 || Loss: 0.9066 || timer: 0.0839 sec.
iter 160180 || Loss: 1.0507 || timer: 0.0892 sec.
iter 160190 || Loss: 0.7067 || timer: 0.0843 sec.
iter 160200 || Loss: 0.7061 || timer: 0.0761 sec.
iter 160210 || Loss: 0.8274 || timer: 0.1025 sec.
iter 160220 || Loss: 0.8750 || timer: 0.0847 sec.
iter 160230 || Loss: 1.1122 || timer: 0.1093 sec.
iter 160240 || Loss: 0.8075 || timer: 0.0937 sec.
iter 160250 || Loss: 1.3655 || timer: 0.0828 sec.
iter 160260 || Loss: 0.7321 || timer: 0.0984 sec.
iter 160270 || Loss: 0.8037 || timer: 0.0135 sec.
iter 160280 || Loss: 1.1532 || timer: 0.0829 sec.
iter 160290 || Loss: 1.1017 || timer: 0.0970 sec.
iter 160300 || Loss: 0.8070 || timer: 0.0851 sec.
iter 160310 || Loss: 1.0069 || timer: 0.0840 sec.
iter 160320 || Loss: 1.5041 || timer: 0.0850 sec.
iter 160330 || Loss: 1.4810 || timer: 0.0923 sec.
iter 160340 || Loss: 1.0090 || timer: 0.0841 sec.
iter 160350 || Loss: 1.4273 || timer: 0.0835 sec.
iter 160360 || Loss: 1.2362 || timer: 0.0873 sec.
iter 160370 || Loss: 1.1970 || timer: 0.1002 sec.
iter 160380 || Loss: 1.3238 || timer: 0.0820 sec.
iter 160390 || Loss: 1.5686 || timer: 0.0853 sec.
iter 160400 || Loss: 1.0078 || timer: 0.0922 sec.
iter 160410 || Loss: 0.8921 || timer: 0.1059 sec.
iter 160420 || Loss: 1.2634 || timer: 0.0816 sec.
iter 160430 || Loss: 1.0921 || timer: 0.0872 sec.
iter 160440 || Loss: 0.9846 || timer: 0.0895 sec.
iter 160450 || Loss: 0.9800 || timer: 0.0919 sec.
iter 160460 || Loss: 1.0340 || timer: 0.0917 sec.
iter 160470 || Loss: 0.6703 || timer: 0.1058 sec.
iter 160480 || Loss: 1.2634 || timer: 0.0875 sec.
iter 160490 || Loss: 1.3896 || timer: 0.0851 sec.
iter 160500 || Loss: 1.1258 || timer: 0.0748 sec.
iter 160510 || Loss: 0.8991 || timer: 0.0912 sec.
iter 160520 || Loss: 0.9789 || timer: 0.0915 sec.
iter 160530 || Loss: 0.8654 || timer: 0.0855 sec.
iter 160540 || Loss: 0.9429 || timer: 0.1036 sec.
iter 160550 || Loss: 0.8954 || timer: 0.1082 sec.
iter 160560 || Loss: 0.7404 || timer: 0.0822 sec.
iter 160570 || Loss: 0.9825 || timer: 0.0825 sec.
iter 160580 || Loss: 0.6488 || timer: 0.1030 sec.
iter 160590 || Loss: 0.8112 || timer: 0.0913 sec.
iter 160600 || Loss: 0.9081 || timer: 0.0210 sec.
iter 160610 || Loss: 0.6729 || timer: 0.0819 sec.
iter 160620 || Loss: 0.8861 || timer: 0.0881 sec.
iter 160630 || Loss: 1.0774 || timer: 0.0851 sec.
iter 160640 || Loss: 0.8858 || timer: 0.0884 sec.
iter 160650 || Loss: 1.2619 || timer: 0.0828 sec.
iter 160660 || Loss: 1.2127 || timer: 0.1075 sec.
iter 160670 || Loss: 1.0852 || timer: 0.1016 sec.
iter 160680 || Loss: 1.0177 || timer: 0.0919 sec.
iter 160690 || Loss: 1.1833 || timer: 0.0851 sec.
iter 160700 || Loss: 1.0833 || timer: 0.1276 sec.
iter 160710 || Loss: 1.3111 || timer: 0.0879 sec.
iter 160720 || Loss: 0.9235 || timer: 0.0906 sec.
iter 160730 || Loss: 0.6340 || timer: 0.0878 sec.
iter 160740 || Loss: 1.0499 || timer: 0.1095 sec.
iter 160750 || Loss: 0.9716 || timer: 0.1067 sec.
iter 160760 || Loss: 0.6882 || timer: 0.1060 sec.
iter 160770 || Loss: 1.1234 || timer: 0.0903 sec.
iter 160780 || Loss: 0.7703 || timer: 0.0902 sec.
iter 160790 || Loss: 1.0083 || timer: 0.0843 sec.
iter 160800 || Loss: 0.8787 || timer: 0.1004 sec.
iter 160810 || Loss: 1.0409 || timer: 0.0917 sec.
iter 160820 || Loss: 0.8992 || timer: 0.0907 sec.
iter 160830 || Loss: 0.8010 || timer: 0.0885 sec.
iter 160840 || Loss: 0.7129 || timer: 0.0911 sec.
iter 160850 || Loss: 1.2794 || timer: 0.0906 sec.
iter 160860 || Loss: 1.0655 || timer: 0.0831 sec.
iter 160870 || Loss: 0.7004 || timer: 0.1038 sec.
iter 160880 || Loss: 0.8456 || timer: 0.0938 sec.
iter 160890 || Loss: 1.0482 || timer: 0.0845 sec.
iter 160900 || Loss: 0.9546 || timer: 0.0839 sec.
iter 160910 || Loss: 0.9005 || timer: 0.0907 sec.
iter 160920 || Loss: 0.9065 || timer: 0.0953 sec.
iter 160930 || Loss: 1.2410 || timer: 0.0312 sec.
iter 160940 || Loss: 2.3140 || timer: 0.0907 sec.
iter 160950 || Loss: 1.1425 || timer: 0.0833 sec.
iter 160960 || Loss: 1.1306 || timer: 0.0861 sec.
iter 160970 || Loss: 1.1447 || timer: 0.0841 sec.
iter 160980 || Loss: 0.9136 || timer: 0.0902 sec.
iter 160990 || Loss: 0.9412 || timer: 0.0810 sec.
iter 161000 || Loss: 0.8052 || timer: 0.0828 sec.
iter 161010 || Loss: 0.9918 || timer: 0.0964 sec.
iter 161020 || Loss: 1.0026 || timer: 0.1008 sec.
iter 161030 || Loss: 1.1012 || timer: 0.1122 sec.
iter 161040 || Loss: 1.1576 || timer: 0.1000 sec.
iter 161050 || Loss: 1.0737 || timer: 0.0845 sec.
iter 161060 || Loss: 0.9419 || timer: 0.0918 sec.
iter 161070 || Loss: 0.8029 || timer: 0.1059 sec.
iter 161080 || Loss: 0.9115 || timer: 0.1129 sec.
iter 161090 || Loss: 0.7400 || timer: 0.0851 sec.
iter 161100 || Loss: 1.1812 || timer: 0.1095 sec.
iter 161110 || Loss: 1.0282 || timer: 0.0837 sec.
iter 161120 || Loss: 0.8666 || timer: 0.1096 sec.
iter 161130 || Loss: 1.0662 || timer: 0.0835 sec.
iter 161140 || Loss: 1.1726 || timer: 0.0897 sec.
iter 161150 || Loss: 0.9127 || timer: 0.0906 sec.
iter 161160 || Loss: 0.8027 || timer: 0.0825 sec.
iter 161170 || Loss: 0.7289 || timer: 0.0888 sec.
iter 161180 || Loss: 0.9066 || timer: 0.0917 sec.
iter 161190 || Loss: 1.0468 || timer: 0.1247 sec.
iter 161200 || Loss: 0.8518 || timer: 0.0880 sec.
iter 161210 || Loss: 0.7962 || timer: 0.1053 sec.
iter 161220 || Loss: 1.0970 || timer: 0.0833 sec.
iter 161230 || Loss: 0.7851 || timer: 0.0907 sec.
iter 161240 || Loss: 1.0023 || timer: 0.0910 sec.
iter 161250 || Loss: 0.9331 || timer: 0.0961 sec.
iter 161260 || Loss: 0.9315 || timer: 0.0174 sec.
iter 161270 || Loss: 1.0096 || timer: 0.0952 sec.
iter 161280 || Loss: 0.8015 || timer: 0.0910 sec.
iter 161290 || Loss: 1.0059 || timer: 0.0894 sec.
iter 161300 || Loss: 0.8337 || timer: 0.0907 sec.
iter 161310 || Loss: 1.0755 || timer: 0.0894 sec.
iter 161320 || Loss: 0.9029 || timer: 0.0892 sec.
iter 161330 || Loss: 0.8545 || timer: 0.0844 sec.
iter 161340 || Loss: 1.0350 || timer: 0.1077 sec.
iter 161350 || Loss: 0.9475 || timer: 0.0824 sec.
iter 161360 || Loss: 1.0305 || timer: 0.1002 sec.
iter 161370 || Loss: 0.8739 || timer: 0.0882 sec.
iter 161380 || Loss: 0.8803 || timer: 0.0868 sec.
iter 161390 || Loss: 1.2019 || timer: 0.0927 sec.
iter 161400 || Loss: 1.3328 || timer: 0.0846 sec.
iter 161410 || Loss: 1.3462 || timer: 0.1128 sec.
iter 161420 || Loss: 1.0157 || timer: 0.0907 sec.
iter 161430 || Loss: 0.9726 || timer: 0.0867 sec.
iter 161440 || Loss: 0.9513 || timer: 0.0855 sec.
iter 161450 || Loss: 1.3825 || timer: 0.1186 sec.
iter 161460 || Loss: 0.9846 || timer: 0.0933 sec.
iter 161470 || Loss: 0.7281 || timer: 0.0911 sec.
iter 161480 || Loss: 0.8733 || timer: 0.1122 sec.
iter 161490 || Loss: 0.9275 || timer: 0.0894 sec.
iter 161500 || Loss: 1.0259 || timer: 0.0947 sec.
iter 161510 || Loss: 1.2516 || timer: 0.0937 sec.
iter 161520 || Loss: 0.8197 || timer: 0.0911 sec.
iter 161530 || Loss: 1.1056 || timer: 0.0860 sec.
iter 161540 || Loss: 1.1185 || timer: 0.0768 sec.
iter 161550 || Loss: 0.6975 || timer: 0.0922 sec.
iter 161560 || Loss: 0.8628 || timer: 0.0913 sec.
iter 161570 || Loss: 1.3597 || timer: 0.0836 sec.
iter 161580 || Loss: 0.7691 || timer: 0.0968 sec.
iter 161590 || Loss: 1.2158 || timer: 0.0173 sec.
iter 161600 || Loss: 0.7401 || timer: 0.0766 sec.
iter 161610 || Loss: 0.9521 || timer: 0.0769 sec.
iter 161620 || Loss: 0.9700 || timer: 0.0918 sec.
iter 161630 || Loss: 0.7807 || timer: 0.0953 sec.
iter 161640 || Loss: 1.1920 || timer: 0.0839 sec.
iter 161650 || Loss: 1.0302 || timer: 0.0964 sec.
iter 161660 || Loss: 1.0206 || timer: 0.0916 sec.
iter 161670 || Loss: 1.3278 || timer: 0.0932 sec.
iter 161680 || Loss: 1.0723 || timer: 0.0912 sec.
iter 161690 || Loss: 0.9266 || timer: 0.0883 sec.
iter 161700 || Loss: 0.8431 || timer: 0.0769 sec.
iter 161710 || Loss: 1.0090 || timer: 0.0842 sec.
iter 161720 || Loss: 0.9383 || timer: 0.0922 sec.
iter 161730 || Loss: 1.0575 || timer: 0.0919 sec.
iter 161740 || Loss: 1.0284 || timer: 0.0838 sec.
iter 161750 || Loss: 1.2185 || timer: 0.0895 sec.
iter 161760 || Loss: 1.3227 || timer: 0.0897 sec.
iter 161770 || Loss: 0.9712 || timer: 0.0904 sec.
iter 161780 || Loss: 0.8403 || timer: 0.0947 sec.
iter 161790 || Loss: 0.8829 || timer: 0.1151 sec.
iter 161800 || Loss: 0.8703 || timer: 0.0956 sec.
iter 161810 || Loss: 0.9794 || timer: 0.0894 sec.
iter 161820 || Loss: 0.8462 || timer: 0.0870 sec.
iter 161830 || Loss: 0.7898 || timer: 0.0839 sec.
iter 161840 || Loss: 0.8672 || timer: 0.1070 sec.
iter 161850 || Loss: 0.7323 || timer: 0.1178 sec.
iter 161860 || Loss: 0.8596 || timer: 0.0962 sec.
iter 161870 || Loss: 1.2719 || timer: 0.0888 sec.
iter 161880 || Loss: 1.1673 || timer: 0.0895 sec.
iter 161890 || Loss: 1.1793 || timer: 0.0921 sec.
iter 161900 || Loss: 1.2193 || timer: 0.0820 sec.
iter 161910 || Loss: 0.8672 || timer: 0.1202 sec.
iter 161920 || Loss: 1.0207 || timer: 0.0166 sec.
iter 161930 || Loss: 0.8687 || timer: 0.0823 sec.
iter 161940 || Loss: 0.6821 || timer: 0.1008 sec.
iter 161950 || Loss: 1.0719 || timer: 0.0892 sec.
iter 161960 || Loss: 0.7443 || timer: 0.1076 sec.
iter 161970 || Loss: 1.5462 || timer: 0.0822 sec.
iter 161980 || Loss: 1.0333 || timer: 0.0824 sec.
iter 161990 || Loss: 0.7984 || timer: 0.0824 sec.
iter 162000 || Loss: 0.7832 || timer: 0.0893 sec.
iter 162010 || Loss: 1.0579 || timer: 0.0867 sec.
iter 162020 || Loss: 0.9581 || timer: 0.1066 sec.
iter 162030 || Loss: 0.9200 || timer: 0.0918 sec.
iter 162040 || Loss: 1.1693 || timer: 0.1058 sec.
iter 162050 || Loss: 0.9297 || timer: 0.0828 sec.
iter 162060 || Loss: 0.9487 || timer: 0.0905 sec.
iter 162070 || Loss: 0.9919 || timer: 0.0892 sec.
iter 162080 || Loss: 0.9403 || timer: 0.0831 sec.
iter 162090 || Loss: 1.1008 || timer: 0.0897 sec.
iter 162100 || Loss: 1.3211 || timer: 0.0864 sec.
iter 162110 || Loss: 0.9352 || timer: 0.0935 sec.
iter 162120 || Loss: 0.9399 || timer: 0.0943 sec.
iter 162130 || Loss: 0.7491 || timer: 0.0895 sec.
iter 162140 || Loss: 0.7834 || timer: 0.0884 sec.
iter 162150 || Loss: 0.7412 || timer: 0.0887 sec.
iter 162160 || Loss: 0.8345 || timer: 0.0940 sec.
iter 162170 || Loss: 1.0261 || timer: 0.0946 sec.
iter 162180 || Loss: 0.9026 || timer: 0.0973 sec.
iter 162190 || Loss: 1.2186 || timer: 0.0878 sec.
iter 162200 || Loss: 1.2359 || timer: 0.0933 sec.
iter 162210 || Loss: 1.0205 || timer: 0.1238 sec.
iter 162220 || Loss: 1.5711 || timer: 0.0903 sec.
iter 162230 || Loss: 1.2753 || timer: 0.0760 sec.
iter 162240 || Loss: 1.3131 || timer: 0.0876 sec.
iter 162250 || Loss: 1.0372 || timer: 0.0171 sec.
iter 162260 || Loss: 1.0877 || timer: 0.1413 sec.
iter 162270 || Loss: 0.8716 || timer: 0.0948 sec.
iter 162280 || Loss: 0.8012 || timer: 0.0841 sec.
iter 162290 || Loss: 0.9208 || timer: 0.1245 sec.
iter 162300 || Loss: 0.7804 || timer: 0.0842 sec.
iter 162310 || Loss: 1.0493 || timer: 0.0757 sec.
iter 162320 || Loss: 1.0376 || timer: 0.0918 sec.
iter 162330 || Loss: 0.9657 || timer: 0.0825 sec.
iter 162340 || Loss: 1.0533 || timer: 0.0915 sec.
iter 162350 || Loss: 0.9651 || timer: 0.1151 sec.
iter 162360 || Loss: 0.7883 || timer: 0.0827 sec.
iter 162370 || Loss: 0.7639 || timer: 0.0890 sec.
iter 162380 || Loss: 0.7995 || timer: 0.0889 sec.
iter 162390 || Loss: 1.0348 || timer: 0.1090 sec.
iter 162400 || Loss: 1.0661 || timer: 0.0915 sec.
iter 162410 || Loss: 0.9804 || timer: 0.0812 sec.
iter 162420 || Loss: 0.9330 || timer: 0.0891 sec.
iter 162430 || Loss: 0.9041 || timer: 0.0889 sec.
iter 162440 || Loss: 1.0769 || timer: 0.0901 sec.
iter 162450 || Loss: 1.6350 || timer: 0.0811 sec.
iter 162460 || Loss: 1.1257 || timer: 0.1012 sec.
iter 162470 || Loss: 0.7050 || timer: 0.0830 sec.
iter 162480 || Loss: 1.0241 || timer: 0.0904 sec.
iter 162490 || Loss: 1.0862 || timer: 0.0819 sec.
iter 162500 || Loss: 0.9320 || timer: 0.0908 sec.
iter 162510 || Loss: 1.3585 || timer: 0.0879 sec.
iter 162520 || Loss: 0.7333 || timer: 0.0928 sec.
iter 162530 || Loss: 0.8705 || timer: 0.0913 sec.
iter 162540 || Loss: 0.7791 || timer: 0.0804 sec.
iter 162550 || Loss: 1.0346 || timer: 0.0826 sec.
iter 162560 || Loss: 0.9394 || timer: 0.0844 sec.
iter 162570 || Loss: 0.8603 || timer: 0.0938 sec.
iter 162580 || Loss: 0.8968 || timer: 0.0249 sec.
iter 162590 || Loss: 0.6877 || timer: 0.0814 sec.
iter 162600 || Loss: 0.9479 || timer: 0.1032 sec.
iter 162610 || Loss: 0.7863 || timer: 0.0812 sec.
iter 162620 || Loss: 0.5809 || timer: 0.1191 sec.
iter 162630 || Loss: 1.1034 || timer: 0.0751 sec.
iter 162640 || Loss: 0.8551 || timer: 0.0750 sec.
iter 162650 || Loss: 0.7132 || timer: 0.0856 sec.
iter 162660 || Loss: 0.8782 || timer: 0.0804 sec.
iter 162670 || Loss: 0.7981 || timer: 0.0820 sec.
iter 162680 || Loss: 1.1373 || timer: 0.0954 sec.
iter 162690 || Loss: 0.6801 || timer: 0.0982 sec.
iter 162700 || Loss: 0.8869 || timer: 0.0886 sec.
iter 162710 || Loss: 0.8106 || timer: 0.0818 sec.
iter 162720 || Loss: 0.9910 || timer: 0.0904 sec.
iter 162730 || Loss: 0.8523 || timer: 0.0754 sec.
iter 162740 || Loss: 0.9780 || timer: 0.0736 sec.
iter 162750 || Loss: 1.0934 || timer: 0.0877 sec.
iter 162760 || Loss: 0.9166 || timer: 0.1011 sec.
iter 162770 || Loss: 0.7995 || timer: 0.0872 sec.
iter 162780 || Loss: 1.1431 || timer: 0.0923 sec.
iter 162790 || Loss: 0.7542 || timer: 0.0751 sec.
iter 162800 || Loss: 0.8742 || timer: 0.0749 sec.
iter 162810 || Loss: 0.6964 || timer: 0.0808 sec.
iter 162820 || Loss: 1.1024 || timer: 0.0876 sec.
iter 162830 || Loss: 0.9499 || timer: 0.0822 sec.
iter 162840 || Loss: 1.0054 || timer: 0.0771 sec.
iter 162850 || Loss: 0.8004 || timer: 0.0897 sec.
iter 162860 || Loss: 1.2568 || timer: 0.0879 sec.
iter 162870 || Loss: 1.3284 || timer: 0.0815 sec.
iter 162880 || Loss: 0.9339 || timer: 0.0826 sec.
iter 162890 || Loss: 1.0941 || timer: 0.0890 sec.
iter 162900 || Loss: 1.1271 || timer: 0.0819 sec.
iter 162910 || Loss: 1.0250 || timer: 0.0134 sec.
iter 162920 || Loss: 1.0601 || timer: 0.0904 sec.
iter 162930 || Loss: 1.0977 || timer: 0.0875 sec.
iter 162940 || Loss: 0.9156 || timer: 0.0913 sec.
iter 162950 || Loss: 0.9351 || timer: 0.0833 sec.
iter 162960 || Loss: 1.0848 || timer: 0.0899 sec.
iter 162970 || Loss: 1.1282 || timer: 0.1032 sec.
iter 162980 || Loss: 1.1414 || timer: 0.0822 sec.
iter 162990 || Loss: 0.7262 || timer: 0.0833 sec.
iter 163000 || Loss: 1.0909 || timer: 0.0908 sec.
iter 163010 || Loss: 1.0714 || timer: 0.1146 sec.
iter 163020 || Loss: 1.0508 || timer: 0.0925 sec.
iter 163030 || Loss: 0.7266 || timer: 0.0822 sec.
iter 163040 || Loss: 0.8944 || timer: 0.0941 sec.
iter 163050 || Loss: 0.7265 || timer: 0.0819 sec.
iter 163060 || Loss: 0.8642 || timer: 0.0909 sec.
iter 163070 || Loss: 1.3428 || timer: 0.0903 sec.
iter 163080 || Loss: 0.8199 || timer: 0.0908 sec.
iter 163090 || Loss: 0.8423 || timer: 0.0910 sec.
iter 163100 || Loss: 0.8864 || timer: 0.0896 sec.
iter 163110 || Loss: 0.8588 || timer: 0.0924 sec.
iter 163120 || Loss: 1.2672 || timer: 0.0891 sec.
iter 163130 || Loss: 1.3223 || timer: 0.0826 sec.
iter 163140 || Loss: 0.9422 || timer: 0.0830 sec.
iter 163150 || Loss: 0.8071 || timer: 0.0858 sec.
iter 163160 || Loss: 0.9650 || timer: 0.0888 sec.
iter 163170 || Loss: 0.8632 || timer: 0.0885 sec.
iter 163180 || Loss: 0.8259 || timer: 0.0831 sec.
iter 163190 || Loss: 1.1781 || timer: 0.0945 sec.
iter 163200 || Loss: 1.0946 || timer: 0.0991 sec.
iter 163210 || Loss: 0.9455 || timer: 0.1038 sec.
iter 163220 || Loss: 0.7246 || timer: 0.0885 sec.
iter 163230 || Loss: 1.0302 || timer: 0.0911 sec.
iter 163240 || Loss: 0.7926 || timer: 0.0181 sec.
iter 163250 || Loss: 0.8725 || timer: 0.0920 sec.
iter 163260 || Loss: 1.0645 || timer: 0.0815 sec.
iter 163270 || Loss: 0.6701 || timer: 0.0807 sec.
iter 163280 || Loss: 1.2359 || timer: 0.0878 sec.
iter 163290 || Loss: 0.9240 || timer: 0.0905 sec.
iter 163300 || Loss: 0.9125 || timer: 0.0827 sec.
iter 163310 || Loss: 1.3020 || timer: 0.1103 sec.
iter 163320 || Loss: 0.8438 || timer: 0.0896 sec.
iter 163330 || Loss: 0.7743 || timer: 0.0865 sec.
iter 163340 || Loss: 0.8360 || timer: 0.0986 sec.
iter 163350 || Loss: 0.9698 || timer: 0.0896 sec.
iter 163360 || Loss: 0.8934 || timer: 0.0900 sec.
iter 163370 || Loss: 1.2349 || timer: 0.0890 sec.
iter 163380 || Loss: 0.9794 || timer: 0.0835 sec.
iter 163390 || Loss: 0.8824 || timer: 0.0823 sec.
iter 163400 || Loss: 0.6815 || timer: 0.0888 sec.
iter 163410 || Loss: 0.8661 || timer: 0.0747 sec.
iter 163420 || Loss: 1.0283 || timer: 0.0815 sec.
iter 163430 || Loss: 0.8363 || timer: 0.0752 sec.
iter 163440 || Loss: 0.8525 || timer: 0.0823 sec.
iter 163450 || Loss: 0.8697 || timer: 0.0828 sec.
iter 163460 || Loss: 0.8396 || timer: 0.1027 sec.
iter 163470 || Loss: 0.7938 || timer: 0.0798 sec.
iter 163480 || Loss: 0.9031 || timer: 0.0902 sec.
iter 163490 || Loss: 1.0153 || timer: 0.0827 sec.
iter 163500 || Loss: 0.7700 || timer: 0.1086 sec.
iter 163510 || Loss: 1.0965 || timer: 0.0825 sec.
iter 163520 || Loss: 0.7975 || timer: 0.0830 sec.
iter 163530 || Loss: 0.8187 || timer: 0.0753 sec.
iter 163540 || Loss: 0.7271 || timer: 0.0826 sec.
iter 163550 || Loss: 0.9927 || timer: 0.0835 sec.
iter 163560 || Loss: 1.1241 || timer: 0.0896 sec.
iter 163570 || Loss: 0.9283 || timer: 0.0168 sec.
iter 163580 || Loss: 0.3971 || timer: 0.0832 sec.
iter 163590 || Loss: 0.9045 || timer: 0.0821 sec.
iter 163600 || Loss: 0.7453 || timer: 0.0830 sec.
iter 163610 || Loss: 1.2441 || timer: 0.0918 sec.
iter 163620 || Loss: 1.0551 || timer: 0.0891 sec.
iter 163630 || Loss: 0.7230 || timer: 0.1120 sec.
iter 163640 || Loss: 0.8606 || timer: 0.0878 sec.
iter 163650 || Loss: 1.0426 || timer: 0.0851 sec.
iter 163660 || Loss: 0.8940 || timer: 0.0832 sec.
iter 163670 || Loss: 0.7909 || timer: 0.0951 sec.
iter 163680 || Loss: 0.9709 || timer: 0.0916 sec.
iter 163690 || Loss: 0.9876 || timer: 0.1112 sec.
iter 163700 || Loss: 0.8892 || timer: 0.0864 sec.
iter 163710 || Loss: 0.9709 || timer: 0.0880 sec.
iter 163720 || Loss: 0.8726 || timer: 0.0888 sec.
iter 163730 || Loss: 0.8991 || timer: 0.0920 sec.
iter 163740 || Loss: 1.2356 || timer: 0.0815 sec.
iter 163750 || Loss: 1.1985 || timer: 0.0823 sec.
iter 163760 || Loss: 0.8553 || timer: 0.0954 sec.
iter 163770 || Loss: 0.8610 || timer: 0.1007 sec.
iter 163780 || Loss: 0.8811 || timer: 0.0755 sec.
iter 163790 || Loss: 0.9988 || timer: 0.0826 sec.
iter 163800 || Loss: 1.4156 || timer: 0.0736 sec.
iter 163810 || Loss: 0.8462 || timer: 0.0900 sec.
iter 163820 || Loss: 0.7812 || timer: 0.1004 sec.
iter 163830 || Loss: 0.8333 || timer: 0.0891 sec.
iter 163840 || Loss: 1.0694 || timer: 0.1016 sec.
iter 163850 || Loss: 0.8588 || timer: 0.0887 sec.
iter 163860 || Loss: 1.1474 || timer: 0.0895 sec.
iter 163870 || Loss: 1.2037 || timer: 0.0905 sec.
iter 163880 || Loss: 0.9345 || timer: 0.1025 sec.
iter 163890 || Loss: 0.8839 || timer: 0.0851 sec.
iter 163900 || Loss: 0.7190 || timer: 0.0208 sec.
iter 163910 || Loss: 0.7756 || timer: 0.1076 sec.
iter 163920 || Loss: 1.2259 || timer: 0.0926 sec.
iter 163930 || Loss: 0.8783 || timer: 0.0737 sec.
iter 163940 || Loss: 1.1671 || timer: 0.0819 sec.
iter 163950 || Loss: 1.0154 || timer: 0.1025 sec.
iter 163960 || Loss: 0.7698 || timer: 0.0861 sec.
iter 163970 || Loss: 0.8457 || timer: 0.0868 sec.
iter 163980 || Loss: 1.0250 || timer: 0.0944 sec.
iter 163990 || Loss: 1.1140 || timer: 0.0821 sec.
iter 164000 || Loss: 1.0426 || timer: 0.1165 sec.
iter 164010 || Loss: 0.9281 || timer: 0.0826 sec.
iter 164020 || Loss: 0.9060 || timer: 0.0850 sec.
iter 164030 || Loss: 1.2299 || timer: 0.0885 sec.
iter 164040 || Loss: 0.8316 || timer: 0.0903 sec.
iter 164050 || Loss: 0.9058 || timer: 0.1044 sec.
iter 164060 || Loss: 1.0303 || timer: 0.0849 sec.
iter 164070 || Loss: 1.0215 || timer: 0.1020 sec.
iter 164080 || Loss: 1.1103 || timer: 0.0884 sec.
iter 164090 || Loss: 0.8145 || timer: 0.0897 sec.
iter 164100 || Loss: 1.0157 || timer: 0.0976 sec.
iter 164110 || Loss: 0.9604 || timer: 0.1309 sec.
iter 164120 || Loss: 1.0826 || timer: 0.0919 sec.
iter 164130 || Loss: 1.1755 || timer: 0.0874 sec.
iter 164140 || Loss: 0.9173 || timer: 0.0886 sec.
iter 164150 || Loss: 0.9742 || timer: 0.0893 sec.
iter 164160 || Loss: 1.0556 || timer: 0.0965 sec.
iter 164170 || Loss: 0.9637 || timer: 0.1362 sec.
iter 164180 || Loss: 0.9623 || timer: 0.0819 sec.
iter 164190 || Loss: 0.7327 || timer: 0.0882 sec.
iter 164200 || Loss: 1.1874 || timer: 0.0892 sec.
iter 164210 || Loss: 1.0200 || timer: 0.1058 sec.
iter 164220 || Loss: 0.9927 || timer: 0.0905 sec.
iter 164230 || Loss: 0.7221 || timer: 0.0218 sec.
iter 164240 || Loss: 0.3804 || timer: 0.1090 sec.
iter 164250 || Loss: 1.1595 || timer: 0.0874 sec.
iter 164260 || Loss: 1.1995 || timer: 0.0821 sec.
iter 164270 || Loss: 0.9027 || timer: 0.0988 sec.
iter 164280 || Loss: 0.7627 || timer: 0.0888 sec.
iter 164290 || Loss: 1.0359 || timer: 0.0973 sec.
iter 164300 || Loss: 0.8076 || timer: 0.0829 sec.
iter 164310 || Loss: 1.0294 || timer: 0.0911 sec.
iter 164320 || Loss: 1.1498 || timer: 0.1153 sec.
iter 164330 || Loss: 0.8315 || timer: 0.0886 sec.
iter 164340 || Loss: 1.3364 || timer: 0.0898 sec.
iter 164350 || Loss: 0.8611 || timer: 0.0929 sec.
iter 164360 || Loss: 0.6671 || timer: 0.0938 sec.
iter 164370 || Loss: 1.1603 || timer: 0.1028 sec.
iter 164380 || Loss: 1.1381 || timer: 0.0932 sec.
iter 164390 || Loss: 1.0397 || timer: 0.1087 sec.
iter 164400 || Loss: 0.7192 || timer: 0.0820 sec.
iter 164410 || Loss: 0.9514 || timer: 0.0766 sec.
iter 164420 || Loss: 0.9889 || timer: 0.1029 sec.
iter 164430 || Loss: 0.9746 || timer: 0.0919 sec.
iter 164440 || Loss: 0.9644 || timer: 0.0910 sec.
iter 164450 || Loss: 0.9283 || timer: 0.1026 sec.
iter 164460 || Loss: 1.1460 || timer: 0.0917 sec.
iter 164470 || Loss: 1.1533 || timer: 0.1124 sec.
iter 164480 || Loss: 0.9994 || timer: 0.0920 sec.
iter 164490 || Loss: 0.7414 || timer: 0.0874 sec.
iter 164500 || Loss: 0.8631 || timer: 0.0969 sec.
iter 164510 || Loss: 0.9897 || timer: 0.0891 sec.
iter 164520 || Loss: 0.9019 || timer: 0.0837 sec.
iter 164530 || Loss: 0.7815 || timer: 0.0815 sec.
iter 164540 || Loss: 0.9622 || timer: 0.0816 sec.
iter 164550 || Loss: 0.9327 || timer: 0.0826 sec.
iter 164560 || Loss: 1.1989 || timer: 0.0214 sec.
iter 164570 || Loss: 0.8270 || timer: 0.0903 sec.
iter 164580 || Loss: 0.7482 || timer: 0.0833 sec.
iter 164590 || Loss: 1.0137 || timer: 0.0857 sec.
iter 164600 || Loss: 1.0543 || timer: 0.0844 sec.
iter 164610 || Loss: 0.9677 || timer: 0.0902 sec.
iter 164620 || Loss: 1.0398 || timer: 0.0883 sec.
iter 164630 || Loss: 0.7413 || timer: 0.0865 sec.
iter 164640 || Loss: 0.9697 || timer: 0.0834 sec.
iter 164650 || Loss: 0.8361 || timer: 0.0838 sec.
iter 164660 || Loss: 0.9206 || timer: 0.0959 sec.
iter 164670 || Loss: 0.7668 || timer: 0.1152 sec.
iter 164680 || Loss: 0.7711 || timer: 0.1032 sec.
iter 164690 || Loss: 0.8643 || timer: 0.0899 sec.
iter 164700 || Loss: 0.7786 || timer: 0.0993 sec.
iter 164710 || Loss: 0.9278 || timer: 0.0927 sec.
iter 164720 || Loss: 1.4034 || timer: 0.0854 sec.
iter 164730 || Loss: 0.9063 || timer: 0.1025 sec.
iter 164740 || Loss: 1.0609 || timer: 0.0895 sec.
iter 164750 || Loss: 1.3455 || timer: 0.1269 sec.
iter 164760 || Loss: 0.9307 || timer: 0.0884 sec.
iter 164770 || Loss: 1.2478 || timer: 0.0921 sec.
iter 164780 || Loss: 1.4226 || timer: 0.0969 sec.
iter 164790 || Loss: 0.7888 || timer: 0.0929 sec.
iter 164800 || Loss: 0.8359 || timer: 0.1093 sec.
iter 164810 || Loss: 1.3069 || timer: 0.0827 sec.
iter 164820 || Loss: 0.7276 || timer: 0.0870 sec.
iter 164830 || Loss: 1.1487 || timer: 0.0880 sec.
iter 164840 || Loss: 1.0158 || timer: 0.0916 sec.
iter 164850 || Loss: 1.0324 || timer: 0.0992 sec.
iter 164860 || Loss: 1.0025 || timer: 0.0906 sec.
iter 164870 || Loss: 0.9154 || timer: 0.0913 sec.
iter 164880 || Loss: 0.9486 || timer: 0.0922 sec.
iter 164890 || Loss: 0.9009 || timer: 0.0263 sec.
iter 164900 || Loss: 0.2422 || timer: 0.0908 sec.
iter 164910 || Loss: 0.8620 || timer: 0.0835 sec.
iter 164920 || Loss: 0.8266 || timer: 0.0836 sec.
iter 164930 || Loss: 0.9246 || timer: 0.0907 sec.
iter 164940 || Loss: 1.2096 || timer: 0.0914 sec.
iter 164950 || Loss: 0.8907 || timer: 0.1084 sec.
iter 164960 || Loss: 0.6311 || timer: 0.1097 sec.
iter 164970 || Loss: 1.0330 || timer: 0.0914 sec.
iter 164980 || Loss: 1.0635 || timer: 0.0897 sec.
iter 164990 || Loss: 1.2539 || timer: 0.0967 sec.
iter 165000 || Loss: 0.8409 || Saving state, iter: 165000
timer: 0.0918 sec.
iter 165010 || Loss: 0.8743 || timer: 0.0873 sec.
iter 165020 || Loss: 0.6835 || timer: 0.0924 sec.
iter 165030 || Loss: 1.3442 || timer: 0.0839 sec.
iter 165040 || Loss: 1.2662 || timer: 0.0836 sec.
iter 165050 || Loss: 0.8028 || timer: 0.0827 sec.
iter 165060 || Loss: 0.8033 || timer: 0.0977 sec.
iter 165070 || Loss: 0.8400 || timer: 0.0880 sec.
iter 165080 || Loss: 0.9164 || timer: 0.1022 sec.
iter 165090 || Loss: 0.9924 || timer: 0.0825 sec.
iter 165100 || Loss: 1.2715 || timer: 0.0829 sec.
iter 165110 || Loss: 0.5971 || timer: 0.0888 sec.
iter 165120 || Loss: 0.9412 || timer: 0.0824 sec.
iter 165130 || Loss: 0.8620 || timer: 0.0885 sec.
iter 165140 || Loss: 1.0162 || timer: 0.1004 sec.
iter 165150 || Loss: 0.8716 || timer: 0.0818 sec.
iter 165160 || Loss: 0.8373 || timer: 0.0829 sec.
iter 165170 || Loss: 0.7434 || timer: 0.0913 sec.
iter 165180 || Loss: 0.8217 || timer: 0.0885 sec.
iter 165190 || Loss: 1.0257 || timer: 0.0840 sec.
iter 165200 || Loss: 1.0180 || timer: 0.0884 sec.
iter 165210 || Loss: 0.9565 || timer: 0.0895 sec.
iter 165220 || Loss: 0.8263 || timer: 0.0150 sec.
iter 165230 || Loss: 0.7870 || timer: 0.0892 sec.
iter 165240 || Loss: 0.8008 || timer: 0.0819 sec.
iter 165250 || Loss: 0.6955 || timer: 0.1306 sec.
iter 165260 || Loss: 0.5945 || timer: 0.0905 sec.
iter 165270 || Loss: 1.1935 || timer: 0.0878 sec.
iter 165280 || Loss: 1.3165 || timer: 0.0902 sec.
iter 165290 || Loss: 0.6986 || timer: 0.0885 sec.
iter 165300 || Loss: 0.7719 || timer: 0.0850 sec.
iter 165310 || Loss: 1.1339 || timer: 0.0913 sec.
iter 165320 || Loss: 0.7419 || timer: 0.0956 sec.
iter 165330 || Loss: 0.8484 || timer: 0.0923 sec.
iter 165340 || Loss: 1.1321 || timer: 0.0835 sec.
iter 165350 || Loss: 1.0691 || timer: 0.1024 sec.
iter 165360 || Loss: 0.8488 || timer: 0.0884 sec.
iter 165370 || Loss: 0.9017 || timer: 0.0851 sec.
iter 165380 || Loss: 0.7506 || timer: 0.0752 sec.
iter 165390 || Loss: 0.7076 || timer: 0.0896 sec.
iter 165400 || Loss: 0.8143 || timer: 0.1131 sec.
iter 165410 || Loss: 1.0885 || timer: 0.0822 sec.
iter 165420 || Loss: 0.5955 || timer: 0.0919 sec.
iter 165430 || Loss: 1.0142 || timer: 0.1021 sec.
iter 165440 || Loss: 0.9889 || timer: 0.0873 sec.
iter 165450 || Loss: 0.7594 || timer: 0.0814 sec.
iter 165460 || Loss: 1.3992 || timer: 0.0850 sec.
iter 165470 || Loss: 1.0106 || timer: 0.0983 sec.
iter 165480 || Loss: 0.8339 || timer: 0.0903 sec.
iter 165490 || Loss: 0.9441 || timer: 0.0920 sec.
iter 165500 || Loss: 0.7837 || timer: 0.1169 sec.
iter 165510 || Loss: 1.0440 || timer: 0.1080 sec.
iter 165520 || Loss: 0.8935 || timer: 0.0919 sec.
iter 165530 || Loss: 0.8994 || timer: 0.0979 sec.
iter 165540 || Loss: 0.7240 || timer: 0.0917 sec.
iter 165550 || Loss: 1.0732 || timer: 0.0240 sec.
iter 165560 || Loss: 0.2982 || timer: 0.0882 sec.
iter 165570 || Loss: 0.8919 || timer: 0.0927 sec.
iter 165580 || Loss: 0.8365 || timer: 0.0963 sec.
iter 165590 || Loss: 0.9924 || timer: 0.0870 sec.
iter 165600 || Loss: 1.1085 || timer: 0.0895 sec.
iter 165610 || Loss: 0.7282 || timer: 0.1148 sec.
iter 165620 || Loss: 1.0234 || timer: 0.0953 sec.
iter 165630 || Loss: 0.8998 || timer: 0.1028 sec.
iter 165640 || Loss: 1.2248 || timer: 0.0896 sec.
iter 165650 || Loss: 0.9966 || timer: 0.0869 sec.
iter 165660 || Loss: 0.7874 || timer: 0.0826 sec.
iter 165670 || Loss: 1.0126 || timer: 0.0833 sec.
iter 165680 || Loss: 0.6413 || timer: 0.0823 sec.
iter 165690 || Loss: 0.6460 || timer: 0.1013 sec.
iter 165700 || Loss: 0.7000 || timer: 0.0898 sec.
iter 165710 || Loss: 1.0258 || timer: 0.0947 sec.
iter 165720 || Loss: 0.7653 || timer: 0.0825 sec.
iter 165730 || Loss: 0.9342 || timer: 0.0960 sec.
iter 165740 || Loss: 1.0024 || timer: 0.0969 sec.
iter 165750 || Loss: 1.0323 || timer: 0.0901 sec.
iter 165760 || Loss: 0.7388 || timer: 0.0926 sec.
iter 165770 || Loss: 1.2279 || timer: 0.1145 sec.
iter 165780 || Loss: 0.9984 || timer: 0.0896 sec.
iter 165790 || Loss: 0.8819 || timer: 0.0985 sec.
iter 165800 || Loss: 1.0144 || timer: 0.0822 sec.
iter 165810 || Loss: 0.9436 || timer: 0.0820 sec.
iter 165820 || Loss: 1.0111 || timer: 0.0850 sec.
iter 165830 || Loss: 1.3887 || timer: 0.0983 sec.
iter 165840 || Loss: 1.3267 || timer: 0.1060 sec.
iter 165850 || Loss: 1.2641 || timer: 0.0888 sec.
iter 165860 || Loss: 0.9379 || timer: 0.0860 sec.
iter 165870 || Loss: 1.1190 || timer: 0.0894 sec.
iter 165880 || Loss: 1.1958 || timer: 0.0177 sec.
iter 165890 || Loss: 2.0757 || timer: 0.0916 sec.
iter 165900 || Loss: 0.8912 || timer: 0.0855 sec.
iter 165910 || Loss: 1.2265 || timer: 0.0751 sec.
iter 165920 || Loss: 0.8427 || timer: 0.0881 sec.
iter 165930 || Loss: 0.7312 || timer: 0.1004 sec.
iter 165940 || Loss: 1.1631 || timer: 0.0893 sec.
iter 165950 || Loss: 1.1553 || timer: 0.0837 sec.
iter 165960 || Loss: 0.8245 || timer: 0.1038 sec.
iter 165970 || Loss: 1.1088 || timer: 0.0913 sec.
iter 165980 || Loss: 1.0864 || timer: 0.1154 sec.
iter 165990 || Loss: 1.0125 || timer: 0.1034 sec.
iter 166000 || Loss: 0.9790 || timer: 0.0832 sec.
iter 166010 || Loss: 0.7647 || timer: 0.0874 sec.
iter 166020 || Loss: 0.8923 || timer: 0.0755 sec.
iter 166030 || Loss: 1.1450 || timer: 0.0839 sec.
iter 166040 || Loss: 1.1533 || timer: 0.0819 sec.
iter 166050 || Loss: 1.0711 || timer: 0.0871 sec.
iter 166060 || Loss: 1.0615 || timer: 0.0826 sec.
iter 166070 || Loss: 0.7202 || timer: 0.0912 sec.
iter 166080 || Loss: 0.9438 || timer: 0.0826 sec.
iter 166090 || Loss: 0.8463 || timer: 0.0845 sec.
iter 166100 || Loss: 0.8941 || timer: 0.0901 sec.
iter 166110 || Loss: 1.0582 || timer: 0.0812 sec.
iter 166120 || Loss: 0.9356 || timer: 0.0897 sec.
iter 166130 || Loss: 1.3419 || timer: 0.0879 sec.
iter 166140 || Loss: 0.9870 || timer: 0.0996 sec.
iter 166150 || Loss: 1.0344 || timer: 0.0874 sec.
iter 166160 || Loss: 0.9890 || timer: 0.0826 sec.
iter 166170 || Loss: 1.0530 || timer: 0.0826 sec.
iter 166180 || Loss: 1.0276 || timer: 0.0959 sec.
iter 166190 || Loss: 0.7733 || timer: 0.0856 sec.
iter 166200 || Loss: 0.7327 || timer: 0.0824 sec.
iter 166210 || Loss: 1.1100 || timer: 0.0246 sec.
iter 166220 || Loss: 0.2649 || timer: 0.0852 sec.
iter 166230 || Loss: 1.1059 || timer: 0.0846 sec.
iter 166240 || Loss: 0.9566 || timer: 0.0880 sec.
iter 166250 || Loss: 1.0661 || timer: 0.1011 sec.
iter 166260 || Loss: 0.9766 || timer: 0.0872 sec.
iter 166270 || Loss: 0.8522 || timer: 0.0820 sec.
iter 166280 || Loss: 0.9473 || timer: 0.0991 sec.
iter 166290 || Loss: 0.9235 || timer: 0.0902 sec.
iter 166300 || Loss: 0.8993 || timer: 0.0882 sec.
iter 166310 || Loss: 0.9130 || timer: 0.0945 sec.
iter 166320 || Loss: 0.8409 || timer: 0.0880 sec.
iter 166330 || Loss: 1.2478 || timer: 0.0906 sec.
iter 166340 || Loss: 1.0832 || timer: 0.0792 sec.
iter 166350 || Loss: 0.8763 || timer: 0.0917 sec.
iter 166360 || Loss: 1.0626 || timer: 0.0909 sec.
iter 166370 || Loss: 1.0935 || timer: 0.0899 sec.
iter 166380 || Loss: 1.0929 || timer: 0.1043 sec.
iter 166390 || Loss: 0.8300 || timer: 0.0819 sec.
iter 166400 || Loss: 0.9190 || timer: 0.1035 sec.
iter 166410 || Loss: 0.8829 || timer: 0.1049 sec.
iter 166420 || Loss: 0.9496 || timer: 0.0861 sec.
iter 166430 || Loss: 1.2409 || timer: 0.0901 sec.
iter 166440 || Loss: 0.8548 || timer: 0.1375 sec.
iter 166450 || Loss: 0.9777 || timer: 0.1372 sec.
iter 166460 || Loss: 0.8931 || timer: 0.0946 sec.
iter 166470 || Loss: 0.8207 || timer: 0.0885 sec.
iter 166480 || Loss: 0.8741 || timer: 0.0922 sec.
iter 166490 || Loss: 1.3565 || timer: 0.0831 sec.
iter 166500 || Loss: 0.8426 || timer: 0.0825 sec.
iter 166510 || Loss: 0.9051 || timer: 0.0825 sec.
iter 166520 || Loss: 1.2818 || timer: 0.0882 sec.
iter 166530 || Loss: 0.8303 || timer: 0.0837 sec.
iter 166540 || Loss: 0.9901 || timer: 0.0179 sec.
iter 166550 || Loss: 0.5946 || timer: 0.0917 sec.
iter 166560 || Loss: 0.9416 || timer: 0.0921 sec.
iter 166570 || Loss: 0.8192 || timer: 0.0877 sec.
iter 166580 || Loss: 1.0426 || timer: 0.0826 sec.
iter 166590 || Loss: 0.7654 || timer: 0.0825 sec.
iter 166600 || Loss: 0.8793 || timer: 0.1137 sec.
iter 166610 || Loss: 0.9660 || timer: 0.1045 sec.
iter 166620 || Loss: 0.9464 || timer: 0.0919 sec.
iter 166630 || Loss: 0.7463 || timer: 0.1060 sec.
iter 166640 || Loss: 1.1297 || timer: 0.0918 sec.
iter 166650 || Loss: 0.7786 || timer: 0.0826 sec.
iter 166660 || Loss: 1.1555 || timer: 0.0837 sec.
iter 166670 || Loss: 1.0402 || timer: 0.0967 sec.
iter 166680 || Loss: 0.9116 || timer: 0.1085 sec.
iter 166690 || Loss: 0.8379 || timer: 0.1101 sec.
iter 166700 || Loss: 1.0307 || timer: 0.0824 sec.
iter 166710 || Loss: 0.9700 || timer: 0.0893 sec.
iter 166720 || Loss: 1.0439 || timer: 0.0942 sec.
iter 166730 || Loss: 1.1076 || timer: 0.0902 sec.
iter 166740 || Loss: 0.9631 || timer: 0.0823 sec.
iter 166750 || Loss: 0.9386 || timer: 0.0831 sec.
iter 166760 || Loss: 0.9169 || timer: 0.1178 sec.
iter 166770 || Loss: 0.8874 || timer: 0.0810 sec.
iter 166780 || Loss: 1.0949 || timer: 0.0957 sec.
iter 166790 || Loss: 1.2648 || timer: 0.1058 sec.
iter 166800 || Loss: 0.7983 || timer: 0.0988 sec.
iter 166810 || Loss: 1.0596 || timer: 0.0832 sec.
iter 166820 || Loss: 1.0468 || timer: 0.1097 sec.
iter 166830 || Loss: 0.9834 || timer: 0.1136 sec.
iter 166840 || Loss: 1.4667 || timer: 0.0831 sec.
iter 166850 || Loss: 0.8145 || timer: 0.0896 sec.
iter 166860 || Loss: 1.2676 || timer: 0.0928 sec.
iter 166870 || Loss: 0.9689 || timer: 0.0176 sec.
iter 166880 || Loss: 1.3709 || timer: 0.1122 sec.
iter 166890 || Loss: 0.9041 || timer: 0.0897 sec.
iter 166900 || Loss: 1.2133 || timer: 0.0829 sec.
iter 166910 || Loss: 1.0283 || timer: 0.0896 sec.
iter 166920 || Loss: 1.1326 || timer: 0.1079 sec.
iter 166930 || Loss: 0.9580 || timer: 0.0891 sec.
iter 166940 || Loss: 0.7480 || timer: 0.0825 sec.
iter 166950 || Loss: 0.8547 || timer: 0.0828 sec.
iter 166960 || Loss: 1.1816 || timer: 0.0846 sec.
iter 166970 || Loss: 0.9193 || timer: 0.1269 sec.
iter 166980 || Loss: 0.6249 || timer: 0.0812 sec.
iter 166990 || Loss: 0.8786 || timer: 0.1042 sec.
iter 167000 || Loss: 0.8930 || timer: 0.0891 sec.
iter 167010 || Loss: 0.9852 || timer: 0.0907 sec.
iter 167020 || Loss: 0.9649 || timer: 0.1014 sec.
iter 167030 || Loss: 0.9657 || timer: 0.0892 sec.
iter 167040 || Loss: 0.7901 || timer: 0.1005 sec.
iter 167050 || Loss: 0.9450 || timer: 0.1146 sec.
iter 167060 || Loss: 0.9432 || timer: 0.0966 sec.
iter 167070 || Loss: 0.8259 || timer: 0.0875 sec.
iter 167080 || Loss: 0.7692 || timer: 0.1039 sec.
iter 167090 || Loss: 1.4582 || timer: 0.0881 sec.
iter 167100 || Loss: 0.8963 || timer: 0.0919 sec.
iter 167110 || Loss: 1.4699 || timer: 0.0827 sec.
iter 167120 || Loss: 0.8583 || timer: 0.0839 sec.
iter 167130 || Loss: 0.7267 || timer: 0.0933 sec.
iter 167140 || Loss: 0.8249 || timer: 0.0911 sec.
iter 167150 || Loss: 1.3483 || timer: 0.0840 sec.
iter 167160 || Loss: 0.7177 || timer: 0.0888 sec.
iter 167170 || Loss: 0.6610 || timer: 0.0845 sec.
iter 167180 || Loss: 0.8441 || timer: 0.0936 sec.
iter 167190 || Loss: 0.9619 || timer: 0.0858 sec.
iter 167200 || Loss: 1.2548 || timer: 0.0247 sec.
iter 167210 || Loss: 0.2655 || timer: 0.0967 sec.
iter 167220 || Loss: 1.4177 || timer: 0.0988 sec.
iter 167230 || Loss: 0.7986 || timer: 0.0830 sec.
iter 167240 || Loss: 0.6638 || timer: 0.0836 sec.
iter 167250 || Loss: 0.7088 || timer: 0.0930 sec.
iter 167260 || Loss: 0.8573 || timer: 0.0812 sec.
iter 167270 || Loss: 0.9303 || timer: 0.0770 sec.
iter 167280 || Loss: 0.7943 || timer: 0.0899 sec.
iter 167290 || Loss: 1.0360 || timer: 0.0905 sec.
iter 167300 || Loss: 0.7140 || timer: 0.1247 sec.
iter 167310 || Loss: 0.8139 || timer: 0.0908 sec.
iter 167320 || Loss: 1.0079 || timer: 0.0891 sec.
iter 167330 || Loss: 1.0392 || timer: 0.0835 sec.
iter 167340 || Loss: 0.8236 || timer: 0.0979 sec.
iter 167350 || Loss: 0.9603 || timer: 0.0914 sec.
iter 167360 || Loss: 1.2246 || timer: 0.0905 sec.
iter 167370 || Loss: 0.7456 || timer: 0.0904 sec.
iter 167380 || Loss: 0.8678 || timer: 0.0953 sec.
iter 167390 || Loss: 0.7060 || timer: 0.0910 sec.
iter 167400 || Loss: 0.7747 || timer: 0.0887 sec.
iter 167410 || Loss: 0.9203 || timer: 0.0919 sec.
iter 167420 || Loss: 0.7613 || timer: 0.0903 sec.
iter 167430 || Loss: 0.7104 || timer: 0.0839 sec.
iter 167440 || Loss: 0.9210 || timer: 0.0979 sec.
iter 167450 || Loss: 1.0821 || timer: 0.0909 sec.
iter 167460 || Loss: 0.7711 || timer: 0.0884 sec.
iter 167470 || Loss: 0.8048 || timer: 0.0895 sec.
iter 167480 || Loss: 0.8292 || timer: 0.0926 sec.
iter 167490 || Loss: 0.9254 || timer: 0.0900 sec.
iter 167500 || Loss: 1.0890 || timer: 0.0838 sec.
iter 167510 || Loss: 0.9695 || timer: 0.0912 sec.
iter 167520 || Loss: 0.8900 || timer: 0.0899 sec.
iter 167530 || Loss: 1.0391 || timer: 0.0236 sec.
iter 167540 || Loss: 1.1365 || timer: 0.0939 sec.
iter 167550 || Loss: 0.8140 || timer: 0.0849 sec.
iter 167560 || Loss: 0.7741 || timer: 0.0833 sec.
iter 167570 || Loss: 1.0062 || timer: 0.1037 sec.
iter 167580 || Loss: 0.8283 || timer: 0.0980 sec.
iter 167590 || Loss: 0.8946 || timer: 0.0869 sec.
iter 167600 || Loss: 1.1206 || timer: 0.0837 sec.
iter 167610 || Loss: 1.0388 || timer: 0.0977 sec.
iter 167620 || Loss: 0.9874 || timer: 0.0842 sec.
iter 167630 || Loss: 0.7587 || timer: 0.1015 sec.
iter 167640 || Loss: 0.5818 || timer: 0.0898 sec.
iter 167650 || Loss: 0.6959 || timer: 0.0965 sec.
iter 167660 || Loss: 0.7858 || timer: 0.0899 sec.
iter 167670 || Loss: 1.0702 || timer: 0.0905 sec.
iter 167680 || Loss: 1.2114 || timer: 0.1034 sec.
iter 167690 || Loss: 0.9748 || timer: 0.0836 sec.
iter 167700 || Loss: 0.9425 || timer: 0.0893 sec.
iter 167710 || Loss: 0.8546 || timer: 0.0918 sec.
iter 167720 || Loss: 0.8377 || timer: 0.0913 sec.
iter 167730 || Loss: 0.9110 || timer: 0.1080 sec.
iter 167740 || Loss: 0.7197 || timer: 0.0907 sec.
iter 167750 || Loss: 1.1396 || timer: 0.1102 sec.
iter 167760 || Loss: 0.8623 || timer: 0.0912 sec.
iter 167770 || Loss: 0.8641 || timer: 0.0834 sec.
iter 167780 || Loss: 0.6830 || timer: 0.0935 sec.
iter 167790 || Loss: 0.7325 || timer: 0.1033 sec.
iter 167800 || Loss: 0.7990 || timer: 0.0847 sec.
iter 167810 || Loss: 0.9810 || timer: 0.0881 sec.
iter 167820 || Loss: 0.9304 || timer: 0.0857 sec.
iter 167830 || Loss: 1.0615 || timer: 0.0919 sec.
iter 167840 || Loss: 0.6588 || timer: 0.0816 sec.
iter 167850 || Loss: 1.1547 || timer: 0.0848 sec.
iter 167860 || Loss: 0.7572 || timer: 0.0268 sec.
iter 167870 || Loss: 2.3713 || timer: 0.0915 sec.
iter 167880 || Loss: 0.5798 || timer: 0.1025 sec.
iter 167890 || Loss: 0.8039 || timer: 0.0889 sec.
iter 167900 || Loss: 0.8335 || timer: 0.0840 sec.
iter 167910 || Loss: 0.8858 || timer: 0.0837 sec.
iter 167920 || Loss: 0.7717 || timer: 0.0878 sec.
iter 167930 || Loss: 0.7935 || timer: 0.0912 sec.
iter 167940 || Loss: 1.1939 || timer: 0.0937 sec.
iter 167950 || Loss: 0.9357 || timer: 0.0882 sec.
iter 167960 || Loss: 0.8984 || timer: 0.0964 sec.
iter 167970 || Loss: 0.8096 || timer: 0.0889 sec.
iter 167980 || Loss: 1.3459 || timer: 0.0911 sec.
iter 167990 || Loss: 1.1914 || timer: 0.0853 sec.
iter 168000 || Loss: 0.7160 || timer: 0.1119 sec.
iter 168010 || Loss: 0.8282 || timer: 0.0841 sec.
iter 168020 || Loss: 0.9384 || timer: 0.0843 sec.
iter 168030 || Loss: 1.1633 || timer: 0.0839 sec.
iter 168040 || Loss: 0.8056 || timer: 0.0906 sec.
iter 168050 || Loss: 0.5848 || timer: 0.0896 sec.
iter 168060 || Loss: 0.7154 || timer: 0.0927 sec.
iter 168070 || Loss: 1.0124 || timer: 0.0881 sec.
iter 168080 || Loss: 0.9303 || timer: 0.0907 sec.
iter 168090 || Loss: 0.7039 || timer: 0.0839 sec.
iter 168100 || Loss: 0.8909 || timer: 0.0866 sec.
iter 168110 || Loss: 0.7069 || timer: 0.0912 sec.
iter 168120 || Loss: 0.8817 || timer: 0.0756 sec.
iter 168130 || Loss: 1.0710 || timer: 0.1034 sec.
iter 168140 || Loss: 1.0458 || timer: 0.1161 sec.
iter 168150 || Loss: 0.9788 || timer: 0.0904 sec.
iter 168160 || Loss: 1.1260 || timer: 0.0913 sec.
iter 168170 || Loss: 0.8885 || timer: 0.0879 sec.
iter 168180 || Loss: 1.2718 || timer: 0.1225 sec.
iter 168190 || Loss: 0.8474 || timer: 0.0246 sec.
iter 168200 || Loss: 4.9967 || timer: 0.0897 sec.
iter 168210 || Loss: 2.0036 || timer: 0.0828 sec.
iter 168220 || Loss: 1.2252 || timer: 0.1197 sec.
iter 168230 || Loss: 1.3177 || timer: 0.0843 sec.
iter 168240 || Loss: 1.7222 || timer: 0.0872 sec.
iter 168250 || Loss: 1.5383 || timer: 0.0886 sec.
iter 168260 || Loss: 1.6663 || timer: 0.0752 sec.
iter 168270 || Loss: 1.1185 || timer: 0.0854 sec.
iter 168280 || Loss: 1.2079 || timer: 0.0969 sec.
iter 168290 || Loss: 0.9376 || timer: 0.1049 sec.
iter 168300 || Loss: 1.1931 || timer: 0.0896 sec.
iter 168310 || Loss: 0.8992 || timer: 0.0983 sec.
iter 168320 || Loss: 0.9721 || timer: 0.0831 sec.
iter 168330 || Loss: 0.8455 || timer: 0.0908 sec.
iter 168340 || Loss: 1.7370 || timer: 0.0862 sec.
iter 168350 || Loss: 1.4233 || timer: 0.0764 sec.
iter 168360 || Loss: 0.9663 || timer: 0.0770 sec.
iter 168370 || Loss: 1.0479 || timer: 0.0922 sec.
iter 168380 || Loss: 1.0234 || timer: 0.0808 sec.
iter 168390 || Loss: 1.0534 || timer: 0.0894 sec.
iter 168400 || Loss: 1.1032 || timer: 0.0897 sec.
iter 168410 || Loss: 1.1190 || timer: 0.0820 sec.
iter 168420 || Loss: 1.2177 || timer: 0.0819 sec.
iter 168430 || Loss: 1.0972 || timer: 0.0849 sec.
iter 168440 || Loss: 1.1880 || timer: 0.0867 sec.
iter 168450 || Loss: 0.7744 || timer: 0.0897 sec.
iter 168460 || Loss: 0.9324 || timer: 0.1078 sec.
iter 168470 || Loss: 0.9919 || timer: 0.1018 sec.
iter 168480 || Loss: 0.9350 || timer: 0.1056 sec.
iter 168490 || Loss: 1.0640 || timer: 0.0892 sec.
iter 168500 || Loss: 1.2975 || timer: 0.0822 sec.
iter 168510 || Loss: 1.1142 || timer: 0.0897 sec.
iter 168520 || Loss: 1.2847 || timer: 0.0223 sec.
iter 168530 || Loss: 4.5739 || timer: 0.1051 sec.
iter 168540 || Loss: 0.9514 || timer: 0.0868 sec.
iter 168550 || Loss: 1.3969 || timer: 0.0892 sec.
iter 168560 || Loss: 0.9770 || timer: 0.1039 sec.
iter 168570 || Loss: 1.3319 || timer: 0.0898 sec.
iter 168580 || Loss: 0.9675 || timer: 0.0907 sec.
iter 168590 || Loss: 0.9770 || timer: 0.0829 sec.
iter 168600 || Loss: 1.0977 || timer: 0.0996 sec.
iter 168610 || Loss: 1.1049 || timer: 0.0798 sec.
iter 168620 || Loss: 0.8865 || timer: 0.1020 sec.
iter 168630 || Loss: 0.5545 || timer: 0.0811 sec.
iter 168640 || Loss: 1.2944 || timer: 0.0836 sec.
iter 168650 || Loss: 0.9820 || timer: 0.0878 sec.
iter 168660 || Loss: 0.8199 || timer: 0.1251 sec.
iter 168670 || Loss: 1.5020 || timer: 0.1039 sec.
iter 168680 || Loss: 1.0142 || timer: 0.0898 sec.
iter 168690 || Loss: 1.0294 || timer: 0.0810 sec.
iter 168700 || Loss: 1.1728 || timer: 0.0890 sec.
iter 168710 || Loss: 0.7281 || timer: 0.0904 sec.
iter 168720 || Loss: 0.9366 || timer: 0.0835 sec.
iter 168730 || Loss: 0.8185 || timer: 0.1062 sec.
iter 168740 || Loss: 1.0467 || timer: 0.0820 sec.
iter 168750 || Loss: 0.8880 || timer: 0.0905 sec.
iter 168760 || Loss: 1.1069 || timer: 0.0868 sec.
iter 168770 || Loss: 0.8670 || timer: 0.0903 sec.
iter 168780 || Loss: 1.1820 || timer: 0.0884 sec.
iter 168790 || Loss: 0.7144 || timer: 0.1009 sec.
iter 168800 || Loss: 1.2092 || timer: 0.0926 sec.
iter 168810 || Loss: 1.1092 || timer: 0.0959 sec.
iter 168820 || Loss: 1.1523 || timer: 0.0875 sec.
iter 168830 || Loss: 0.8002 || timer: 0.0893 sec.
iter 168840 || Loss: 1.0331 || timer: 0.0902 sec.
iter 168850 || Loss: 1.1811 || timer: 0.0171 sec.
iter 168860 || Loss: 0.3727 || timer: 0.0823 sec.
iter 168870 || Loss: 1.1968 || timer: 0.0839 sec.
iter 168880 || Loss: 0.7120 || timer: 0.0910 sec.
iter 168890 || Loss: 0.9928 || timer: 0.0890 sec.
iter 168900 || Loss: 0.9104 || timer: 0.1182 sec.
iter 168910 || Loss: 0.7827 || timer: 0.1044 sec.
iter 168920 || Loss: 1.1734 || timer: 0.0911 sec.
iter 168930 || Loss: 0.7631 || timer: 0.0789 sec.
iter 168940 || Loss: 0.8331 || timer: 0.0889 sec.
iter 168950 || Loss: 1.3521 || timer: 0.1222 sec.
iter 168960 || Loss: 0.9338 || timer: 0.0917 sec.
iter 168970 || Loss: 1.4834 || timer: 0.0901 sec.
iter 168980 || Loss: 1.1680 || timer: 0.0822 sec.
iter 168990 || Loss: 0.7874 || timer: 0.0912 sec.
iter 169000 || Loss: 0.7996 || timer: 0.0842 sec.
iter 169010 || Loss: 1.0475 || timer: 0.0846 sec.
iter 169020 || Loss: 1.0636 || timer: 0.0906 sec.
iter 169030 || Loss: 0.7564 || timer: 0.0849 sec.
iter 169040 || Loss: 0.7655 || timer: 0.0901 sec.
iter 169050 || Loss: 0.8779 || timer: 0.0868 sec.
iter 169060 || Loss: 0.9899 || timer: 0.0903 sec.
iter 169070 || Loss: 1.1117 || timer: 0.0911 sec.
iter 169080 || Loss: 0.8532 || timer: 0.0903 sec.
iter 169090 || Loss: 1.0802 || timer: 0.0853 sec.
iter 169100 || Loss: 0.8461 || timer: 0.0874 sec.
iter 169110 || Loss: 1.0441 || timer: 0.0976 sec.
iter 169120 || Loss: 1.0318 || timer: 0.0844 sec.
iter 169130 || Loss: 0.8301 || timer: 0.0905 sec.
iter 169140 || Loss: 0.9103 || timer: 0.0901 sec.
iter 169150 || Loss: 0.7935 || timer: 0.0840 sec.
iter 169160 || Loss: 0.7125 || timer: 0.0859 sec.
iter 169170 || Loss: 0.9293 || timer: 0.0895 sec.
iter 169180 || Loss: 0.7872 || timer: 0.0239 sec.
iter 169190 || Loss: 0.4975 || timer: 0.0957 sec.
iter 169200 || Loss: 0.9447 || timer: 0.0945 sec.
iter 169210 || Loss: 1.2707 || timer: 0.0828 sec.
iter 169220 || Loss: 0.7551 || timer: 0.1045 sec.
iter 169230 || Loss: 0.8177 || timer: 0.0890 sec.
iter 169240 || Loss: 0.7055 || timer: 0.1046 sec.
iter 169250 || Loss: 1.5516 || timer: 0.0854 sec.
iter 169260 || Loss: 0.9417 || timer: 0.0929 sec.
iter 169270 || Loss: 0.8781 || timer: 0.1041 sec.
iter 169280 || Loss: 0.8776 || timer: 0.1228 sec.
iter 169290 || Loss: 0.9339 || timer: 0.0854 sec.
iter 169300 || Loss: 0.9336 || timer: 0.0799 sec.
iter 169310 || Loss: 1.1617 || timer: 0.0921 sec.
iter 169320 || Loss: 1.0269 || timer: 0.0833 sec.
iter 169330 || Loss: 1.0415 || timer: 0.0902 sec.
iter 169340 || Loss: 1.0382 || timer: 0.1577 sec.
iter 169350 || Loss: 1.3552 || timer: 0.0904 sec.
iter 169360 || Loss: 0.8927 || timer: 0.0945 sec.
iter 169370 || Loss: 0.7511 || timer: 0.0940 sec.
iter 169380 || Loss: 0.9020 || timer: 0.0968 sec.
iter 169390 || Loss: 1.1432 || timer: 0.0903 sec.
iter 169400 || Loss: 0.9033 || timer: 0.0912 sec.
iter 169410 || Loss: 0.9569 || timer: 0.0814 sec.
iter 169420 || Loss: 0.8706 || timer: 0.0913 sec.
iter 169430 || Loss: 0.7965 || timer: 0.1041 sec.
iter 169440 || Loss: 0.8407 || timer: 0.0930 sec.
iter 169450 || Loss: 0.8224 || timer: 0.0899 sec.
iter 169460 || Loss: 0.9489 || timer: 0.0908 sec.
iter 169470 || Loss: 0.9702 || timer: 0.0825 sec.
iter 169480 || Loss: 0.8623 || timer: 0.0863 sec.
iter 169490 || Loss: 0.6632 || timer: 0.0838 sec.
iter 169500 || Loss: 0.7275 || timer: 0.0757 sec.
iter 169510 || Loss: 0.9097 || timer: 0.0240 sec.
iter 169520 || Loss: 0.6125 || timer: 0.1060 sec.
iter 169530 || Loss: 0.9084 || timer: 0.1100 sec.
iter 169540 || Loss: 0.7858 || timer: 0.1147 sec.
iter 169550 || Loss: 0.6039 || timer: 0.1057 sec.
iter 169560 || Loss: 0.8650 || timer: 0.0808 sec.
iter 169570 || Loss: 0.6875 || timer: 0.0915 sec.
iter 169580 || Loss: 1.1821 || timer: 0.0878 sec.
iter 169590 || Loss: 0.8639 || timer: 0.0921 sec.
iter 169600 || Loss: 1.1747 || timer: 0.0826 sec.
iter 169610 || Loss: 0.7687 || timer: 0.0983 sec.
iter 169620 || Loss: 0.6964 || timer: 0.0897 sec.
iter 169630 || Loss: 1.0547 || timer: 0.0907 sec.
iter 169640 || Loss: 0.8710 || timer: 0.0873 sec.
iter 169650 || Loss: 0.8706 || timer: 0.0835 sec.
iter 169660 || Loss: 1.1357 || timer: 0.1068 sec.
iter 169670 || Loss: 0.9485 || timer: 0.0919 sec.
iter 169680 || Loss: 1.1253 || timer: 0.1032 sec.
iter 169690 || Loss: 0.8136 || timer: 0.0912 sec.
iter 169700 || Loss: 0.8125 || timer: 0.0832 sec.
iter 169710 || Loss: 0.9261 || timer: 0.0847 sec.
iter 169720 || Loss: 0.9672 || timer: 0.0811 sec.
iter 169730 || Loss: 0.9827 || timer: 0.0923 sec.
iter 169740 || Loss: 0.8635 || timer: 0.0757 sec.
iter 169750 || Loss: 0.7219 || timer: 0.0887 sec.
iter 169760 || Loss: 1.2037 || timer: 0.0855 sec.
iter 169770 || Loss: 0.7431 || timer: 0.1071 sec.
iter 169780 || Loss: 0.9298 || timer: 0.0994 sec.
iter 169790 || Loss: 1.8932 || timer: 0.0899 sec.
iter 169800 || Loss: 1.4685 || timer: 0.0819 sec.
iter 169810 || Loss: 1.0155 || timer: 0.0824 sec.
iter 169820 || Loss: 0.9433 || timer: 0.0904 sec.
iter 169830 || Loss: 1.0995 || timer: 0.0830 sec.
iter 169840 || Loss: 1.7177 || timer: 0.0166 sec.
iter 169850 || Loss: 4.2124 || timer: 0.0833 sec.
iter 169860 || Loss: 1.1681 || timer: 0.0881 sec.
iter 169870 || Loss: 0.7818 || timer: 0.0902 sec.
iter 169880 || Loss: 0.9868 || timer: 0.0902 sec.
iter 169890 || Loss: 1.0740 || timer: 0.1247 sec.
iter 169900 || Loss: 1.2380 || timer: 0.1215 sec.
iter 169910 || Loss: 1.2080 || timer: 0.0905 sec.
iter 169920 || Loss: 1.1218 || timer: 0.0925 sec.
iter 169930 || Loss: 0.8766 || timer: 0.0983 sec.
iter 169940 || Loss: 0.9750 || timer: 0.0898 sec.
iter 169950 || Loss: 0.9735 || timer: 0.0837 sec.
iter 169960 || Loss: 0.8484 || timer: 0.0903 sec.
iter 169970 || Loss: 1.0619 || timer: 0.0841 sec.
iter 169980 || Loss: 0.9179 || timer: 0.0976 sec.
iter 169990 || Loss: 1.0097 || timer: 0.0889 sec.
iter 170000 || Loss: 1.0392 || Saving state, iter: 170000
timer: 0.0823 sec.
iter 170010 || Loss: 0.7858 || timer: 0.0886 sec.
iter 170020 || Loss: 1.5372 || timer: 0.0886 sec.
iter 170030 || Loss: 0.9272 || timer: 0.0851 sec.
iter 170040 || Loss: 1.1471 || timer: 0.0918 sec.
iter 170050 || Loss: 0.8754 || timer: 0.0888 sec.
iter 170060 || Loss: 1.1518 || timer: 0.0908 sec.
iter 170070 || Loss: 0.9495 || timer: 0.0905 sec.
iter 170080 || Loss: 0.7341 || timer: 0.0828 sec.
iter 170090 || Loss: 1.2488 || timer: 0.0798 sec.
iter 170100 || Loss: 1.0962 || timer: 0.0828 sec.
iter 170110 || Loss: 0.8834 || timer: 0.0904 sec.
iter 170120 || Loss: 1.0856 || timer: 0.0827 sec.
iter 170130 || Loss: 0.8876 || timer: 0.0921 sec.
iter 170140 || Loss: 1.0053 || timer: 0.0871 sec.
iter 170150 || Loss: 0.9418 || timer: 0.0814 sec.
iter 170160 || Loss: 1.2128 || timer: 0.1038 sec.
iter 170170 || Loss: 1.0801 || timer: 0.0251 sec.
iter 170180 || Loss: 1.6231 || timer: 0.0849 sec.
iter 170190 || Loss: 1.3037 || timer: 0.0821 sec.
iter 170200 || Loss: 1.0738 || timer: 0.0887 sec.
iter 170210 || Loss: 0.8983 || timer: 0.0906 sec.
iter 170220 || Loss: 1.2365 || timer: 0.1147 sec.
iter 170230 || Loss: 1.2676 || timer: 0.1098 sec.
iter 170240 || Loss: 0.8954 || timer: 0.0883 sec.
iter 170250 || Loss: 0.7894 || timer: 0.0835 sec.
iter 170260 || Loss: 0.9789 || timer: 0.0840 sec.
iter 170270 || Loss: 0.9772 || timer: 0.1020 sec.
iter 170280 || Loss: 1.1303 || timer: 0.0821 sec.
iter 170290 || Loss: 0.8397 || timer: 0.0822 sec.
iter 170300 || Loss: 0.9158 || timer: 0.1028 sec.
iter 170310 || Loss: 0.9736 || timer: 0.0943 sec.
iter 170320 || Loss: 0.7968 || timer: 0.0963 sec.
iter 170330 || Loss: 0.9422 || timer: 0.0833 sec.
iter 170340 || Loss: 0.9677 || timer: 0.0861 sec.
iter 170350 || Loss: 1.5997 || timer: 0.1051 sec.
iter 170360 || Loss: 1.1325 || timer: 0.0932 sec.
iter 170370 || Loss: 1.0884 || timer: 0.0845 sec.
iter 170380 || Loss: 1.3146 || timer: 0.1041 sec.
iter 170390 || Loss: 0.7590 || timer: 0.0900 sec.
iter 170400 || Loss: 0.9773 || timer: 0.0915 sec.
iter 170410 || Loss: 0.7614 || timer: 0.0889 sec.
iter 170420 || Loss: 0.9933 || timer: 0.0892 sec.
iter 170430 || Loss: 0.7731 || timer: 0.0881 sec.
iter 170440 || Loss: 1.1303 || timer: 0.0769 sec.
iter 170450 || Loss: 1.0281 || timer: 0.0849 sec.
iter 170460 || Loss: 1.1021 || timer: 0.0851 sec.
iter 170470 || Loss: 0.9702 || timer: 0.0914 sec.
iter 170480 || Loss: 0.8080 || timer: 0.0902 sec.
iter 170490 || Loss: 0.6944 || timer: 0.1048 sec.
iter 170500 || Loss: 0.8045 || timer: 0.0189 sec.
iter 170510 || Loss: 1.4156 || timer: 0.1063 sec.
iter 170520 || Loss: 1.2307 || timer: 0.1011 sec.
iter 170530 || Loss: 0.8645 || timer: 0.0909 sec.
iter 170540 || Loss: 0.8193 || timer: 0.0771 sec.
iter 170550 || Loss: 0.7844 || timer: 0.0897 sec.
iter 170560 || Loss: 1.1480 || timer: 0.0942 sec.
iter 170570 || Loss: 0.7837 || timer: 0.0852 sec.
iter 170580 || Loss: 0.7734 || timer: 0.0762 sec.
iter 170590 || Loss: 1.0194 || timer: 0.0866 sec.
iter 170600 || Loss: 0.8686 || timer: 0.1084 sec.
iter 170610 || Loss: 1.2143 || timer: 0.0852 sec.
iter 170620 || Loss: 1.1459 || timer: 0.0941 sec.
iter 170630 || Loss: 1.2432 || timer: 0.0913 sec.
iter 170640 || Loss: 0.7258 || timer: 0.0917 sec.
iter 170650 || Loss: 1.0801 || timer: 0.1017 sec.
iter 170660 || Loss: 1.1257 || timer: 0.0859 sec.
iter 170670 || Loss: 1.5233 || timer: 0.0978 sec.
iter 170680 || Loss: 1.0035 || timer: 0.1075 sec.
iter 170690 || Loss: 1.0725 || timer: 0.0825 sec.
iter 170700 || Loss: 0.6328 || timer: 0.0925 sec.
iter 170710 || Loss: 1.1070 || timer: 0.1081 sec.
iter 170720 || Loss: 1.1261 || timer: 0.0847 sec.
iter 170730 || Loss: 0.8941 || timer: 0.0751 sec.
iter 170740 || Loss: 0.6870 || timer: 0.0867 sec.
iter 170750 || Loss: 0.6406 || timer: 0.0865 sec.
iter 170760 || Loss: 1.1921 || timer: 0.0839 sec.
iter 170770 || Loss: 1.0822 || timer: 0.0929 sec.
iter 170780 || Loss: 0.8725 || timer: 0.1147 sec.
iter 170790 || Loss: 0.7336 || timer: 0.0884 sec.
iter 170800 || Loss: 0.9797 || timer: 0.0772 sec.
iter 170810 || Loss: 0.7678 || timer: 0.0841 sec.
iter 170820 || Loss: 0.9577 || timer: 0.1046 sec.
iter 170830 || Loss: 0.7241 || timer: 0.0188 sec.
iter 170840 || Loss: 1.4684 || timer: 0.0843 sec.
iter 170850 || Loss: 1.5161 || timer: 0.0915 sec.
iter 170860 || Loss: 1.0582 || timer: 0.0838 sec.
iter 170870 || Loss: 0.9535 || timer: 0.0889 sec.
iter 170880 || Loss: 1.0686 || timer: 0.0837 sec.
iter 170890 || Loss: 0.8426 || timer: 0.0845 sec.
iter 170900 || Loss: 1.1488 || timer: 0.0991 sec.
iter 170910 || Loss: 0.8650 || timer: 0.0921 sec.
iter 170920 || Loss: 1.1867 || timer: 0.0944 sec.
iter 170930 || Loss: 0.7244 || timer: 0.0985 sec.
iter 170940 || Loss: 0.9868 || timer: 0.0885 sec.
iter 170950 || Loss: 0.9793 || timer: 0.1005 sec.
iter 170960 || Loss: 0.8522 || timer: 0.0888 sec.
iter 170970 || Loss: 0.8683 || timer: 0.0840 sec.
iter 170980 || Loss: 1.3895 || timer: 0.0939 sec.
iter 170990 || Loss: 0.8318 || timer: 0.0956 sec.
iter 171000 || Loss: 0.9217 || timer: 0.0910 sec.
iter 171010 || Loss: 0.9636 || timer: 0.0803 sec.
iter 171020 || Loss: 1.1605 || timer: 0.0827 sec.
iter 171030 || Loss: 0.8270 || timer: 0.0848 sec.
iter 171040 || Loss: 0.9526 || timer: 0.0834 sec.
iter 171050 || Loss: 0.7288 || timer: 0.0896 sec.
iter 171060 || Loss: 1.1608 || timer: 0.0846 sec.
iter 171070 || Loss: 0.6482 || timer: 0.0919 sec.
iter 171080 || Loss: 0.8512 || timer: 0.0893 sec.
iter 171090 || Loss: 1.0786 || timer: 0.0923 sec.
iter 171100 || Loss: 1.0643 || timer: 0.1048 sec.
iter 171110 || Loss: 0.8941 || timer: 0.0813 sec.
iter 171120 || Loss: 0.7848 || timer: 0.0849 sec.
iter 171130 || Loss: 0.7541 || timer: 0.0858 sec.
iter 171140 || Loss: 0.9887 || timer: 0.0926 sec.
iter 171150 || Loss: 1.2335 || timer: 0.0905 sec.
iter 171160 || Loss: 0.9420 || timer: 0.0181 sec.
iter 171170 || Loss: 0.5877 || timer: 0.0839 sec.
iter 171180 || Loss: 0.7639 || timer: 0.0843 sec.
iter 171190 || Loss: 1.0533 || timer: 0.0919 sec.
iter 171200 || Loss: 1.2499 || timer: 0.1062 sec.
iter 171210 || Loss: 1.0274 || timer: 0.0901 sec.
iter 171220 || Loss: 0.5854 || timer: 0.1143 sec.
iter 171230 || Loss: 0.8129 || timer: 0.0895 sec.
iter 171240 || Loss: 0.9516 || timer: 0.0839 sec.
iter 171250 || Loss: 0.6211 || timer: 0.1079 sec.
iter 171260 || Loss: 0.7374 || timer: 0.1191 sec.
iter 171270 || Loss: 0.9430 || timer: 0.0926 sec.
iter 171280 || Loss: 1.1615 || timer: 0.0925 sec.
iter 171290 || Loss: 0.6519 || timer: 0.0831 sec.
iter 171300 || Loss: 0.6932 || timer: 0.0910 sec.
iter 171310 || Loss: 0.8670 || timer: 0.0924 sec.
iter 171320 || Loss: 0.9940 || timer: 0.0989 sec.
iter 171330 || Loss: 0.9021 || timer: 0.0836 sec.
iter 171340 || Loss: 0.7339 || timer: 0.1017 sec.
iter 171350 || Loss: 0.8458 || timer: 0.0832 sec.
iter 171360 || Loss: 0.7536 || timer: 0.0893 sec.
iter 171370 || Loss: 0.9200 || timer: 0.0979 sec.
iter 171380 || Loss: 0.8398 || timer: 0.0918 sec.
iter 171390 || Loss: 1.0613 || timer: 0.0829 sec.
iter 171400 || Loss: 1.0727 || timer: 0.0902 sec.
iter 171410 || Loss: 1.0599 || timer: 0.0914 sec.
iter 171420 || Loss: 0.7334 || timer: 0.0832 sec.
iter 171430 || Loss: 0.7717 || timer: 0.0850 sec.
iter 171440 || Loss: 0.7878 || timer: 0.0916 sec.
iter 171450 || Loss: 0.9445 || timer: 0.0841 sec.
iter 171460 || Loss: 1.0776 || timer: 0.0981 sec.
iter 171470 || Loss: 0.8694 || timer: 0.0849 sec.
iter 171480 || Loss: 0.6310 || timer: 0.0869 sec.
iter 171490 || Loss: 0.9092 || timer: 0.0247 sec.
iter 171500 || Loss: 1.2381 || timer: 0.1181 sec.
iter 171510 || Loss: 1.0382 || timer: 0.1193 sec.
iter 171520 || Loss: 1.0709 || timer: 0.0821 sec.
iter 171530 || Loss: 0.8101 || timer: 0.0928 sec.
iter 171540 || Loss: 1.0478 || timer: 0.0865 sec.
iter 171550 || Loss: 0.9891 || timer: 0.1057 sec.
iter 171560 || Loss: 0.9992 || timer: 0.0932 sec.
iter 171570 || Loss: 1.1214 || timer: 0.0916 sec.
iter 171580 || Loss: 0.8242 || timer: 0.0842 sec.
iter 171590 || Loss: 0.9857 || timer: 0.0966 sec.
iter 171600 || Loss: 0.7160 || timer: 0.0879 sec.
iter 171610 || Loss: 0.9375 || timer: 0.0864 sec.
iter 171620 || Loss: 0.9564 || timer: 0.0833 sec.
iter 171630 || Loss: 0.9580 || timer: 0.0859 sec.
iter 171640 || Loss: 0.8065 || timer: 0.0897 sec.
iter 171650 || Loss: 0.8453 || timer: 0.1006 sec.
iter 171660 || Loss: 0.7748 || timer: 0.0887 sec.
iter 171670 || Loss: 0.8702 || timer: 0.1060 sec.
iter 171680 || Loss: 1.2027 || timer: 0.1013 sec.
iter 171690 || Loss: 1.3155 || timer: 0.0844 sec.
iter 171700 || Loss: 0.9214 || timer: 0.0895 sec.
iter 171710 || Loss: 0.9374 || timer: 0.0819 sec.
iter 171720 || Loss: 1.3800 || timer: 0.0856 sec.
iter 171730 || Loss: 0.8427 || timer: 0.0832 sec.
iter 171740 || Loss: 1.0647 || timer: 0.0830 sec.
iter 171750 || Loss: 0.9590 || timer: 0.0987 sec.
iter 171760 || Loss: 1.1704 || timer: 0.0985 sec.
iter 171770 || Loss: 1.3362 || timer: 0.0916 sec.
iter 171780 || Loss: 1.0658 || timer: 0.0831 sec.
iter 171790 || Loss: 0.9772 || timer: 0.0859 sec.
iter 171800 || Loss: 0.9144 || timer: 0.1058 sec.
iter 171810 || Loss: 0.9608 || timer: 0.0818 sec.
iter 171820 || Loss: 0.7704 || timer: 0.0285 sec.
iter 171830 || Loss: 0.3259 || timer: 0.0900 sec.
iter 171840 || Loss: 1.0483 || timer: 0.0880 sec.
iter 171850 || Loss: 1.1460 || timer: 0.0838 sec.
iter 171860 || Loss: 0.8189 || timer: 0.0915 sec.
iter 171870 || Loss: 1.7655 || timer: 0.0915 sec.
iter 171880 || Loss: 0.7744 || timer: 0.0911 sec.
iter 171890 || Loss: 1.4947 || timer: 0.0903 sec.
iter 171900 || Loss: 0.8264 || timer: 0.0851 sec.
iter 171910 || Loss: 1.0500 || timer: 0.1119 sec.
iter 171920 || Loss: 0.8978 || timer: 0.0935 sec.
iter 171930 || Loss: 1.0255 || timer: 0.0913 sec.
iter 171940 || Loss: 1.0345 || timer: 0.0833 sec.
iter 171950 || Loss: 0.7576 || timer: 0.0840 sec.
iter 171960 || Loss: 0.7195 || timer: 0.0898 sec.
iter 171970 || Loss: 0.9901 || timer: 0.0900 sec.
iter 171980 || Loss: 1.0195 || timer: 0.0869 sec.
iter 171990 || Loss: 0.8635 || timer: 0.0904 sec.
iter 172000 || Loss: 0.7584 || timer: 0.0954 sec.
iter 172010 || Loss: 1.1234 || timer: 0.0758 sec.
iter 172020 || Loss: 0.8951 || timer: 0.0816 sec.
iter 172030 || Loss: 1.1344 || timer: 0.0902 sec.
iter 172040 || Loss: 0.9470 || timer: 0.0890 sec.
iter 172050 || Loss: 1.0103 || timer: 0.0910 sec.
iter 172060 || Loss: 1.0166 || timer: 0.0919 sec.
iter 172070 || Loss: 0.7665 || timer: 0.0907 sec.
iter 172080 || Loss: 0.9289 || timer: 0.0836 sec.
iter 172090 || Loss: 0.7292 || timer: 0.0924 sec.
iter 172100 || Loss: 0.8038 || timer: 0.0886 sec.
iter 172110 || Loss: 1.1056 || timer: 0.0904 sec.
iter 172120 || Loss: 1.0428 || timer: 0.0933 sec.
iter 172130 || Loss: 1.1719 || timer: 0.0972 sec.
iter 172140 || Loss: 0.9476 || timer: 0.0899 sec.
iter 172150 || Loss: 1.0380 || timer: 0.0254 sec.
iter 172160 || Loss: 1.5140 || timer: 0.0862 sec.
iter 172170 || Loss: 1.0159 || timer: 0.0834 sec.
iter 172180 || Loss: 1.1491 || timer: 0.0880 sec.
iter 172190 || Loss: 0.7475 || timer: 0.0877 sec.
iter 172200 || Loss: 0.7200 || timer: 0.0940 sec.
iter 172210 || Loss: 0.8327 || timer: 0.0988 sec.
iter 172220 || Loss: 1.0427 || timer: 0.0840 sec.
iter 172230 || Loss: 0.7703 || timer: 0.0893 sec.
iter 172240 || Loss: 0.9517 || timer: 0.0904 sec.
iter 172250 || Loss: 1.1977 || timer: 0.1056 sec.
iter 172260 || Loss: 0.7879 || timer: 0.0885 sec.
iter 172270 || Loss: 1.0302 || timer: 0.1079 sec.
iter 172280 || Loss: 0.8864 || timer: 0.0913 sec.
iter 172290 || Loss: 0.8265 || timer: 0.1381 sec.
iter 172300 || Loss: 0.6148 || timer: 0.0896 sec.
iter 172310 || Loss: 1.0652 || timer: 0.1472 sec.
iter 172320 || Loss: 0.9839 || timer: 0.1049 sec.
iter 172330 || Loss: 1.1709 || timer: 0.0925 sec.
iter 172340 || Loss: 0.7784 || timer: 0.0887 sec.
iter 172350 || Loss: 0.6531 || timer: 0.0845 sec.
iter 172360 || Loss: 1.0139 || timer: 0.0848 sec.
iter 172370 || Loss: 1.0089 || timer: 0.0767 sec.
iter 172380 || Loss: 0.7536 || timer: 0.0829 sec.
iter 172390 || Loss: 0.8450 || timer: 0.0822 sec.
iter 172400 || Loss: 1.1086 || timer: 0.1029 sec.
iter 172410 || Loss: 1.2707 || timer: 0.0808 sec.
iter 172420 || Loss: 0.7756 || timer: 0.0754 sec.
iter 172430 || Loss: 0.8922 || timer: 0.0750 sec.
iter 172440 || Loss: 0.9409 || timer: 0.0751 sec.
iter 172450 || Loss: 1.0955 || timer: 0.0897 sec.
iter 172460 || Loss: 0.8828 || timer: 0.0819 sec.
iter 172470 || Loss: 1.2261 || timer: 0.0831 sec.
iter 172480 || Loss: 1.2464 || timer: 0.0193 sec.
iter 172490 || Loss: 1.7250 || timer: 0.0864 sec.
iter 172500 || Loss: 1.1825 || timer: 0.0890 sec.
iter 172510 || Loss: 1.0584 || timer: 0.0815 sec.
iter 172520 || Loss: 0.7945 || timer: 0.0835 sec.
iter 172530 || Loss: 0.8806 || timer: 0.0898 sec.
iter 172540 || Loss: 1.1225 || timer: 0.0917 sec.
iter 172550 || Loss: 1.5652 || timer: 0.1058 sec.
iter 172560 || Loss: 0.8578 || timer: 0.0823 sec.
iter 172570 || Loss: 0.9022 || timer: 0.0826 sec.
iter 172580 || Loss: 0.6244 || timer: 0.1163 sec.
iter 172590 || Loss: 1.0243 || timer: 0.0820 sec.
iter 172600 || Loss: 0.9053 || timer: 0.1039 sec.
iter 172610 || Loss: 0.8229 || timer: 0.0841 sec.
iter 172620 || Loss: 0.9348 || timer: 0.0828 sec.
iter 172630 || Loss: 0.7372 || timer: 0.0824 sec.
iter 172640 || Loss: 0.8930 || timer: 0.0896 sec.
iter 172650 || Loss: 0.7750 || timer: 0.0880 sec.
iter 172660 || Loss: 1.1373 || timer: 0.0829 sec.
iter 172670 || Loss: 0.7522 || timer: 0.0910 sec.
iter 172680 || Loss: 1.0993 || timer: 0.0826 sec.
iter 172690 || Loss: 0.7171 || timer: 0.0926 sec.
iter 172700 || Loss: 0.9293 || timer: 0.0861 sec.
iter 172710 || Loss: 0.7880 || timer: 0.0814 sec.
iter 172720 || Loss: 0.7915 || timer: 0.0892 sec.
iter 172730 || Loss: 0.7809 || timer: 0.0848 sec.
iter 172740 || Loss: 0.9196 || timer: 0.0907 sec.
iter 172750 || Loss: 1.1283 || timer: 0.0914 sec.
iter 172760 || Loss: 1.0448 || timer: 0.0821 sec.
iter 172770 || Loss: 1.1628 || timer: 0.0892 sec.
iter 172780 || Loss: 0.9193 || timer: 0.1081 sec.
iter 172790 || Loss: 0.7425 || timer: 0.0847 sec.
iter 172800 || Loss: 1.0705 || timer: 0.0889 sec.
iter 172810 || Loss: 1.1497 || timer: 0.0234 sec.
iter 172820 || Loss: 0.8352 || timer: 0.0865 sec.
iter 172830 || Loss: 0.7901 || timer: 0.1030 sec.
iter 172840 || Loss: 0.7464 || timer: 0.0813 sec.
iter 172850 || Loss: 1.1361 || timer: 0.0988 sec.
iter 172860 || Loss: 0.8815 || timer: 0.0858 sec.
iter 172870 || Loss: 0.9701 || timer: 0.1090 sec.
iter 172880 || Loss: 0.9829 || timer: 0.0902 sec.
iter 172890 || Loss: 1.0655 || timer: 0.0760 sec.
iter 172900 || Loss: 0.8904 || timer: 0.0850 sec.
iter 172910 || Loss: 1.2662 || timer: 0.1199 sec.
iter 172920 || Loss: 0.8144 || timer: 0.0996 sec.
iter 172930 || Loss: 1.1669 || timer: 0.0897 sec.
iter 172940 || Loss: 1.1885 || timer: 0.0814 sec.
iter 172950 || Loss: 0.6920 || timer: 0.0884 sec.
iter 172960 || Loss: 0.6900 || timer: 0.0900 sec.
iter 172970 || Loss: 1.2059 || timer: 0.0769 sec.
iter 172980 || Loss: 1.0682 || timer: 0.0866 sec.
iter 172990 || Loss: 1.1091 || timer: 0.0933 sec.
iter 173000 || Loss: 1.0282 || timer: 0.1196 sec.
iter 173010 || Loss: 1.0627 || timer: 0.0935 sec.
iter 173020 || Loss: 1.1707 || timer: 0.1082 sec.
iter 173030 || Loss: 1.2619 || timer: 0.0906 sec.
iter 173040 || Loss: 0.8377 || timer: 0.0865 sec.
iter 173050 || Loss: 0.6144 || timer: 0.0905 sec.
iter 173060 || Loss: 0.9032 || timer: 0.0929 sec.
iter 173070 || Loss: 0.6333 || timer: 0.0912 sec.
iter 173080 || Loss: 0.7377 || timer: 0.0907 sec.
iter 173090 || Loss: 0.8899 || timer: 0.0857 sec.
iter 173100 || Loss: 0.9882 || timer: 0.1026 sec.
iter 173110 || Loss: 0.8651 || timer: 0.0780 sec.
iter 173120 || Loss: 1.5666 || timer: 0.1007 sec.
iter 173130 || Loss: 0.6437 || timer: 0.0813 sec.
iter 173140 || Loss: 1.0825 || timer: 0.0197 sec.
iter 173150 || Loss: 1.1081 || timer: 0.0838 sec.
iter 173160 || Loss: 0.7332 || timer: 0.0909 sec.
iter 173170 || Loss: 1.1123 || timer: 0.1059 sec.
iter 173180 || Loss: 0.8613 || timer: 0.0907 sec.
iter 173190 || Loss: 1.0326 || timer: 0.0875 sec.
iter 173200 || Loss: 1.2102 || timer: 0.0824 sec.
iter 173210 || Loss: 0.7260 || timer: 0.0824 sec.
iter 173220 || Loss: 0.8731 || timer: 0.0899 sec.
iter 173230 || Loss: 1.1708 || timer: 0.0826 sec.
iter 173240 || Loss: 0.5706 || timer: 0.0965 sec.
iter 173250 || Loss: 0.6840 || timer: 0.0856 sec.
iter 173260 || Loss: 0.8789 || timer: 0.0938 sec.
iter 173270 || Loss: 0.6979 || timer: 0.0809 sec.
iter 173280 || Loss: 0.8673 || timer: 0.0763 sec.
iter 173290 || Loss: 0.9235 || timer: 0.0825 sec.
iter 173300 || Loss: 0.7949 || timer: 0.0834 sec.
iter 173310 || Loss: 0.9573 || timer: 0.0900 sec.
iter 173320 || Loss: 0.9427 || timer: 0.0761 sec.
iter 173330 || Loss: 0.6259 || timer: 0.1064 sec.
iter 173340 || Loss: 0.8317 || timer: 0.0812 sec.
iter 173350 || Loss: 0.8861 || timer: 0.0988 sec.
iter 173360 || Loss: 0.8670 || timer: 0.1009 sec.
iter 173370 || Loss: 1.0233 || timer: 0.0872 sec.
iter 173380 || Loss: 0.9494 || timer: 0.0812 sec.
iter 173390 || Loss: 1.1533 || timer: 0.0917 sec.
iter 173400 || Loss: 0.9000 || timer: 0.0888 sec.
iter 173410 || Loss: 1.4568 || timer: 0.0831 sec.
iter 173420 || Loss: 1.2856 || timer: 0.0846 sec.
iter 173430 || Loss: 1.1699 || timer: 0.0811 sec.
iter 173440 || Loss: 0.8868 || timer: 0.0821 sec.
iter 173450 || Loss: 0.8694 || timer: 0.0821 sec.
iter 173460 || Loss: 0.9377 || timer: 0.0817 sec.
iter 173470 || Loss: 1.0708 || timer: 0.0275 sec.
iter 173480 || Loss: 0.5581 || timer: 0.0890 sec.
iter 173490 || Loss: 0.8851 || timer: 0.0883 sec.
iter 173500 || Loss: 0.6391 || timer: 0.0836 sec.
iter 173510 || Loss: 1.1171 || timer: 0.0812 sec.
iter 173520 || Loss: 0.9891 || timer: 0.0808 sec.
iter 173530 || Loss: 0.7775 || timer: 0.0902 sec.
iter 173540 || Loss: 0.8221 || timer: 0.0917 sec.
iter 173550 || Loss: 0.7911 || timer: 0.0826 sec.
iter 173560 || Loss: 0.9368 || timer: 0.1069 sec.
iter 173570 || Loss: 0.7252 || timer: 0.0932 sec.
iter 173580 || Loss: 1.1454 || timer: 0.0828 sec.
iter 173590 || Loss: 1.2109 || timer: 0.1088 sec.
iter 173600 || Loss: 0.9067 || timer: 0.0879 sec.
iter 173610 || Loss: 0.8659 || timer: 0.0883 sec.
iter 173620 || Loss: 1.1051 || timer: 0.0879 sec.
iter 173630 || Loss: 0.7200 || timer: 0.0852 sec.
iter 173640 || Loss: 0.9496 || timer: 0.0843 sec.
iter 173650 || Loss: 0.9893 || timer: 0.1093 sec.
iter 173660 || Loss: 0.8313 || timer: 0.0813 sec.
iter 173670 || Loss: 0.9094 || timer: 0.0924 sec.
iter 173680 || Loss: 0.8320 || timer: 0.0841 sec.
iter 173690 || Loss: 1.1486 || timer: 0.0832 sec.
iter 173700 || Loss: 1.0306 || timer: 0.0812 sec.
iter 173710 || Loss: 0.6774 || timer: 0.0926 sec.
iter 173720 || Loss: 0.8395 || timer: 0.0900 sec.
iter 173730 || Loss: 0.8691 || timer: 0.0958 sec.
iter 173740 || Loss: 1.0054 || timer: 0.0825 sec.
iter 173750 || Loss: 0.9635 || timer: 0.0830 sec.
iter 173760 || Loss: 1.2029 || timer: 0.0915 sec.
iter 173770 || Loss: 1.0540 || timer: 0.0875 sec.
iter 173780 || Loss: 0.6832 || timer: 0.0825 sec.
iter 173790 || Loss: 0.9781 || timer: 0.0845 sec.
iter 173800 || Loss: 0.7145 || timer: 0.0276 sec.
iter 173810 || Loss: 0.4250 || timer: 0.0906 sec.
iter 173820 || Loss: 0.8396 || timer: 0.0909 sec.
iter 173830 || Loss: 1.0652 || timer: 0.0837 sec.
iter 173840 || Loss: 0.7875 || timer: 0.0839 sec.
iter 173850 || Loss: 0.8991 || timer: 0.1015 sec.
iter 173860 || Loss: 0.7688 || timer: 0.0854 sec.
iter 173870 || Loss: 0.7972 || timer: 0.0859 sec.
iter 173880 || Loss: 0.9780 || timer: 0.0906 sec.
iter 173890 || Loss: 1.0682 || timer: 0.0829 sec.
iter 173900 || Loss: 0.7511 || timer: 0.1055 sec.
iter 173910 || Loss: 0.9720 || timer: 0.0898 sec.
iter 173920 || Loss: 0.9183 || timer: 0.1206 sec.
iter 173930 || Loss: 1.0018 || timer: 0.0937 sec.
iter 173940 || Loss: 0.9815 || timer: 0.0933 sec.
iter 173950 || Loss: 0.8656 || timer: 0.1025 sec.
iter 173960 || Loss: 0.6201 || timer: 0.0908 sec.
iter 173970 || Loss: 0.8081 || timer: 0.0912 sec.
iter 173980 || Loss: 0.6700 || timer: 0.1049 sec.
iter 173990 || Loss: 1.0495 || timer: 0.0874 sec.
iter 174000 || Loss: 1.0595 || timer: 0.1067 sec.
iter 174010 || Loss: 1.3487 || timer: 0.0955 sec.
iter 174020 || Loss: 0.9802 || timer: 0.0927 sec.
iter 174030 || Loss: 0.8138 || timer: 0.0927 sec.
iter 174040 || Loss: 0.9124 || timer: 0.0902 sec.
iter 174050 || Loss: 0.9757 || timer: 0.0900 sec.
iter 174060 || Loss: 0.8496 || timer: 0.0915 sec.
iter 174070 || Loss: 0.7132 || timer: 0.1078 sec.
iter 174080 || Loss: 0.8371 || timer: 0.0936 sec.
iter 174090 || Loss: 0.8070 || timer: 0.1124 sec.
iter 174100 || Loss: 0.5868 || timer: 0.0906 sec.
iter 174110 || Loss: 0.8247 || timer: 0.0905 sec.
iter 174120 || Loss: 0.9843 || timer: 0.0899 sec.
iter 174130 || Loss: 0.8146 || timer: 0.0163 sec.
iter 174140 || Loss: 0.8038 || timer: 0.0887 sec.
iter 174150 || Loss: 0.9391 || timer: 0.1198 sec.
iter 174160 || Loss: 1.0035 || timer: 0.1053 sec.
iter 174170 || Loss: 1.1046 || timer: 0.1050 sec.
iter 174180 || Loss: 0.9135 || timer: 0.1020 sec.
iter 174190 || Loss: 0.8004 || timer: 0.0887 sec.
iter 174200 || Loss: 0.8210 || timer: 0.0861 sec.
iter 174210 || Loss: 0.7103 || timer: 0.0917 sec.
iter 174220 || Loss: 1.0410 || timer: 0.0916 sec.
iter 174230 || Loss: 0.7235 || timer: 0.1160 sec.
iter 174240 || Loss: 0.6218 || timer: 0.0904 sec.
iter 174250 || Loss: 0.6710 || timer: 0.0903 sec.
iter 174260 || Loss: 0.6462 || timer: 0.0861 sec.
iter 174270 || Loss: 0.7700 || timer: 0.0909 sec.
iter 174280 || Loss: 0.9124 || timer: 0.0925 sec.
iter 174290 || Loss: 0.9430 || timer: 0.0939 sec.
iter 174300 || Loss: 0.9254 || timer: 0.1049 sec.
iter 174310 || Loss: 0.9548 || timer: 0.1073 sec.
iter 174320 || Loss: 0.9456 || timer: 0.1124 sec.
iter 174330 || Loss: 0.9670 || timer: 0.0957 sec.
iter 174340 || Loss: 0.9605 || timer: 0.0830 sec.
iter 174350 || Loss: 0.9417 || timer: 0.0829 sec.
iter 174360 || Loss: 0.7810 || timer: 0.0915 sec.
iter 174370 || Loss: 1.0623 || timer: 0.0807 sec.
iter 174380 || Loss: 0.9617 || timer: 0.0862 sec.
iter 174390 || Loss: 0.7459 || timer: 0.0832 sec.
iter 174400 || Loss: 1.0224 || timer: 0.0900 sec.
iter 174410 || Loss: 1.1106 || timer: 0.0978 sec.
iter 174420 || Loss: 0.9528 || timer: 0.0826 sec.
iter 174430 || Loss: 0.9546 || timer: 0.0844 sec.
iter 174440 || Loss: 0.9094 || timer: 0.0819 sec.
iter 174450 || Loss: 1.6508 || timer: 0.0823 sec.
iter 174460 || Loss: 0.9099 || timer: 0.0271 sec.
iter 174470 || Loss: 1.4419 || timer: 0.0904 sec.
iter 174480 || Loss: 1.0779 || timer: 0.1114 sec.
iter 174490 || Loss: 0.8203 || timer: 0.0844 sec.
iter 174500 || Loss: 0.9944 || timer: 0.1081 sec.
iter 174510 || Loss: 1.0054 || timer: 0.0901 sec.
iter 174520 || Loss: 0.9296 || timer: 0.0887 sec.
iter 174530 || Loss: 0.7857 || timer: 0.0861 sec.
iter 174540 || Loss: 0.8393 || timer: 0.0906 sec.
iter 174550 || Loss: 1.0267 || timer: 0.0997 sec.
iter 174560 || Loss: 0.9017 || timer: 0.1019 sec.
iter 174570 || Loss: 0.8069 || timer: 0.1104 sec.
iter 174580 || Loss: 0.7378 || timer: 0.0904 sec.
iter 174590 || Loss: 0.8517 || timer: 0.0843 sec.
iter 174600 || Loss: 0.7723 || timer: 0.0837 sec.
iter 174610 || Loss: 0.7566 || timer: 0.1050 sec.
iter 174620 || Loss: 0.7340 || timer: 0.0924 sec.
iter 174630 || Loss: 0.9813 || timer: 0.0843 sec.
iter 174640 || Loss: 1.0365 || timer: 0.0827 sec.
iter 174650 || Loss: 0.9563 || timer: 0.0985 sec.
iter 174660 || Loss: 0.9949 || timer: 0.0837 sec.
iter 174670 || Loss: 0.9597 || timer: 0.0893 sec.
iter 174680 || Loss: 1.2406 || timer: 0.0988 sec.
iter 174690 || Loss: 0.7329 || timer: 0.0894 sec.
iter 174700 || Loss: 0.8110 || timer: 0.0925 sec.
iter 174710 || Loss: 0.9757 || timer: 0.0932 sec.
iter 174720 || Loss: 1.1621 || timer: 0.0824 sec.
iter 174730 || Loss: 0.9131 || timer: 0.0897 sec.
iter 174740 || Loss: 1.2721 || timer: 0.1145 sec.
iter 174750 || Loss: 0.8935 || timer: 0.0885 sec.
iter 174760 || Loss: 0.9085 || timer: 0.0905 sec.
iter 174770 || Loss: 0.7840 || timer: 0.0910 sec.
iter 174780 || Loss: 1.1772 || timer: 0.0826 sec.
iter 174790 || Loss: 0.9208 || timer: 0.0245 sec.
iter 174800 || Loss: 1.5120 || timer: 0.0759 sec.
iter 174810 || Loss: 1.1249 || timer: 0.1065 sec.
iter 174820 || Loss: 1.0453 || timer: 0.0901 sec.
iter 174830 || Loss: 0.6642 || timer: 0.0894 sec.
iter 174840 || Loss: 0.9221 || timer: 0.0899 sec.
iter 174850 || Loss: 1.0067 || timer: 0.1009 sec.
iter 174860 || Loss: 0.9194 || timer: 0.0824 sec.
iter 174870 || Loss: 1.0107 || timer: 0.0824 sec.
iter 174880 || Loss: 0.8297 || timer: 0.0896 sec.
iter 174890 || Loss: 0.7884 || timer: 0.0957 sec.
iter 174900 || Loss: 1.1590 || timer: 0.0883 sec.
iter 174910 || Loss: 1.1469 || timer: 0.0875 sec.
iter 174920 || Loss: 0.7998 || timer: 0.0748 sec.
iter 174930 || Loss: 0.5739 || timer: 0.0908 sec.
iter 174940 || Loss: 0.9202 || timer: 0.0901 sec.
iter 174950 || Loss: 0.8329 || timer: 0.0920 sec.
iter 174960 || Loss: 0.8827 || timer: 0.1075 sec.
iter 174970 || Loss: 0.9138 || timer: 0.0904 sec.
iter 174980 || Loss: 0.9372 || timer: 0.0994 sec.
iter 174990 || Loss: 0.8673 || timer: 0.0906 sec.
iter 175000 || Loss: 1.0366 || Saving state, iter: 175000
timer: 0.1066 sec.
iter 175010 || Loss: 0.8824 || timer: 0.0826 sec.
iter 175020 || Loss: 1.1213 || timer: 0.0817 sec.
iter 175030 || Loss: 0.9790 || timer: 0.0928 sec.
iter 175040 || Loss: 1.2100 || timer: 0.0754 sec.
iter 175050 || Loss: 0.7939 || timer: 0.0856 sec.
iter 175060 || Loss: 0.7916 || timer: 0.0886 sec.
iter 175070 || Loss: 0.9392 || timer: 0.0897 sec.
iter 175080 || Loss: 0.9313 || timer: 0.0829 sec.
iter 175090 || Loss: 0.7973 || timer: 0.0901 sec.
iter 175100 || Loss: 0.9845 || timer: 0.0897 sec.
iter 175110 || Loss: 1.0406 || timer: 0.1240 sec.
iter 175120 || Loss: 0.8372 || timer: 0.0263 sec.
iter 175130 || Loss: 1.0647 || timer: 0.0836 sec.
iter 175140 || Loss: 1.0407 || timer: 0.1010 sec.
iter 175150 || Loss: 1.1590 || timer: 0.0846 sec.
iter 175160 || Loss: 1.1082 || timer: 0.0833 sec.
iter 175170 || Loss: 1.3447 || timer: 0.1062 sec.
iter 175180 || Loss: 0.7516 || timer: 0.0839 sec.
iter 175190 || Loss: 0.8198 || timer: 0.0901 sec.
iter 175200 || Loss: 1.0457 || timer: 0.1016 sec.
iter 175210 || Loss: 0.6090 || timer: 0.0898 sec.
iter 175220 || Loss: 1.0646 || timer: 0.1019 sec.
iter 175230 || Loss: 0.8689 || timer: 0.0961 sec.
iter 175240 || Loss: 0.8059 || timer: 0.0897 sec.
iter 175250 || Loss: 0.9094 || timer: 0.0821 sec.
iter 175260 || Loss: 0.9003 || timer: 0.0915 sec.
iter 175270 || Loss: 0.9769 || timer: 0.0824 sec.
iter 175280 || Loss: 1.0337 || timer: 0.0871 sec.
iter 175290 || Loss: 0.7917 || timer: 0.0820 sec.
iter 175300 || Loss: 1.0075 || timer: 0.0924 sec.
iter 175310 || Loss: 0.9125 || timer: 0.0909 sec.
iter 175320 || Loss: 0.7005 || timer: 0.1095 sec.
iter 175330 || Loss: 1.0052 || timer: 0.0810 sec.
iter 175340 || Loss: 1.1044 || timer: 0.0833 sec.
iter 175350 || Loss: 1.0069 || timer: 0.0888 sec.
iter 175360 || Loss: 1.1020 || timer: 0.0816 sec.
iter 175370 || Loss: 0.9058 || timer: 0.0937 sec.
iter 175380 || Loss: 0.9856 || timer: 0.0927 sec.
iter 175390 || Loss: 1.1504 || timer: 0.0921 sec.
iter 175400 || Loss: 0.8718 || timer: 0.0940 sec.
iter 175410 || Loss: 1.0632 || timer: 0.0750 sec.
iter 175420 || Loss: 0.7078 || timer: 0.0849 sec.
iter 175430 || Loss: 1.1541 || timer: 0.0878 sec.
iter 175440 || Loss: 0.9143 || timer: 0.0864 sec.
iter 175450 || Loss: 0.8020 || timer: 0.0188 sec.
iter 175460 || Loss: 0.3333 || timer: 0.0913 sec.
iter 175470 || Loss: 0.9067 || timer: 0.0899 sec.
iter 175480 || Loss: 0.9046 || timer: 0.1052 sec.
iter 175490 || Loss: 0.8617 || timer: 0.0833 sec.
iter 175500 || Loss: 1.1449 || timer: 0.0890 sec.
iter 175510 || Loss: 0.8096 || timer: 0.0822 sec.
iter 175520 || Loss: 1.2182 || timer: 0.0853 sec.
iter 175530 || Loss: 1.0987 || timer: 0.0915 sec.
iter 175540 || Loss: 0.8706 || timer: 0.0897 sec.
iter 175550 || Loss: 0.9923 || timer: 0.1101 sec.
iter 175560 || Loss: 1.1357 || timer: 0.0915 sec.
iter 175570 || Loss: 0.9381 || timer: 0.1086 sec.
iter 175580 || Loss: 1.1716 || timer: 0.0899 sec.
iter 175590 || Loss: 0.7316 || timer: 0.0906 sec.
iter 175600 || Loss: 1.0445 || timer: 0.1150 sec.
iter 175610 || Loss: 0.7868 || timer: 0.0898 sec.
iter 175620 || Loss: 1.4840 || timer: 0.0896 sec.
iter 175630 || Loss: 0.8872 || timer: 0.0858 sec.
iter 175640 || Loss: 0.9349 || timer: 0.0981 sec.
iter 175650 || Loss: 0.9639 || timer: 0.0817 sec.
iter 175660 || Loss: 0.9965 || timer: 0.0830 sec.
iter 175670 || Loss: 1.1144 || timer: 0.0994 sec.
iter 175680 || Loss: 0.9811 || timer: 0.0888 sec.
iter 175690 || Loss: 1.0564 || timer: 0.1222 sec.
iter 175700 || Loss: 0.9342 || timer: 0.0971 sec.
iter 175710 || Loss: 0.7589 || timer: 0.0907 sec.
iter 175720 || Loss: 0.6716 || timer: 0.0963 sec.
iter 175730 || Loss: 0.7796 || timer: 0.0902 sec.
iter 175740 || Loss: 0.6388 || timer: 0.0844 sec.
iter 175750 || Loss: 0.9843 || timer: 0.0923 sec.
iter 175760 || Loss: 1.0674 || timer: 0.0937 sec.
iter 175770 || Loss: 0.8545 || timer: 0.0868 sec.
iter 175780 || Loss: 0.6974 || timer: 0.0171 sec.
iter 175790 || Loss: 0.5844 || timer: 0.0912 sec.
iter 175800 || Loss: 0.8235 || timer: 0.1078 sec.
iter 175810 || Loss: 0.8340 || timer: 0.0938 sec.
iter 175820 || Loss: 0.6984 || timer: 0.0916 sec.
iter 175830 || Loss: 1.0078 || timer: 0.0897 sec.
iter 175840 || Loss: 1.0790 || timer: 0.0969 sec.
iter 175850 || Loss: 1.1816 || timer: 0.0908 sec.
iter 175860 || Loss: 0.7871 || timer: 0.0912 sec.
iter 175870 || Loss: 0.9284 || timer: 0.0891 sec.
iter 175880 || Loss: 0.9810 || timer: 0.0972 sec.
iter 175890 || Loss: 0.7205 || timer: 0.0892 sec.
iter 175900 || Loss: 0.7495 || timer: 0.0911 sec.
iter 175910 || Loss: 0.7717 || timer: 0.0895 sec.
iter 175920 || Loss: 1.0052 || timer: 0.0914 sec.
iter 175930 || Loss: 0.7287 || timer: 0.0903 sec.
iter 175940 || Loss: 0.9945 || timer: 0.0899 sec.
iter 175950 || Loss: 1.1306 || timer: 0.0948 sec.
iter 175960 || Loss: 1.1027 || timer: 0.0935 sec.
iter 175970 || Loss: 1.1710 || timer: 0.0839 sec.
iter 175980 || Loss: 0.8901 || timer: 0.0979 sec.
iter 175990 || Loss: 1.0273 || timer: 0.0899 sec.
iter 176000 || Loss: 0.8225 || timer: 0.0840 sec.
iter 176010 || Loss: 0.6467 || timer: 0.0895 sec.
iter 176020 || Loss: 0.8711 || timer: 0.0929 sec.
iter 176030 || Loss: 0.8229 || timer: 0.1121 sec.
iter 176040 || Loss: 1.2090 || timer: 0.0869 sec.
iter 176050 || Loss: 1.0203 || timer: 0.0885 sec.
iter 176060 || Loss: 0.9413 || timer: 0.0982 sec.
iter 176070 || Loss: 0.8445 || timer: 0.1113 sec.
iter 176080 || Loss: 0.9628 || timer: 0.0814 sec.
iter 176090 || Loss: 1.3016 || timer: 0.0756 sec.
iter 176100 || Loss: 0.8970 || timer: 0.0840 sec.
iter 176110 || Loss: 0.9202 || timer: 0.0150 sec.
iter 176120 || Loss: 2.6609 || timer: 0.0837 sec.
iter 176130 || Loss: 1.0226 || timer: 0.1053 sec.
iter 176140 || Loss: 0.7664 || timer: 0.0831 sec.
iter 176150 || Loss: 0.8940 || timer: 0.0852 sec.
iter 176160 || Loss: 1.5481 || timer: 0.0889 sec.
iter 176170 || Loss: 0.8628 || timer: 0.0910 sec.
iter 176180 || Loss: 0.7439 || timer: 0.1261 sec.
iter 176190 || Loss: 0.7948 || timer: 0.0929 sec.
iter 176200 || Loss: 1.0871 || timer: 0.0967 sec.
iter 176210 || Loss: 0.9064 || timer: 0.0967 sec.
iter 176220 || Loss: 0.8765 || timer: 0.0841 sec.
iter 176230 || Loss: 0.8407 || timer: 0.0897 sec.
iter 176240 || Loss: 0.6786 || timer: 0.1161 sec.
iter 176250 || Loss: 0.8285 || timer: 0.0916 sec.
iter 176260 || Loss: 0.8861 || timer: 0.0899 sec.
iter 176270 || Loss: 0.9177 || timer: 0.0909 sec.
iter 176280 || Loss: 0.9314 || timer: 0.0920 sec.
iter 176290 || Loss: 1.0447 || timer: 0.0847 sec.
iter 176300 || Loss: 0.6944 || timer: 0.0915 sec.
iter 176310 || Loss: 0.8648 || timer: 0.0833 sec.
iter 176320 || Loss: 1.1092 || timer: 0.0840 sec.
iter 176330 || Loss: 1.1268 || timer: 0.0929 sec.
iter 176340 || Loss: 1.2474 || timer: 0.0933 sec.
iter 176350 || Loss: 0.9923 || timer: 0.0832 sec.
iter 176360 || Loss: 0.7621 || timer: 0.0901 sec.
iter 176370 || Loss: 0.8609 || timer: 0.0902 sec.
iter 176380 || Loss: 0.7995 || timer: 0.0840 sec.
iter 176390 || Loss: 1.1536 || timer: 0.0913 sec.
iter 176400 || Loss: 1.1291 || timer: 0.0925 sec.
iter 176410 || Loss: 0.9961 || timer: 0.0900 sec.
iter 176420 || Loss: 0.7951 || timer: 0.0975 sec.
iter 176430 || Loss: 0.8436 || timer: 0.0975 sec.
iter 176440 || Loss: 0.9866 || timer: 0.0219 sec.
iter 176450 || Loss: 0.2869 || timer: 0.1091 sec.
iter 176460 || Loss: 0.8835 || timer: 0.1134 sec.
iter 176470 || Loss: 0.9389 || timer: 0.0908 sec.
iter 176480 || Loss: 0.8808 || timer: 0.1129 sec.
iter 176490 || Loss: 0.8352 || timer: 0.0921 sec.
iter 176500 || Loss: 0.6183 || timer: 0.1012 sec.
iter 176510 || Loss: 1.4517 || timer: 0.0831 sec.
iter 176520 || Loss: 0.9311 || timer: 0.0923 sec.
iter 176530 || Loss: 0.9813 || timer: 0.0878 sec.
iter 176540 || Loss: 1.0539 || timer: 0.0870 sec.
iter 176550 || Loss: 1.0282 || timer: 0.0830 sec.
iter 176560 || Loss: 1.5440 || timer: 0.0770 sec.
iter 176570 || Loss: 1.0957 || timer: 0.1104 sec.
iter 176580 || Loss: 0.9413 || timer: 0.0904 sec.
iter 176590 || Loss: 0.7537 || timer: 0.0840 sec.
iter 176600 || Loss: 0.8242 || timer: 0.0827 sec.
iter 176610 || Loss: 0.9074 || timer: 0.0929 sec.
iter 176620 || Loss: 1.1324 || timer: 0.0887 sec.
iter 176630 || Loss: 0.8189 || timer: 0.0905 sec.
iter 176640 || Loss: 0.8499 || timer: 0.0841 sec.
iter 176650 || Loss: 0.6250 || timer: 0.0832 sec.
iter 176660 || Loss: 0.7461 || timer: 0.0815 sec.
iter 176670 || Loss: 0.8829 || timer: 0.0915 sec.
iter 176680 || Loss: 0.6968 || timer: 0.0901 sec.
iter 176690 || Loss: 1.2714 || timer: 0.0911 sec.
iter 176700 || Loss: 0.7764 || timer: 0.0907 sec.
iter 176710 || Loss: 0.9115 || timer: 0.0932 sec.
iter 176720 || Loss: 1.1909 || timer: 0.0983 sec.
iter 176730 || Loss: 0.9416 || timer: 0.0842 sec.
iter 176740 || Loss: 0.8662 || timer: 0.0838 sec.
iter 176750 || Loss: 0.8979 || timer: 0.1110 sec.
iter 176760 || Loss: 1.2980 || timer: 0.0842 sec.
iter 176770 || Loss: 1.1042 || timer: 0.0175 sec.
iter 176780 || Loss: 1.0530 || timer: 0.0909 sec.
iter 176790 || Loss: 0.8281 || timer: 0.0830 sec.
iter 176800 || Loss: 0.9330 || timer: 0.0902 sec.
iter 176810 || Loss: 0.9096 || timer: 0.1045 sec.
iter 176820 || Loss: 1.1057 || timer: 0.0895 sec.
iter 176830 || Loss: 1.0575 || timer: 0.0914 sec.
iter 176840 || Loss: 0.7959 || timer: 0.1101 sec.
iter 176850 || Loss: 0.7800 || timer: 0.0908 sec.
iter 176860 || Loss: 0.8111 || timer: 0.1112 sec.
iter 176870 || Loss: 1.1005 || timer: 0.1016 sec.
iter 176880 || Loss: 0.9005 || timer: 0.0825 sec.
iter 176890 || Loss: 0.7822 || timer: 0.0912 sec.
iter 176900 || Loss: 0.7840 || timer: 0.0910 sec.
iter 176910 || Loss: 0.7245 || timer: 0.0835 sec.
iter 176920 || Loss: 0.7373 || timer: 0.0892 sec.
iter 176930 || Loss: 0.7369 || timer: 0.0821 sec.
iter 176940 || Loss: 1.2295 || timer: 0.0824 sec.
iter 176950 || Loss: 0.9731 || timer: 0.1125 sec.
iter 176960 || Loss: 0.9373 || timer: 0.0929 sec.
iter 176970 || Loss: 0.9064 || timer: 0.0840 sec.
iter 176980 || Loss: 0.7841 || timer: 0.0824 sec.
iter 176990 || Loss: 0.9776 || timer: 0.0837 sec.
iter 177000 || Loss: 1.0716 || timer: 0.0900 sec.
iter 177010 || Loss: 0.8521 || timer: 0.0818 sec.
iter 177020 || Loss: 0.7468 || timer: 0.0835 sec.
iter 177030 || Loss: 0.7547 || timer: 0.0912 sec.
iter 177040 || Loss: 0.8250 || timer: 0.0826 sec.
iter 177050 || Loss: 1.1461 || timer: 0.0901 sec.
iter 177060 || Loss: 0.9785 || timer: 0.0837 sec.
iter 177070 || Loss: 0.8521 || timer: 0.0919 sec.
iter 177080 || Loss: 1.3011 || timer: 0.0813 sec.
iter 177090 || Loss: 0.9027 || timer: 0.0808 sec.
iter 177100 || Loss: 0.9990 || timer: 0.0261 sec.
iter 177110 || Loss: 0.7238 || timer: 0.0938 sec.
iter 177120 || Loss: 0.7995 || timer: 0.0903 sec.
iter 177130 || Loss: 0.8268 || timer: 0.0932 sec.
iter 177140 || Loss: 0.9788 || timer: 0.1001 sec.
iter 177150 || Loss: 0.9043 || timer: 0.0897 sec.
iter 177160 || Loss: 1.1641 || timer: 0.0837 sec.
iter 177170 || Loss: 1.0118 || timer: 0.0768 sec.
iter 177180 || Loss: 1.0505 || timer: 0.0934 sec.
iter 177190 || Loss: 0.9618 || timer: 0.0841 sec.
iter 177200 || Loss: 0.8306 || timer: 0.0937 sec.
iter 177210 || Loss: 0.9315 || timer: 0.0838 sec.
iter 177220 || Loss: 1.1427 || timer: 0.0899 sec.
iter 177230 || Loss: 1.0388 || timer: 0.0922 sec.
iter 177240 || Loss: 0.9080 || timer: 0.0974 sec.
iter 177250 || Loss: 1.1525 || timer: 0.0929 sec.
iter 177260 || Loss: 1.1547 || timer: 0.0811 sec.
iter 177270 || Loss: 1.2981 || timer: 0.0841 sec.
iter 177280 || Loss: 1.0513 || timer: 0.0839 sec.
iter 177290 || Loss: 0.8996 || timer: 0.0904 sec.
iter 177300 || Loss: 0.7555 || timer: 0.0859 sec.
iter 177310 || Loss: 0.7518 || timer: 0.0910 sec.
iter 177320 || Loss: 1.2169 || timer: 0.1036 sec.
iter 177330 || Loss: 0.9430 || timer: 0.0837 sec.
iter 177340 || Loss: 0.6973 || timer: 0.1128 sec.
iter 177350 || Loss: 0.9332 || timer: 0.0844 sec.
iter 177360 || Loss: 1.1234 || timer: 0.1073 sec.
iter 177370 || Loss: 1.1707 || timer: 0.0826 sec.
iter 177380 || Loss: 1.1762 || timer: 0.0812 sec.
iter 177390 || Loss: 0.9576 || timer: 0.0817 sec.
iter 177400 || Loss: 0.8393 || timer: 0.1032 sec.
iter 177410 || Loss: 0.8904 || timer: 0.0928 sec.
iter 177420 || Loss: 1.4337 || timer: 0.1063 sec.
iter 177430 || Loss: 0.9632 || timer: 0.0258 sec.
iter 177440 || Loss: 0.5708 || timer: 0.0770 sec.
iter 177450 || Loss: 0.7660 || timer: 0.0845 sec.
iter 177460 || Loss: 1.0468 || timer: 0.0910 sec.
iter 177470 || Loss: 0.8124 || timer: 0.1034 sec.
iter 177480 || Loss: 1.2014 || timer: 0.0919 sec.
iter 177490 || Loss: 0.9035 || timer: 0.0861 sec.
iter 177500 || Loss: 1.3525 || timer: 0.0836 sec.
iter 177510 || Loss: 1.2418 || timer: 0.0928 sec.
iter 177520 || Loss: 0.6824 || timer: 0.0846 sec.
iter 177530 || Loss: 1.1445 || timer: 0.0969 sec.
iter 177540 || Loss: 0.7677 || timer: 0.0844 sec.
iter 177550 || Loss: 1.0522 || timer: 0.0856 sec.
iter 177560 || Loss: 1.2547 || timer: 0.0805 sec.
iter 177570 || Loss: 1.0275 || timer: 0.0784 sec.
iter 177580 || Loss: 0.7416 || timer: 0.0980 sec.
iter 177590 || Loss: 0.7232 || timer: 0.0815 sec.
iter 177600 || Loss: 0.9981 || timer: 0.0911 sec.
iter 177610 || Loss: 0.7583 || timer: 0.0875 sec.
iter 177620 || Loss: 0.8695 || timer: 0.0754 sec.
iter 177630 || Loss: 1.2860 || timer: 0.0844 sec.
iter 177640 || Loss: 0.8283 || timer: 0.0835 sec.
iter 177650 || Loss: 0.8433 || timer: 0.0913 sec.
iter 177660 || Loss: 0.6614 || timer: 0.0908 sec.
iter 177670 || Loss: 1.5792 || timer: 0.0952 sec.
iter 177680 || Loss: 0.8681 || timer: 0.1073 sec.
iter 177690 || Loss: 1.1181 || timer: 0.0841 sec.
iter 177700 || Loss: 0.9365 || timer: 0.1059 sec.
iter 177710 || Loss: 0.6591 || timer: 0.0983 sec.
iter 177720 || Loss: 0.9385 || timer: 0.0881 sec.
iter 177730 || Loss: 1.0934 || timer: 0.0849 sec.
iter 177740 || Loss: 1.0134 || timer: 0.0844 sec.
iter 177750 || Loss: 0.9249 || timer: 0.0861 sec.
iter 177760 || Loss: 0.7494 || timer: 0.0212 sec.
iter 177770 || Loss: 0.1750 || timer: 0.0857 sec.
iter 177780 || Loss: 1.2269 || timer: 0.0925 sec.
iter 177790 || Loss: 0.8323 || timer: 0.0910 sec.
iter 177800 || Loss: 0.8807 || timer: 0.0938 sec.
iter 177810 || Loss: 0.6801 || timer: 0.0912 sec.
iter 177820 || Loss: 0.9365 || timer: 0.0937 sec.
iter 177830 || Loss: 0.9153 || timer: 0.0916 sec.
iter 177840 || Loss: 0.6503 || timer: 0.1063 sec.
iter 177850 || Loss: 1.0716 || timer: 0.0916 sec.
iter 177860 || Loss: 1.0292 || timer: 0.0973 sec.
iter 177870 || Loss: 1.3278 || timer: 0.0901 sec.
iter 177880 || Loss: 1.3450 || timer: 0.0837 sec.
iter 177890 || Loss: 0.7935 || timer: 0.0883 sec.
iter 177900 || Loss: 0.7090 || timer: 0.0993 sec.
iter 177910 || Loss: 1.0556 || timer: 0.0915 sec.
iter 177920 || Loss: 0.9540 || timer: 0.0932 sec.
iter 177930 || Loss: 0.8091 || timer: 0.0905 sec.
iter 177940 || Loss: 1.0215 || timer: 0.0903 sec.
iter 177950 || Loss: 0.7372 || timer: 0.0967 sec.
iter 177960 || Loss: 0.8993 || timer: 0.1142 sec.
iter 177970 || Loss: 0.7436 || timer: 0.0945 sec.
iter 177980 || Loss: 0.8150 || timer: 0.0841 sec.
iter 177990 || Loss: 0.8947 || timer: 0.0892 sec.
iter 178000 || Loss: 0.6603 || timer: 0.0998 sec.
iter 178010 || Loss: 0.9442 || timer: 0.0918 sec.
iter 178020 || Loss: 0.9590 || timer: 0.0881 sec.
iter 178030 || Loss: 1.1040 || timer: 0.0886 sec.
iter 178040 || Loss: 1.1628 || timer: 0.0897 sec.
iter 178050 || Loss: 0.9192 || timer: 0.0912 sec.
iter 178060 || Loss: 0.9490 || timer: 0.0855 sec.
iter 178070 || Loss: 1.1988 || timer: 0.0919 sec.
iter 178080 || Loss: 1.1702 || timer: 0.0905 sec.
iter 178090 || Loss: 1.1844 || timer: 0.0234 sec.
iter 178100 || Loss: 0.5539 || timer: 0.0920 sec.
iter 178110 || Loss: 0.9804 || timer: 0.0978 sec.
iter 178120 || Loss: 1.0295 || timer: 0.0880 sec.
iter 178130 || Loss: 0.8034 || timer: 0.0917 sec.
iter 178140 || Loss: 0.7924 || timer: 0.0937 sec.
iter 178150 || Loss: 0.6991 || timer: 0.0910 sec.
iter 178160 || Loss: 0.9295 || timer: 0.0852 sec.
iter 178170 || Loss: 0.9922 || timer: 0.0822 sec.
iter 178180 || Loss: 1.0773 || timer: 0.1162 sec.
iter 178190 || Loss: 1.0643 || timer: 0.0974 sec.
iter 178200 || Loss: 0.9712 || timer: 0.0911 sec.
iter 178210 || Loss: 0.8232 || timer: 0.0901 sec.
iter 178220 || Loss: 0.8148 || timer: 0.0760 sec.
iter 178230 || Loss: 0.9251 || timer: 0.1016 sec.
iter 178240 || Loss: 0.9299 || timer: 0.0842 sec.
iter 178250 || Loss: 0.7417 || timer: 0.0846 sec.
iter 178260 || Loss: 1.2809 || timer: 0.0913 sec.
iter 178270 || Loss: 0.8812 || timer: 0.0841 sec.
iter 178280 || Loss: 1.0522 || timer: 0.0883 sec.
iter 178290 || Loss: 0.6828 || timer: 0.0938 sec.
iter 178300 || Loss: 0.8621 || timer: 0.0920 sec.
iter 178310 || Loss: 0.6618 || timer: 0.0843 sec.
iter 178320 || Loss: 1.1549 || timer: 0.0914 sec.
iter 178330 || Loss: 0.9966 || timer: 0.0933 sec.
iter 178340 || Loss: 0.9264 || timer: 0.0903 sec.
iter 178350 || Loss: 1.0245 || timer: 0.0933 sec.
iter 178360 || Loss: 0.7497 || timer: 0.1009 sec.
iter 178370 || Loss: 0.7662 || timer: 0.0935 sec.
iter 178380 || Loss: 0.8660 || timer: 0.0915 sec.
iter 178390 || Loss: 1.1632 || timer: 0.0921 sec.
iter 178400 || Loss: 0.8303 || timer: 0.0838 sec.
iter 178410 || Loss: 0.7009 || timer: 0.0932 sec.
iter 178420 || Loss: 0.8354 || timer: 0.0268 sec.
iter 178430 || Loss: 2.6991 || timer: 0.0924 sec.
iter 178440 || Loss: 1.5590 || timer: 0.0907 sec.
iter 178450 || Loss: 1.1702 || timer: 0.1029 sec.
iter 178460 || Loss: 1.0945 || timer: 0.0902 sec.
iter 178470 || Loss: 0.9940 || timer: 0.0864 sec.
iter 178480 || Loss: 1.2300 || timer: 0.0915 sec.
iter 178490 || Loss: 0.9204 || timer: 0.0806 sec.
iter 178500 || Loss: 1.0440 || timer: 0.1059 sec.
iter 178510 || Loss: 1.0588 || timer: 0.0848 sec.
iter 178520 || Loss: 1.0421 || timer: 0.0887 sec.
iter 178530 || Loss: 0.9497 || timer: 0.0789 sec.
iter 178540 || Loss: 1.4715 || timer: 0.0843 sec.
iter 178550 || Loss: 0.7668 || timer: 0.0958 sec.
iter 178560 || Loss: 0.7013 || timer: 0.1034 sec.
iter 178570 || Loss: 1.1693 || timer: 0.0833 sec.
iter 178580 || Loss: 1.1321 || timer: 0.0914 sec.
iter 178590 || Loss: 0.8846 || timer: 0.0880 sec.
iter 178600 || Loss: 0.9675 || timer: 0.0861 sec.
iter 178610 || Loss: 0.7289 || timer: 0.0918 sec.
iter 178620 || Loss: 0.7740 || timer: 0.0839 sec.
iter 178630 || Loss: 0.7439 || timer: 0.0843 sec.
iter 178640 || Loss: 0.9231 || timer: 0.0990 sec.
iter 178650 || Loss: 0.9502 || timer: 0.0837 sec.
iter 178660 || Loss: 1.2332 || timer: 0.0837 sec.
iter 178670 || Loss: 1.3351 || timer: 0.1128 sec.
iter 178680 || Loss: 0.9271 || timer: 0.0810 sec.
iter 178690 || Loss: 0.9645 || timer: 0.0867 sec.
iter 178700 || Loss: 0.8141 || timer: 0.0913 sec.
iter 178710 || Loss: 0.7883 || timer: 0.0867 sec.
iter 178720 || Loss: 0.8067 || timer: 0.1091 sec.
iter 178730 || Loss: 0.7508 || timer: 0.0907 sec.
iter 178740 || Loss: 0.9389 || timer: 0.0928 sec.
iter 178750 || Loss: 0.9140 || timer: 0.0177 sec.
iter 178760 || Loss: 0.2184 || timer: 0.0918 sec.
iter 178770 || Loss: 1.1008 || timer: 0.1065 sec.
iter 178780 || Loss: 0.9812 || timer: 0.0917 sec.
iter 178790 || Loss: 0.8780 || timer: 0.0830 sec.
iter 178800 || Loss: 1.0864 || timer: 0.0885 sec.
iter 178810 || Loss: 0.9437 || timer: 0.0833 sec.
iter 178820 || Loss: 1.0483 || timer: 0.0890 sec.
iter 178830 || Loss: 0.7000 || timer: 0.0964 sec.
iter 178840 || Loss: 1.1089 || timer: 0.1017 sec.
iter 178850 || Loss: 1.3921 || timer: 0.0986 sec.
iter 178860 || Loss: 0.7449 || timer: 0.0953 sec.
iter 178870 || Loss: 0.7141 || timer: 0.0836 sec.
iter 178880 || Loss: 1.1072 || timer: 0.0754 sec.
iter 178890 || Loss: 0.9791 || timer: 0.0910 sec.
iter 178900 || Loss: 1.1589 || timer: 0.0969 sec.
iter 178910 || Loss: 0.7679 || timer: 0.0837 sec.
iter 178920 || Loss: 0.9164 || timer: 0.0826 sec.
iter 178930 || Loss: 0.8504 || timer: 0.0835 sec.
iter 178940 || Loss: 0.7406 || timer: 0.0923 sec.
iter 178950 || Loss: 1.0308 || timer: 0.0843 sec.
iter 178960 || Loss: 0.8873 || timer: 0.1061 sec.
iter 178970 || Loss: 0.9881 || timer: 0.0961 sec.
iter 178980 || Loss: 0.7930 || timer: 0.0833 sec.
iter 178990 || Loss: 0.8487 || timer: 0.0932 sec.
iter 179000 || Loss: 1.0853 || timer: 0.0836 sec.
iter 179010 || Loss: 1.0801 || timer: 0.0875 sec.
iter 179020 || Loss: 0.9042 || timer: 0.0857 sec.
iter 179030 || Loss: 0.8691 || timer: 0.0843 sec.
iter 179040 || Loss: 0.8524 || timer: 0.0916 sec.
iter 179050 || Loss: 0.9591 || timer: 0.0941 sec.
iter 179060 || Loss: 0.6661 || timer: 0.0896 sec.
iter 179070 || Loss: 0.9787 || timer: 0.0910 sec.
iter 179080 || Loss: 0.7783 || timer: 0.0206 sec.
iter 179090 || Loss: 0.5698 || timer: 0.0836 sec.
iter 179100 || Loss: 0.9638 || timer: 0.0899 sec.
iter 179110 || Loss: 0.9422 || timer: 0.0844 sec.
iter 179120 || Loss: 0.9579 || timer: 0.0855 sec.
iter 179130 || Loss: 0.6979 || timer: 0.0855 sec.
iter 179140 || Loss: 1.5895 || timer: 0.0843 sec.
iter 179150 || Loss: 0.9629 || timer: 0.0823 sec.
iter 179160 || Loss: 0.8177 || timer: 0.0910 sec.
iter 179170 || Loss: 0.8649 || timer: 0.1032 sec.
iter 179180 || Loss: 0.7978 || timer: 0.1128 sec.
iter 179190 || Loss: 0.7586 || timer: 0.0845 sec.
iter 179200 || Loss: 0.8638 || timer: 0.1094 sec.
iter 179210 || Loss: 1.4395 || timer: 0.0890 sec.
iter 179220 || Loss: 0.7353 || timer: 0.0841 sec.
iter 179230 || Loss: 0.8054 || timer: 0.0949 sec.
iter 179240 || Loss: 0.4518 || timer: 0.1090 sec.
iter 179250 || Loss: 0.7724 || timer: 0.1042 sec.
iter 179260 || Loss: 0.6952 || timer: 0.0910 sec.
iter 179270 || Loss: 0.7349 || timer: 0.0843 sec.
iter 179280 || Loss: 1.1945 || timer: 0.1064 sec.
iter 179290 || Loss: 0.9170 || timer: 0.0916 sec.
iter 179300 || Loss: 0.9213 || timer: 0.1155 sec.
iter 179310 || Loss: 1.2447 || timer: 0.0894 sec.
iter 179320 || Loss: 0.7412 || timer: 0.0910 sec.
iter 179330 || Loss: 1.1924 || timer: 0.1123 sec.
iter 179340 || Loss: 0.9953 || timer: 0.1050 sec.
iter 179350 || Loss: 0.7876 || timer: 0.1078 sec.
iter 179360 || Loss: 1.2818 || timer: 0.0913 sec.
iter 179370 || Loss: 0.6876 || timer: 0.0915 sec.
iter 179380 || Loss: 0.9476 || timer: 0.1004 sec.
iter 179390 || Loss: 1.1732 || timer: 0.0907 sec.
iter 179400 || Loss: 1.1393 || timer: 0.1014 sec.
iter 179410 || Loss: 1.2618 || timer: 0.0183 sec.
iter 179420 || Loss: 2.5913 || timer: 0.0881 sec.
iter 179430 || Loss: 1.1152 || timer: 0.0902 sec.
iter 179440 || Loss: 0.9107 || timer: 0.0998 sec.
iter 179450 || Loss: 0.8168 || timer: 0.0841 sec.
iter 179460 || Loss: 0.8487 || timer: 0.1093 sec.
iter 179470 || Loss: 0.9109 || timer: 0.0815 sec.
iter 179480 || Loss: 1.1094 || timer: 0.1007 sec.
iter 179490 || Loss: 1.1392 || timer: 0.1116 sec.
iter 179500 || Loss: 0.8475 || timer: 0.0904 sec.
iter 179510 || Loss: 0.7959 || timer: 0.0971 sec.
iter 179520 || Loss: 1.1058 || timer: 0.0925 sec.
iter 179530 || Loss: 1.1224 || timer: 0.0826 sec.
iter 179540 || Loss: 1.1120 || timer: 0.0829 sec.
iter 179550 || Loss: 0.8586 || timer: 0.0910 sec.
iter 179560 || Loss: 0.9503 || timer: 0.0852 sec.
iter 179570 || Loss: 1.3837 || timer: 0.0914 sec.
iter 179580 || Loss: 0.8337 || timer: 0.1030 sec.
iter 179590 || Loss: 1.0325 || timer: 0.0888 sec.
iter 179600 || Loss: 1.1159 || timer: 0.1041 sec.
iter 179610 || Loss: 1.5989 || timer: 0.0841 sec.
iter 179620 || Loss: 0.9275 || timer: 0.0891 sec.
iter 179630 || Loss: 0.7474 || timer: 0.0896 sec.
iter 179640 || Loss: 1.2499 || timer: 0.0825 sec.
iter 179650 || Loss: 1.2619 || timer: 0.0916 sec.
iter 179660 || Loss: 0.6711 || timer: 0.1135 sec.
iter 179670 || Loss: 0.7371 || timer: 0.1051 sec.
iter 179680 || Loss: 0.8799 || timer: 0.0884 sec.
iter 179690 || Loss: 0.9806 || timer: 0.1074 sec.
iter 179700 || Loss: 0.7427 || timer: 0.0826 sec.
iter 179710 || Loss: 0.9492 || timer: 0.0889 sec.
iter 179720 || Loss: 1.3280 || timer: 0.0880 sec.
iter 179730 || Loss: 1.1683 || timer: 0.1100 sec.
iter 179740 || Loss: 1.0690 || timer: 0.0259 sec.
iter 179750 || Loss: 0.4336 || timer: 0.0858 sec.
iter 179760 || Loss: 0.6291 || timer: 0.0796 sec.
iter 179770 || Loss: 0.9544 || timer: 0.0893 sec.
iter 179780 || Loss: 0.8002 || timer: 0.0946 sec.
iter 179790 || Loss: 0.8311 || timer: 0.0906 sec.
iter 179800 || Loss: 1.1280 || timer: 0.0859 sec.
iter 179810 || Loss: 0.9853 || timer: 0.0902 sec.
iter 179820 || Loss: 1.0061 || timer: 0.0824 sec.
iter 179830 || Loss: 0.9490 || timer: 0.1107 sec.
iter 179840 || Loss: 0.8719 || timer: 0.1270 sec.
iter 179850 || Loss: 0.7290 || timer: 0.0939 sec.
iter 179860 || Loss: 1.2577 || timer: 0.0752 sec.
iter 179870 || Loss: 1.0593 || timer: 0.0897 sec.
iter 179880 || Loss: 0.7894 || timer: 0.1014 sec.
iter 179890 || Loss: 1.1197 || timer: 0.0867 sec.
iter 179900 || Loss: 0.7768 || timer: 0.0821 sec.
iter 179910 || Loss: 0.8641 || timer: 0.0829 sec.
iter 179920 || Loss: 1.1664 || timer: 0.0822 sec.
iter 179930 || Loss: 0.9579 || timer: 0.1043 sec.
iter 179940 || Loss: 0.8581 || timer: 0.0921 sec.
iter 179950 || Loss: 0.7913 || timer: 0.0906 sec.
iter 179960 || Loss: 0.9886 || timer: 0.0825 sec.
iter 179970 || Loss: 0.7676 || timer: 0.0879 sec.
iter 179980 || Loss: 0.9723 || timer: 0.0821 sec.
iter 179990 || Loss: 1.0776 || timer: 0.0884 sec.
iter 180000 || Loss: 0.9808 || Saving state, iter: 180000
timer: 0.1097 sec.
iter 180010 || Loss: 0.7881 || timer: 0.0919 sec.
iter 180020 || Loss: 1.5793 || timer: 0.0919 sec.
iter 180030 || Loss: 1.1286 || timer: 0.0843 sec.
iter 180040 || Loss: 0.9290 || timer: 0.0887 sec.
iter 180050 || Loss: 0.8048 || timer: 0.0889 sec.
iter 180060 || Loss: 0.9814 || timer: 0.0819 sec.
iter 180070 || Loss: 1.1851 || timer: 0.0235 sec.
iter 180080 || Loss: 0.6637 || timer: 0.0924 sec.
iter 180090 || Loss: 0.8636 || timer: 0.0915 sec.
iter 180100 || Loss: 1.0060 || timer: 0.0759 sec.
iter 180110 || Loss: 0.9423 || timer: 0.0875 sec.
iter 180120 || Loss: 1.4828 || timer: 0.0882 sec.
iter 180130 || Loss: 0.9773 || timer: 0.1241 sec.
iter 180140 || Loss: 0.9403 || timer: 0.0961 sec.
iter 180150 || Loss: 1.1130 || timer: 0.0860 sec.
iter 180160 || Loss: 0.8942 || timer: 0.0911 sec.
iter 180170 || Loss: 0.8450 || timer: 0.1154 sec.
iter 180180 || Loss: 0.9931 || timer: 0.0806 sec.
iter 180190 || Loss: 1.0544 || timer: 0.0890 sec.
iter 180200 || Loss: 0.9869 || timer: 0.0789 sec.
iter 180210 || Loss: 0.9941 || timer: 0.0828 sec.
iter 180220 || Loss: 1.0105 || timer: 0.0834 sec.
iter 180230 || Loss: 0.9542 || timer: 0.0828 sec.
iter 180240 || Loss: 0.6626 || timer: 0.0888 sec.
iter 180250 || Loss: 0.9139 || timer: 0.0762 sec.
iter 180260 || Loss: 0.7878 || timer: 0.0738 sec.
iter 180270 || Loss: 0.8084 || timer: 0.0935 sec.
iter 180280 || Loss: 0.7500 || timer: 0.0916 sec.
iter 180290 || Loss: 0.5640 || timer: 0.0942 sec.
iter 180300 || Loss: 0.7783 || timer: 0.0917 sec.
iter 180310 || Loss: 0.8072 || timer: 0.0899 sec.
iter 180320 || Loss: 1.1613 || timer: 0.0886 sec.
iter 180330 || Loss: 1.1657 || timer: 0.0914 sec.
iter 180340 || Loss: 0.9883 || timer: 0.0887 sec.
iter 180350 || Loss: 1.5392 || timer: 0.0847 sec.
iter 180360 || Loss: 1.2301 || timer: 0.0806 sec.
iter 180370 || Loss: 0.9730 || timer: 0.0847 sec.
iter 180380 || Loss: 1.2505 || timer: 0.0899 sec.
iter 180390 || Loss: 1.0705 || timer: 0.0895 sec.
iter 180400 || Loss: 0.8815 || timer: 0.0177 sec.
iter 180410 || Loss: 2.2039 || timer: 0.0874 sec.
iter 180420 || Loss: 0.8646 || timer: 0.0911 sec.
iter 180430 || Loss: 1.1772 || timer: 0.0904 sec.
iter 180440 || Loss: 1.3857 || timer: 0.0894 sec.
iter 180450 || Loss: 0.7567 || timer: 0.1116 sec.
iter 180460 || Loss: 0.9400 || timer: 0.0811 sec.
iter 180470 || Loss: 1.1132 || timer: 0.0928 sec.
iter 180480 || Loss: 1.0707 || timer: 0.1150 sec.
iter 180490 || Loss: 0.8400 || timer: 0.0852 sec.
iter 180500 || Loss: 0.8963 || timer: 0.1200 sec.
iter 180510 || Loss: 0.6931 || timer: 0.1066 sec.
iter 180520 || Loss: 1.0396 || timer: 0.1121 sec.
iter 180530 || Loss: 0.7533 || timer: 0.0837 sec.
iter 180540 || Loss: 0.6873 || timer: 0.0992 sec.
iter 180550 || Loss: 0.9464 || timer: 0.0868 sec.
iter 180560 || Loss: 0.9092 || timer: 0.0855 sec.
iter 180570 || Loss: 0.8738 || timer: 0.0905 sec.
iter 180580 || Loss: 0.7625 || timer: 0.1038 sec.
iter 180590 || Loss: 0.9866 || timer: 0.1087 sec.
iter 180600 || Loss: 0.9287 || timer: 0.0909 sec.
iter 180610 || Loss: 1.0919 || timer: 0.0828 sec.
iter 180620 || Loss: 0.8254 || timer: 0.0913 sec.
iter 180630 || Loss: 0.7351 || timer: 0.0836 sec.
iter 180640 || Loss: 0.9513 || timer: 0.0966 sec.
iter 180650 || Loss: 1.1664 || timer: 0.1011 sec.
iter 180660 || Loss: 1.0074 || timer: 0.0891 sec.
iter 180670 || Loss: 0.9808 || timer: 0.0829 sec.
iter 180680 || Loss: 1.6595 || timer: 0.0908 sec.
iter 180690 || Loss: 0.9573 || timer: 0.0914 sec.
iter 180700 || Loss: 1.3024 || timer: 0.1088 sec.
iter 180710 || Loss: 1.2815 || timer: 0.1068 sec.
iter 180720 || Loss: 0.9980 || timer: 0.0918 sec.
iter 180730 || Loss: 0.7840 || timer: 0.0262 sec.
iter 180740 || Loss: 1.0329 || timer: 0.0860 sec.
iter 180750 || Loss: 1.1459 || timer: 0.1045 sec.
iter 180760 || Loss: 0.7953 || timer: 0.0919 sec.
iter 180770 || Loss: 0.7871 || timer: 0.0848 sec.
iter 180780 || Loss: 0.7319 || timer: 0.0836 sec.
iter 180790 || Loss: 0.7595 || timer: 0.0838 sec.
iter 180800 || Loss: 0.9337 || timer: 0.0933 sec.
iter 180810 || Loss: 0.7038 || timer: 0.1065 sec.
iter 180820 || Loss: 0.8274 || timer: 0.0858 sec.
iter 180830 || Loss: 1.1264 || timer: 0.1053 sec.
iter 180840 || Loss: 0.9760 || timer: 0.1028 sec.
iter 180850 || Loss: 0.9473 || timer: 0.0946 sec.
iter 180860 || Loss: 0.9688 || timer: 0.0892 sec.
iter 180870 || Loss: 1.0718 || timer: 0.1027 sec.
iter 180880 || Loss: 0.9378 || timer: 0.1059 sec.
iter 180890 || Loss: 1.0575 || timer: 0.0836 sec.
iter 180900 || Loss: 0.8978 || timer: 0.1109 sec.
iter 180910 || Loss: 0.9567 || timer: 0.1084 sec.
iter 180920 || Loss: 1.2557 || timer: 0.0834 sec.
iter 180930 || Loss: 0.7813 || timer: 0.0931 sec.
iter 180940 || Loss: 0.8429 || timer: 0.0917 sec.
iter 180950 || Loss: 1.0601 || timer: 0.0911 sec.
iter 180960 || Loss: 0.8733 || timer: 0.0896 sec.
iter 180970 || Loss: 1.1346 || timer: 0.0926 sec.
iter 180980 || Loss: 1.1829 || timer: 0.1145 sec.
iter 180990 || Loss: 1.0801 || timer: 0.0915 sec.
iter 181000 || Loss: 0.9042 || timer: 0.0867 sec.
iter 181010 || Loss: 1.3280 || timer: 0.0946 sec.
iter 181020 || Loss: 0.9277 || timer: 0.0930 sec.
iter 181030 || Loss: 1.1688 || timer: 0.0807 sec.
iter 181040 || Loss: 0.7206 || timer: 0.1033 sec.
iter 181050 || Loss: 0.5998 || timer: 0.1041 sec.
iter 181060 || Loss: 0.8209 || timer: 0.0173 sec.
iter 181070 || Loss: 0.4977 || timer: 0.0847 sec.
iter 181080 || Loss: 0.6312 || timer: 0.0772 sec.
iter 181090 || Loss: 1.0368 || timer: 0.0829 sec.
iter 181100 || Loss: 0.9026 || timer: 0.0942 sec.
iter 181110 || Loss: 1.3703 || timer: 0.0912 sec.
iter 181120 || Loss: 0.6484 || timer: 0.0882 sec.
iter 181130 || Loss: 1.1751 || timer: 0.0886 sec.
iter 181140 || Loss: 0.9214 || timer: 0.0876 sec.
iter 181150 || Loss: 0.8571 || timer: 0.0912 sec.
iter 181160 || Loss: 1.0883 || timer: 0.1202 sec.
iter 181170 || Loss: 0.8981 || timer: 0.0914 sec.
iter 181180 || Loss: 1.7832 || timer: 0.0912 sec.
iter 181190 || Loss: 1.0776 || timer: 0.0840 sec.
iter 181200 || Loss: 1.4326 || timer: 0.0912 sec.
iter 181210 || Loss: 0.6402 || timer: 0.0945 sec.
iter 181220 || Loss: 1.1562 || timer: 0.0841 sec.
iter 181230 || Loss: 1.1098 || timer: 0.0897 sec.
iter 181240 || Loss: 1.0590 || timer: 0.0853 sec.
iter 181250 || Loss: 0.9499 || timer: 0.0920 sec.
iter 181260 || Loss: 0.7970 || timer: 0.0922 sec.
iter 181270 || Loss: 0.5409 || timer: 0.0926 sec.
iter 181280 || Loss: 1.0803 || timer: 0.0844 sec.
iter 181290 || Loss: 0.8208 || timer: 0.0821 sec.
iter 181300 || Loss: 0.9400 || timer: 0.0844 sec.
iter 181310 || Loss: 1.4149 || timer: 0.0838 sec.
iter 181320 || Loss: 1.2667 || timer: 0.0924 sec.
iter 181330 || Loss: 0.6582 || timer: 0.0832 sec.
iter 181340 || Loss: 1.1234 || timer: 0.0847 sec.
iter 181350 || Loss: 0.9949 || timer: 0.1001 sec.
iter 181360 || Loss: 1.4097 || timer: 0.0919 sec.
iter 181370 || Loss: 0.8938 || timer: 0.0918 sec.
iter 181380 || Loss: 0.7533 || timer: 0.0832 sec.
iter 181390 || Loss: 0.8140 || timer: 0.0220 sec.
iter 181400 || Loss: 0.9057 || timer: 0.0928 sec.
iter 181410 || Loss: 1.1689 || timer: 0.0917 sec.
iter 181420 || Loss: 1.0815 || timer: 0.0904 sec.
iter 181430 || Loss: 0.9499 || timer: 0.0890 sec.
iter 181440 || Loss: 0.8820 || timer: 0.0831 sec.
iter 181450 || Loss: 1.1163 || timer: 0.0888 sec.
iter 181460 || Loss: 0.8699 || timer: 0.0827 sec.
iter 181470 || Loss: 0.7647 || timer: 0.0895 sec.
iter 181480 || Loss: 1.2351 || timer: 0.0834 sec.
iter 181490 || Loss: 0.9077 || timer: 0.0961 sec.
iter 181500 || Loss: 0.7118 || timer: 0.0894 sec.
iter 181510 || Loss: 1.0173 || timer: 0.0826 sec.
iter 181520 || Loss: 0.9887 || timer: 0.0817 sec.
iter 181530 || Loss: 1.1718 || timer: 0.0902 sec.
iter 181540 || Loss: 1.0889 || timer: 0.0919 sec.
iter 181550 || Loss: 0.7527 || timer: 0.0928 sec.
iter 181560 || Loss: 0.7203 || timer: 0.0835 sec.
iter 181570 || Loss: 1.1067 || timer: 0.0901 sec.
iter 181580 || Loss: 1.1415 || timer: 0.1017 sec.
iter 181590 || Loss: 0.8760 || timer: 0.0903 sec.
iter 181600 || Loss: 0.9662 || timer: 0.0893 sec.
iter 181610 || Loss: 1.0837 || timer: 0.0895 sec.
iter 181620 || Loss: 0.9103 || timer: 0.0850 sec.
iter 181630 || Loss: 0.6817 || timer: 0.1015 sec.
iter 181640 || Loss: 0.8866 || timer: 0.0938 sec.
iter 181650 || Loss: 0.8994 || timer: 0.0915 sec.
iter 181660 || Loss: 1.0205 || timer: 0.0819 sec.
iter 181670 || Loss: 0.7483 || timer: 0.0894 sec.
iter 181680 || Loss: 1.0477 || timer: 0.0910 sec.
iter 181690 || Loss: 1.1911 || timer: 0.1236 sec.
iter 181700 || Loss: 1.1171 || timer: 0.0894 sec.
iter 181710 || Loss: 0.8533 || timer: 0.0833 sec.
iter 181720 || Loss: 0.9572 || timer: 0.0216 sec.
iter 181730 || Loss: 0.1089 || timer: 0.1016 sec.
iter 181740 || Loss: 0.8182 || timer: 0.0930 sec.
iter 181750 || Loss: 1.0869 || timer: 0.0955 sec.
iter 181760 || Loss: 0.8736 || timer: 0.0901 sec.
iter 181770 || Loss: 0.9733 || timer: 0.0875 sec.
iter 181780 || Loss: 0.9678 || timer: 0.0915 sec.
iter 181790 || Loss: 0.7008 || timer: 0.0770 sec.
iter 181800 || Loss: 0.6757 || timer: 0.0774 sec.
iter 181810 || Loss: 0.7703 || timer: 0.0849 sec.
iter 181820 || Loss: 0.8644 || timer: 0.1010 sec.
iter 181830 || Loss: 1.2521 || timer: 0.0842 sec.
iter 181840 || Loss: 2.1688 || timer: 0.0869 sec.
iter 181850 || Loss: 1.3927 || timer: 0.0900 sec.
iter 181860 || Loss: 1.0878 || timer: 0.0895 sec.
iter 181870 || Loss: 1.7001 || timer: 0.0832 sec.
iter 181880 || Loss: 1.1681 || timer: 0.0870 sec.
iter 181890 || Loss: 1.0867 || timer: 0.0839 sec.
iter 181900 || Loss: 0.9597 || timer: 0.0859 sec.
iter 181910 || Loss: 0.8872 || timer: 0.0812 sec.
iter 181920 || Loss: 0.7205 || timer: 0.0846 sec.
iter 181930 || Loss: 1.0005 || timer: 0.0887 sec.
iter 181940 || Loss: 1.2489 || timer: 0.1063 sec.
iter 181950 || Loss: 0.8408 || timer: 0.0882 sec.
iter 181960 || Loss: 0.8039 || timer: 0.0910 sec.
iter 181970 || Loss: 1.2551 || timer: 0.0894 sec.
iter 181980 || Loss: 0.9611 || timer: 0.0881 sec.
iter 181990 || Loss: 1.2444 || timer: 0.0897 sec.
iter 182000 || Loss: 0.9607 || timer: 0.0897 sec.
iter 182010 || Loss: 1.0190 || timer: 0.0917 sec.
iter 182020 || Loss: 0.9127 || timer: 0.0926 sec.
iter 182030 || Loss: 0.8482 || timer: 0.0913 sec.
iter 182040 || Loss: 0.8822 || timer: 0.0866 sec.
iter 182050 || Loss: 0.9706 || timer: 0.0203 sec.
iter 182060 || Loss: 0.2447 || timer: 0.0888 sec.
iter 182070 || Loss: 1.0919 || timer: 0.1054 sec.
iter 182080 || Loss: 1.2735 || timer: 0.0979 sec.
iter 182090 || Loss: 0.7098 || timer: 0.1132 sec.
iter 182100 || Loss: 0.9584 || timer: 0.0914 sec.
iter 182110 || Loss: 1.1802 || timer: 0.0908 sec.
iter 182120 || Loss: 1.2570 || timer: 0.0849 sec.
iter 182130 || Loss: 0.9098 || timer: 0.0923 sec.
iter 182140 || Loss: 0.8776 || timer: 0.0837 sec.
iter 182150 || Loss: 0.7112 || timer: 0.1184 sec.
iter 182160 || Loss: 1.1173 || timer: 0.0842 sec.
iter 182170 || Loss: 0.6530 || timer: 0.0881 sec.
iter 182180 || Loss: 1.0797 || timer: 0.0890 sec.
iter 182190 || Loss: 0.7810 || timer: 0.0891 sec.
iter 182200 || Loss: 0.9146 || timer: 0.0881 sec.
iter 182210 || Loss: 1.1287 || timer: 0.0905 sec.
iter 182220 || Loss: 0.9047 || timer: 0.0926 sec.
iter 182230 || Loss: 1.2274 || timer: 0.1024 sec.
iter 182240 || Loss: 0.8152 || timer: 0.0865 sec.
iter 182250 || Loss: 1.0721 || timer: 0.0843 sec.
iter 182260 || Loss: 0.7542 || timer: 0.0914 sec.
iter 182270 || Loss: 1.0904 || timer: 0.1159 sec.
iter 182280 || Loss: 1.0719 || timer: 0.0917 sec.
iter 182290 || Loss: 0.8395 || timer: 0.0932 sec.
iter 182300 || Loss: 1.0268 || timer: 0.0934 sec.
iter 182310 || Loss: 0.8327 || timer: 0.0896 sec.
iter 182320 || Loss: 1.0647 || timer: 0.0916 sec.
iter 182330 || Loss: 1.0149 || timer: 0.0855 sec.
iter 182340 || Loss: 1.1120 || timer: 0.0775 sec.
iter 182350 || Loss: 0.9390 || timer: 0.0813 sec.
iter 182360 || Loss: 0.8002 || timer: 0.0826 sec.
iter 182370 || Loss: 0.7660 || timer: 0.0792 sec.
iter 182380 || Loss: 0.9137 || timer: 0.0217 sec.
iter 182390 || Loss: 0.2526 || timer: 0.0917 sec.
iter 182400 || Loss: 0.8946 || timer: 0.0894 sec.
iter 182410 || Loss: 0.9370 || timer: 0.0825 sec.
iter 182420 || Loss: 0.7993 || timer: 0.0770 sec.
iter 182430 || Loss: 0.7975 || timer: 0.0942 sec.
iter 182440 || Loss: 1.0668 || timer: 0.0838 sec.
iter 182450 || Loss: 1.2433 || timer: 0.0928 sec.
iter 182460 || Loss: 1.0118 || timer: 0.0842 sec.
iter 182470 || Loss: 0.7035 || timer: 0.0877 sec.
iter 182480 || Loss: 0.8469 || timer: 0.1115 sec.
iter 182490 || Loss: 1.3744 || timer: 0.1034 sec.
iter 182500 || Loss: 1.1339 || timer: 0.0920 sec.
iter 182510 || Loss: 1.2650 || timer: 0.0901 sec.
iter 182520 || Loss: 1.0112 || timer: 0.0834 sec.
iter 182530 || Loss: 1.1724 || timer: 0.0845 sec.
iter 182540 || Loss: 1.0268 || timer: 0.0814 sec.
iter 182550 || Loss: 1.4794 || timer: 0.0912 sec.
iter 182560 || Loss: 0.8576 || timer: 0.0924 sec.
iter 182570 || Loss: 1.1136 || timer: 0.0863 sec.
iter 182580 || Loss: 1.0314 || timer: 0.0837 sec.
iter 182590 || Loss: 0.7808 || timer: 0.0828 sec.
iter 182600 || Loss: 0.9970 || timer: 0.0838 sec.
iter 182610 || Loss: 0.7100 || timer: 0.0838 sec.
iter 182620 || Loss: 1.0220 || timer: 0.1025 sec.
iter 182630 || Loss: 0.8946 || timer: 0.0910 sec.
iter 182640 || Loss: 0.8600 || timer: 0.1058 sec.
iter 182650 || Loss: 0.7848 || timer: 0.0995 sec.
iter 182660 || Loss: 1.0581 || timer: 0.1053 sec.
iter 182670 || Loss: 1.0290 || timer: 0.0904 sec.
iter 182680 || Loss: 1.2031 || timer: 0.0777 sec.
iter 182690 || Loss: 0.9250 || timer: 0.1071 sec.
iter 182700 || Loss: 0.9754 || timer: 0.0842 sec.
iter 182710 || Loss: 1.2655 || timer: 0.0150 sec.
iter 182720 || Loss: 0.4766 || timer: 0.0918 sec.
iter 182730 || Loss: 1.1970 || timer: 0.0960 sec.
iter 182740 || Loss: 0.9413 || timer: 0.0829 sec.
iter 182750 || Loss: 0.8955 || timer: 0.0924 sec.
iter 182760 || Loss: 0.7011 || timer: 0.0825 sec.
iter 182770 || Loss: 0.6312 || timer: 0.0920 sec.
iter 182780 || Loss: 1.0360 || timer: 0.0851 sec.
iter 182790 || Loss: 1.3812 || timer: 0.0913 sec.
iter 182800 || Loss: 0.9809 || timer: 0.0917 sec.
iter 182810 || Loss: 0.9808 || timer: 0.1087 sec.
iter 182820 || Loss: 0.8127 || timer: 0.0913 sec.
iter 182830 || Loss: 1.0131 || timer: 0.1102 sec.
iter 182840 || Loss: 0.8466 || timer: 0.0912 sec.
iter 182850 || Loss: 1.1582 || timer: 0.0838 sec.
iter 182860 || Loss: 0.6994 || timer: 0.0899 sec.
iter 182870 || Loss: 0.9197 || timer: 0.0955 sec.
iter 182880 || Loss: 0.8120 || timer: 0.0922 sec.
iter 182890 || Loss: 0.7254 || timer: 0.1021 sec.
iter 182900 || Loss: 0.7714 || timer: 0.0764 sec.
iter 182910 || Loss: 0.9889 || timer: 0.0913 sec.
iter 182920 || Loss: 1.1388 || timer: 0.0916 sec.
iter 182930 || Loss: 0.9782 || timer: 0.0838 sec.
iter 182940 || Loss: 1.3124 || timer: 0.0838 sec.
iter 182950 || Loss: 0.9957 || timer: 0.1055 sec.
iter 182960 || Loss: 1.0867 || timer: 0.0934 sec.
iter 182970 || Loss: 0.8095 || timer: 0.0913 sec.
iter 182980 || Loss: 0.8758 || timer: 0.0998 sec.
iter 182990 || Loss: 1.1645 || timer: 0.1123 sec.
iter 183000 || Loss: 0.9653 || timer: 0.0762 sec.
iter 183010 || Loss: 0.9039 || timer: 0.0900 sec.
iter 183020 || Loss: 0.6817 || timer: 0.1176 sec.
iter 183030 || Loss: 0.9434 || timer: 0.1074 sec.
iter 183040 || Loss: 1.0557 || timer: 0.0182 sec.
iter 183050 || Loss: 1.0752 || timer: 0.0844 sec.
iter 183060 || Loss: 0.6459 || timer: 0.0915 sec.
iter 183070 || Loss: 0.9069 || timer: 0.0927 sec.
iter 183080 || Loss: 0.8586 || timer: 0.0823 sec.
iter 183090 || Loss: 1.0471 || timer: 0.0900 sec.
iter 183100 || Loss: 1.0450 || timer: 0.0980 sec.
iter 183110 || Loss: 0.6633 || timer: 0.0892 sec.
iter 183120 || Loss: 0.7861 || timer: 0.0901 sec.
iter 183130 || Loss: 0.9468 || timer: 0.0818 sec.
iter 183140 || Loss: 0.8294 || timer: 0.1096 sec.
iter 183150 || Loss: 0.9115 || timer: 0.1124 sec.
iter 183160 || Loss: 0.8235 || timer: 0.0914 sec.
iter 183170 || Loss: 0.7988 || timer: 0.0870 sec.
iter 183180 || Loss: 1.9668 || timer: 0.0983 sec.
iter 183190 || Loss: 0.8483 || timer: 0.0850 sec.
iter 183200 || Loss: 0.7907 || timer: 0.0895 sec.
iter 183210 || Loss: 0.9285 || timer: 0.1052 sec.
iter 183220 || Loss: 0.6718 || timer: 0.0947 sec.
iter 183230 || Loss: 0.9636 || timer: 0.1091 sec.
iter 183240 || Loss: 0.9452 || timer: 0.0982 sec.
iter 183250 || Loss: 0.9843 || timer: 0.1005 sec.
iter 183260 || Loss: 0.8672 || timer: 0.0870 sec.
iter 183270 || Loss: 0.9385 || timer: 0.0873 sec.
iter 183280 || Loss: 1.1062 || timer: 0.0878 sec.
iter 183290 || Loss: 1.0620 || timer: 0.0894 sec.
iter 183300 || Loss: 0.7086 || timer: 0.0901 sec.
iter 183310 || Loss: 0.7733 || timer: 0.0908 sec.
iter 183320 || Loss: 1.0454 || timer: 0.0885 sec.
iter 183330 || Loss: 0.8532 || timer: 0.0932 sec.
iter 183340 || Loss: 0.8960 || timer: 0.0858 sec.
iter 183350 || Loss: 0.9286 || timer: 0.0874 sec.
iter 183360 || Loss: 0.9153 || timer: 0.0899 sec.
iter 183370 || Loss: 1.0147 || timer: 0.0213 sec.
iter 183380 || Loss: 0.5768 || timer: 0.1020 sec.
iter 183390 || Loss: 0.7954 || timer: 0.0875 sec.
iter 183400 || Loss: 1.1571 || timer: 0.1042 sec.
iter 183410 || Loss: 1.1709 || timer: 0.0821 sec.
iter 183420 || Loss: 0.8655 || timer: 0.0894 sec.
iter 183430 || Loss: 1.0158 || timer: 0.1024 sec.
iter 183440 || Loss: 0.8496 || timer: 0.0914 sec.
iter 183450 || Loss: 0.8690 || timer: 0.0826 sec.
iter 183460 || Loss: 1.1785 || timer: 0.0816 sec.
iter 183470 || Loss: 1.0837 || timer: 0.0969 sec.
iter 183480 || Loss: 1.0396 || timer: 0.0871 sec.
iter 183490 || Loss: 1.0356 || timer: 0.0810 sec.
iter 183500 || Loss: 0.9014 || timer: 0.0822 sec.
iter 183510 || Loss: 0.8353 || timer: 0.0831 sec.
iter 183520 || Loss: 0.8270 || timer: 0.0753 sec.
iter 183530 || Loss: 0.7708 || timer: 0.0835 sec.
iter 183540 || Loss: 0.8059 || timer: 0.1057 sec.
iter 183550 || Loss: 0.8670 || timer: 0.0962 sec.
iter 183560 || Loss: 0.8586 || timer: 0.0829 sec.
iter 183570 || Loss: 0.8709 || timer: 0.0852 sec.
iter 183580 || Loss: 0.7853 || timer: 0.0834 sec.
iter 183590 || Loss: 0.6771 || timer: 0.1288 sec.
iter 183600 || Loss: 0.9964 || timer: 0.0891 sec.
iter 183610 || Loss: 0.8300 || timer: 0.0910 sec.
iter 183620 || Loss: 1.0763 || timer: 0.0865 sec.
iter 183630 || Loss: 0.8170 || timer: 0.0915 sec.
iter 183640 || Loss: 0.9294 || timer: 0.1041 sec.
iter 183650 || Loss: 1.0565 || timer: 0.0929 sec.
iter 183660 || Loss: 1.0161 || timer: 0.0820 sec.
iter 183670 || Loss: 0.9866 || timer: 0.0957 sec.
iter 183680 || Loss: 0.7780 || timer: 0.0933 sec.
iter 183690 || Loss: 0.7376 || timer: 0.1193 sec.
iter 183700 || Loss: 0.8295 || timer: 0.0183 sec.
iter 183710 || Loss: 1.0673 || timer: 0.0926 sec.
iter 183720 || Loss: 0.8779 || timer: 0.1062 sec.
iter 183730 || Loss: 0.7339 || timer: 0.0904 sec.
iter 183740 || Loss: 0.7299 || timer: 0.0901 sec.
iter 183750 || Loss: 0.8318 || timer: 0.0906 sec.
iter 183760 || Loss: 0.6491 || timer: 0.0911 sec.
iter 183770 || Loss: 1.1840 || timer: 0.0825 sec.
iter 183780 || Loss: 0.8687 || timer: 0.1021 sec.
iter 183790 || Loss: 0.7123 || timer: 0.0919 sec.
iter 183800 || Loss: 0.7980 || timer: 0.1451 sec.
iter 183810 || Loss: 1.0022 || timer: 0.0920 sec.
iter 183820 || Loss: 0.7004 || timer: 0.0909 sec.
iter 183830 || Loss: 0.9640 || timer: 0.0809 sec.
iter 183840 || Loss: 0.6842 || timer: 0.0810 sec.
iter 183850 || Loss: 0.6933 || timer: 0.0913 sec.
iter 183860 || Loss: 0.8801 || timer: 0.0914 sec.
iter 183870 || Loss: 0.8549 || timer: 0.1062 sec.
iter 183880 || Loss: 1.0106 || timer: 0.0937 sec.
iter 183890 || Loss: 0.9621 || timer: 0.0904 sec.
iter 183900 || Loss: 1.0285 || timer: 0.0918 sec.
iter 183910 || Loss: 0.7862 || timer: 0.0975 sec.
iter 183920 || Loss: 0.6273 || timer: 0.0926 sec.
iter 183930 || Loss: 0.8593 || timer: 0.0949 sec.
iter 183940 || Loss: 0.7643 || timer: 0.1221 sec.
iter 183950 || Loss: 1.1724 || timer: 0.0825 sec.
iter 183960 || Loss: 1.2283 || timer: 0.0960 sec.
iter 183970 || Loss: 1.2838 || timer: 0.0828 sec.
iter 183980 || Loss: 1.0855 || timer: 0.0823 sec.
iter 183990 || Loss: 1.0818 || timer: 0.0901 sec.
iter 184000 || Loss: 1.0883 || timer: 0.0807 sec.
iter 184010 || Loss: 0.8257 || timer: 0.0843 sec.
iter 184020 || Loss: 0.9299 || timer: 0.0827 sec.
iter 184030 || Loss: 0.8735 || timer: 0.0259 sec.
iter 184040 || Loss: 1.0670 || timer: 0.0817 sec.
iter 184050 || Loss: 0.9016 || timer: 0.0938 sec.
iter 184060 || Loss: 1.0105 || timer: 0.0897 sec.
iter 184070 || Loss: 0.9997 || timer: 0.1128 sec.
iter 184080 || Loss: 0.9818 || timer: 0.0943 sec.
iter 184090 || Loss: 1.3385 || timer: 0.0882 sec.
iter 184100 || Loss: 0.9656 || timer: 0.0957 sec.
iter 184110 || Loss: 1.0272 || timer: 0.0998 sec.
iter 184120 || Loss: 0.7617 || timer: 0.0919 sec.
iter 184130 || Loss: 1.2378 || timer: 0.1012 sec.
iter 184140 || Loss: 0.8581 || timer: 0.0944 sec.
iter 184150 || Loss: 0.6867 || timer: 0.0893 sec.
iter 184160 || Loss: 1.0153 || timer: 0.0758 sec.
iter 184170 || Loss: 1.0360 || timer: 0.0754 sec.
iter 184180 || Loss: 1.0781 || timer: 0.0882 sec.
iter 184190 || Loss: 0.6364 || timer: 0.1263 sec.
iter 184200 || Loss: 0.8511 || timer: 0.0844 sec.
iter 184210 || Loss: 1.1098 || timer: 0.0845 sec.
iter 184220 || Loss: 0.7218 || timer: 0.0918 sec.
iter 184230 || Loss: 0.9569 || timer: 0.0927 sec.
iter 184240 || Loss: 1.1621 || timer: 0.1150 sec.
iter 184250 || Loss: 1.0499 || timer: 0.0884 sec.
iter 184260 || Loss: 1.0654 || timer: 0.0885 sec.
iter 184270 || Loss: 0.9469 || timer: 0.0816 sec.
iter 184280 || Loss: 0.8994 || timer: 0.0975 sec.
iter 184290 || Loss: 0.9725 || timer: 0.1020 sec.
iter 184300 || Loss: 0.6907 || timer: 0.0828 sec.
iter 184310 || Loss: 1.0206 || timer: 0.0831 sec.
iter 184320 || Loss: 0.8011 || timer: 0.0909 sec.
iter 184330 || Loss: 0.6016 || timer: 0.0851 sec.
iter 184340 || Loss: 0.8253 || timer: 0.0890 sec.
iter 184350 || Loss: 0.7628 || timer: 0.0847 sec.
iter 184360 || Loss: 1.0183 || timer: 0.0195 sec.
iter 184370 || Loss: 0.8719 || timer: 0.0840 sec.
iter 184380 || Loss: 0.9455 || timer: 0.1079 sec.
iter 184390 || Loss: 0.8902 || timer: 0.0846 sec.
iter 184400 || Loss: 0.9250 || timer: 0.0817 sec.
iter 184410 || Loss: 0.9366 || timer: 0.0964 sec.
iter 184420 || Loss: 0.9270 || timer: 0.0909 sec.
iter 184430 || Loss: 0.9550 || timer: 0.0922 sec.
iter 184440 || Loss: 1.1409 || timer: 0.0998 sec.
iter 184450 || Loss: 0.7839 || timer: 0.0856 sec.
iter 184460 || Loss: 1.0816 || timer: 0.1046 sec.
iter 184470 || Loss: 0.8505 || timer: 0.0841 sec.
iter 184480 || Loss: 0.9565 || timer: 0.0942 sec.
iter 184490 || Loss: 0.9503 || timer: 0.0900 sec.
iter 184500 || Loss: 1.4228 || timer: 0.0940 sec.
iter 184510 || Loss: 0.9088 || timer: 0.1147 sec.
iter 184520 || Loss: 0.9177 || timer: 0.0802 sec.
iter 184530 || Loss: 0.8213 || timer: 0.0915 sec.
iter 184540 || Loss: 0.8612 || timer: 0.0836 sec.
iter 184550 || Loss: 0.7250 || timer: 0.0846 sec.
iter 184560 || Loss: 0.9149 || timer: 0.0772 sec.
iter 184570 || Loss: 0.6145 || timer: 0.0831 sec.
iter 184580 || Loss: 0.5942 || timer: 0.1001 sec.
iter 184590 || Loss: 0.8374 || timer: 0.0906 sec.
iter 184600 || Loss: 0.9187 || timer: 0.0841 sec.
iter 184610 || Loss: 0.7779 || timer: 0.0892 sec.
iter 184620 || Loss: 1.0222 || timer: 0.0903 sec.
iter 184630 || Loss: 1.1972 || timer: 0.0887 sec.
iter 184640 || Loss: 0.8528 || timer: 0.1070 sec.
iter 184650 || Loss: 0.9622 || timer: 0.0914 sec.
iter 184660 || Loss: 1.0638 || timer: 0.0898 sec.
iter 184670 || Loss: 0.8267 || timer: 0.0835 sec.
iter 184680 || Loss: 0.7293 || timer: 0.0896 sec.
iter 184690 || Loss: 0.9455 || timer: 0.0263 sec.
iter 184700 || Loss: 3.2403 || timer: 0.1005 sec.
iter 184710 || Loss: 0.6348 || timer: 0.0905 sec.
iter 184720 || Loss: 0.9755 || timer: 0.0914 sec.
iter 184730 || Loss: 0.8427 || timer: 0.0876 sec.
iter 184740 || Loss: 1.0683 || timer: 0.0906 sec.
iter 184750 || Loss: 1.0451 || timer: 0.0857 sec.
iter 184760 || Loss: 0.8133 || timer: 0.0915 sec.
iter 184770 || Loss: 0.9148 || timer: 0.1100 sec.
iter 184780 || Loss: 1.0092 || timer: 0.1073 sec.
iter 184790 || Loss: 0.9525 || timer: 0.0989 sec.
iter 184800 || Loss: 0.8137 || timer: 0.0931 sec.
iter 184810 || Loss: 0.9764 || timer: 0.0888 sec.
iter 184820 || Loss: 0.8272 || timer: 0.0900 sec.
iter 184830 || Loss: 0.8361 || timer: 0.0883 sec.
iter 184840 || Loss: 1.1374 || timer: 0.1023 sec.
iter 184850 || Loss: 0.8671 || timer: 0.1065 sec.
iter 184860 || Loss: 0.9174 || timer: 0.1017 sec.
iter 184870 || Loss: 1.2073 || timer: 0.0844 sec.
iter 184880 || Loss: 1.0169 || timer: 0.0839 sec.
iter 184890 || Loss: 0.7276 || timer: 0.0915 sec.
iter 184900 || Loss: 0.9440 || timer: 0.1021 sec.
iter 184910 || Loss: 0.6871 || timer: 0.0910 sec.
iter 184920 || Loss: 0.8479 || timer: 0.0830 sec.
iter 184930 || Loss: 1.0777 || timer: 0.0834 sec.
iter 184940 || Loss: 0.9320 || timer: 0.0952 sec.
iter 184950 || Loss: 0.8819 || timer: 0.0837 sec.
iter 184960 || Loss: 0.9360 || timer: 0.0816 sec.
iter 184970 || Loss: 1.1966 || timer: 0.1191 sec.
iter 184980 || Loss: 0.7070 || timer: 0.0916 sec.
iter 184990 || Loss: 0.8174 || timer: 0.0908 sec.
iter 185000 || Loss: 0.9973 || Saving state, iter: 185000
timer: 0.0881 sec.
iter 185010 || Loss: 0.9413 || timer: 0.0908 sec.
iter 185020 || Loss: 0.9894 || timer: 0.0217 sec.
iter 185030 || Loss: 0.3530 || timer: 0.0916 sec.
iter 185040 || Loss: 1.0862 || timer: 0.0839 sec.
iter 185050 || Loss: 0.7217 || timer: 0.0844 sec.
iter 185060 || Loss: 0.8319 || timer: 0.0881 sec.
iter 185070 || Loss: 1.0735 || timer: 0.0841 sec.
iter 185080 || Loss: 1.2372 || timer: 0.0971 sec.
iter 185090 || Loss: 1.3383 || timer: 0.0911 sec.
iter 185100 || Loss: 0.7533 || timer: 0.0939 sec.
iter 185110 || Loss: 1.0301 || timer: 0.0903 sec.
iter 185120 || Loss: 0.9930 || timer: 0.0892 sec.
iter 185130 || Loss: 1.0599 || timer: 0.0839 sec.
iter 185140 || Loss: 1.1071 || timer: 0.0839 sec.
iter 185150 || Loss: 0.9319 || timer: 0.0843 sec.
iter 185160 || Loss: 0.9455 || timer: 0.0928 sec.
iter 185170 || Loss: 0.7468 || timer: 0.0912 sec.
iter 185180 || Loss: 0.8316 || timer: 0.1109 sec.
iter 185190 || Loss: 0.6498 || timer: 0.0892 sec.
iter 185200 || Loss: 0.9870 || timer: 0.0910 sec.
iter 185210 || Loss: 0.7286 || timer: 0.1039 sec.
iter 185220 || Loss: 0.9247 || timer: 0.0922 sec.
iter 185230 || Loss: 0.9058 || timer: 0.0843 sec.
iter 185240 || Loss: 0.9937 || timer: 0.0842 sec.
iter 185250 || Loss: 0.9945 || timer: 0.0896 sec.
iter 185260 || Loss: 1.1620 || timer: 0.1105 sec.
iter 185270 || Loss: 0.7587 || timer: 0.0906 sec.
iter 185280 || Loss: 1.1976 || timer: 0.0828 sec.
iter 185290 || Loss: 0.8170 || timer: 0.0912 sec.
iter 185300 || Loss: 0.8667 || timer: 0.1040 sec.
iter 185310 || Loss: 0.8960 || timer: 0.0839 sec.
iter 185320 || Loss: 1.1279 || timer: 0.0850 sec.
iter 185330 || Loss: 1.0476 || timer: 0.0839 sec.
iter 185340 || Loss: 0.9504 || timer: 0.0910 sec.
iter 185350 || Loss: 0.9859 || timer: 0.0255 sec.
iter 185360 || Loss: 0.1162 || timer: 0.0912 sec.
iter 185370 || Loss: 0.6801 || timer: 0.0839 sec.
iter 185380 || Loss: 0.6990 || timer: 0.1036 sec.
iter 185390 || Loss: 0.8601 || timer: 0.0916 sec.
iter 185400 || Loss: 1.2020 || timer: 0.0840 sec.
iter 185410 || Loss: 0.7797 || timer: 0.0772 sec.
iter 185420 || Loss: 1.0678 || timer: 0.0921 sec.
iter 185430 || Loss: 0.6880 || timer: 0.0897 sec.
iter 185440 || Loss: 0.9460 || timer: 0.0912 sec.
iter 185450 || Loss: 0.6717 || timer: 0.0967 sec.
iter 185460 || Loss: 0.7623 || timer: 0.0953 sec.
iter 185470 || Loss: 0.9607 || timer: 0.0912 sec.
iter 185480 || Loss: 1.2741 || timer: 0.0913 sec.
iter 185490 || Loss: 1.1991 || timer: 0.1030 sec.
iter 185500 || Loss: 1.1800 || timer: 0.0906 sec.
iter 185510 || Loss: 1.0204 || timer: 0.0837 sec.
iter 185520 || Loss: 1.0664 || timer: 0.0964 sec.
iter 185530 || Loss: 0.8264 || timer: 0.0838 sec.
iter 185540 || Loss: 1.0819 || timer: 0.0836 sec.
iter 185550 || Loss: 0.8511 || timer: 0.0903 sec.
iter 185560 || Loss: 1.0667 || timer: 0.0960 sec.
iter 185570 || Loss: 0.7096 || timer: 0.0935 sec.
iter 185580 || Loss: 0.9298 || timer: 0.0755 sec.
iter 185590 || Loss: 0.8739 || timer: 0.0936 sec.
iter 185600 || Loss: 0.7170 || timer: 0.0842 sec.
iter 185610 || Loss: 1.4812 || timer: 0.0862 sec.
iter 185620 || Loss: 1.3026 || timer: 0.0930 sec.
iter 185630 || Loss: 1.0281 || timer: 0.0936 sec.
iter 185640 || Loss: 0.7670 || timer: 0.0836 sec.
iter 185650 || Loss: 0.7998 || timer: 0.0910 sec.
iter 185660 || Loss: 0.6889 || timer: 0.0913 sec.
iter 185670 || Loss: 0.6575 || timer: 0.1088 sec.
iter 185680 || Loss: 0.7552 || timer: 0.0205 sec.
iter 185690 || Loss: 2.7829 || timer: 0.0843 sec.
iter 185700 || Loss: 1.0901 || timer: 0.0925 sec.
iter 185710 || Loss: 0.8416 || timer: 0.0828 sec.
iter 185720 || Loss: 0.8007 || timer: 0.0838 sec.
iter 185730 || Loss: 0.7627 || timer: 0.0926 sec.
iter 185740 || Loss: 0.7396 || timer: 0.0908 sec.
iter 185750 || Loss: 0.9605 || timer: 0.0906 sec.
iter 185760 || Loss: 0.4852 || timer: 0.0836 sec.
iter 185770 || Loss: 1.1129 || timer: 0.0839 sec.
iter 185780 || Loss: 0.9207 || timer: 0.0966 sec.
iter 185790 || Loss: 1.0454 || timer: 0.0913 sec.
iter 185800 || Loss: 0.8390 || timer: 0.0827 sec.
iter 185810 || Loss: 0.7068 || timer: 0.0830 sec.
iter 185820 || Loss: 0.7815 || timer: 0.0857 sec.
iter 185830 || Loss: 0.9600 || timer: 0.0830 sec.
iter 185840 || Loss: 0.8416 || timer: 0.0941 sec.
iter 185850 || Loss: 0.6834 || timer: 0.0920 sec.
iter 185860 || Loss: 0.8664 || timer: 0.0924 sec.
iter 185870 || Loss: 0.8993 || timer: 0.0912 sec.
iter 185880 || Loss: 0.9821 || timer: 0.0899 sec.
iter 185890 || Loss: 0.9515 || timer: 0.0841 sec.
iter 185900 || Loss: 0.8018 || timer: 0.0767 sec.
iter 185910 || Loss: 0.8752 || timer: 0.0839 sec.
iter 185920 || Loss: 1.1708 || timer: 0.1093 sec.
iter 185930 || Loss: 1.0497 || timer: 0.0922 sec.
iter 185940 || Loss: 0.9102 || timer: 0.0900 sec.
iter 185950 || Loss: 0.8610 || timer: 0.0842 sec.
iter 185960 || Loss: 0.8183 || timer: 0.0920 sec.
iter 185970 || Loss: 0.9747 || timer: 0.0918 sec.
iter 185980 || Loss: 0.8778 || timer: 0.0843 sec.
iter 185990 || Loss: 0.5981 || timer: 0.0894 sec.
iter 186000 || Loss: 1.0327 || timer: 0.0922 sec.
iter 186010 || Loss: 0.9509 || timer: 0.0207 sec.
iter 186020 || Loss: 0.3367 || timer: 0.0839 sec.
iter 186030 || Loss: 0.8814 || timer: 0.1146 sec.
iter 186040 || Loss: 0.8634 || timer: 0.0839 sec.
iter 186050 || Loss: 1.1089 || timer: 0.0914 sec.
iter 186060 || Loss: 0.9240 || timer: 0.0919 sec.
iter 186070 || Loss: 1.1213 || timer: 0.0823 sec.
iter 186080 || Loss: 1.0367 || timer: 0.0901 sec.
iter 186090 || Loss: 1.1548 || timer: 0.0842 sec.
iter 186100 || Loss: 1.0019 || timer: 0.1090 sec.
iter 186110 || Loss: 1.0273 || timer: 0.1110 sec.
iter 186120 || Loss: 0.7772 || timer: 0.0916 sec.
iter 186130 || Loss: 0.8197 || timer: 0.0927 sec.
iter 186140 || Loss: 1.3155 || timer: 0.0910 sec.
iter 186150 || Loss: 0.7798 || timer: 0.1118 sec.
iter 186160 || Loss: 1.0924 || timer: 0.0957 sec.
iter 186170 || Loss: 0.8130 || timer: 0.0914 sec.
iter 186180 || Loss: 0.9761 || timer: 0.0931 sec.
iter 186190 || Loss: 0.8311 || timer: 0.1091 sec.
iter 186200 || Loss: 0.9998 || timer: 0.0865 sec.
iter 186210 || Loss: 1.0633 || timer: 0.0927 sec.
iter 186220 || Loss: 0.8742 || timer: 0.0895 sec.
iter 186230 || Loss: 0.8677 || timer: 0.1027 sec.
iter 186240 || Loss: 1.1226 || timer: 0.0814 sec.
iter 186250 || Loss: 1.0643 || timer: 0.0938 sec.
iter 186260 || Loss: 0.7547 || timer: 0.0943 sec.
iter 186270 || Loss: 0.9710 || timer: 0.0813 sec.
iter 186280 || Loss: 0.8145 || timer: 0.0919 sec.
iter 186290 || Loss: 0.8962 || timer: 0.0949 sec.
iter 186300 || Loss: 1.0778 || timer: 0.1232 sec.
iter 186310 || Loss: 1.0973 || timer: 0.0755 sec.
iter 186320 || Loss: 0.9675 || timer: 0.0814 sec.
iter 186330 || Loss: 0.6538 || timer: 0.0910 sec.
iter 186340 || Loss: 0.7267 || timer: 0.0225 sec.
iter 186350 || Loss: 2.0196 || timer: 0.0841 sec.
iter 186360 || Loss: 1.1874 || timer: 0.0931 sec.
iter 186370 || Loss: 0.7667 || timer: 0.0839 sec.
iter 186380 || Loss: 0.7799 || timer: 0.0877 sec.
iter 186390 || Loss: 1.0747 || timer: 0.1046 sec.
iter 186400 || Loss: 0.8865 || timer: 0.0914 sec.
iter 186410 || Loss: 0.7953 || timer: 0.0998 sec.
iter 186420 || Loss: 1.4246 || timer: 0.0906 sec.
iter 186430 || Loss: 0.8700 || timer: 0.0870 sec.
iter 186440 || Loss: 1.0058 || timer: 0.0958 sec.
iter 186450 || Loss: 0.9986 || timer: 0.0897 sec.
iter 186460 || Loss: 0.7201 || timer: 0.0885 sec.
iter 186470 || Loss: 0.9171 || timer: 0.0850 sec.
iter 186480 || Loss: 1.0433 || timer: 0.0915 sec.
iter 186490 || Loss: 0.7895 || timer: 0.0822 sec.
iter 186500 || Loss: 0.5046 || timer: 0.0824 sec.
iter 186510 || Loss: 1.1031 || timer: 0.0937 sec.
iter 186520 || Loss: 1.1415 || timer: 0.0749 sec.
iter 186530 || Loss: 0.8366 || timer: 0.0994 sec.
iter 186540 || Loss: 1.0904 || timer: 0.0930 sec.
iter 186550 || Loss: 1.1095 || timer: 0.0901 sec.
iter 186560 || Loss: 0.8637 || timer: 0.1074 sec.
iter 186570 || Loss: 0.9238 || timer: 0.0849 sec.
iter 186580 || Loss: 1.1704 || timer: 0.0755 sec.
iter 186590 || Loss: 0.9038 || timer: 0.1270 sec.
iter 186600 || Loss: 0.9115 || timer: 0.0853 sec.
iter 186610 || Loss: 0.9608 || timer: 0.1081 sec.
iter 186620 || Loss: 0.7241 || timer: 0.1121 sec.
iter 186630 || Loss: 1.0376 || timer: 0.0998 sec.
iter 186640 || Loss: 1.1148 || timer: 0.0840 sec.
iter 186650 || Loss: 1.0852 || timer: 0.1047 sec.
iter 186660 || Loss: 0.9960 || timer: 0.0901 sec.
iter 186670 || Loss: 0.9325 || timer: 0.0268 sec.
iter 186680 || Loss: 0.3011 || timer: 0.0828 sec.
iter 186690 || Loss: 1.0798 || timer: 0.0765 sec.
iter 186700 || Loss: 0.9313 || timer: 0.0858 sec.
iter 186710 || Loss: 0.7970 || timer: 0.1033 sec.
iter 186720 || Loss: 0.9474 || timer: 0.0897 sec.
iter 186730 || Loss: 0.8496 || timer: 0.0873 sec.
iter 186740 || Loss: 1.0190 || timer: 0.0844 sec.
iter 186750 || Loss: 0.6220 || timer: 0.0956 sec.
iter 186760 || Loss: 0.9996 || timer: 0.0911 sec.
iter 186770 || Loss: 1.0121 || timer: 0.0939 sec.
iter 186780 || Loss: 0.7972 || timer: 0.0891 sec.
iter 186790 || Loss: 1.2796 || timer: 0.0948 sec.
iter 186800 || Loss: 0.7435 || timer: 0.0822 sec.
iter 186810 || Loss: 0.9001 || timer: 0.1147 sec.
iter 186820 || Loss: 0.7306 || timer: 0.0816 sec.
iter 186830 || Loss: 1.1784 || timer: 0.0893 sec.
iter 186840 || Loss: 1.0452 || timer: 0.1012 sec.
iter 186850 || Loss: 1.2059 || timer: 0.0880 sec.
iter 186860 || Loss: 0.8535 || timer: 0.0955 sec.
iter 186870 || Loss: 1.1370 || timer: 0.0839 sec.
iter 186880 || Loss: 0.7434 || timer: 0.0922 sec.
iter 186890 || Loss: 0.7814 || timer: 0.0852 sec.
iter 186900 || Loss: 0.7969 || timer: 0.0733 sec.
iter 186910 || Loss: 0.8783 || timer: 0.0937 sec.
iter 186920 || Loss: 1.0245 || timer: 0.1177 sec.
iter 186930 || Loss: 0.8130 || timer: 0.1076 sec.
iter 186940 || Loss: 1.0917 || timer: 0.0869 sec.
iter 186950 || Loss: 0.9090 || timer: 0.0926 sec.
iter 186960 || Loss: 0.8955 || timer: 0.1162 sec.
iter 186970 || Loss: 0.7372 || timer: 0.0832 sec.
iter 186980 || Loss: 0.9365 || timer: 0.1012 sec.
iter 186990 || Loss: 1.0581 || timer: 0.0911 sec.
iter 187000 || Loss: 0.9451 || timer: 0.0193 sec.
iter 187010 || Loss: 1.3662 || timer: 0.1009 sec.
iter 187020 || Loss: 0.8416 || timer: 0.1165 sec.
iter 187030 || Loss: 0.7825 || timer: 0.0907 sec.
iter 187040 || Loss: 1.0654 || timer: 0.0915 sec.
iter 187050 || Loss: 0.7818 || timer: 0.0986 sec.
iter 187060 || Loss: 0.7824 || timer: 0.0845 sec.
iter 187070 || Loss: 0.7731 || timer: 0.0875 sec.
iter 187080 || Loss: 0.6658 || timer: 0.0850 sec.
iter 187090 || Loss: 0.6963 || timer: 0.0799 sec.
iter 187100 || Loss: 1.1499 || timer: 0.1227 sec.
iter 187110 || Loss: 1.0953 || timer: 0.0905 sec.
iter 187120 || Loss: 0.7223 || timer: 0.0997 sec.
iter 187130 || Loss: 1.0281 || timer: 0.0908 sec.
iter 187140 || Loss: 0.9056 || timer: 0.0838 sec.
iter 187150 || Loss: 0.9899 || timer: 0.0842 sec.
iter 187160 || Loss: 0.8505 || timer: 0.1115 sec.
iter 187170 || Loss: 1.2421 || timer: 0.0894 sec.
iter 187180 || Loss: 0.6794 || timer: 0.0899 sec.
iter 187190 || Loss: 0.9001 || timer: 0.0810 sec.
iter 187200 || Loss: 0.8239 || timer: 0.0835 sec.
iter 187210 || Loss: 0.8361 || timer: 0.0761 sec.
iter 187220 || Loss: 1.1256 || timer: 0.0825 sec.
iter 187230 || Loss: 0.8629 || timer: 0.0812 sec.
iter 187240 || Loss: 0.8811 || timer: 0.0854 sec.
iter 187250 || Loss: 0.9800 || timer: 0.0911 sec.
iter 187260 || Loss: 1.0319 || timer: 0.1170 sec.
iter 187270 || Loss: 0.9453 || timer: 0.0844 sec.
iter 187280 || Loss: 0.8327 || timer: 0.0988 sec.
iter 187290 || Loss: 1.1587 || timer: 0.0965 sec.
iter 187300 || Loss: 0.6850 || timer: 0.0839 sec.
iter 187310 || Loss: 0.8478 || timer: 0.0823 sec.
iter 187320 || Loss: 0.9569 || timer: 0.0910 sec.
iter 187330 || Loss: 0.9160 || timer: 0.0166 sec.
iter 187340 || Loss: 1.9146 || timer: 0.0888 sec.
iter 187350 || Loss: 0.7436 || timer: 0.0824 sec.
iter 187360 || Loss: 1.0403 || timer: 0.0891 sec.
iter 187370 || Loss: 0.7912 || timer: 0.0921 sec.
iter 187380 || Loss: 0.5808 || timer: 0.0894 sec.
iter 187390 || Loss: 1.0730 || timer: 0.1111 sec.
iter 187400 || Loss: 0.7505 || timer: 0.0836 sec.
iter 187410 || Loss: 0.8745 || timer: 0.0887 sec.
iter 187420 || Loss: 0.7282 || timer: 0.0876 sec.
iter 187430 || Loss: 0.8806 || timer: 0.0967 sec.
iter 187440 || Loss: 1.0156 || timer: 0.0917 sec.
iter 187450 || Loss: 0.8638 || timer: 0.0846 sec.
iter 187460 || Loss: 0.9522 || timer: 0.0819 sec.
iter 187470 || Loss: 0.9856 || timer: 0.0831 sec.
iter 187480 || Loss: 0.9284 || timer: 0.0954 sec.
iter 187490 || Loss: 0.8477 || timer: 0.0821 sec.
iter 187500 || Loss: 0.8585 || timer: 0.0933 sec.
iter 187510 || Loss: 1.1288 || timer: 0.0884 sec.
iter 187520 || Loss: 0.8740 || timer: 0.0821 sec.
iter 187530 || Loss: 0.6524 || timer: 0.0925 sec.
iter 187540 || Loss: 0.9743 || timer: 0.0999 sec.
iter 187550 || Loss: 1.0025 || timer: 0.0882 sec.
iter 187560 || Loss: 0.6620 || timer: 0.0826 sec.
iter 187570 || Loss: 0.7878 || timer: 0.1010 sec.
iter 187580 || Loss: 0.7975 || timer: 0.0825 sec.
iter 187590 || Loss: 0.7176 || timer: 0.0821 sec.
iter 187600 || Loss: 1.0508 || timer: 0.0888 sec.
iter 187610 || Loss: 0.7568 || timer: 0.0906 sec.
iter 187620 || Loss: 1.0327 || timer: 0.0915 sec.
iter 187630 || Loss: 0.6060 || timer: 0.0861 sec.
iter 187640 || Loss: 0.8167 || timer: 0.0853 sec.
iter 187650 || Loss: 0.7895 || timer: 0.0832 sec.
iter 187660 || Loss: 1.0106 || timer: 0.0164 sec.
iter 187670 || Loss: 1.3736 || timer: 0.0920 sec.
iter 187680 || Loss: 1.0683 || timer: 0.0819 sec.
iter 187690 || Loss: 0.9752 || timer: 0.0871 sec.
iter 187700 || Loss: 1.0300 || timer: 0.0827 sec.
iter 187710 || Loss: 1.1171 || timer: 0.0833 sec.
iter 187720 || Loss: 0.9684 || timer: 0.0894 sec.
iter 187730 || Loss: 0.8407 || timer: 0.0940 sec.
iter 187740 || Loss: 0.8230 || timer: 0.0981 sec.
iter 187750 || Loss: 0.7687 || timer: 0.0893 sec.
iter 187760 || Loss: 0.9883 || timer: 0.1102 sec.
iter 187770 || Loss: 0.8514 || timer: 0.1041 sec.
iter 187780 || Loss: 1.1761 || timer: 0.1138 sec.
iter 187790 || Loss: 1.3740 || timer: 0.0920 sec.
iter 187800 || Loss: 0.7850 || timer: 0.0822 sec.
iter 187810 || Loss: 0.8626 || timer: 0.0823 sec.
iter 187820 || Loss: 0.8937 || timer: 0.0901 sec.
iter 187830 || Loss: 0.7393 || timer: 0.1035 sec.
iter 187840 || Loss: 0.7174 || timer: 0.0879 sec.
iter 187850 || Loss: 0.6744 || timer: 0.0861 sec.
iter 187860 || Loss: 0.8106 || timer: 0.0947 sec.
iter 187870 || Loss: 1.3166 || timer: 0.1357 sec.
iter 187880 || Loss: 1.1300 || timer: 0.0846 sec.
iter 187890 || Loss: 0.8975 || timer: 0.0833 sec.
iter 187900 || Loss: 1.1586 || timer: 0.0836 sec.
iter 187910 || Loss: 0.6723 || timer: 0.0768 sec.
iter 187920 || Loss: 0.7432 || timer: 0.0960 sec.
iter 187930 || Loss: 0.7932 || timer: 0.0916 sec.
iter 187940 || Loss: 0.7762 || timer: 0.1070 sec.
iter 187950 || Loss: 0.6377 || timer: 0.0923 sec.
iter 187960 || Loss: 1.0634 || timer: 0.1047 sec.
iter 187970 || Loss: 0.7570 || timer: 0.0913 sec.
iter 187980 || Loss: 0.7359 || timer: 0.0771 sec.
iter 187990 || Loss: 1.0312 || timer: 0.0225 sec.
iter 188000 || Loss: 2.2128 || timer: 0.0822 sec.
iter 188010 || Loss: 0.9820 || timer: 0.1109 sec.
iter 188020 || Loss: 0.7931 || timer: 0.0998 sec.
iter 188030 || Loss: 0.8111 || timer: 0.0913 sec.
iter 188040 || Loss: 1.2389 || timer: 0.0905 sec.
iter 188050 || Loss: 0.9487 || timer: 0.0911 sec.
iter 188060 || Loss: 1.2161 || timer: 0.0918 sec.
iter 188070 || Loss: 0.8800 || timer: 0.1093 sec.
iter 188080 || Loss: 1.0224 || timer: 0.1050 sec.
iter 188090 || Loss: 1.0711 || timer: 0.1000 sec.
iter 188100 || Loss: 0.7538 || timer: 0.0882 sec.
iter 188110 || Loss: 0.9446 || timer: 0.1035 sec.
iter 188120 || Loss: 0.6106 || timer: 0.0909 sec.
iter 188130 || Loss: 1.0275 || timer: 0.0891 sec.
iter 188140 || Loss: 1.0331 || timer: 0.0884 sec.
iter 188150 || Loss: 0.9971 || timer: 0.1251 sec.
iter 188160 || Loss: 0.9235 || timer: 0.0921 sec.
iter 188170 || Loss: 1.0666 || timer: 0.0942 sec.
iter 188180 || Loss: 0.6244 || timer: 0.0886 sec.
iter 188190 || Loss: 0.9654 || timer: 0.0917 sec.
iter 188200 || Loss: 1.2334 || timer: 0.0922 sec.
iter 188210 || Loss: 0.6597 || timer: 0.0927 sec.
iter 188220 || Loss: 0.7321 || timer: 0.0879 sec.
iter 188230 || Loss: 0.8491 || timer: 0.0839 sec.
iter 188240 || Loss: 0.9381 || timer: 0.0927 sec.
iter 188250 || Loss: 1.0467 || timer: 0.0897 sec.
iter 188260 || Loss: 0.9538 || timer: 0.0891 sec.
iter 188270 || Loss: 0.8733 || timer: 0.0831 sec.
iter 188280 || Loss: 1.0019 || timer: 0.1196 sec.
iter 188290 || Loss: 0.8879 || timer: 0.0908 sec.
iter 188300 || Loss: 0.8248 || timer: 0.0793 sec.
iter 188310 || Loss: 0.8039 || timer: 0.0906 sec.
iter 188320 || Loss: 1.0150 || timer: 0.0199 sec.
iter 188330 || Loss: 0.5317 || timer: 0.0934 sec.
iter 188340 || Loss: 1.2517 || timer: 0.1066 sec.
iter 188350 || Loss: 0.6761 || timer: 0.0886 sec.
iter 188360 || Loss: 0.7977 || timer: 0.0886 sec.
iter 188370 || Loss: 0.8845 || timer: 0.1082 sec.
iter 188380 || Loss: 0.8369 || timer: 0.0846 sec.
iter 188390 || Loss: 0.9004 || timer: 0.1008 sec.
iter 188400 || Loss: 1.0731 || timer: 0.0835 sec.
iter 188410 || Loss: 1.0768 || timer: 0.0816 sec.
iter 188420 || Loss: 0.8260 || timer: 0.0968 sec.
iter 188430 || Loss: 0.9478 || timer: 0.0763 sec.
iter 188440 || Loss: 0.6487 || timer: 0.1081 sec.
iter 188450 || Loss: 0.7312 || timer: 0.0840 sec.
iter 188460 || Loss: 0.5976 || timer: 0.0910 sec.
iter 188470 || Loss: 0.7812 || timer: 0.0985 sec.
iter 188480 || Loss: 0.8205 || timer: 0.0897 sec.
iter 188490 || Loss: 1.1249 || timer: 0.0910 sec.
iter 188500 || Loss: 0.9932 || timer: 0.0909 sec.
iter 188510 || Loss: 0.9575 || timer: 0.0979 sec.
iter 188520 || Loss: 1.1266 || timer: 0.0850 sec.
iter 188530 || Loss: 0.8289 || timer: 0.0846 sec.
iter 188540 || Loss: 0.8470 || timer: 0.0921 sec.
iter 188550 || Loss: 1.0559 || timer: 0.0927 sec.
iter 188560 || Loss: 0.6300 || timer: 0.1035 sec.
iter 188570 || Loss: 0.8158 || timer: 0.1074 sec.
iter 188580 || Loss: 1.1851 || timer: 0.1002 sec.
iter 188590 || Loss: 0.9740 || timer: 0.1143 sec.
iter 188600 || Loss: 1.0411 || timer: 0.1193 sec.
iter 188610 || Loss: 0.9798 || timer: 0.0856 sec.
iter 188620 || Loss: 1.1539 || timer: 0.0872 sec.
iter 188630 || Loss: 0.8887 || timer: 0.0896 sec.
iter 188640 || Loss: 0.7725 || timer: 0.0986 sec.
iter 188650 || Loss: 0.7220 || timer: 0.0156 sec.
iter 188660 || Loss: 0.8460 || timer: 0.1192 sec.
iter 188670 || Loss: 0.7777 || timer: 0.0825 sec.
iter 188680 || Loss: 1.0975 || timer: 0.0820 sec.
iter 188690 || Loss: 0.8097 || timer: 0.0894 sec.
iter 188700 || Loss: 1.1127 || timer: 0.0896 sec.
iter 188710 || Loss: 0.9833 || timer: 0.0827 sec.
iter 188720 || Loss: 1.0290 || timer: 0.0829 sec.
iter 188730 || Loss: 0.8025 || timer: 0.0849 sec.
iter 188740 || Loss: 0.7311 || timer: 0.0845 sec.
iter 188750 || Loss: 0.5684 || timer: 0.1175 sec.
iter 188760 || Loss: 0.8199 || timer: 0.0849 sec.
iter 188770 || Loss: 0.7273 || timer: 0.0920 sec.
iter 188780 || Loss: 0.7339 || timer: 0.0882 sec.
iter 188790 || Loss: 0.6968 || timer: 0.0838 sec.
iter 188800 || Loss: 1.0945 || timer: 0.0965 sec.
iter 188810 || Loss: 1.1788 || timer: 0.0873 sec.
iter 188820 || Loss: 0.7607 || timer: 0.0882 sec.
iter 188830 || Loss: 0.9073 || timer: 0.0932 sec.
iter 188840 || Loss: 1.0843 || timer: 0.0835 sec.
iter 188850 || Loss: 1.1161 || timer: 0.0773 sec.
iter 188860 || Loss: 0.8465 || timer: 0.0844 sec.
iter 188870 || Loss: 0.9832 || timer: 0.0833 sec.
iter 188880 || Loss: 0.8771 || timer: 0.0844 sec.
iter 188890 || Loss: 0.5248 || timer: 0.0926 sec.
iter 188900 || Loss: 1.0627 || timer: 0.0845 sec.
iter 188910 || Loss: 0.8377 || timer: 0.0910 sec.
iter 188920 || Loss: 0.9053 || timer: 0.0855 sec.
iter 188930 || Loss: 1.1009 || timer: 0.0840 sec.
iter 188940 || Loss: 0.6518 || timer: 0.0915 sec.
iter 188950 || Loss: 1.0298 || timer: 0.0867 sec.
iter 188960 || Loss: 0.8120 || timer: 0.0907 sec.
iter 188970 || Loss: 0.7885 || timer: 0.0924 sec.
iter 188980 || Loss: 1.0411 || timer: 0.0178 sec.
iter 188990 || Loss: 0.7954 || timer: 0.0776 sec.
iter 189000 || Loss: 0.9704 || timer: 0.0844 sec.
iter 189010 || Loss: 1.0397 || timer: 0.0863 sec.
iter 189020 || Loss: 0.8222 || timer: 0.1045 sec.
iter 189030 || Loss: 0.8243 || timer: 0.0907 sec.
iter 189040 || Loss: 0.9473 || timer: 0.0893 sec.
iter 189050 || Loss: 0.8139 || timer: 0.0834 sec.
iter 189060 || Loss: 0.8656 || timer: 0.0898 sec.
iter 189070 || Loss: 0.8648 || timer: 0.0913 sec.
iter 189080 || Loss: 0.9966 || timer: 0.0962 sec.
iter 189090 || Loss: 0.8306 || timer: 0.0912 sec.
iter 189100 || Loss: 0.7941 || timer: 0.0900 sec.
iter 189110 || Loss: 0.8814 || timer: 0.0886 sec.
iter 189120 || Loss: 0.9405 || timer: 0.0911 sec.
iter 189130 || Loss: 0.6639 || timer: 0.0821 sec.
iter 189140 || Loss: 1.1028 || timer: 0.0896 sec.
iter 189150 || Loss: 0.7228 || timer: 0.0937 sec.
iter 189160 || Loss: 1.1343 || timer: 0.0892 sec.
iter 189170 || Loss: 0.8097 || timer: 0.0900 sec.
iter 189180 || Loss: 0.9100 || timer: 0.0900 sec.
iter 189190 || Loss: 0.7008 || timer: 0.0895 sec.
iter 189200 || Loss: 0.6977 || timer: 0.1025 sec.
iter 189210 || Loss: 0.7498 || timer: 0.0821 sec.
iter 189220 || Loss: 0.8755 || timer: 0.0990 sec.
iter 189230 || Loss: 0.7846 || timer: 0.0875 sec.
iter 189240 || Loss: 0.8384 || timer: 0.0941 sec.
iter 189250 || Loss: 1.2549 || timer: 0.1028 sec.
iter 189260 || Loss: 1.1062 || timer: 0.1032 sec.
iter 189270 || Loss: 0.8641 || timer: 0.0883 sec.
iter 189280 || Loss: 1.0320 || timer: 0.1023 sec.
iter 189290 || Loss: 0.8825 || timer: 0.0901 sec.
iter 189300 || Loss: 0.7211 || timer: 0.0904 sec.
iter 189310 || Loss: 0.8971 || timer: 0.0224 sec.
iter 189320 || Loss: 1.5273 || timer: 0.0914 sec.
iter 189330 || Loss: 0.6452 || timer: 0.0986 sec.
iter 189340 || Loss: 0.9242 || timer: 0.0827 sec.
iter 189350 || Loss: 0.5519 || timer: 0.0907 sec.
iter 189360 || Loss: 0.9000 || timer: 0.0939 sec.
iter 189370 || Loss: 0.6633 || timer: 0.0904 sec.
iter 189380 || Loss: 1.0094 || timer: 0.0914 sec.
iter 189390 || Loss: 1.1517 || timer: 0.1039 sec.
iter 189400 || Loss: 0.9456 || timer: 0.0913 sec.
iter 189410 || Loss: 1.0050 || timer: 0.0973 sec.
iter 189420 || Loss: 1.0431 || timer: 0.0910 sec.
iter 189430 || Loss: 0.9720 || timer: 0.0829 sec.
iter 189440 || Loss: 1.0744 || timer: 0.0835 sec.
iter 189450 || Loss: 1.0307 || timer: 0.0853 sec.
iter 189460 || Loss: 1.2227 || timer: 0.0838 sec.
iter 189470 || Loss: 0.6679 || timer: 0.0825 sec.
iter 189480 || Loss: 1.1079 || timer: 0.0922 sec.
iter 189490 || Loss: 0.8155 || timer: 0.1024 sec.
iter 189500 || Loss: 0.7550 || timer: 0.1039 sec.
iter 189510 || Loss: 0.8673 || timer: 0.0839 sec.
iter 189520 || Loss: 0.6806 || timer: 0.0853 sec.
iter 189530 || Loss: 0.8198 || timer: 0.0910 sec.
iter 189540 || Loss: 1.1861 || timer: 0.0914 sec.
iter 189550 || Loss: 0.7278 || timer: 0.1053 sec.
iter 189560 || Loss: 0.9819 || timer: 0.0842 sec.
iter 189570 || Loss: 0.9592 || timer: 0.0905 sec.
iter 189580 || Loss: 0.9361 || timer: 0.0910 sec.
iter 189590 || Loss: 0.8795 || timer: 0.0845 sec.
iter 189600 || Loss: 0.8638 || timer: 0.0866 sec.
iter 189610 || Loss: 0.9490 || timer: 0.0922 sec.
iter 189620 || Loss: 0.8021 || timer: 0.0833 sec.
iter 189630 || Loss: 0.8839 || timer: 0.1115 sec.
iter 189640 || Loss: 0.7341 || timer: 0.0207 sec.
iter 189650 || Loss: 0.4310 || timer: 0.0910 sec.
iter 189660 || Loss: 0.8935 || timer: 0.0905 sec.
iter 189670 || Loss: 0.6753 || timer: 0.1060 sec.
iter 189680 || Loss: 0.9322 || timer: 0.0994 sec.
iter 189690 || Loss: 0.7084 || timer: 0.0926 sec.
iter 189700 || Loss: 0.5592 || timer: 0.0845 sec.
iter 189710 || Loss: 1.4337 || timer: 0.0842 sec.
iter 189720 || Loss: 1.3484 || timer: 0.0916 sec.
iter 189730 || Loss: 0.9908 || timer: 0.0912 sec.
iter 189740 || Loss: 0.9402 || timer: 0.0963 sec.
iter 189750 || Loss: 1.0104 || timer: 0.0838 sec.
iter 189760 || Loss: 0.9855 || timer: 0.0919 sec.
iter 189770 || Loss: 1.1971 || timer: 0.0923 sec.
iter 189780 || Loss: 1.0218 || timer: 0.1014 sec.
iter 189790 || Loss: 1.0330 || timer: 0.1144 sec.
iter 189800 || Loss: 0.7181 || timer: 0.1036 sec.
iter 189810 || Loss: 1.0966 || timer: 0.0851 sec.
iter 189820 || Loss: 0.8427 || timer: 0.0914 sec.
iter 189830 || Loss: 1.1415 || timer: 0.0928 sec.
iter 189840 || Loss: 1.0715 || timer: 0.1010 sec.
iter 189850 || Loss: 0.6794 || timer: 0.0840 sec.
iter 189860 || Loss: 1.3794 || timer: 0.1102 sec.
iter 189870 || Loss: 0.8584 || timer: 0.0912 sec.
iter 189880 || Loss: 0.9756 || timer: 0.0902 sec.
iter 189890 || Loss: 0.9102 || timer: 0.0896 sec.
iter 189900 || Loss: 0.7939 || timer: 0.0839 sec.
iter 189910 || Loss: 0.8923 || timer: 0.0823 sec.
iter 189920 || Loss: 0.9094 || timer: 0.1005 sec.
iter 189930 || Loss: 0.9933 || timer: 0.1054 sec.
iter 189940 || Loss: 1.0539 || timer: 0.0911 sec.
iter 189950 || Loss: 0.9273 || timer: 0.1094 sec.
iter 189960 || Loss: 0.6978 || timer: 0.1094 sec.
iter 189970 || Loss: 1.0580 || timer: 0.0183 sec.
iter 189980 || Loss: 3.7512 || timer: 0.0848 sec.
iter 189990 || Loss: 0.8592 || timer: 0.0836 sec.
iter 190000 || Loss: 0.9021 || Saving state, iter: 190000
timer: 0.0918 sec.
iter 190010 || Loss: 0.8245 || timer: 0.0839 sec.
iter 190020 || Loss: 0.8654 || timer: 0.0933 sec.
iter 190030 || Loss: 0.7726 || timer: 0.0887 sec.
iter 190040 || Loss: 0.8592 || timer: 0.0924 sec.
iter 190050 || Loss: 0.7261 || timer: 0.0775 sec.
iter 190060 || Loss: 1.2472 || timer: 0.0859 sec.
iter 190070 || Loss: 1.2889 || timer: 0.1244 sec.
iter 190080 || Loss: 1.0731 || timer: 0.0915 sec.
iter 190090 || Loss: 0.9599 || timer: 0.1068 sec.
iter 190100 || Loss: 0.9800 || timer: 0.0916 sec.
iter 190110 || Loss: 0.7524 || timer: 0.0839 sec.
iter 190120 || Loss: 1.2314 || timer: 0.0938 sec.
iter 190130 || Loss: 1.0125 || timer: 0.0834 sec.
iter 190140 || Loss: 0.8383 || timer: 0.0828 sec.
iter 190150 || Loss: 1.2538 || timer: 0.0835 sec.
iter 190160 || Loss: 1.4774 || timer: 0.0982 sec.
iter 190170 || Loss: 1.0757 || timer: 0.0840 sec.
iter 190180 || Loss: 0.7104 || timer: 0.0914 sec.
iter 190190 || Loss: 0.9530 || timer: 0.0840 sec.
iter 190200 || Loss: 1.2363 || timer: 0.0841 sec.
iter 190210 || Loss: 1.1460 || timer: 0.0934 sec.
iter 190220 || Loss: 0.9216 || timer: 0.0915 sec.
iter 190230 || Loss: 0.8989 || timer: 0.0890 sec.
iter 190240 || Loss: 0.7019 || timer: 0.0844 sec.
iter 190250 || Loss: 0.8025 || timer: 0.0863 sec.
iter 190260 || Loss: 0.7413 || timer: 0.0843 sec.
iter 190270 || Loss: 1.1740 || timer: 0.0761 sec.
iter 190280 || Loss: 0.8149 || timer: 0.0795 sec.
iter 190290 || Loss: 0.9426 || timer: 0.0828 sec.
iter 190300 || Loss: 0.9420 || timer: 0.0274 sec.
iter 190310 || Loss: 0.5096 || timer: 0.0908 sec.
iter 190320 || Loss: 0.9552 || timer: 0.0908 sec.
iter 190330 || Loss: 0.8986 || timer: 0.0898 sec.
iter 190340 || Loss: 0.8184 || timer: 0.0851 sec.
iter 190350 || Loss: 0.8488 || timer: 0.0983 sec.
iter 190360 || Loss: 1.3589 || timer: 0.1198 sec.
iter 190370 || Loss: 0.9393 || timer: 0.1060 sec.
iter 190380 || Loss: 1.0424 || timer: 0.0892 sec.
iter 190390 || Loss: 0.9109 || timer: 0.0844 sec.
iter 190400 || Loss: 1.1420 || timer: 0.1130 sec.
iter 190410 || Loss: 0.9045 || timer: 0.0830 sec.
iter 190420 || Loss: 0.8937 || timer: 0.0882 sec.
iter 190430 || Loss: 0.9932 || timer: 0.0831 sec.
iter 190440 || Loss: 0.8405 || timer: 0.0856 sec.
iter 190450 || Loss: 0.8268 || timer: 0.1046 sec.
iter 190460 || Loss: 0.7853 || timer: 0.0865 sec.
iter 190470 || Loss: 0.9038 || timer: 0.0910 sec.
iter 190480 || Loss: 1.2674 || timer: 0.1063 sec.
iter 190490 || Loss: 1.1085 || timer: 0.1036 sec.
iter 190500 || Loss: 1.1569 || timer: 0.0839 sec.
iter 190510 || Loss: 1.1077 || timer: 0.1264 sec.
iter 190520 || Loss: 1.2803 || timer: 0.0898 sec.
iter 190530 || Loss: 0.8032 || timer: 0.0821 sec.
iter 190540 || Loss: 1.1469 || timer: 0.0994 sec.
iter 190550 || Loss: 0.9275 || timer: 0.0854 sec.
iter 190560 || Loss: 0.7437 || timer: 0.0860 sec.
iter 190570 || Loss: 0.9602 || timer: 0.0930 sec.
iter 190580 || Loss: 1.3462 || timer: 0.0770 sec.
iter 190590 || Loss: 0.9495 || timer: 0.0813 sec.
iter 190600 || Loss: 0.7418 || timer: 0.0940 sec.
iter 190610 || Loss: 0.9216 || timer: 0.0876 sec.
iter 190620 || Loss: 1.0024 || timer: 0.0837 sec.
iter 190630 || Loss: 0.7140 || timer: 0.0194 sec.
iter 190640 || Loss: 2.3952 || timer: 0.0899 sec.
iter 190650 || Loss: 0.7949 || timer: 0.0899 sec.
iter 190660 || Loss: 1.0173 || timer: 0.1086 sec.
iter 190670 || Loss: 0.7680 || timer: 0.0839 sec.
iter 190680 || Loss: 0.9985 || timer: 0.0961 sec.
iter 190690 || Loss: 1.0199 || timer: 0.0880 sec.
iter 190700 || Loss: 0.8186 || timer: 0.0846 sec.
iter 190710 || Loss: 0.9947 || timer: 0.0939 sec.
iter 190720 || Loss: 1.4303 || timer: 0.0843 sec.
iter 190730 || Loss: 0.7063 || timer: 0.1165 sec.
iter 190740 || Loss: 0.8925 || timer: 0.0838 sec.
iter 190750 || Loss: 0.9633 || timer: 0.0840 sec.
iter 190760 || Loss: 0.9220 || timer: 0.0920 sec.
iter 190770 || Loss: 0.8643 || timer: 0.0978 sec.
iter 190780 || Loss: 0.6585 || timer: 0.1031 sec.
iter 190790 || Loss: 0.9336 || timer: 0.0903 sec.
iter 190800 || Loss: 0.9536 || timer: 0.0985 sec.
iter 190810 || Loss: 0.7066 || timer: 0.0900 sec.
iter 190820 || Loss: 0.7543 || timer: 0.0848 sec.
iter 190830 || Loss: 0.9117 || timer: 0.0877 sec.
iter 190840 || Loss: 1.2442 || timer: 0.1002 sec.
iter 190850 || Loss: 1.1022 || timer: 0.0890 sec.
iter 190860 || Loss: 0.7324 || timer: 0.0876 sec.
iter 190870 || Loss: 1.3319 || timer: 0.0961 sec.
iter 190880 || Loss: 1.1490 || timer: 0.0828 sec.
iter 190890 || Loss: 0.5208 || timer: 0.0901 sec.
iter 190900 || Loss: 0.8148 || timer: 0.0841 sec.
iter 190910 || Loss: 1.2318 || timer: 0.0934 sec.
iter 190920 || Loss: 0.8877 || timer: 0.0915 sec.
iter 190930 || Loss: 0.8353 || timer: 0.0957 sec.
iter 190940 || Loss: 1.0151 || timer: 0.0913 sec.
iter 190950 || Loss: 1.0348 || timer: 0.0915 sec.
iter 190960 || Loss: 0.7972 || timer: 0.0181 sec.
iter 190970 || Loss: 1.4410 || timer: 0.0848 sec.
iter 190980 || Loss: 1.2443 || timer: 0.0846 sec.
iter 190990 || Loss: 1.2648 || timer: 0.0951 sec.
iter 191000 || Loss: 0.8936 || timer: 0.0775 sec.
iter 191010 || Loss: 0.7351 || timer: 0.0909 sec.
iter 191020 || Loss: 0.9081 || timer: 0.0922 sec.
iter 191030 || Loss: 1.0355 || timer: 0.0916 sec.
iter 191040 || Loss: 0.7515 || timer: 0.0927 sec.
iter 191050 || Loss: 1.0436 || timer: 0.0772 sec.
iter 191060 || Loss: 1.0475 || timer: 0.1065 sec.
iter 191070 || Loss: 0.8419 || timer: 0.1040 sec.
iter 191080 || Loss: 0.9245 || timer: 0.1020 sec.
iter 191090 || Loss: 0.7430 || timer: 0.0755 sec.
iter 191100 || Loss: 0.8214 || timer: 0.0823 sec.
iter 191110 || Loss: 1.0190 || timer: 0.0969 sec.
iter 191120 || Loss: 0.9850 || timer: 0.0756 sec.
iter 191130 || Loss: 0.7969 || timer: 0.0840 sec.
iter 191140 || Loss: 0.9297 || timer: 0.0865 sec.
iter 191150 || Loss: 0.7360 || timer: 0.0903 sec.
iter 191160 || Loss: 0.7744 || timer: 0.0840 sec.
iter 191170 || Loss: 0.9180 || timer: 0.0935 sec.
iter 191180 || Loss: 0.9163 || timer: 0.1089 sec.
iter 191190 || Loss: 0.9443 || timer: 0.0842 sec.
iter 191200 || Loss: 0.8823 || timer: 0.1189 sec.
iter 191210 || Loss: 0.6358 || timer: 0.0848 sec.
iter 191220 || Loss: 1.0606 || timer: 0.0897 sec.
iter 191230 || Loss: 0.8115 || timer: 0.0909 sec.
iter 191240 || Loss: 1.0678 || timer: 0.0791 sec.
iter 191250 || Loss: 0.7057 || timer: 0.0832 sec.
iter 191260 || Loss: 1.0770 || timer: 0.0898 sec.
iter 191270 || Loss: 0.7959 || timer: 0.0900 sec.
iter 191280 || Loss: 1.0789 || timer: 0.0903 sec.
iter 191290 || Loss: 0.7581 || timer: 0.0250 sec.
iter 191300 || Loss: 0.7640 || timer: 0.0844 sec.
iter 191310 || Loss: 1.1116 || timer: 0.0765 sec.
iter 191320 || Loss: 1.1010 || timer: 0.0750 sec.
iter 191330 || Loss: 0.8555 || timer: 0.0874 sec.
iter 191340 || Loss: 1.1133 || timer: 0.0904 sec.
iter 191350 || Loss: 0.8378 || timer: 0.0978 sec.
iter 191360 || Loss: 0.7343 || timer: 0.0840 sec.
iter 191370 || Loss: 0.9017 || timer: 0.0823 sec.
iter 191380 || Loss: 0.8136 || timer: 0.1085 sec.
iter 191390 || Loss: 1.3450 || timer: 0.1022 sec.
iter 191400 || Loss: 0.8978 || timer: 0.1073 sec.
iter 191410 || Loss: 1.0068 || timer: 0.0906 sec.
iter 191420 || Loss: 0.8261 || timer: 0.0911 sec.
iter 191430 || Loss: 0.9503 || timer: 0.0872 sec.
iter 191440 || Loss: 0.6158 || timer: 0.0863 sec.
iter 191450 || Loss: 0.8949 || timer: 0.0901 sec.
iter 191460 || Loss: 0.9035 || timer: 0.0898 sec.
iter 191470 || Loss: 0.8780 || timer: 0.0827 sec.
iter 191480 || Loss: 1.0092 || timer: 0.1088 sec.
iter 191490 || Loss: 0.8051 || timer: 0.0909 sec.
iter 191500 || Loss: 0.8024 || timer: 0.0837 sec.
iter 191510 || Loss: 1.0798 || timer: 0.0843 sec.
iter 191520 || Loss: 0.7429 || timer: 0.0755 sec.
iter 191530 || Loss: 0.5577 || timer: 0.0986 sec.
iter 191540 || Loss: 1.3095 || timer: 0.1043 sec.
iter 191550 || Loss: 0.7536 || timer: 0.1067 sec.
iter 191560 || Loss: 0.6314 || timer: 0.0905 sec.
iter 191570 || Loss: 0.7370 || timer: 0.0839 sec.
iter 191580 || Loss: 0.8339 || timer: 0.0909 sec.
iter 191590 || Loss: 0.9877 || timer: 0.0835 sec.
iter 191600 || Loss: 1.1008 || timer: 0.1075 sec.
iter 191610 || Loss: 1.2488 || timer: 0.1135 sec.
iter 191620 || Loss: 0.9866 || timer: 0.0287 sec.
iter 191630 || Loss: 1.6632 || timer: 0.1032 sec.
iter 191640 || Loss: 0.9243 || timer: 0.0940 sec.
iter 191650 || Loss: 0.9936 || timer: 0.0936 sec.
iter 191660 || Loss: 0.6425 || timer: 0.0891 sec.
iter 191670 || Loss: 0.9567 || timer: 0.0968 sec.
iter 191680 || Loss: 0.5881 || timer: 0.0824 sec.
iter 191690 || Loss: 1.0469 || timer: 0.0929 sec.
iter 191700 || Loss: 1.1509 || timer: 0.1035 sec.
iter 191710 || Loss: 1.0143 || timer: 0.0844 sec.
iter 191720 || Loss: 0.8588 || timer: 0.0970 sec.
iter 191730 || Loss: 0.7338 || timer: 0.0907 sec.
iter 191740 || Loss: 0.7520 || timer: 0.0826 sec.
iter 191750 || Loss: 0.9467 || timer: 0.0810 sec.
iter 191760 || Loss: 1.0825 || timer: 0.0748 sec.
iter 191770 || Loss: 0.8277 || timer: 0.1030 sec.
iter 191780 || Loss: 1.0712 || timer: 0.0845 sec.
iter 191790 || Loss: 0.9537 || timer: 0.0904 sec.
iter 191800 || Loss: 0.8547 || timer: 0.1060 sec.
iter 191810 || Loss: 1.0322 || timer: 0.0845 sec.
iter 191820 || Loss: 1.0020 || timer: 0.0843 sec.
iter 191830 || Loss: 1.0804 || timer: 0.0841 sec.
iter 191840 || Loss: 1.1550 || timer: 0.1004 sec.
iter 191850 || Loss: 1.0962 || timer: 0.0820 sec.
iter 191860 || Loss: 0.9550 || timer: 0.0849 sec.
iter 191870 || Loss: 0.8359 || timer: 0.0911 sec.
iter 191880 || Loss: 1.0719 || timer: 0.0828 sec.
iter 191890 || Loss: 1.0339 || timer: 0.0840 sec.
iter 191900 || Loss: 0.9969 || timer: 0.0856 sec.
iter 191910 || Loss: 1.0835 || timer: 0.0845 sec.
iter 191920 || Loss: 0.7153 || timer: 0.0911 sec.
iter 191930 || Loss: 1.0892 || timer: 0.1140 sec.
iter 191940 || Loss: 0.8576 || timer: 0.1068 sec.
iter 191950 || Loss: 1.3169 || timer: 0.0225 sec.
iter 191960 || Loss: 0.1079 || timer: 0.0846 sec.
iter 191970 || Loss: 0.9344 || timer: 0.0839 sec.
iter 191980 || Loss: 0.8514 || timer: 0.0844 sec.
iter 191990 || Loss: 0.7900 || timer: 0.0873 sec.
iter 192000 || Loss: 1.0018 || timer: 0.0814 sec.
iter 192010 || Loss: 0.8011 || timer: 0.0895 sec.
iter 192020 || Loss: 0.7840 || timer: 0.0829 sec.
iter 192030 || Loss: 1.1145 || timer: 0.0910 sec.
iter 192040 || Loss: 1.1419 || timer: 0.1043 sec.
iter 192050 || Loss: 0.8917 || timer: 0.1098 sec.
iter 192060 || Loss: 1.2160 || timer: 0.0888 sec.
iter 192070 || Loss: 1.3718 || timer: 0.0767 sec.
iter 192080 || Loss: 0.9723 || timer: 0.0755 sec.
iter 192090 || Loss: 0.7658 || timer: 0.0973 sec.
iter 192100 || Loss: 0.8608 || timer: 0.0842 sec.
iter 192110 || Loss: 0.8252 || timer: 0.1150 sec.
iter 192120 || Loss: 0.7292 || timer: 0.0845 sec.
iter 192130 || Loss: 0.9534 || timer: 0.0875 sec.
iter 192140 || Loss: 0.7519 || timer: 0.0912 sec.
iter 192150 || Loss: 0.7159 || timer: 0.0947 sec.
iter 192160 || Loss: 1.3006 || timer: 0.0905 sec.
iter 192170 || Loss: 1.1363 || timer: 0.0904 sec.
iter 192180 || Loss: 1.1178 || timer: 0.0840 sec.
iter 192190 || Loss: 0.7297 || timer: 0.1039 sec.
iter 192200 || Loss: 1.0148 || timer: 0.1032 sec.
iter 192210 || Loss: 1.2008 || timer: 0.0900 sec.
iter 192220 || Loss: 0.9606 || timer: 0.0896 sec.
iter 192230 || Loss: 1.0325 || timer: 0.0906 sec.
iter 192240 || Loss: 1.1741 || timer: 0.0856 sec.
iter 192250 || Loss: 1.4148 || timer: 0.0894 sec.
iter 192260 || Loss: 1.1037 || timer: 0.0904 sec.
iter 192270 || Loss: 0.9818 || timer: 0.0914 sec.
iter 192280 || Loss: 0.7819 || timer: 0.0209 sec.
iter 192290 || Loss: 0.2605 || timer: 0.1036 sec.
iter 192300 || Loss: 0.9155 || timer: 0.0842 sec.
iter 192310 || Loss: 0.7369 || timer: 0.0836 sec.
iter 192320 || Loss: 0.6705 || timer: 0.0952 sec.
iter 192330 || Loss: 0.8901 || timer: 0.0981 sec.
iter 192340 || Loss: 0.9226 || timer: 0.0893 sec.
iter 192350 || Loss: 0.8888 || timer: 0.1139 sec.
iter 192360 || Loss: 0.7668 || timer: 0.0863 sec.
iter 192370 || Loss: 0.9212 || timer: 0.0879 sec.
iter 192380 || Loss: 0.7709 || timer: 0.1232 sec.
iter 192390 || Loss: 0.8458 || timer: 0.0958 sec.
iter 192400 || Loss: 1.1948 || timer: 0.0871 sec.
iter 192410 || Loss: 0.9388 || timer: 0.0907 sec.
iter 192420 || Loss: 0.9435 || timer: 0.0912 sec.
iter 192430 || Loss: 1.0605 || timer: 0.0910 sec.
iter 192440 || Loss: 0.8528 || timer: 0.0844 sec.
iter 192450 || Loss: 0.8604 || timer: 0.0912 sec.
iter 192460 || Loss: 1.2984 || timer: 0.0838 sec.
iter 192470 || Loss: 0.9265 || timer: 0.0939 sec.
iter 192480 || Loss: 0.6380 || timer: 0.0835 sec.
iter 192490 || Loss: 0.8070 || timer: 0.0916 sec.
iter 192500 || Loss: 1.1734 || timer: 0.0842 sec.
iter 192510 || Loss: 1.1466 || timer: 0.0911 sec.
iter 192520 || Loss: 0.9969 || timer: 0.0912 sec.
iter 192530 || Loss: 0.9327 || timer: 0.1039 sec.
iter 192540 || Loss: 0.7783 || timer: 0.0922 sec.
iter 192550 || Loss: 1.0183 || timer: 0.0907 sec.
iter 192560 || Loss: 0.8260 || timer: 0.0913 sec.
iter 192570 || Loss: 1.0292 || timer: 0.0923 sec.
iter 192580 || Loss: 0.6942 || timer: 0.0906 sec.
iter 192590 || Loss: 0.6547 || timer: 0.0854 sec.
iter 192600 || Loss: 0.9639 || timer: 0.0829 sec.
iter 192610 || Loss: 0.9388 || timer: 0.0242 sec.
iter 192620 || Loss: 0.2155 || timer: 0.0983 sec.
iter 192630 || Loss: 0.8414 || timer: 0.0968 sec.
iter 192640 || Loss: 0.6420 || timer: 0.0906 sec.
iter 192650 || Loss: 0.9085 || timer: 0.1190 sec.
iter 192660 || Loss: 1.0632 || timer: 0.0885 sec.
iter 192670 || Loss: 0.8931 || timer: 0.0923 sec.
iter 192680 || Loss: 1.1661 || timer: 0.0840 sec.
iter 192690 || Loss: 0.8250 || timer: 0.0942 sec.
iter 192700 || Loss: 0.8128 || timer: 0.1111 sec.
iter 192710 || Loss: 0.9168 || timer: 0.1028 sec.
iter 192720 || Loss: 0.6233 || timer: 0.0878 sec.
iter 192730 || Loss: 0.8274 || timer: 0.0953 sec.
iter 192740 || Loss: 0.7299 || timer: 0.0866 sec.
iter 192750 || Loss: 1.2154 || timer: 0.0918 sec.
iter 192760 || Loss: 0.9663 || timer: 0.1124 sec.
iter 192770 || Loss: 0.7576 || timer: 0.1026 sec.
iter 192780 || Loss: 1.0089 || timer: 0.0892 sec.
iter 192790 || Loss: 1.0428 || timer: 0.0995 sec.
iter 192800 || Loss: 0.9568 || timer: 0.0985 sec.
iter 192810 || Loss: 0.7116 || timer: 0.0840 sec.
iter 192820 || Loss: 0.7218 || timer: 0.0906 sec.
iter 192830 || Loss: 0.9021 || timer: 0.0974 sec.
iter 192840 || Loss: 0.9365 || timer: 0.0821 sec.
iter 192850 || Loss: 1.0484 || timer: 0.0912 sec.
iter 192860 || Loss: 0.9080 || timer: 0.0903 sec.
iter 192870 || Loss: 0.8365 || timer: 0.0841 sec.
iter 192880 || Loss: 0.7113 || timer: 0.0839 sec.
iter 192890 || Loss: 0.9705 || timer: 0.0884 sec.
iter 192900 || Loss: 0.9249 || timer: 0.1021 sec.
iter 192910 || Loss: 1.6477 || timer: 0.0838 sec.
iter 192920 || Loss: 0.9933 || timer: 0.1053 sec.
iter 192930 || Loss: 0.8481 || timer: 0.0918 sec.
iter 192940 || Loss: 0.9855 || timer: 0.0286 sec.
iter 192950 || Loss: 0.9117 || timer: 0.0942 sec.
iter 192960 || Loss: 0.6704 || timer: 0.0914 sec.
iter 192970 || Loss: 1.1590 || timer: 0.0852 sec.
iter 192980 || Loss: 0.8802 || timer: 0.0907 sec.
iter 192990 || Loss: 0.9471 || timer: 0.0940 sec.
iter 193000 || Loss: 1.0831 || timer: 0.0837 sec.
iter 193010 || Loss: 0.8508 || timer: 0.0911 sec.
iter 193020 || Loss: 0.9653 || timer: 0.0935 sec.
iter 193030 || Loss: 1.0174 || timer: 0.0908 sec.
iter 193040 || Loss: 0.7051 || timer: 0.0978 sec.
iter 193050 || Loss: 0.7108 || timer: 0.0913 sec.
iter 193060 || Loss: 0.9167 || timer: 0.0952 sec.
iter 193070 || Loss: 0.9159 || timer: 0.0761 sec.
iter 193080 || Loss: 0.8949 || timer: 0.0767 sec.
iter 193090 || Loss: 1.0652 || timer: 0.0827 sec.
iter 193100 || Loss: 0.9302 || timer: 0.1057 sec.
iter 193110 || Loss: 0.9291 || timer: 0.0905 sec.
iter 193120 || Loss: 1.1899 || timer: 0.0769 sec.
iter 193130 || Loss: 0.4980 || timer: 0.0861 sec.
iter 193140 || Loss: 0.6092 || timer: 0.0918 sec.
iter 193150 || Loss: 1.2274 || timer: 0.1246 sec.
iter 193160 || Loss: 0.9016 || timer: 0.0940 sec.
iter 193170 || Loss: 0.9054 || timer: 0.0911 sec.
iter 193180 || Loss: 1.1810 || timer: 0.0836 sec.
iter 193190 || Loss: 1.0020 || timer: 0.0879 sec.
iter 193200 || Loss: 0.8926 || timer: 0.0822 sec.
iter 193210 || Loss: 0.9084 || timer: 0.0887 sec.
iter 193220 || Loss: 1.0042 || timer: 0.1152 sec.
iter 193230 || Loss: 0.8029 || timer: 0.0900 sec.
iter 193240 || Loss: 0.8881 || timer: 0.0839 sec.
iter 193250 || Loss: 1.0048 || timer: 0.0884 sec.
iter 193260 || Loss: 1.0524 || timer: 0.0892 sec.
iter 193270 || Loss: 1.0016 || timer: 0.0244 sec.
iter 193280 || Loss: 0.7134 || timer: 0.0929 sec.
iter 193290 || Loss: 0.7133 || timer: 0.1003 sec.
iter 193300 || Loss: 0.7788 || timer: 0.0822 sec.
iter 193310 || Loss: 0.6641 || timer: 0.1023 sec.
iter 193320 || Loss: 0.9350 || timer: 0.0941 sec.
iter 193330 || Loss: 0.8582 || timer: 0.0902 sec.
iter 193340 || Loss: 0.6820 || timer: 0.1082 sec.
iter 193350 || Loss: 0.8933 || timer: 0.0885 sec.
iter 193360 || Loss: 0.9854 || timer: 0.0933 sec.
iter 193370 || Loss: 1.0457 || timer: 0.0942 sec.
iter 193380 || Loss: 0.9016 || timer: 0.0892 sec.
iter 193390 || Loss: 0.7385 || timer: 0.0926 sec.
iter 193400 || Loss: 0.7862 || timer: 0.0919 sec.
iter 193410 || Loss: 0.8462 || timer: 0.0922 sec.
iter 193420 || Loss: 0.9043 || timer: 0.0900 sec.
iter 193430 || Loss: 0.7972 || timer: 0.0909 sec.
iter 193440 || Loss: 0.7833 || timer: 0.1036 sec.
iter 193450 || Loss: 1.4053 || timer: 0.1043 sec.
iter 193460 || Loss: 0.8935 || timer: 0.0911 sec.
iter 193470 || Loss: 1.1052 || timer: 0.0901 sec.
iter 193480 || Loss: 0.8577 || timer: 0.0829 sec.
iter 193490 || Loss: 0.9531 || timer: 0.1025 sec.
iter 193500 || Loss: 0.8482 || timer: 0.0937 sec.
iter 193510 || Loss: 0.9675 || timer: 0.0910 sec.
iter 193520 || Loss: 0.9842 || timer: 0.0963 sec.
iter 193530 || Loss: 0.6747 || timer: 0.0832 sec.
iter 193540 || Loss: 0.7792 || timer: 0.0918 sec.
iter 193550 || Loss: 1.3106 || timer: 0.0931 sec.
iter 193560 || Loss: 0.9669 || timer: 0.0840 sec.
iter 193570 || Loss: 0.6883 || timer: 0.0897 sec.
iter 193580 || Loss: 1.5375 || timer: 0.0845 sec.
iter 193590 || Loss: 0.8789 || timer: 0.0924 sec.
iter 193600 || Loss: 0.8255 || timer: 0.0241 sec.
iter 193610 || Loss: 1.1983 || timer: 0.0887 sec.
iter 193620 || Loss: 0.8666 || timer: 0.0845 sec.
iter 193630 || Loss: 0.9315 || timer: 0.0829 sec.
iter 193640 || Loss: 0.7806 || timer: 0.0852 sec.
iter 193650 || Loss: 0.9162 || timer: 0.0975 sec.
iter 193660 || Loss: 0.8947 || timer: 0.0836 sec.
iter 193670 || Loss: 0.8673 || timer: 0.1067 sec.
iter 193680 || Loss: 1.2609 || timer: 0.0953 sec.
iter 193690 || Loss: 0.9101 || timer: 0.1140 sec.
iter 193700 || Loss: 1.0051 || timer: 0.0981 sec.
iter 193710 || Loss: 0.6660 || timer: 0.0841 sec.
iter 193720 || Loss: 0.6945 || timer: 0.0915 sec.
iter 193730 || Loss: 1.1140 || timer: 0.0927 sec.
iter 193740 || Loss: 0.6656 || timer: 0.0919 sec.
iter 193750 || Loss: 0.8978 || timer: 0.0916 sec.
iter 193760 || Loss: 1.0598 || timer: 0.0877 sec.
iter 193770 || Loss: 0.9861 || timer: 0.1112 sec.
iter 193780 || Loss: 0.9297 || timer: 0.0844 sec.
iter 193790 || Loss: 0.7689 || timer: 0.0908 sec.
iter 193800 || Loss: 0.9047 || timer: 0.0843 sec.
iter 193810 || Loss: 1.0291 || timer: 0.0898 sec.
iter 193820 || Loss: 1.2530 || timer: 0.0933 sec.
iter 193830 || Loss: 0.8422 || timer: 0.0914 sec.
iter 193840 || Loss: 1.0098 || timer: 0.0929 sec.
iter 193850 || Loss: 0.7936 || timer: 0.0994 sec.
iter 193860 || Loss: 1.2233 || timer: 0.0869 sec.
iter 193870 || Loss: 1.0599 || timer: 0.0910 sec.
iter 193880 || Loss: 1.1086 || timer: 0.1306 sec.
iter 193890 || Loss: 0.8392 || timer: 0.0803 sec.
iter 193900 || Loss: 0.7077 || timer: 0.0949 sec.
iter 193910 || Loss: 0.6128 || timer: 0.0898 sec.
iter 193920 || Loss: 0.9679 || timer: 0.0928 sec.
iter 193930 || Loss: 1.0106 || timer: 0.0212 sec.
iter 193940 || Loss: 0.3884 || timer: 0.0821 sec.
iter 193950 || Loss: 0.7962 || timer: 0.1027 sec.
iter 193960 || Loss: 1.0502 || timer: 0.0921 sec.
iter 193970 || Loss: 1.3778 || timer: 0.0892 sec.
iter 193980 || Loss: 0.8209 || timer: 0.0819 sec.
iter 193990 || Loss: 0.7842 || timer: 0.0894 sec.
iter 194000 || Loss: 0.7067 || timer: 0.0878 sec.
iter 194010 || Loss: 0.6601 || timer: 0.0821 sec.
iter 194020 || Loss: 0.5471 || timer: 0.1109 sec.
iter 194030 || Loss: 0.8676 || timer: 0.1089 sec.
iter 194040 || Loss: 0.8193 || timer: 0.0892 sec.
iter 194050 || Loss: 0.8270 || timer: 0.0879 sec.
iter 194060 || Loss: 0.6735 || timer: 0.0902 sec.
iter 194070 || Loss: 1.2046 || timer: 0.1169 sec.
iter 194080 || Loss: 0.9612 || timer: 0.0967 sec.
iter 194090 || Loss: 1.0719 || timer: 0.0825 sec.
iter 194100 || Loss: 1.0572 || timer: 0.0966 sec.
iter 194110 || Loss: 0.9991 || timer: 0.0897 sec.
iter 194120 || Loss: 0.9916 || timer: 0.0904 sec.
iter 194130 || Loss: 1.1002 || timer: 0.0960 sec.
iter 194140 || Loss: 0.8975 || timer: 0.0832 sec.
iter 194150 || Loss: 0.7476 || timer: 0.0836 sec.
iter 194160 || Loss: 0.8546 || timer: 0.0850 sec.
iter 194170 || Loss: 0.7102 || timer: 0.0910 sec.
iter 194180 || Loss: 1.2118 || timer: 0.0865 sec.
iter 194190 || Loss: 0.9869 || timer: 0.0940 sec.
iter 194200 || Loss: 0.7512 || timer: 0.0837 sec.
iter 194210 || Loss: 0.8388 || timer: 0.1024 sec.
iter 194220 || Loss: 0.9949 || timer: 0.0906 sec.
iter 194230 || Loss: 0.8690 || timer: 0.0909 sec.
iter 194240 || Loss: 1.0335 || timer: 0.0915 sec.
iter 194250 || Loss: 0.7058 || timer: 0.1029 sec.
iter 194260 || Loss: 0.8415 || timer: 0.0279 sec.
iter 194270 || Loss: 0.8412 || timer: 0.0891 sec.
iter 194280 || Loss: 0.7585 || timer: 0.1249 sec.
iter 194290 || Loss: 0.8189 || timer: 0.0906 sec.
iter 194300 || Loss: 1.1112 || timer: 0.0951 sec.
iter 194310 || Loss: 1.1779 || timer: 0.0911 sec.
iter 194320 || Loss: 0.9591 || timer: 0.0944 sec.
iter 194330 || Loss: 1.3041 || timer: 0.0924 sec.
iter 194340 || Loss: 0.8356 || timer: 0.0905 sec.
iter 194350 || Loss: 0.7027 || timer: 0.0905 sec.
iter 194360 || Loss: 0.8614 || timer: 0.1124 sec.
iter 194370 || Loss: 0.9717 || timer: 0.0935 sec.
iter 194380 || Loss: 1.3217 || timer: 0.1108 sec.
iter 194390 || Loss: 0.8266 || timer: 0.0837 sec.
iter 194400 || Loss: 0.8993 || timer: 0.0902 sec.
iter 194410 || Loss: 0.8277 || timer: 0.0900 sec.
iter 194420 || Loss: 0.7191 || timer: 0.0917 sec.
iter 194430 || Loss: 1.2522 || timer: 0.0919 sec.
iter 194440 || Loss: 0.7918 || timer: 0.0903 sec.
iter 194450 || Loss: 0.9480 || timer: 0.0852 sec.
iter 194460 || Loss: 0.9544 || timer: 0.0850 sec.
iter 194470 || Loss: 0.7790 || timer: 0.0840 sec.
iter 194480 || Loss: 0.5440 || timer: 0.0898 sec.
iter 194490 || Loss: 0.9692 || timer: 0.0813 sec.
iter 194500 || Loss: 0.9187 || timer: 0.0890 sec.
iter 194510 || Loss: 0.7599 || timer: 0.0903 sec.
iter 194520 || Loss: 0.7226 || timer: 0.0975 sec.
iter 194530 || Loss: 0.6933 || timer: 0.0898 sec.
iter 194540 || Loss: 0.9625 || timer: 0.0993 sec.
iter 194550 || Loss: 0.9389 || timer: 0.0885 sec.
iter 194560 || Loss: 1.0154 || timer: 0.0892 sec.
iter 194570 || Loss: 0.9196 || timer: 0.0808 sec.
iter 194580 || Loss: 1.2146 || timer: 0.0824 sec.
iter 194590 || Loss: 0.9064 || timer: 0.0230 sec.
iter 194600 || Loss: 0.5452 || timer: 0.0935 sec.
iter 194610 || Loss: 0.6559 || timer: 0.0941 sec.
iter 194620 || Loss: 0.9384 || timer: 0.0903 sec.
iter 194630 || Loss: 0.8263 || timer: 0.0964 sec.
iter 194640 || Loss: 0.9474 || timer: 0.1252 sec.
iter 194650 || Loss: 0.6452 || timer: 0.1064 sec.
iter 194660 || Loss: 0.9772 || timer: 0.0908 sec.
iter 194670 || Loss: 1.0451 || timer: 0.0930 sec.
iter 194680 || Loss: 1.0988 || timer: 0.0872 sec.
iter 194690 || Loss: 0.9717 || timer: 0.1154 sec.
iter 194700 || Loss: 0.8249 || timer: 0.0825 sec.
iter 194710 || Loss: 0.7772 || timer: 0.0898 sec.
iter 194720 || Loss: 0.8358 || timer: 0.0820 sec.
iter 194730 || Loss: 0.8466 || timer: 0.0919 sec.
iter 194740 || Loss: 0.9868 || timer: 0.0819 sec.
iter 194750 || Loss: 0.8740 || timer: 0.0911 sec.
iter 194760 || Loss: 0.7022 || timer: 0.0837 sec.
iter 194770 || Loss: 0.9297 || timer: 0.0845 sec.
iter 194780 || Loss: 0.9026 || timer: 0.1017 sec.
iter 194790 || Loss: 1.2327 || timer: 0.1189 sec.
iter 194800 || Loss: 1.5323 || timer: 0.0845 sec.
iter 194810 || Loss: 1.6187 || timer: 0.0888 sec.
iter 194820 || Loss: 1.3594 || timer: 0.0810 sec.
iter 194830 || Loss: 1.3466 || timer: 0.1100 sec.
iter 194840 || Loss: 1.0434 || timer: 0.0930 sec.
iter 194850 || Loss: 1.2079 || timer: 0.0828 sec.
iter 194860 || Loss: 0.9371 || timer: 0.0829 sec.
iter 194870 || Loss: 1.3370 || timer: 0.0890 sec.
iter 194880 || Loss: 1.0626 || timer: 0.0931 sec.
iter 194890 || Loss: 0.9739 || timer: 0.0771 sec.
iter 194900 || Loss: 1.8927 || timer: 0.0904 sec.
iter 194910 || Loss: 1.0304 || timer: 0.1089 sec.
iter 194920 || Loss: 0.9943 || timer: 0.0239 sec.
iter 194930 || Loss: 0.8671 || timer: 0.0971 sec.
iter 194940 || Loss: 1.0859 || timer: 0.0843 sec.
iter 194950 || Loss: 0.7836 || timer: 0.0758 sec.
iter 194960 || Loss: 0.8928 || timer: 0.0874 sec.
iter 194970 || Loss: 1.1626 || timer: 0.0895 sec.
iter 194980 || Loss: 0.9549 || timer: 0.0907 sec.
iter 194990 || Loss: 0.7928 || timer: 0.0844 sec.
iter 195000 || Loss: 1.2818 || Saving state, iter: 195000
timer: 0.1017 sec.
iter 195010 || Loss: 0.8104 || timer: 0.0834 sec.
iter 195020 || Loss: 1.0387 || timer: 0.1085 sec.
iter 195030 || Loss: 0.8683 || timer: 0.1148 sec.
iter 195040 || Loss: 1.2140 || timer: 0.0941 sec.
iter 195050 || Loss: 0.7186 || timer: 0.0904 sec.
iter 195060 || Loss: 0.9166 || timer: 0.0842 sec.
iter 195070 || Loss: 0.9680 || timer: 0.0853 sec.
iter 195080 || Loss: 0.6499 || timer: 0.0881 sec.
iter 195090 || Loss: 0.9639 || timer: 0.0914 sec.
iter 195100 || Loss: 0.9796 || timer: 0.0849 sec.
iter 195110 || Loss: 0.9503 || timer: 0.0900 sec.
iter 195120 || Loss: 0.8853 || timer: 0.1036 sec.
iter 195130 || Loss: 1.3228 || timer: 0.0844 sec.
iter 195140 || Loss: 0.9602 || timer: 0.0923 sec.
iter 195150 || Loss: 0.8227 || timer: 0.0907 sec.
iter 195160 || Loss: 0.7965 || timer: 0.0901 sec.
iter 195170 || Loss: 0.8083 || timer: 0.0763 sec.
iter 195180 || Loss: 1.0640 || timer: 0.0841 sec.
iter 195190 || Loss: 1.0017 || timer: 0.0814 sec.
iter 195200 || Loss: 1.1714 || timer: 0.0898 sec.
iter 195210 || Loss: 1.2216 || timer: 0.0948 sec.
iter 195220 || Loss: 1.0032 || timer: 0.1065 sec.
iter 195230 || Loss: 1.1525 || timer: 0.0908 sec.
iter 195240 || Loss: 0.9670 || timer: 0.1076 sec.
iter 195250 || Loss: 0.7790 || timer: 0.0191 sec.
iter 195260 || Loss: 0.7921 || timer: 0.1038 sec.
iter 195270 || Loss: 1.0112 || timer: 0.0841 sec.
iter 195280 || Loss: 0.8956 || timer: 0.0901 sec.
iter 195290 || Loss: 0.9961 || timer: 0.0831 sec.
iter 195300 || Loss: 0.8091 || timer: 0.1166 sec.
iter 195310 || Loss: 0.8920 || timer: 0.0879 sec.
iter 195320 || Loss: 0.8644 || timer: 0.0899 sec.
iter 195330 || Loss: 1.0624 || timer: 0.0840 sec.
iter 195340 || Loss: 0.6132 || timer: 0.0916 sec.
iter 195350 || Loss: 1.0389 || timer: 0.1200 sec.
iter 195360 || Loss: 0.9649 || timer: 0.0955 sec.
iter 195370 || Loss: 0.7954 || timer: 0.0832 sec.
iter 195380 || Loss: 1.1604 || timer: 0.0924 sec.
iter 195390 || Loss: 0.8152 || timer: 0.0882 sec.
iter 195400 || Loss: 0.5453 || timer: 0.0846 sec.
iter 195410 || Loss: 0.6870 || timer: 0.0918 sec.
iter 195420 || Loss: 0.9323 || timer: 0.0846 sec.
iter 195430 || Loss: 0.8142 || timer: 0.1009 sec.
iter 195440 || Loss: 0.8513 || timer: 0.0866 sec.
iter 195450 || Loss: 0.8699 || timer: 0.0862 sec.
iter 195460 || Loss: 1.0072 || timer: 0.1032 sec.
iter 195470 || Loss: 1.0603 || timer: 0.0908 sec.
iter 195480 || Loss: 0.9106 || timer: 0.0907 sec.
iter 195490 || Loss: 1.0112 || timer: 0.0848 sec.
iter 195500 || Loss: 0.9630 || timer: 0.0980 sec.
iter 195510 || Loss: 0.7272 || timer: 0.0930 sec.
iter 195520 || Loss: 1.0236 || timer: 0.0897 sec.
iter 195530 || Loss: 0.7054 || timer: 0.0930 sec.
iter 195540 || Loss: 1.0763 || timer: 0.0927 sec.
iter 195550 || Loss: 0.7480 || timer: 0.0846 sec.
iter 195560 || Loss: 0.8024 || timer: 0.0837 sec.
iter 195570 || Loss: 1.2287 || timer: 0.0905 sec.
iter 195580 || Loss: 0.8088 || timer: 0.0248 sec.
iter 195590 || Loss: 0.5218 || timer: 0.0842 sec.
iter 195600 || Loss: 0.9590 || timer: 0.0909 sec.
iter 195610 || Loss: 0.5941 || timer: 0.0915 sec.
iter 195620 || Loss: 0.8669 || timer: 0.0841 sec.
iter 195630 || Loss: 0.9024 || timer: 0.0908 sec.
iter 195640 || Loss: 0.7665 || timer: 0.0834 sec.
iter 195650 || Loss: 0.8092 || timer: 0.1090 sec.
iter 195660 || Loss: 0.8947 || timer: 0.1014 sec.
iter 195670 || Loss: 1.1034 || timer: 0.0894 sec.
iter 195680 || Loss: 1.0307 || timer: 0.1121 sec.
iter 195690 || Loss: 0.7110 || timer: 0.0843 sec.
iter 195700 || Loss: 0.9025 || timer: 0.1029 sec.
iter 195710 || Loss: 0.9024 || timer: 0.0846 sec.
iter 195720 || Loss: 0.7333 || timer: 0.1294 sec.
iter 195730 || Loss: 1.0292 || timer: 0.0972 sec.
iter 195740 || Loss: 0.9183 || timer: 0.0928 sec.
iter 195750 || Loss: 0.8341 || timer: 0.0829 sec.
iter 195760 || Loss: 0.7293 || timer: 0.0933 sec.
iter 195770 || Loss: 0.7215 || timer: 0.0892 sec.
iter 195780 || Loss: 1.2698 || timer: 0.1150 sec.
iter 195790 || Loss: 1.0633 || timer: 0.0927 sec.
iter 195800 || Loss: 0.7190 || timer: 0.0999 sec.
iter 195810 || Loss: 0.8056 || timer: 0.0838 sec.
iter 195820 || Loss: 1.2394 || timer: 0.0991 sec.
iter 195830 || Loss: 0.7638 || timer: 0.1037 sec.
iter 195840 || Loss: 0.8539 || timer: 0.0937 sec.
iter 195850 || Loss: 1.0084 || timer: 0.0920 sec.
iter 195860 || Loss: 1.0576 || timer: 0.0971 sec.
iter 195870 || Loss: 0.8633 || timer: 0.1100 sec.
iter 195880 || Loss: 0.8896 || timer: 0.0924 sec.
iter 195890 || Loss: 1.4960 || timer: 0.1061 sec.
iter 195900 || Loss: 1.1255 || timer: 0.1080 sec.
iter 195910 || Loss: 1.0694 || timer: 0.0173 sec.
iter 195920 || Loss: 1.2683 || timer: 0.0828 sec.
iter 195930 || Loss: 0.8192 || timer: 0.1106 sec.
iter 195940 || Loss: 0.7411 || timer: 0.0899 sec.
iter 195950 || Loss: 0.6463 || timer: 0.0838 sec.
iter 195960 || Loss: 0.7951 || timer: 0.1018 sec.
iter 195970 || Loss: 1.3386 || timer: 0.0995 sec.
iter 195980 || Loss: 0.9578 || timer: 0.1105 sec.
iter 195990 || Loss: 0.8830 || timer: 0.0841 sec.
iter 196000 || Loss: 0.7430 || timer: 0.0952 sec.
iter 196010 || Loss: 1.3379 || timer: 0.0958 sec.
iter 196020 || Loss: 1.3503 || timer: 0.0962 sec.
iter 196030 || Loss: 0.9429 || timer: 0.0839 sec.
iter 196040 || Loss: 1.0561 || timer: 0.0902 sec.
iter 196050 || Loss: 0.9112 || timer: 0.0830 sec.
iter 196060 || Loss: 0.7414 || timer: 0.0819 sec.
iter 196070 || Loss: 0.8242 || timer: 0.0882 sec.
iter 196080 || Loss: 1.4252 || timer: 0.1102 sec.
iter 196090 || Loss: 1.2162 || timer: 0.0842 sec.
iter 196100 || Loss: 0.7531 || timer: 0.1138 sec.
iter 196110 || Loss: 0.9258 || timer: 0.0838 sec.
iter 196120 || Loss: 0.8429 || timer: 0.0913 sec.
iter 196130 || Loss: 1.0995 || timer: 0.0838 sec.
iter 196140 || Loss: 0.7754 || timer: 0.0822 sec.
iter 196150 || Loss: 0.7823 || timer: 0.0837 sec.
iter 196160 || Loss: 1.0740 || timer: 0.0839 sec.
iter 196170 || Loss: 0.9329 || timer: 0.0907 sec.
iter 196180 || Loss: 0.8811 || timer: 0.0854 sec.
iter 196190 || Loss: 0.8702 || timer: 0.0868 sec.
iter 196200 || Loss: 1.0183 || timer: 0.0909 sec.
iter 196210 || Loss: 1.5130 || timer: 0.0838 sec.
iter 196220 || Loss: 0.8347 || timer: 0.0827 sec.
iter 196230 || Loss: 0.7822 || timer: 0.0911 sec.
iter 196240 || Loss: 0.8914 || timer: 0.0292 sec.
iter 196250 || Loss: 1.1069 || timer: 0.1147 sec.
iter 196260 || Loss: 1.0276 || timer: 0.1090 sec.
iter 196270 || Loss: 0.8417 || timer: 0.0897 sec.
iter 196280 || Loss: 1.1292 || timer: 0.1137 sec.
iter 196290 || Loss: 0.9011 || timer: 0.1053 sec.
iter 196300 || Loss: 0.8586 || timer: 0.1014 sec.
iter 196310 || Loss: 0.8066 || timer: 0.0891 sec.
iter 196320 || Loss: 0.8967 || timer: 0.1039 sec.
iter 196330 || Loss: 0.9771 || timer: 0.0767 sec.
iter 196340 || Loss: 0.8405 || timer: 0.1135 sec.
iter 196350 || Loss: 0.9724 || timer: 0.0905 sec.
iter 196360 || Loss: 0.9342 || timer: 0.0858 sec.
iter 196370 || Loss: 1.0408 || timer: 0.0809 sec.
iter 196380 || Loss: 0.7066 || timer: 0.0897 sec.
iter 196390 || Loss: 0.7364 || timer: 0.1069 sec.
iter 196400 || Loss: 1.0160 || timer: 0.0841 sec.
iter 196410 || Loss: 0.8849 || timer: 0.0929 sec.
iter 196420 || Loss: 0.9063 || timer: 0.0907 sec.
iter 196430 || Loss: 0.7603 || timer: 0.0819 sec.
iter 196440 || Loss: 0.8947 || timer: 0.0891 sec.
iter 196450 || Loss: 0.8557 || timer: 0.1012 sec.
iter 196460 || Loss: 1.3954 || timer: 0.1060 sec.
iter 196470 || Loss: 0.9155 || timer: 0.0902 sec.
iter 196480 || Loss: 0.8651 || timer: 0.0938 sec.
iter 196490 || Loss: 0.9366 || timer: 0.0891 sec.
iter 196500 || Loss: 0.7834 || timer: 0.1002 sec.
iter 196510 || Loss: 0.9466 || timer: 0.1071 sec.
iter 196520 || Loss: 1.0368 || timer: 0.0826 sec.
iter 196530 || Loss: 0.9644 || timer: 0.0840 sec.
iter 196540 || Loss: 1.0579 || timer: 0.0894 sec.
iter 196550 || Loss: 1.0038 || timer: 0.0920 sec.
iter 196560 || Loss: 1.0317 || timer: 0.0837 sec.
iter 196570 || Loss: 0.9162 || timer: 0.0238 sec.
iter 196580 || Loss: 0.3119 || timer: 0.0898 sec.
iter 196590 || Loss: 0.6640 || timer: 0.0835 sec.
iter 196600 || Loss: 0.9082 || timer: 0.0878 sec.
iter 196610 || Loss: 0.8396 || timer: 0.0930 sec.
iter 196620 || Loss: 1.0221 || timer: 0.0905 sec.
iter 196630 || Loss: 0.9650 || timer: 0.1056 sec.
iter 196640 || Loss: 0.9122 || timer: 0.0845 sec.
iter 196650 || Loss: 1.0531 || timer: 0.0842 sec.
iter 196660 || Loss: 1.0163 || timer: 0.1006 sec.
iter 196670 || Loss: 1.3147 || timer: 0.1031 sec.
iter 196680 || Loss: 0.7128 || timer: 0.0912 sec.
iter 196690 || Loss: 0.8017 || timer: 0.0882 sec.
iter 196700 || Loss: 0.7321 || timer: 0.0909 sec.
iter 196710 || Loss: 1.0395 || timer: 0.0882 sec.
iter 196720 || Loss: 0.8893 || timer: 0.1089 sec.
iter 196730 || Loss: 0.8993 || timer: 0.0905 sec.
iter 196740 || Loss: 0.8034 || timer: 0.0903 sec.
iter 196750 || Loss: 1.1171 || timer: 0.0923 sec.
iter 196760 || Loss: 0.4453 || timer: 0.0909 sec.
iter 196770 || Loss: 0.7508 || timer: 0.1167 sec.
iter 196780 || Loss: 0.5339 || timer: 0.0901 sec.
iter 196790 || Loss: 1.2037 || timer: 0.0908 sec.
iter 196800 || Loss: 0.8712 || timer: 0.0880 sec.
iter 196810 || Loss: 1.1732 || timer: 0.0841 sec.
iter 196820 || Loss: 1.0523 || timer: 0.0907 sec.
iter 196830 || Loss: 0.8284 || timer: 0.0909 sec.
iter 196840 || Loss: 1.0363 || timer: 0.0848 sec.
iter 196850 || Loss: 0.7495 || timer: 0.0911 sec.
iter 196860 || Loss: 0.6440 || timer: 0.1090 sec.
iter 196870 || Loss: 0.8336 || timer: 0.0910 sec.
iter 196880 || Loss: 0.7423 || timer: 0.1117 sec.
iter 196890 || Loss: 1.1221 || timer: 0.0847 sec.
iter 196900 || Loss: 1.3340 || timer: 0.0293 sec.
iter 196910 || Loss: 1.1415 || timer: 0.0884 sec.
iter 196920 || Loss: 1.0499 || timer: 0.0930 sec.
iter 196930 || Loss: 0.8495 || timer: 0.0856 sec.
iter 196940 || Loss: 0.7177 || timer: 0.1053 sec.
iter 196950 || Loss: 1.0490 || timer: 0.0908 sec.
iter 196960 || Loss: 1.0079 || timer: 0.0866 sec.
iter 196970 || Loss: 0.8744 || timer: 0.0926 sec.
iter 196980 || Loss: 0.8280 || timer: 0.0900 sec.
iter 196990 || Loss: 0.8371 || timer: 0.0800 sec.
iter 197000 || Loss: 0.5929 || timer: 0.0939 sec.
iter 197010 || Loss: 0.7287 || timer: 0.0824 sec.
iter 197020 || Loss: 1.1968 || timer: 0.0747 sec.
iter 197030 || Loss: 0.7971 || timer: 0.0902 sec.
iter 197040 || Loss: 0.7840 || timer: 0.0926 sec.
iter 197050 || Loss: 1.0623 || timer: 0.0802 sec.
iter 197060 || Loss: 0.8014 || timer: 0.1079 sec.
iter 197070 || Loss: 0.6050 || timer: 0.0922 sec.
iter 197080 || Loss: 1.0481 || timer: 0.0860 sec.
iter 197090 || Loss: 0.7528 || timer: 0.0760 sec.
iter 197100 || Loss: 0.8191 || timer: 0.0840 sec.
iter 197110 || Loss: 0.8847 || timer: 0.0943 sec.
iter 197120 || Loss: 1.1625 || timer: 0.0853 sec.
iter 197130 || Loss: 0.8297 || timer: 0.0850 sec.
iter 197140 || Loss: 0.8655 || timer: 0.0780 sec.
iter 197150 || Loss: 1.0177 || timer: 0.0912 sec.
iter 197160 || Loss: 1.1907 || timer: 0.0930 sec.
iter 197170 || Loss: 0.7080 || timer: 0.0946 sec.
iter 197180 || Loss: 1.0424 || timer: 0.0904 sec.
iter 197190 || Loss: 0.9428 || timer: 0.0873 sec.
iter 197200 || Loss: 0.9816 || timer: 0.1128 sec.
iter 197210 || Loss: 0.8038 || timer: 0.0845 sec.
iter 197220 || Loss: 1.2902 || timer: 0.1009 sec.
iter 197230 || Loss: 0.8143 || timer: 0.0268 sec.
iter 197240 || Loss: 0.6812 || timer: 0.0842 sec.
iter 197250 || Loss: 0.9370 || timer: 0.0835 sec.
iter 197260 || Loss: 0.8152 || timer: 0.0833 sec.
iter 197270 || Loss: 1.2500 || timer: 0.0839 sec.
iter 197280 || Loss: 0.8656 || timer: 0.0831 sec.
iter 197290 || Loss: 1.0206 || timer: 0.1008 sec.
iter 197300 || Loss: 0.9445 || timer: 0.0842 sec.
iter 197310 || Loss: 0.9356 || timer: 0.1046 sec.
iter 197320 || Loss: 0.9584 || timer: 0.0814 sec.
iter 197330 || Loss: 1.1670 || timer: 0.1091 sec.
iter 197340 || Loss: 0.7177 || timer: 0.0902 sec.
iter 197350 || Loss: 1.1894 || timer: 0.0864 sec.
iter 197360 || Loss: 0.7745 || timer: 0.0910 sec.
iter 197370 || Loss: 0.9162 || timer: 0.0836 sec.
iter 197380 || Loss: 1.1437 || timer: 0.0875 sec.
iter 197390 || Loss: 0.6600 || timer: 0.0920 sec.
iter 197400 || Loss: 0.9495 || timer: 0.0814 sec.
iter 197410 || Loss: 0.8301 || timer: 0.0928 sec.
iter 197420 || Loss: 0.8487 || timer: 0.0896 sec.
iter 197430 || Loss: 0.9231 || timer: 0.1014 sec.
iter 197440 || Loss: 0.7673 || timer: 0.0834 sec.
iter 197450 || Loss: 0.7869 || timer: 0.0851 sec.
iter 197460 || Loss: 0.9298 || timer: 0.0842 sec.
iter 197470 || Loss: 0.8498 || timer: 0.0852 sec.
iter 197480 || Loss: 0.7523 || timer: 0.0922 sec.
iter 197490 || Loss: 0.7403 || timer: 0.1162 sec.
iter 197500 || Loss: 0.9058 || timer: 0.0840 sec.
iter 197510 || Loss: 0.8839 || timer: 0.0919 sec.
iter 197520 || Loss: 1.0918 || timer: 0.0842 sec.
iter 197530 || Loss: 0.9220 || timer: 0.0896 sec.
iter 197540 || Loss: 0.8148 || timer: 0.0872 sec.
iter 197550 || Loss: 0.7752 || timer: 0.0926 sec.
iter 197560 || Loss: 1.0651 || timer: 0.0249 sec.
iter 197570 || Loss: 0.9600 || timer: 0.0832 sec.
iter 197580 || Loss: 0.8156 || timer: 0.0814 sec.
iter 197590 || Loss: 0.5844 || timer: 0.0990 sec.
iter 197600 || Loss: 0.8125 || timer: 0.1068 sec.
iter 197610 || Loss: 0.9764 || timer: 0.0915 sec.
iter 197620 || Loss: 0.8113 || timer: 0.1044 sec.
iter 197630 || Loss: 0.8540 || timer: 0.1041 sec.
iter 197640 || Loss: 1.0855 || timer: 0.0912 sec.
iter 197650 || Loss: 0.7754 || timer: 0.0823 sec.
iter 197660 || Loss: 0.6037 || timer: 0.1168 sec.
iter 197670 || Loss: 1.2314 || timer: 0.0946 sec.
iter 197680 || Loss: 1.1134 || timer: 0.0900 sec.
iter 197690 || Loss: 0.8460 || timer: 0.0902 sec.
iter 197700 || Loss: 0.7635 || timer: 0.0767 sec.
iter 197710 || Loss: 0.6687 || timer: 0.0960 sec.
iter 197720 || Loss: 0.7144 || timer: 0.0903 sec.
iter 197730 || Loss: 0.7927 || timer: 0.0940 sec.
iter 197740 || Loss: 1.1255 || timer: 0.0841 sec.
iter 197750 || Loss: 0.9047 || timer: 0.1005 sec.
iter 197760 || Loss: 0.9061 || timer: 0.0839 sec.
iter 197770 || Loss: 0.8074 || timer: 0.1052 sec.
iter 197780 || Loss: 1.0631 || timer: 0.0901 sec.
iter 197790 || Loss: 0.8811 || timer: 0.0828 sec.
iter 197800 || Loss: 0.7576 || timer: 0.1045 sec.
iter 197810 || Loss: 1.4656 || timer: 0.0991 sec.
iter 197820 || Loss: 0.8544 || timer: 0.0816 sec.
iter 197830 || Loss: 0.9254 || timer: 0.0817 sec.
iter 197840 || Loss: 0.9570 || timer: 0.1047 sec.
iter 197850 || Loss: 1.0001 || timer: 0.1041 sec.
iter 197860 || Loss: 1.0219 || timer: 0.0887 sec.
iter 197870 || Loss: 1.0060 || timer: 0.0863 sec.
iter 197880 || Loss: 0.9743 || timer: 0.0918 sec.
iter 197890 || Loss: 0.6174 || timer: 0.0162 sec.
iter 197900 || Loss: 0.4009 || timer: 0.0830 sec.
iter 197910 || Loss: 0.7249 || timer: 0.0825 sec.
iter 197920 || Loss: 0.8566 || timer: 0.0825 sec.
iter 197930 || Loss: 0.7173 || timer: 0.0827 sec.
iter 197940 || Loss: 1.1081 || timer: 0.0849 sec.
iter 197950 || Loss: 1.1252 || timer: 0.0923 sec.
iter 197960 || Loss: 0.9194 || timer: 0.0939 sec.
iter 197970 || Loss: 1.0929 || timer: 0.1015 sec.
iter 197980 || Loss: 0.8503 || timer: 0.0911 sec.
iter 197990 || Loss: 0.8585 || timer: 0.1195 sec.
iter 198000 || Loss: 1.0838 || timer: 0.0829 sec.
iter 198010 || Loss: 1.0822 || timer: 0.0879 sec.
iter 198020 || Loss: 1.1879 || timer: 0.0991 sec.
iter 198030 || Loss: 1.1458 || timer: 0.0869 sec.
iter 198040 || Loss: 1.1016 || timer: 0.0901 sec.
iter 198050 || Loss: 1.2221 || timer: 0.0800 sec.
iter 198060 || Loss: 0.8044 || timer: 0.0912 sec.
iter 198070 || Loss: 0.8949 || timer: 0.1049 sec.
iter 198080 || Loss: 0.7587 || timer: 0.0883 sec.
iter 198090 || Loss: 0.9734 || timer: 0.0979 sec.
iter 198100 || Loss: 0.9062 || timer: 0.1038 sec.
iter 198110 || Loss: 1.1870 || timer: 0.0830 sec.
iter 198120 || Loss: 0.8350 || timer: 0.0839 sec.
iter 198130 || Loss: 0.8247 || timer: 0.0910 sec.
iter 198140 || Loss: 0.6905 || timer: 0.0821 sec.
iter 198150 || Loss: 0.8021 || timer: 0.1085 sec.
iter 198160 || Loss: 0.7991 || timer: 0.0922 sec.
iter 198170 || Loss: 0.9873 || timer: 0.0889 sec.
iter 198180 || Loss: 1.0995 || timer: 0.0813 sec.
iter 198190 || Loss: 0.9485 || timer: 0.0930 sec.
iter 198200 || Loss: 0.6751 || timer: 0.0890 sec.
iter 198210 || Loss: 1.2942 || timer: 0.0891 sec.
iter 198220 || Loss: 0.6917 || timer: 0.0167 sec.
iter 198230 || Loss: 1.7285 || timer: 0.0833 sec.
iter 198240 || Loss: 1.4148 || timer: 0.0912 sec.
iter 198250 || Loss: 0.9648 || timer: 0.0829 sec.
iter 198260 || Loss: 1.0932 || timer: 0.0889 sec.
iter 198270 || Loss: 1.1476 || timer: 0.0857 sec.
iter 198280 || Loss: 1.3226 || timer: 0.0983 sec.
iter 198290 || Loss: 0.8504 || timer: 0.0826 sec.
iter 198300 || Loss: 1.0971 || timer: 0.0868 sec.
iter 198310 || Loss: 0.5720 || timer: 0.1077 sec.
iter 198320 || Loss: 1.0574 || timer: 0.1180 sec.
iter 198330 || Loss: 0.9145 || timer: 0.0829 sec.
iter 198340 || Loss: 0.8349 || timer: 0.0814 sec.
iter 198350 || Loss: 1.2646 || timer: 0.0827 sec.
iter 198360 || Loss: 0.8736 || timer: 0.1154 sec.
iter 198370 || Loss: 0.9568 || timer: 0.0898 sec.
iter 198380 || Loss: 0.6863 || timer: 0.0884 sec.
iter 198390 || Loss: 0.8312 || timer: 0.1037 sec.
iter 198400 || Loss: 1.0661 || timer: 0.0824 sec.
iter 198410 || Loss: 0.9803 || timer: 0.0977 sec.
iter 198420 || Loss: 0.9897 || timer: 0.0902 sec.
iter 198430 || Loss: 0.6576 || timer: 0.0897 sec.
iter 198440 || Loss: 1.1837 || timer: 0.0882 sec.
iter 198450 || Loss: 1.1603 || timer: 0.0859 sec.
iter 198460 || Loss: 0.8775 || timer: 0.0765 sec.
iter 198470 || Loss: 0.9215 || timer: 0.0963 sec.
iter 198480 || Loss: 0.8582 || timer: 0.0810 sec.
iter 198490 || Loss: 0.6945 || timer: 0.0824 sec.
iter 198500 || Loss: 0.6939 || timer: 0.0918 sec.
iter 198510 || Loss: 0.8688 || timer: 0.0957 sec.
iter 198520 || Loss: 0.6470 || timer: 0.0835 sec.
iter 198530 || Loss: 0.8928 || timer: 0.0818 sec.
iter 198540 || Loss: 0.8279 || timer: 0.0869 sec.
iter 198550 || Loss: 0.7687 || timer: 0.0218 sec.
iter 198560 || Loss: 0.5308 || timer: 0.0907 sec.
iter 198570 || Loss: 0.6934 || timer: 0.0738 sec.
iter 198580 || Loss: 1.0882 || timer: 0.0797 sec.
iter 198590 || Loss: 1.0462 || timer: 0.0835 sec.
iter 198600 || Loss: 0.7278 || timer: 0.0830 sec.
iter 198610 || Loss: 0.9583 || timer: 0.0828 sec.
iter 198620 || Loss: 0.8445 || timer: 0.0823 sec.
iter 198630 || Loss: 1.0688 || timer: 0.0740 sec.
iter 198640 || Loss: 0.9110 || timer: 0.1094 sec.
iter 198650 || Loss: 0.6552 || timer: 0.0961 sec.
iter 198660 || Loss: 1.1088 || timer: 0.0866 sec.
iter 198670 || Loss: 1.0618 || timer: 0.0813 sec.
iter 198680 || Loss: 0.9663 || timer: 0.0885 sec.
iter 198690 || Loss: 1.0257 || timer: 0.0830 sec.
iter 198700 || Loss: 0.7915 || timer: 0.0810 sec.
iter 198710 || Loss: 0.8028 || timer: 0.1078 sec.
iter 198720 || Loss: 0.7922 || timer: 0.0812 sec.
iter 198730 || Loss: 0.8969 || timer: 0.0877 sec.
iter 198740 || Loss: 0.5667 || timer: 0.1289 sec.
iter 198750 || Loss: 0.7718 || timer: 0.0907 sec.
iter 198760 || Loss: 0.7720 || timer: 0.0891 sec.
iter 198770 || Loss: 0.7853 || timer: 0.0751 sec.
iter 198780 || Loss: 0.4693 || timer: 0.0826 sec.
iter 198790 || Loss: 0.9545 || timer: 0.0926 sec.
iter 198800 || Loss: 0.8480 || timer: 0.0829 sec.
iter 198810 || Loss: 0.8307 || timer: 0.0878 sec.
iter 198820 || Loss: 0.9439 || timer: 0.0827 sec.
iter 198830 || Loss: 0.9211 || timer: 0.0826 sec.
iter 198840 || Loss: 0.8772 || timer: 0.1046 sec.
iter 198850 || Loss: 1.1004 || timer: 0.0830 sec.
iter 198860 || Loss: 0.7530 || timer: 0.1128 sec.
iter 198870 || Loss: 1.4513 || timer: 0.0897 sec.
iter 198880 || Loss: 0.5399 || timer: 0.0167 sec.
iter 198890 || Loss: 0.9775 || timer: 0.0911 sec.
iter 198900 || Loss: 1.2998 || timer: 0.0994 sec.
iter 198910 || Loss: 1.0030 || timer: 0.0887 sec.
iter 198920 || Loss: 1.0299 || timer: 0.0819 sec.
iter 198930 || Loss: 1.1175 || timer: 0.0961 sec.
iter 198940 || Loss: 1.1568 || timer: 0.0906 sec.
iter 198950 || Loss: 0.9075 || timer: 0.1035 sec.
iter 198960 || Loss: 0.7624 || timer: 0.0755 sec.
iter 198970 || Loss: 0.9959 || timer: 0.1144 sec.
iter 198980 || Loss: 1.2800 || timer: 0.1007 sec.
iter 198990 || Loss: 0.9307 || timer: 0.1011 sec.
iter 199000 || Loss: 0.7226 || timer: 0.0816 sec.
iter 199010 || Loss: 0.9194 || timer: 0.0830 sec.
iter 199020 || Loss: 0.7893 || timer: 0.0823 sec.
iter 199030 || Loss: 0.8862 || timer: 0.0886 sec.
iter 199040 || Loss: 0.9216 || timer: 0.1021 sec.
iter 199050 || Loss: 0.7668 || timer: 0.0840 sec.
iter 199060 || Loss: 0.8665 || timer: 0.0986 sec.
iter 199070 || Loss: 0.9009 || timer: 0.0885 sec.
iter 199080 || Loss: 0.8419 || timer: 0.0825 sec.
iter 199090 || Loss: 0.9531 || timer: 0.0904 sec.
iter 199100 || Loss: 0.8530 || timer: 0.0828 sec.
iter 199110 || Loss: 0.8483 || timer: 0.1046 sec.
iter 199120 || Loss: 1.0462 || timer: 0.0922 sec.
iter 199130 || Loss: 0.8541 || timer: 0.1021 sec.
iter 199140 || Loss: 1.1602 || timer: 0.1033 sec.
iter 199150 || Loss: 0.7131 || timer: 0.0905 sec.
iter 199160 || Loss: 0.9269 || timer: 0.0835 sec.
iter 199170 || Loss: 0.9558 || timer: 0.0903 sec.
iter 199180 || Loss: 0.6670 || timer: 0.0921 sec.
iter 199190 || Loss: 1.0596 || timer: 0.0911 sec.
iter 199200 || Loss: 0.8337 || timer: 0.0878 sec.
iter 199210 || Loss: 1.0045 || timer: 0.0256 sec.
iter 199220 || Loss: 1.0921 || timer: 0.0895 sec.
iter 199230 || Loss: 1.0068 || timer: 0.0929 sec.
iter 199240 || Loss: 1.0189 || timer: 0.0885 sec.
iter 199250 || Loss: 0.7631 || timer: 0.0924 sec.
iter 199260 || Loss: 0.9185 || timer: 0.0850 sec.
iter 199270 || Loss: 1.0313 || timer: 0.0908 sec.
iter 199280 || Loss: 0.8358 || timer: 0.0844 sec.
iter 199290 || Loss: 0.9029 || timer: 0.0844 sec.
iter 199300 || Loss: 0.9117 || timer: 0.0913 sec.
iter 199310 || Loss: 1.1747 || timer: 0.0968 sec.
iter 199320 || Loss: 0.8494 || timer: 0.1037 sec.
iter 199330 || Loss: 0.7415 || timer: 0.0881 sec.
iter 199340 || Loss: 1.0874 || timer: 0.0896 sec.
iter 199350 || Loss: 0.7114 || timer: 0.0821 sec.
iter 199360 || Loss: 0.9343 || timer: 0.0888 sec.
iter 199370 || Loss: 0.6734 || timer: 0.0823 sec.
iter 199380 || Loss: 0.8686 || timer: 0.1256 sec.
iter 199390 || Loss: 0.8305 || timer: 0.0913 sec.
iter 199400 || Loss: 0.8427 || timer: 0.0843 sec.
iter 199410 || Loss: 0.6301 || timer: 0.0895 sec.
iter 199420 || Loss: 0.7398 || timer: 0.0888 sec.
iter 199430 || Loss: 1.4006 || timer: 0.0992 sec.
iter 199440 || Loss: 0.9869 || timer: 0.0838 sec.
iter 199450 || Loss: 1.0068 || timer: 0.0897 sec.
iter 199460 || Loss: 1.0096 || timer: 0.0903 sec.
iter 199470 || Loss: 0.6913 || timer: 0.1211 sec.
iter 199480 || Loss: 1.1624 || timer: 0.1047 sec.
iter 199490 || Loss: 0.8043 || timer: 0.0841 sec.
iter 199500 || Loss: 0.9159 || timer: 0.0837 sec.
iter 199510 || Loss: 0.7689 || timer: 0.0928 sec.
iter 199520 || Loss: 1.0899 || timer: 0.0913 sec.
iter 199530 || Loss: 1.1144 || timer: 0.0934 sec.
iter 199540 || Loss: 0.8881 || timer: 0.0172 sec.
iter 199550 || Loss: 1.8918 || timer: 0.0912 sec.
iter 199560 || Loss: 0.8707 || timer: 0.1174 sec.
iter 199570 || Loss: 0.7496 || timer: 0.0817 sec.
iter 199580 || Loss: 0.8497 || timer: 0.0811 sec.
iter 199590 || Loss: 1.1785 || timer: 0.0837 sec.
iter 199600 || Loss: 0.7862 || timer: 0.0913 sec.
iter 199610 || Loss: 1.2856 || timer: 0.0838 sec.
iter 199620 || Loss: 1.0401 || timer: 0.0915 sec.
iter 199630 || Loss: 0.8621 || timer: 0.0912 sec.
iter 199640 || Loss: 0.8983 || timer: 0.0974 sec.
iter 199650 || Loss: 0.5473 || timer: 0.0906 sec.
iter 199660 || Loss: 1.1745 || timer: 0.0921 sec.
iter 199670 || Loss: 0.7917 || timer: 0.0895 sec.
iter 199680 || Loss: 1.0774 || timer: 0.0865 sec.
iter 199690 || Loss: 0.6901 || timer: 0.0999 sec.
iter 199700 || Loss: 0.9793 || timer: 0.0879 sec.
iter 199710 || Loss: 0.8827 || timer: 0.1085 sec.
iter 199720 || Loss: 0.8471 || timer: 0.1014 sec.
iter 199730 || Loss: 0.8302 || timer: 0.0834 sec.
iter 199740 || Loss: 0.7803 || timer: 0.0829 sec.
iter 199750 || Loss: 0.8149 || timer: 0.0907 sec.
iter 199760 || Loss: 0.9784 || timer: 0.0776 sec.
iter 199770 || Loss: 1.0540 || timer: 0.0785 sec.
iter 199780 || Loss: 0.7616 || timer: 0.0906 sec.
iter 199790 || Loss: 0.8145 || timer: 0.0841 sec.
iter 199800 || Loss: 1.1998 || timer: 0.1041 sec.
iter 199810 || Loss: 1.0397 || timer: 0.1039 sec.
iter 199820 || Loss: 1.7345 || timer: 0.0788 sec.
iter 199830 || Loss: 1.0955 || timer: 0.0831 sec.
iter 199840 || Loss: 0.9077 || timer: 0.0883 sec.
iter 199850 || Loss: 0.9800 || timer: 0.0851 sec.
iter 199860 || Loss: 0.9187 || timer: 0.0772 sec.
iter 199870 || Loss: 0.8777 || timer: 0.0203 sec.
iter 199880 || Loss: 0.9885 || timer: 0.0920 sec.
iter 199890 || Loss: 0.7388 || timer: 0.0895 sec.
iter 199900 || Loss: 1.1099 || timer: 0.0930 sec.
iter 199910 || Loss: 0.8231 || timer: 0.0853 sec.
iter 199920 || Loss: 0.8867 || timer: 0.0882 sec.
iter 199930 || Loss: 0.9705 || timer: 0.0891 sec.
iter 199940 || Loss: 1.1204 || timer: 0.0916 sec.
iter 199950 || Loss: 0.8996 || timer: 0.1057 sec.
iter 199960 || Loss: 0.8535 || timer: 0.0910 sec.
iter 199970 || Loss: 1.0376 || timer: 0.0966 sec.
iter 199980 || Loss: 1.1698 || timer: 0.0924 sec.
iter 199990 || Loss: 0.9000 || timer: 0.0986 sec.
iter 200000 || Loss: 0.7231 || Saving state, iter: 200000
timer: 0.0876 sec.
iter 200010 || Loss: 0.6682 || timer: 0.1013 sec.
iter 200020 || Loss: 0.7728 || timer: 0.0931 sec.
iter 200030 || Loss: 0.9814 || timer: 0.0884 sec.
iter 200040 || Loss: 1.0905 || timer: 0.0888 sec.
iter 200050 || Loss: 0.7527 || timer: 0.0912 sec.
iter 200060 || Loss: 1.0591 || timer: 0.0887 sec.
iter 200070 || Loss: 0.5493 || timer: 0.0838 sec.
iter 200080 || Loss: 0.9928 || timer: 0.1010 sec.
iter 200090 || Loss: 0.8231 || timer: 0.1047 sec.
iter 200100 || Loss: 1.3335 || timer: 0.0848 sec.
iter 200110 || Loss: 0.7733 || timer: 0.0893 sec.
iter 200120 || Loss: 1.0999 || timer: 0.0912 sec.
iter 200130 || Loss: 0.8810 || timer: 0.0839 sec.
iter 200140 || Loss: 0.9149 || timer: 0.0843 sec.
iter 200150 || Loss: 0.7893 || timer: 0.1009 sec.
iter 200160 || Loss: 0.9808 || timer: 0.0846 sec.
iter 200170 || Loss: 1.0687 || timer: 0.0840 sec.
iter 200180 || Loss: 1.1612 || timer: 0.0912 sec.
iter 200190 || Loss: 0.7237 || timer: 0.0998 sec.
iter 200200 || Loss: 1.0210 || timer: 0.0261 sec.
iter 200210 || Loss: 0.6026 || timer: 0.0849 sec.
iter 200220 || Loss: 0.8223 || timer: 0.0839 sec.
iter 200230 || Loss: 0.7100 || timer: 0.1086 sec.
iter 200240 || Loss: 1.0619 || timer: 0.1043 sec.
iter 200250 || Loss: 1.0516 || timer: 0.0920 sec.
iter 200260 || Loss: 0.9801 || timer: 0.0898 sec.
iter 200270 || Loss: 1.0808 || timer: 0.0843 sec.
iter 200280 || Loss: 0.6622 || timer: 0.0874 sec.
iter 200290 || Loss: 1.4986 || timer: 0.0841 sec.
iter 200300 || Loss: 0.9198 || timer: 0.0969 sec.
iter 200310 || Loss: 0.7015 || timer: 0.0825 sec.
iter 200320 || Loss: 0.8448 || timer: 0.0849 sec.
iter 200330 || Loss: 0.8117 || timer: 0.0892 sec.
iter 200340 || Loss: 0.7427 || timer: 0.0913 sec.
iter 200350 || Loss: 1.0083 || timer: 0.0904 sec.
iter 200360 || Loss: 1.2090 || timer: 0.0962 sec.
iter 200370 || Loss: 0.9672 || timer: 0.1164 sec.
iter 200380 || Loss: 1.2961 || timer: 0.1078 sec.
iter 200390 || Loss: 0.9505 || timer: 0.1039 sec.
iter 200400 || Loss: 1.2810 || timer: 0.0838 sec.
iter 200410 || Loss: 0.9510 || timer: 0.0917 sec.
iter 200420 || Loss: 0.7509 || timer: 0.0888 sec.
iter 200430 || Loss: 0.7256 || timer: 0.0844 sec.
iter 200440 || Loss: 1.2326 || timer: 0.0875 sec.
iter 200450 || Loss: 0.8318 || timer: 0.0885 sec.
iter 200460 || Loss: 0.6783 || timer: 0.0802 sec.
iter 200470 || Loss: 1.0781 || timer: 0.0838 sec.
iter 200480 || Loss: 1.0336 || timer: 0.0824 sec.
iter 200490 || Loss: 0.8014 || timer: 0.0921 sec.
iter 200500 || Loss: 0.8087 || timer: 0.0849 sec.
iter 200510 || Loss: 1.0088 || timer: 0.0840 sec.
iter 200520 || Loss: 0.6889 || timer: 0.0852 sec.
iter 200530 || Loss: 0.8885 || timer: 0.0270 sec.
iter 200540 || Loss: 0.7257 || timer: 0.0943 sec.
iter 200550 || Loss: 0.6993 || timer: 0.0920 sec.
iter 200560 || Loss: 0.8414 || timer: 0.0826 sec.
iter 200570 || Loss: 1.0124 || timer: 0.0904 sec.
iter 200580 || Loss: 1.0010 || timer: 0.0904 sec.
iter 200590 || Loss: 1.1279 || timer: 0.0824 sec.
iter 200600 || Loss: 0.7615 || timer: 0.1023 sec.
iter 200610 || Loss: 0.8872 || timer: 0.0907 sec.
iter 200620 || Loss: 1.1342 || timer: 0.1030 sec.
iter 200630 || Loss: 0.7666 || timer: 0.0971 sec.
iter 200640 || Loss: 1.0700 || timer: 0.0841 sec.
iter 200650 || Loss: 1.1535 || timer: 0.1060 sec.
iter 200660 || Loss: 0.9956 || timer: 0.0839 sec.
iter 200670 || Loss: 0.9083 || timer: 0.0916 sec.
iter 200680 || Loss: 0.7284 || timer: 0.0903 sec.
iter 200690 || Loss: 0.7768 || timer: 0.0838 sec.
iter 200700 || Loss: 1.2237 || timer: 0.0931 sec.
iter 200710 || Loss: 0.8020 || timer: 0.0906 sec.
iter 200720 || Loss: 0.7544 || timer: 0.0905 sec.
iter 200730 || Loss: 0.8544 || timer: 0.0957 sec.
iter 200740 || Loss: 1.0853 || timer: 0.0834 sec.
iter 200750 || Loss: 1.0154 || timer: 0.0951 sec.
iter 200760 || Loss: 0.9030 || timer: 0.0850 sec.
iter 200770 || Loss: 0.8140 || timer: 0.0908 sec.
iter 200780 || Loss: 1.0627 || timer: 0.0813 sec.
iter 200790 || Loss: 0.7567 || timer: 0.0901 sec.
iter 200800 || Loss: 0.8478 || timer: 0.0896 sec.
iter 200810 || Loss: 0.9793 || timer: 0.0959 sec.
iter 200820 || Loss: 0.8638 || timer: 0.0823 sec.
iter 200830 || Loss: 0.7759 || timer: 0.0917 sec.
iter 200840 || Loss: 1.0923 || timer: 0.1088 sec.
iter 200850 || Loss: 1.0243 || timer: 0.0894 sec.
iter 200860 || Loss: 0.6982 || timer: 0.0263 sec.
iter 200870 || Loss: 4.5414 || timer: 0.0848 sec.
iter 200880 || Loss: 1.1970 || timer: 0.1071 sec.
iter 200890 || Loss: 1.2363 || timer: 0.0823 sec.
iter 200900 || Loss: 1.2039 || timer: 0.0917 sec.
iter 200910 || Loss: 0.6089 || timer: 0.0887 sec.
iter 200920 || Loss: 1.1682 || timer: 0.0872 sec.
iter 200930 || Loss: 0.9685 || timer: 0.1416 sec.
iter 200940 || Loss: 1.1905 || timer: 0.0938 sec.
iter 200950 || Loss: 0.9228 || timer: 0.0979 sec.
iter 200960 || Loss: 1.1515 || timer: 0.0944 sec.
iter 200970 || Loss: 1.1011 || timer: 0.0918 sec.
iter 200980 || Loss: 1.0830 || timer: 0.0848 sec.
iter 200990 || Loss: 0.8394 || timer: 0.0894 sec.
iter 201000 || Loss: 0.9064 || timer: 0.0833 sec.
iter 201010 || Loss: 1.2067 || timer: 0.0825 sec.
iter 201020 || Loss: 1.0719 || timer: 0.0895 sec.
iter 201030 || Loss: 1.1706 || timer: 0.0959 sec.
iter 201040 || Loss: 0.7813 || timer: 0.0940 sec.
iter 201050 || Loss: 0.9541 || timer: 0.0889 sec.
iter 201060 || Loss: 1.0877 || timer: 0.0981 sec.
iter 201070 || Loss: 1.0892 || timer: 0.0830 sec.
iter 201080 || Loss: 0.6992 || timer: 0.1203 sec.
iter 201090 || Loss: 1.1748 || timer: 0.0920 sec.
iter 201100 || Loss: 0.8634 || timer: 0.0906 sec.
iter 201110 || Loss: 0.7784 || timer: 0.0888 sec.
iter 201120 || Loss: 0.7226 || timer: 0.1032 sec.
iter 201130 || Loss: 1.1713 || timer: 0.0965 sec.
iter 201140 || Loss: 0.9227 || timer: 0.1416 sec.
iter 201150 || Loss: 0.8887 || timer: 0.1003 sec.
iter 201160 || Loss: 0.9290 || timer: 0.0963 sec.
iter 201170 || Loss: 1.2686 || timer: 0.0960 sec.
iter 201180 || Loss: 0.7545 || timer: 0.1100 sec.
iter 201190 || Loss: 0.7340 || timer: 0.0179 sec.
iter 201200 || Loss: 1.2398 || timer: 0.0914 sec.
iter 201210 || Loss: 1.0512 || timer: 0.0924 sec.
iter 201220 || Loss: 0.9205 || timer: 0.0925 sec.
iter 201230 || Loss: 0.8133 || timer: 0.0887 sec.
iter 201240 || Loss: 0.7618 || timer: 0.0903 sec.
iter 201250 || Loss: 1.0155 || timer: 0.0920 sec.
iter 201260 || Loss: 0.5499 || timer: 0.0934 sec.
iter 201270 || Loss: 1.0681 || timer: 0.1023 sec.
iter 201280 || Loss: 1.3437 || timer: 0.0923 sec.
iter 201290 || Loss: 0.7937 || timer: 0.0998 sec.
iter 201300 || Loss: 0.9431 || timer: 0.0826 sec.
iter 201310 || Loss: 1.1856 || timer: 0.1021 sec.
iter 201320 || Loss: 0.8783 || timer: 0.0826 sec.
iter 201330 || Loss: 0.6822 || timer: 0.1045 sec.
iter 201340 || Loss: 1.0697 || timer: 0.0845 sec.
iter 201350 || Loss: 0.6951 || timer: 0.0955 sec.
iter 201360 || Loss: 0.8031 || timer: 0.0854 sec.
iter 201370 || Loss: 1.0350 || timer: 0.0851 sec.
iter 201380 || Loss: 0.9825 || timer: 0.0916 sec.
iter 201390 || Loss: 0.8213 || timer: 0.1022 sec.
iter 201400 || Loss: 1.0139 || timer: 0.0868 sec.
iter 201410 || Loss: 1.0157 || timer: 0.1001 sec.
iter 201420 || Loss: 1.0897 || timer: 0.0873 sec.
iter 201430 || Loss: 0.9678 || timer: 0.1092 sec.
iter 201440 || Loss: 0.7417 || timer: 0.0836 sec.
iter 201450 || Loss: 0.6608 || timer: 0.0852 sec.
iter 201460 || Loss: 0.5988 || timer: 0.1019 sec.
iter 201470 || Loss: 1.1404 || timer: 0.0915 sec.
iter 201480 || Loss: 0.9969 || timer: 0.0903 sec.
iter 201490 || Loss: 1.0574 || timer: 0.0905 sec.
iter 201500 || Loss: 1.0344 || timer: 0.0872 sec.
iter 201510 || Loss: 0.8838 || timer: 0.0907 sec.
iter 201520 || Loss: 0.6721 || timer: 0.0195 sec.
iter 201530 || Loss: 0.7141 || timer: 0.1197 sec.
iter 201540 || Loss: 1.1239 || timer: 0.1099 sec.
iter 201550 || Loss: 0.8716 || timer: 0.0906 sec.
iter 201560 || Loss: 0.8461 || timer: 0.1093 sec.
iter 201570 || Loss: 1.2276 || timer: 0.0888 sec.
iter 201580 || Loss: 0.8475 || timer: 0.0913 sec.
iter 201590 || Loss: 0.7729 || timer: 0.0923 sec.
iter 201600 || Loss: 0.9705 || timer: 0.0884 sec.
iter 201610 || Loss: 1.3292 || timer: 0.0902 sec.
iter 201620 || Loss: 1.0501 || timer: 0.0902 sec.
iter 201630 || Loss: 1.1261 || timer: 0.0914 sec.
iter 201640 || Loss: 0.7913 || timer: 0.0844 sec.
iter 201650 || Loss: 1.0608 || timer: 0.1107 sec.
iter 201660 || Loss: 1.0843 || timer: 0.0839 sec.
iter 201670 || Loss: 0.9305 || timer: 0.0944 sec.
iter 201680 || Loss: 0.8879 || timer: 0.0827 sec.
iter 201690 || Loss: 1.2866 || timer: 0.0870 sec.
iter 201700 || Loss: 1.0137 || timer: 0.0913 sec.
iter 201710 || Loss: 0.8296 || timer: 0.1100 sec.
iter 201720 || Loss: 0.9941 || timer: 0.0849 sec.
iter 201730 || Loss: 1.0014 || timer: 0.0853 sec.
iter 201740 || Loss: 0.9652 || timer: 0.0882 sec.
iter 201750 || Loss: 0.9035 || timer: 0.0737 sec.
iter 201760 || Loss: 0.7575 || timer: 0.0912 sec.
iter 201770 || Loss: 1.1251 || timer: 0.0916 sec.
iter 201780 || Loss: 0.9724 || timer: 0.0929 sec.
iter 201790 || Loss: 0.9930 || timer: 0.0934 sec.
iter 201800 || Loss: 0.9018 || timer: 0.0893 sec.
iter 201810 || Loss: 1.0295 || timer: 0.1084 sec.
iter 201820 || Loss: 0.9954 || timer: 0.0841 sec.
iter 201830 || Loss: 0.7614 || timer: 0.0844 sec.
iter 201840 || Loss: 1.1567 || timer: 0.0848 sec.
iter 201850 || Loss: 1.0669 || timer: 0.0189 sec.
iter 201860 || Loss: 1.0697 || timer: 0.0878 sec.
iter 201870 || Loss: 0.7211 || timer: 0.0899 sec.
iter 201880 || Loss: 0.6985 || timer: 0.0848 sec.
iter 201890 || Loss: 0.9344 || timer: 0.1230 sec.
iter 201900 || Loss: 0.8001 || timer: 0.0891 sec.
iter 201910 || Loss: 0.9210 || timer: 0.1102 sec.
iter 201920 || Loss: 1.1384 || timer: 0.0911 sec.
iter 201930 || Loss: 0.7774 || timer: 0.0896 sec.
iter 201940 || Loss: 0.8976 || timer: 0.0922 sec.
iter 201950 || Loss: 0.8216 || timer: 0.1008 sec.
iter 201960 || Loss: 1.0926 || timer: 0.0891 sec.
iter 201970 || Loss: 0.8305 || timer: 0.0876 sec.
iter 201980 || Loss: 1.0409 || timer: 0.0900 sec.
iter 201990 || Loss: 0.7460 || timer: 0.0998 sec.
iter 202000 || Loss: 0.8030 || timer: 0.0916 sec.
iter 202010 || Loss: 0.7537 || timer: 0.0898 sec.
iter 202020 || Loss: 0.6046 || timer: 0.0835 sec.
iter 202030 || Loss: 1.0662 || timer: 0.0918 sec.
iter 202040 || Loss: 0.7537 || timer: 0.0905 sec.
iter 202050 || Loss: 0.6926 || timer: 0.0842 sec.
iter 202060 || Loss: 0.9200 || timer: 0.0920 sec.
iter 202070 || Loss: 0.7176 || timer: 0.0900 sec.
iter 202080 || Loss: 0.9303 || timer: 0.0879 sec.
iter 202090 || Loss: 0.8781 || timer: 0.0945 sec.
iter 202100 || Loss: 0.5982 || timer: 0.1039 sec.
iter 202110 || Loss: 0.8762 || timer: 0.1041 sec.
iter 202120 || Loss: 0.8977 || timer: 0.0923 sec.
iter 202130 || Loss: 0.7674 || timer: 0.0930 sec.
iter 202140 || Loss: 1.0435 || timer: 0.0929 sec.
iter 202150 || Loss: 0.8453 || timer: 0.0888 sec.
iter 202160 || Loss: 0.7099 || timer: 0.0911 sec.
iter 202170 || Loss: 1.0454 || timer: 0.0901 sec.
iter 202180 || Loss: 0.9227 || timer: 0.0185 sec.
iter 202190 || Loss: 0.3515 || timer: 0.0818 sec.
iter 202200 || Loss: 1.0196 || timer: 0.0915 sec.
iter 202210 || Loss: 0.8280 || timer: 0.1017 sec.
iter 202220 || Loss: 0.8801 || timer: 0.0981 sec.
iter 202230 || Loss: 0.9955 || timer: 0.0857 sec.
iter 202240 || Loss: 0.8019 || timer: 0.0902 sec.
iter 202250 || Loss: 0.8257 || timer: 0.0824 sec.
iter 202260 || Loss: 1.2625 || timer: 0.0849 sec.
iter 202270 || Loss: 0.7549 || timer: 0.0848 sec.
iter 202280 || Loss: 1.1227 || timer: 0.1007 sec.
iter 202290 || Loss: 0.9775 || timer: 0.0901 sec.
iter 202300 || Loss: 0.9400 || timer: 0.0915 sec.
iter 202310 || Loss: 0.8066 || timer: 0.0873 sec.
iter 202320 || Loss: 0.8164 || timer: 0.0881 sec.
iter 202330 || Loss: 1.0641 || timer: 0.0827 sec.
iter 202340 || Loss: 1.0126 || timer: 0.0923 sec.
iter 202350 || Loss: 0.8295 || timer: 0.1066 sec.
iter 202360 || Loss: 1.1629 || timer: 0.0845 sec.
iter 202370 || Loss: 1.0220 || timer: 0.0931 sec.
iter 202380 || Loss: 0.7600 || timer: 0.0919 sec.
iter 202390 || Loss: 0.9008 || timer: 0.0941 sec.
iter 202400 || Loss: 0.8992 || timer: 0.0925 sec.
iter 202410 || Loss: 0.6117 || timer: 0.0875 sec.
iter 202420 || Loss: 0.7721 || timer: 0.0826 sec.
iter 202430 || Loss: 0.9024 || timer: 0.1109 sec.
iter 202440 || Loss: 0.9635 || timer: 0.0896 sec.
iter 202450 || Loss: 1.0553 || timer: 0.0914 sec.
iter 202460 || Loss: 1.1133 || timer: 0.0909 sec.
iter 202470 || Loss: 0.6753 || timer: 0.1011 sec.
iter 202480 || Loss: 0.8656 || timer: 0.1019 sec.
iter 202490 || Loss: 1.0444 || timer: 0.0991 sec.
iter 202500 || Loss: 0.9003 || timer: 0.0892 sec.
iter 202510 || Loss: 0.9022 || timer: 0.0353 sec.
iter 202520 || Loss: 0.6258 || timer: 0.1064 sec.
iter 202530 || Loss: 1.0164 || timer: 0.0835 sec.
iter 202540 || Loss: 0.8267 || timer: 0.0903 sec.
iter 202550 || Loss: 0.8232 || timer: 0.0898 sec.
iter 202560 || Loss: 0.7988 || timer: 0.0947 sec.
iter 202570 || Loss: 0.8726 || timer: 0.0903 sec.
iter 202580 || Loss: 0.8557 || timer: 0.0826 sec.
iter 202590 || Loss: 0.9527 || timer: 0.0933 sec.
iter 202600 || Loss: 0.8174 || timer: 0.0865 sec.
iter 202610 || Loss: 1.3242 || timer: 0.0961 sec.
iter 202620 || Loss: 1.1317 || timer: 0.0856 sec.
iter 202630 || Loss: 0.7060 || timer: 0.0921 sec.
iter 202640 || Loss: 0.8869 || timer: 0.0851 sec.
iter 202650 || Loss: 1.0704 || timer: 0.0838 sec.
iter 202660 || Loss: 0.9279 || timer: 0.0846 sec.
iter 202670 || Loss: 0.8212 || timer: 0.0843 sec.
iter 202680 || Loss: 1.0661 || timer: 0.0929 sec.
iter 202690 || Loss: 0.8200 || timer: 0.0902 sec.
iter 202700 || Loss: 0.9578 || timer: 0.0946 sec.
iter 202710 || Loss: 1.0009 || timer: 0.1042 sec.
iter 202720 || Loss: 1.1205 || timer: 0.0914 sec.
iter 202730 || Loss: 1.1160 || timer: 0.0892 sec.
iter 202740 || Loss: 0.8874 || timer: 0.0835 sec.
iter 202750 || Loss: 0.7830 || timer: 0.1194 sec.
iter 202760 || Loss: 1.2219 || timer: 0.0901 sec.
iter 202770 || Loss: 1.1000 || timer: 0.0874 sec.
iter 202780 || Loss: 0.7875 || timer: 0.0818 sec.
iter 202790 || Loss: 0.7997 || timer: 0.0910 sec.
iter 202800 || Loss: 0.9990 || timer: 0.0890 sec.
iter 202810 || Loss: 0.7213 || timer: 0.0835 sec.
iter 202820 || Loss: 1.2861 || timer: 0.0939 sec.
iter 202830 || Loss: 0.8794 || timer: 0.0884 sec.
iter 202840 || Loss: 1.1340 || timer: 0.0294 sec.
iter 202850 || Loss: 3.3461 || timer: 0.0830 sec.
iter 202860 || Loss: 1.3580 || timer: 0.0846 sec.
iter 202870 || Loss: 1.0307 || timer: 0.0864 sec.
iter 202880 || Loss: 1.1177 || timer: 0.0939 sec.
iter 202890 || Loss: 1.1154 || timer: 0.1153 sec.
iter 202900 || Loss: 0.6485 || timer: 0.0924 sec.
iter 202910 || Loss: 0.8459 || timer: 0.0943 sec.
iter 202920 || Loss: 0.9894 || timer: 0.0905 sec.
iter 202930 || Loss: 0.9192 || timer: 0.1142 sec.
iter 202940 || Loss: 1.0230 || timer: 0.1143 sec.
iter 202950 || Loss: 0.8146 || timer: 0.0870 sec.
iter 202960 || Loss: 1.0137 || timer: 0.1028 sec.
iter 202970 || Loss: 1.1114 || timer: 0.0899 sec.
iter 202980 || Loss: 1.0723 || timer: 0.0932 sec.
iter 202990 || Loss: 1.2482 || timer: 0.0982 sec.
iter 203000 || Loss: 1.1466 || timer: 0.0848 sec.
iter 203010 || Loss: 1.1952 || timer: 0.0960 sec.
iter 203020 || Loss: 1.1454 || timer: 0.0907 sec.
iter 203030 || Loss: 0.8328 || timer: 0.0913 sec.
iter 203040 || Loss: 1.1151 || timer: 0.0828 sec.
iter 203050 || Loss: 1.2075 || timer: 0.0917 sec.
iter 203060 || Loss: 0.8420 || timer: 0.0835 sec.
iter 203070 || Loss: 0.6995 || timer: 0.0836 sec.
iter 203080 || Loss: 1.1034 || timer: 0.0902 sec.
iter 203090 || Loss: 0.7364 || timer: 0.0837 sec.
iter 203100 || Loss: 1.0616 || timer: 0.0920 sec.
iter 203110 || Loss: 0.8342 || timer: 0.0837 sec.
iter 203120 || Loss: 1.2787 || timer: 0.0890 sec.
iter 203130 || Loss: 0.7945 || timer: 0.1121 sec.
iter 203140 || Loss: 0.7690 || timer: 0.0829 sec.
iter 203150 || Loss: 0.8246 || timer: 0.0809 sec.
iter 203160 || Loss: 1.0421 || timer: 0.0893 sec.
iter 203170 || Loss: 0.9730 || timer: 0.0252 sec.
iter 203180 || Loss: 0.8373 || timer: 0.0887 sec.
iter 203190 || Loss: 0.7769 || timer: 0.0955 sec.
iter 203200 || Loss: 1.1197 || timer: 0.0930 sec.
iter 203210 || Loss: 0.9230 || timer: 0.0886 sec.
iter 203220 || Loss: 1.0828 || timer: 0.0879 sec.
iter 203230 || Loss: 0.6579 || timer: 0.0830 sec.
iter 203240 || Loss: 0.9483 || timer: 0.0840 sec.
iter 203250 || Loss: 1.0171 || timer: 0.0947 sec.
iter 203260 || Loss: 0.9398 || timer: 0.0906 sec.
iter 203270 || Loss: 0.7851 || timer: 0.0964 sec.
iter 203280 || Loss: 1.0720 || timer: 0.0902 sec.
iter 203290 || Loss: 1.2561 || timer: 0.1073 sec.
iter 203300 || Loss: 0.9768 || timer: 0.0910 sec.
iter 203310 || Loss: 1.1033 || timer: 0.0907 sec.
iter 203320 || Loss: 1.2878 || timer: 0.0823 sec.
iter 203330 || Loss: 0.6877 || timer: 0.0956 sec.
iter 203340 || Loss: 1.0034 || timer: 0.0877 sec.
iter 203350 || Loss: 0.9278 || timer: 0.1194 sec.
iter 203360 || Loss: 0.8621 || timer: 0.0896 sec.
iter 203370 || Loss: 1.3528 || timer: 0.0996 sec.
iter 203380 || Loss: 0.9528 || timer: 0.0936 sec.
iter 203390 || Loss: 0.8052 || timer: 0.0915 sec.
iter 203400 || Loss: 0.9871 || timer: 0.0942 sec.
iter 203410 || Loss: 0.7179 || timer: 0.0892 sec.
iter 203420 || Loss: 0.9970 || timer: 0.0904 sec.
iter 203430 || Loss: 0.8364 || timer: 0.0894 sec.
iter 203440 || Loss: 0.7072 || timer: 0.0897 sec.
iter 203450 || Loss: 1.0539 || timer: 0.0924 sec.
iter 203460 || Loss: 0.9913 || timer: 0.0909 sec.
iter 203470 || Loss: 1.0078 || timer: 0.0911 sec.
iter 203480 || Loss: 0.7666 || timer: 0.0909 sec.
iter 203490 || Loss: 1.0263 || timer: 0.1050 sec.
iter 203500 || Loss: 1.2121 || timer: 0.0209 sec.
iter 203510 || Loss: 0.6286 || timer: 0.0922 sec.
iter 203520 || Loss: 0.7448 || timer: 0.0836 sec.
iter 203530 || Loss: 1.1917 || timer: 0.0830 sec.
iter 203540 || Loss: 0.9286 || timer: 0.0919 sec.
iter 203550 || Loss: 0.6864 || timer: 0.0898 sec.
iter 203560 || Loss: 0.8141 || timer: 0.0889 sec.
iter 203570 || Loss: 0.6445 || timer: 0.0825 sec.
iter 203580 || Loss: 0.7475 || timer: 0.0829 sec.
iter 203590 || Loss: 0.9668 || timer: 0.0841 sec.
iter 203600 || Loss: 0.8523 || timer: 0.0964 sec.
iter 203610 || Loss: 0.8380 || timer: 0.0837 sec.
iter 203620 || Loss: 0.8741 || timer: 0.0829 sec.
iter 203630 || Loss: 0.9519 || timer: 0.0941 sec.
iter 203640 || Loss: 1.0576 || timer: 0.0824 sec.
iter 203650 || Loss: 1.0322 || timer: 0.0907 sec.
iter 203660 || Loss: 0.8279 || timer: 0.0897 sec.
iter 203670 || Loss: 0.6606 || timer: 0.0919 sec.
iter 203680 || Loss: 0.7123 || timer: 0.1067 sec.
iter 203690 || Loss: 0.7247 || timer: 0.0866 sec.
iter 203700 || Loss: 0.9238 || timer: 0.0896 sec.
iter 203710 || Loss: 0.8622 || timer: 0.0889 sec.
iter 203720 || Loss: 1.2955 || timer: 0.1000 sec.
iter 203730 || Loss: 0.5490 || timer: 0.0863 sec.
iter 203740 || Loss: 0.6673 || timer: 0.0908 sec.
iter 203750 || Loss: 1.3225 || timer: 0.0934 sec.
iter 203760 || Loss: 1.0936 || timer: 0.1057 sec.
iter 203770 || Loss: 0.8168 || timer: 0.0831 sec.
iter 203780 || Loss: 0.8739 || timer: 0.1139 sec.
iter 203790 || Loss: 0.9480 || timer: 0.1201 sec.
iter 203800 || Loss: 1.0316 || timer: 0.0945 sec.
iter 203810 || Loss: 0.8272 || timer: 0.0979 sec.
iter 203820 || Loss: 0.9042 || timer: 0.0886 sec.
iter 203830 || Loss: 0.6153 || timer: 0.0270 sec.
iter 203840 || Loss: 0.3686 || timer: 0.0949 sec.
iter 203850 || Loss: 0.8780 || timer: 0.0947 sec.
iter 203860 || Loss: 0.9886 || timer: 0.1037 sec.
iter 203870 || Loss: 0.9150 || timer: 0.0827 sec.
iter 203880 || Loss: 0.8764 || timer: 0.0842 sec.
iter 203890 || Loss: 1.0634 || timer: 0.0844 sec.
iter 203900 || Loss: 0.6335 || timer: 0.0905 sec.
iter 203910 || Loss: 0.9088 || timer: 0.0840 sec.
iter 203920 || Loss: 0.8484 || timer: 0.0920 sec.
iter 203930 || Loss: 0.7468 || timer: 0.0933 sec.
iter 203940 || Loss: 0.8458 || timer: 0.1048 sec.
iter 203950 || Loss: 0.7181 || timer: 0.0816 sec.
iter 203960 || Loss: 0.6948 || timer: 0.0905 sec.
iter 203970 || Loss: 0.9350 || timer: 0.0827 sec.
iter 203980 || Loss: 0.7565 || timer: 0.0902 sec.
iter 203990 || Loss: 0.8008 || timer: 0.1091 sec.
iter 204000 || Loss: 0.8029 || timer: 0.0903 sec.
iter 204010 || Loss: 1.1842 || timer: 0.1225 sec.
iter 204020 || Loss: 0.9722 || timer: 0.0906 sec.
iter 204030 || Loss: 0.8981 || timer: 0.0898 sec.
iter 204040 || Loss: 1.0819 || timer: 0.0849 sec.
iter 204050 || Loss: 0.6948 || timer: 0.0885 sec.
iter 204060 || Loss: 1.2339 || timer: 0.0902 sec.
iter 204070 || Loss: 0.6314 || timer: 0.0921 sec.
iter 204080 || Loss: 1.0968 || timer: 0.0878 sec.
iter 204090 || Loss: 1.1921 || timer: 0.1020 sec.
iter 204100 || Loss: 1.0618 || timer: 0.0831 sec.
iter 204110 || Loss: 1.4756 || timer: 0.0899 sec.
iter 204120 || Loss: 0.8466 || timer: 0.1156 sec.
iter 204130 || Loss: 0.7581 || timer: 0.0839 sec.
iter 204140 || Loss: 0.9515 || timer: 0.0923 sec.
iter 204150 || Loss: 1.2541 || timer: 0.0869 sec.
iter 204160 || Loss: 0.8606 || timer: 0.0229 sec.
iter 204170 || Loss: 0.3052 || timer: 0.0934 sec.
iter 204180 || Loss: 1.0403 || timer: 0.0832 sec.
iter 204190 || Loss: 1.1355 || timer: 0.0912 sec.
iter 204200 || Loss: 0.9512 || timer: 0.0828 sec.
iter 204210 || Loss: 0.8140 || timer: 0.1133 sec.
iter 204220 || Loss: 0.9130 || timer: 0.0897 sec.
iter 204230 || Loss: 0.9620 || timer: 0.0902 sec.
iter 204240 || Loss: 1.0224 || timer: 0.0901 sec.
iter 204250 || Loss: 0.8829 || timer: 0.0897 sec.
iter 204260 || Loss: 0.9384 || timer: 0.1261 sec.
iter 204270 || Loss: 0.9260 || timer: 0.0823 sec.
iter 204280 || Loss: 0.8868 || timer: 0.0925 sec.
iter 204290 || Loss: 0.9593 || timer: 0.0899 sec.
iter 204300 || Loss: 0.8545 || timer: 0.0831 sec.
iter 204310 || Loss: 0.6382 || timer: 0.0824 sec.
iter 204320 || Loss: 0.9943 || timer: 0.0864 sec.
iter 204330 || Loss: 0.8033 || timer: 0.0916 sec.
iter 204340 || Loss: 1.2551 || timer: 0.0829 sec.
iter 204350 || Loss: 0.7256 || timer: 0.0975 sec.
iter 204360 || Loss: 0.6720 || timer: 0.0889 sec.
iter 204370 || Loss: 0.9740 || timer: 0.0887 sec.
iter 204380 || Loss: 0.9271 || timer: 0.0887 sec.
iter 204390 || Loss: 0.8660 || timer: 0.0903 sec.
iter 204400 || Loss: 0.8866 || timer: 0.0914 sec.
iter 204410 || Loss: 0.7829 || timer: 0.1087 sec.
iter 204420 || Loss: 0.8283 || timer: 0.0910 sec.
iter 204430 || Loss: 0.9087 || timer: 0.0903 sec.
iter 204440 || Loss: 0.9595 || timer: 0.0906 sec.
iter 204450 || Loss: 0.9374 || timer: 0.0892 sec.
iter 204460 || Loss: 0.9337 || timer: 0.0906 sec.
iter 204470 || Loss: 0.9397 || timer: 0.0835 sec.
iter 204480 || Loss: 0.7663 || timer: 0.1075 sec.
iter 204490 || Loss: 0.7773 || timer: 0.0172 sec.
iter 204500 || Loss: 1.3379 || timer: 0.0901 sec.
iter 204510 || Loss: 0.8835 || timer: 0.0825 sec.
iter 204520 || Loss: 0.8053 || timer: 0.0918 sec.
iter 204530 || Loss: 1.1470 || timer: 0.0926 sec.
iter 204540 || Loss: 1.0235 || timer: 0.1066 sec.
iter 204550 || Loss: 0.7121 || timer: 0.0886 sec.
iter 204560 || Loss: 0.8363 || timer: 0.0938 sec.
iter 204570 || Loss: 0.9488 || timer: 0.0903 sec.
iter 204580 || Loss: 0.6953 || timer: 0.0907 sec.
iter 204590 || Loss: 0.6198 || timer: 0.0956 sec.
iter 204600 || Loss: 0.8319 || timer: 0.0824 sec.
iter 204610 || Loss: 1.0204 || timer: 0.1096 sec.
iter 204620 || Loss: 1.2854 || timer: 0.1074 sec.
iter 204630 || Loss: 0.7098 || timer: 0.1031 sec.
iter 204640 || Loss: 1.1386 || timer: 0.0902 sec.
iter 204650 || Loss: 1.1070 || timer: 0.0878 sec.
iter 204660 || Loss: 0.9363 || timer: 0.0887 sec.
iter 204670 || Loss: 0.9597 || timer: 0.0904 sec.
iter 204680 || Loss: 0.8928 || timer: 0.1002 sec.
iter 204690 || Loss: 0.8861 || timer: 0.1025 sec.
iter 204700 || Loss: 0.8871 || timer: 0.1037 sec.
iter 204710 || Loss: 0.9559 || timer: 0.0900 sec.
iter 204720 || Loss: 1.3038 || timer: 0.0912 sec.
iter 204730 || Loss: 0.7634 || timer: 0.0922 sec.
iter 204740 || Loss: 1.0963 || timer: 0.0840 sec.
iter 204750 || Loss: 0.9290 || timer: 0.1120 sec.
iter 204760 || Loss: 1.0647 || timer: 0.0920 sec.
iter 204770 || Loss: 1.2134 || timer: 0.0901 sec.
iter 204780 || Loss: 0.9795 || timer: 0.0825 sec.
iter 204790 || Loss: 0.7119 || timer: 0.0889 sec.
iter 204800 || Loss: 0.7618 || timer: 0.0899 sec.
iter 204810 || Loss: 0.7836 || timer: 0.0902 sec.
iter 204820 || Loss: 0.7899 || timer: 0.0262 sec.
iter 204830 || Loss: 0.5786 || timer: 0.0886 sec.
iter 204840 || Loss: 0.9150 || timer: 0.1344 sec.
iter 204850 || Loss: 0.8621 || timer: 0.0910 sec.
iter 204860 || Loss: 0.9101 || timer: 0.0894 sec.
iter 204870 || Loss: 0.8688 || timer: 0.0991 sec.
iter 204880 || Loss: 0.6241 || timer: 0.0911 sec.
iter 204890 || Loss: 1.0504 || timer: 0.0878 sec.
iter 204900 || Loss: 0.8686 || timer: 0.1093 sec.
iter 204910 || Loss: 0.8981 || timer: 0.0889 sec.
iter 204920 || Loss: 0.8259 || timer: 0.1022 sec.
iter 204930 || Loss: 0.6102 || timer: 0.0863 sec.
iter 204940 || Loss: 0.8277 || timer: 0.1154 sec.
iter 204950 || Loss: 0.7633 || timer: 0.0842 sec.
iter 204960 || Loss: 0.6054 || timer: 0.0896 sec.
iter 204970 || Loss: 0.8215 || timer: 0.0868 sec.
iter 204980 || Loss: 0.9468 || timer: 0.0836 sec.
iter 204990 || Loss: 0.7975 || timer: 0.0887 sec.
iter 205000 || Loss: 0.9744 || Saving state, iter: 205000
timer: 0.1098 sec.
iter 205010 || Loss: 0.6623 || timer: 0.0917 sec.
iter 205020 || Loss: 0.7824 || timer: 0.0823 sec.
iter 205030 || Loss: 0.8202 || timer: 0.0826 sec.
iter 205040 || Loss: 0.9215 || timer: 0.0897 sec.
iter 205050 || Loss: 0.7926 || timer: 0.0881 sec.
iter 205060 || Loss: 0.8138 || timer: 0.0919 sec.
iter 205070 || Loss: 0.6922 || timer: 0.0909 sec.
iter 205080 || Loss: 0.9091 || timer: 0.0868 sec.
iter 205090 || Loss: 0.9913 || timer: 0.0898 sec.
iter 205100 || Loss: 0.9943 || timer: 0.0826 sec.
iter 205110 || Loss: 0.9234 || timer: 0.0905 sec.
iter 205120 || Loss: 0.8387 || timer: 0.1051 sec.
iter 205130 || Loss: 0.8424 || timer: 0.0932 sec.
iter 205140 || Loss: 0.9481 || timer: 0.0892 sec.
iter 205150 || Loss: 0.8308 || timer: 0.0220 sec.
iter 205160 || Loss: 0.2151 || timer: 0.0954 sec.
iter 205170 || Loss: 0.7404 || timer: 0.0989 sec.
iter 205180 || Loss: 0.9063 || timer: 0.0828 sec.
iter 205190 || Loss: 1.1644 || timer: 0.0901 sec.
iter 205200 || Loss: 0.8871 || timer: 0.0910 sec.
iter 205210 || Loss: 1.0023 || timer: 0.0918 sec.
iter 205220 || Loss: 1.0348 || timer: 0.0880 sec.
iter 205230 || Loss: 0.9753 || timer: 0.0895 sec.
iter 205240 || Loss: 0.6592 || timer: 0.0929 sec.
iter 205250 || Loss: 0.7458 || timer: 0.0955 sec.
iter 205260 || Loss: 0.8511 || timer: 0.0979 sec.
iter 205270 || Loss: 1.0401 || timer: 0.0904 sec.
iter 205280 || Loss: 0.7629 || timer: 0.0913 sec.
iter 205290 || Loss: 0.6970 || timer: 0.0892 sec.
iter 205300 || Loss: 1.3030 || timer: 0.0894 sec.
iter 205310 || Loss: 1.1736 || timer: 0.0990 sec.
iter 205320 || Loss: 0.9747 || timer: 0.0941 sec.
iter 205330 || Loss: 0.9933 || timer: 0.0901 sec.
iter 205340 || Loss: 1.3956 || timer: 0.0929 sec.
iter 205350 || Loss: 0.9125 || timer: 0.0837 sec.
iter 205360 || Loss: 1.0176 || timer: 0.0832 sec.
iter 205370 || Loss: 0.9126 || timer: 0.0902 sec.
iter 205380 || Loss: 0.9187 || timer: 0.0929 sec.
iter 205390 || Loss: 0.8516 || timer: 0.0901 sec.
iter 205400 || Loss: 0.7483 || timer: 0.0911 sec.
iter 205410 || Loss: 0.6737 || timer: 0.0968 sec.
iter 205420 || Loss: 0.9511 || timer: 0.0844 sec.
iter 205430 || Loss: 0.7778 || timer: 0.0881 sec.
iter 205440 || Loss: 0.8743 || timer: 0.0845 sec.
iter 205450 || Loss: 1.3536 || timer: 0.1106 sec.
iter 205460 || Loss: 1.1155 || timer: 0.0899 sec.
iter 205470 || Loss: 0.9138 || timer: 0.0884 sec.
iter 205480 || Loss: 1.3326 || timer: 0.0196 sec.
iter 205490 || Loss: 3.2427 || timer: 0.0826 sec.
iter 205500 || Loss: 1.0900 || timer: 0.0839 sec.
iter 205510 || Loss: 1.0841 || timer: 0.0824 sec.
iter 205520 || Loss: 0.8459 || timer: 0.0903 sec.
iter 205530 || Loss: 0.9489 || timer: 0.0824 sec.
iter 205540 || Loss: 1.0007 || timer: 0.0916 sec.
iter 205550 || Loss: 0.7610 || timer: 0.0936 sec.
iter 205560 || Loss: 1.0287 || timer: 0.1050 sec.
iter 205570 || Loss: 1.0711 || timer: 0.0828 sec.
iter 205580 || Loss: 0.7922 || timer: 0.1184 sec.
iter 205590 || Loss: 1.1021 || timer: 0.0882 sec.
iter 205600 || Loss: 1.2721 || timer: 0.0952 sec.
iter 205610 || Loss: 0.8936 || timer: 0.0909 sec.
iter 205620 || Loss: 0.9190 || timer: 0.0886 sec.
iter 205630 || Loss: 0.7548 || timer: 0.0899 sec.
iter 205640 || Loss: 0.7151 || timer: 0.1439 sec.
iter 205650 || Loss: 0.9205 || timer: 0.0831 sec.
iter 205660 || Loss: 0.7830 || timer: 0.0820 sec.
iter 205670 || Loss: 0.9204 || timer: 0.0821 sec.
iter 205680 || Loss: 0.8023 || timer: 0.1101 sec.
iter 205690 || Loss: 0.7534 || timer: 0.0899 sec.
iter 205700 || Loss: 1.3787 || timer: 0.0923 sec.
iter 205710 || Loss: 0.8106 || timer: 0.0906 sec.
iter 205720 || Loss: 0.9110 || timer: 0.1068 sec.
iter 205730 || Loss: 1.1650 || timer: 0.0912 sec.
iter 205740 || Loss: 0.7726 || timer: 0.0895 sec.
iter 205750 || Loss: 0.6350 || timer: 0.1048 sec.
iter 205760 || Loss: 1.5659 || timer: 0.0905 sec.
iter 205770 || Loss: 1.3700 || timer: 0.0840 sec.
iter 205780 || Loss: 0.8376 || timer: 0.1093 sec.
iter 205790 || Loss: 0.7853 || timer: 0.0873 sec.
iter 205800 || Loss: 1.2511 || timer: 0.0901 sec.
iter 205810 || Loss: 0.8930 || timer: 0.0251 sec.
iter 205820 || Loss: 0.7374 || timer: 0.0914 sec.
iter 205830 || Loss: 0.8640 || timer: 0.0929 sec.
iter 205840 || Loss: 0.7335 || timer: 0.0835 sec.
iter 205850 || Loss: 1.3294 || timer: 0.0870 sec.
iter 205860 || Loss: 0.8757 || timer: 0.0827 sec.
iter 205870 || Loss: 1.0581 || timer: 0.0899 sec.
iter 205880 || Loss: 0.9457 || timer: 0.0920 sec.
iter 205890 || Loss: 1.0012 || timer: 0.0946 sec.
iter 205900 || Loss: 0.9031 || timer: 0.0842 sec.
iter 205910 || Loss: 0.8205 || timer: 0.1132 sec.
iter 205920 || Loss: 0.8636 || timer: 0.0895 sec.
iter 205930 || Loss: 1.0085 || timer: 0.0916 sec.
iter 205940 || Loss: 0.7666 || timer: 0.0917 sec.
iter 205950 || Loss: 0.8822 || timer: 0.0928 sec.
iter 205960 || Loss: 0.8314 || timer: 0.0927 sec.
iter 205970 || Loss: 0.7851 || timer: 0.0833 sec.
iter 205980 || Loss: 1.0720 || timer: 0.0902 sec.
iter 205990 || Loss: 0.7379 || timer: 0.0824 sec.
iter 206000 || Loss: 0.9304 || timer: 0.0825 sec.
iter 206010 || Loss: 0.9076 || timer: 0.0825 sec.
iter 206020 || Loss: 0.9526 || timer: 0.0877 sec.
iter 206030 || Loss: 0.6830 || timer: 0.0998 sec.
iter 206040 || Loss: 0.7914 || timer: 0.0831 sec.
iter 206050 || Loss: 0.9045 || timer: 0.0893 sec.
iter 206060 || Loss: 0.8502 || timer: 0.0880 sec.
iter 206070 || Loss: 0.7419 || timer: 0.0957 sec.
iter 206080 || Loss: 1.0326 || timer: 0.0830 sec.
iter 206090 || Loss: 0.6737 || timer: 0.0862 sec.
iter 206100 || Loss: 0.9212 || timer: 0.0887 sec.
iter 206110 || Loss: 0.5882 || timer: 0.0908 sec.
iter 206120 || Loss: 1.0114 || timer: 0.0828 sec.
iter 206130 || Loss: 0.7353 || timer: 0.0906 sec.
iter 206140 || Loss: 0.8246 || timer: 0.0203 sec.
iter 206150 || Loss: 0.5911 || timer: 0.0897 sec.
iter 206160 || Loss: 1.2929 || timer: 0.0920 sec.
iter 206170 || Loss: 0.7784 || timer: 0.0911 sec.
iter 206180 || Loss: 1.1199 || timer: 0.0902 sec.
iter 206190 || Loss: 1.0658 || timer: 0.0905 sec.
iter 206200 || Loss: 0.6865 || timer: 0.1215 sec.
iter 206210 || Loss: 1.1702 || timer: 0.0862 sec.
iter 206220 || Loss: 1.0248 || timer: 0.0910 sec.
iter 206230 || Loss: 1.2467 || timer: 0.1110 sec.
iter 206240 || Loss: 1.0788 || timer: 0.0997 sec.
iter 206250 || Loss: 0.7019 || timer: 0.0925 sec.
iter 206260 || Loss: 0.9219 || timer: 0.0916 sec.
iter 206270 || Loss: 0.8352 || timer: 0.0842 sec.
iter 206280 || Loss: 0.7687 || timer: 0.0892 sec.
iter 206290 || Loss: 1.1578 || timer: 0.1105 sec.
iter 206300 || Loss: 0.8556 || timer: 0.0833 sec.
iter 206310 || Loss: 0.9878 || timer: 0.0829 sec.
iter 206320 || Loss: 0.9856 || timer: 0.1314 sec.
iter 206330 || Loss: 0.9883 || timer: 0.0823 sec.
iter 206340 || Loss: 1.2216 || timer: 0.0944 sec.
iter 206350 || Loss: 0.9798 || timer: 0.1021 sec.
iter 206360 || Loss: 0.7241 || timer: 0.1002 sec.
iter 206370 || Loss: 0.9910 || timer: 0.0840 sec.
iter 206380 || Loss: 0.8662 || timer: 0.0963 sec.
iter 206390 || Loss: 0.8660 || timer: 0.1027 sec.
iter 206400 || Loss: 1.0027 || timer: 0.0891 sec.
iter 206410 || Loss: 1.0911 || timer: 0.0891 sec.
iter 206420 || Loss: 1.0148 || timer: 0.1126 sec.
iter 206430 || Loss: 1.0065 || timer: 0.0951 sec.
iter 206440 || Loss: 0.8540 || timer: 0.0865 sec.
iter 206450 || Loss: 0.7677 || timer: 0.0904 sec.
iter 206460 || Loss: 0.8451 || timer: 0.0891 sec.
iter 206470 || Loss: 0.8438 || timer: 0.0269 sec.
iter 206480 || Loss: 0.9471 || timer: 0.0818 sec.
iter 206490 || Loss: 0.6779 || timer: 0.0838 sec.
iter 206500 || Loss: 0.8662 || timer: 0.0895 sec.
iter 206510 || Loss: 0.8104 || timer: 0.0899 sec.
iter 206520 || Loss: 0.6711 || timer: 0.0844 sec.
iter 206530 || Loss: 0.8054 || timer: 0.1000 sec.
iter 206540 || Loss: 0.9028 || timer: 0.0973 sec.
iter 206550 || Loss: 1.1403 || timer: 0.0821 sec.
iter 206560 || Loss: 0.6044 || timer: 0.0975 sec.
iter 206570 || Loss: 0.6961 || timer: 0.1166 sec.
iter 206580 || Loss: 0.5710 || timer: 0.0903 sec.
iter 206590 || Loss: 0.6519 || timer: 0.0903 sec.
iter 206600 || Loss: 0.9677 || timer: 0.0925 sec.
iter 206610 || Loss: 0.6284 || timer: 0.0897 sec.
iter 206620 || Loss: 0.7687 || timer: 0.0918 sec.
iter 206630 || Loss: 1.1416 || timer: 0.0900 sec.
iter 206640 || Loss: 0.5035 || timer: 0.0875 sec.
iter 206650 || Loss: 1.0042 || timer: 0.0897 sec.
iter 206660 || Loss: 1.0104 || timer: 0.0958 sec.
iter 206670 || Loss: 1.1463 || timer: 0.1029 sec.
iter 206680 || Loss: 0.9339 || timer: 0.0911 sec.
iter 206690 || Loss: 1.0372 || timer: 0.0923 sec.
iter 206700 || Loss: 1.1218 || timer: 0.0828 sec.
iter 206710 || Loss: 0.7686 || timer: 0.0806 sec.
iter 206720 || Loss: 0.7459 || timer: 0.0906 sec.
iter 206730 || Loss: 1.2540 || timer: 0.0907 sec.
iter 206740 || Loss: 0.9614 || timer: 0.1019 sec.
iter 206750 || Loss: 0.9297 || timer: 0.0947 sec.
iter 206760 || Loss: 0.9749 || timer: 0.0901 sec.
iter 206770 || Loss: 0.7503 || timer: 0.0851 sec.
iter 206780 || Loss: 0.4970 || timer: 0.1099 sec.
iter 206790 || Loss: 0.9187 || timer: 0.0917 sec.
iter 206800 || Loss: 0.8789 || timer: 0.0218 sec.
iter 206810 || Loss: 0.7287 || timer: 0.0949 sec.
iter 206820 || Loss: 0.6816 || timer: 0.0895 sec.
iter 206830 || Loss: 0.7621 || timer: 0.0881 sec.
iter 206840 || Loss: 0.9470 || timer: 0.0834 sec.
iter 206850 || Loss: 0.7157 || timer: 0.0910 sec.
iter 206860 || Loss: 1.1087 || timer: 0.0936 sec.
iter 206870 || Loss: 1.0434 || timer: 0.1080 sec.
iter 206880 || Loss: 0.9905 || timer: 0.1020 sec.
iter 206890 || Loss: 0.8073 || timer: 0.1068 sec.
iter 206900 || Loss: 0.7716 || timer: 0.0996 sec.
iter 206910 || Loss: 0.7762 || timer: 0.0889 sec.
iter 206920 || Loss: 1.0131 || timer: 0.0935 sec.
iter 206930 || Loss: 0.8645 || timer: 0.0923 sec.
iter 206940 || Loss: 1.1119 || timer: 0.1124 sec.
iter 206950 || Loss: 1.1352 || timer: 0.0897 sec.
iter 206960 || Loss: 0.9351 || timer: 0.0833 sec.
iter 206970 || Loss: 1.0616 || timer: 0.0902 sec.
iter 206980 || Loss: 0.6962 || timer: 0.1108 sec.
iter 206990 || Loss: 0.8888 || timer: 0.0853 sec.
iter 207000 || Loss: 1.0430 || timer: 0.0896 sec.
iter 207010 || Loss: 0.8518 || timer: 0.0900 sec.
iter 207020 || Loss: 0.8442 || timer: 0.1147 sec.
iter 207030 || Loss: 0.7883 || timer: 0.0899 sec.
iter 207040 || Loss: 1.0111 || timer: 0.1142 sec.
iter 207050 || Loss: 0.8987 || timer: 0.0922 sec.
iter 207060 || Loss: 0.8579 || timer: 0.0912 sec.
iter 207070 || Loss: 0.7012 || timer: 0.1110 sec.
iter 207080 || Loss: 1.0495 || timer: 0.0838 sec.
iter 207090 || Loss: 0.8346 || timer: 0.0874 sec.
iter 207100 || Loss: 0.6772 || timer: 0.0908 sec.
iter 207110 || Loss: 0.7911 || timer: 0.0913 sec.
iter 207120 || Loss: 0.9357 || timer: 0.0858 sec.
iter 207130 || Loss: 0.7464 || timer: 0.0252 sec.
iter 207140 || Loss: 1.3758 || timer: 0.0881 sec.
iter 207150 || Loss: 0.9135 || timer: 0.1318 sec.
iter 207160 || Loss: 0.7508 || timer: 0.0895 sec.
iter 207170 || Loss: 0.9531 || timer: 0.0917 sec.
iter 207180 || Loss: 1.3536 || timer: 0.0898 sec.
iter 207190 || Loss: 0.6640 || timer: 0.0865 sec.
iter 207200 || Loss: 0.9742 || timer: 0.0915 sec.
iter 207210 || Loss: 0.8430 || timer: 0.0834 sec.
iter 207220 || Loss: 0.9246 || timer: 0.1050 sec.
iter 207230 || Loss: 0.5190 || timer: 0.1155 sec.
iter 207240 || Loss: 1.1355 || timer: 0.0823 sec.
iter 207250 || Loss: 1.2714 || timer: 0.1041 sec.
iter 207260 || Loss: 0.8909 || timer: 0.0847 sec.
iter 207270 || Loss: 1.0220 || timer: 0.0919 sec.
iter 207280 || Loss: 0.8642 || timer: 0.0822 sec.
iter 207290 || Loss: 0.7819 || timer: 0.0821 sec.
iter 207300 || Loss: 0.7920 || timer: 0.1069 sec.
iter 207310 || Loss: 1.2200 || timer: 0.0871 sec.
iter 207320 || Loss: 1.1878 || timer: 0.0985 sec.
iter 207330 || Loss: 1.1953 || timer: 0.0820 sec.
iter 207340 || Loss: 0.6416 || timer: 0.0891 sec.
iter 207350 || Loss: 0.8250 || timer: 0.0834 sec.
iter 207360 || Loss: 0.8771 || timer: 0.0915 sec.
iter 207370 || Loss: 1.3051 || timer: 0.0921 sec.
iter 207380 || Loss: 0.9446 || timer: 0.0824 sec.
iter 207390 || Loss: 0.8541 || timer: 0.0826 sec.
iter 207400 || Loss: 0.8872 || timer: 0.0901 sec.
iter 207410 || Loss: 0.9318 || timer: 0.1126 sec.
iter 207420 || Loss: 0.6846 || timer: 0.0897 sec.
iter 207430 || Loss: 0.7090 || timer: 0.1098 sec.
iter 207440 || Loss: 0.9086 || timer: 0.0903 sec.
iter 207450 || Loss: 1.1743 || timer: 0.0921 sec.
iter 207460 || Loss: 0.8378 || timer: 0.0201 sec.
iter 207470 || Loss: 1.3794 || timer: 0.0907 sec.
iter 207480 || Loss: 1.1999 || timer: 0.0921 sec.
iter 207490 || Loss: 0.9369 || timer: 0.0825 sec.
iter 207500 || Loss: 0.6951 || timer: 0.0916 sec.
iter 207510 || Loss: 1.3400 || timer: 0.0979 sec.
iter 207520 || Loss: 1.0414 || timer: 0.0985 sec.
iter 207530 || Loss: 1.3528 || timer: 0.0823 sec.
iter 207540 || Loss: 0.7093 || timer: 0.0861 sec.
iter 207550 || Loss: 0.7459 || timer: 0.1185 sec.
iter 207560 || Loss: 0.7540 || timer: 0.1206 sec.
iter 207570 || Loss: 0.8527 || timer: 0.0903 sec.
iter 207580 || Loss: 0.9160 || timer: 0.1056 sec.
iter 207590 || Loss: 1.0225 || timer: 0.1145 sec.
iter 207600 || Loss: 0.9956 || timer: 0.0886 sec.
iter 207610 || Loss: 0.8713 || timer: 0.1028 sec.
iter 207620 || Loss: 0.9865 || timer: 0.0897 sec.
iter 207630 || Loss: 0.6375 || timer: 0.0963 sec.
iter 207640 || Loss: 0.7989 || timer: 0.0903 sec.
iter 207650 || Loss: 0.7085 || timer: 0.0896 sec.
iter 207660 || Loss: 0.7074 || timer: 0.0980 sec.
iter 207670 || Loss: 0.8040 || timer: 0.0829 sec.
iter 207680 || Loss: 0.8280 || timer: 0.0896 sec.
iter 207690 || Loss: 0.7666 || timer: 0.0922 sec.
iter 207700 || Loss: 0.8313 || timer: 0.0901 sec.
iter 207710 || Loss: 0.7959 || timer: 0.1003 sec.
iter 207720 || Loss: 0.6295 || timer: 0.0908 sec.
iter 207730 || Loss: 0.8311 || timer: 0.1129 sec.
iter 207740 || Loss: 0.8031 || timer: 0.0765 sec.
iter 207750 || Loss: 0.8782 || timer: 0.0907 sec.
iter 207760 || Loss: 0.6418 || timer: 0.0826 sec.
iter 207770 || Loss: 0.8443 || timer: 0.0828 sec.
iter 207780 || Loss: 1.0687 || timer: 0.0910 sec.
iter 207790 || Loss: 0.7461 || timer: 0.0193 sec.
iter 207800 || Loss: 0.8094 || timer: 0.0855 sec.
iter 207810 || Loss: 0.8837 || timer: 0.1097 sec.
iter 207820 || Loss: 0.8399 || timer: 0.0948 sec.
iter 207830 || Loss: 0.9998 || timer: 0.0923 sec.
iter 207840 || Loss: 1.0089 || timer: 0.0870 sec.
iter 207850 || Loss: 0.8363 || timer: 0.0883 sec.
iter 207860 || Loss: 0.8008 || timer: 0.0833 sec.
iter 207870 || Loss: 0.5585 || timer: 0.0871 sec.
iter 207880 || Loss: 0.6975 || timer: 0.0904 sec.
iter 207890 || Loss: 0.8640 || timer: 0.0942 sec.
iter 207900 || Loss: 0.9820 || timer: 0.0914 sec.
iter 207910 || Loss: 0.9001 || timer: 0.0874 sec.
iter 207920 || Loss: 0.6894 || timer: 0.0909 sec.
iter 207930 || Loss: 1.0370 || timer: 0.0902 sec.
iter 207940 || Loss: 0.8776 || timer: 0.0897 sec.
iter 207950 || Loss: 0.8364 || timer: 0.0893 sec.
iter 207960 || Loss: 0.8607 || timer: 0.1051 sec.
iter 207970 || Loss: 0.9849 || timer: 0.0876 sec.
iter 207980 || Loss: 0.9256 || timer: 0.0915 sec.
iter 207990 || Loss: 0.6800 || timer: 0.0988 sec.
iter 208000 || Loss: 0.8773 || timer: 0.1031 sec.
iter 208010 || Loss: 1.0132 || timer: 0.0925 sec.
iter 208020 || Loss: 1.0372 || timer: 0.0897 sec.
iter 208030 || Loss: 0.7662 || timer: 0.0834 sec.
iter 208040 || Loss: 1.0125 || timer: 0.0915 sec.
iter 208050 || Loss: 0.7748 || timer: 0.0915 sec.
iter 208060 || Loss: 1.0343 || timer: 0.0832 sec.
iter 208070 || Loss: 0.7299 || timer: 0.0854 sec.
iter 208080 || Loss: 0.9618 || timer: 0.0823 sec.
iter 208090 || Loss: 0.7874 || timer: 0.0945 sec.
iter 208100 || Loss: 1.0733 || timer: 0.0823 sec.
iter 208110 || Loss: 0.8069 || timer: 0.0904 sec.
iter 208120 || Loss: 0.9907 || timer: 0.0201 sec.
iter 208130 || Loss: 1.9777 || timer: 0.0891 sec.
iter 208140 || Loss: 1.4159 || timer: 0.0906 sec.
iter 208150 || Loss: 0.7112 || timer: 0.0864 sec.
iter 208160 || Loss: 0.6182 || timer: 0.0830 sec.
iter 208170 || Loss: 0.7774 || timer: 0.0927 sec.
iter 208180 || Loss: 0.9193 || timer: 0.1034 sec.
iter 208190 || Loss: 0.9476 || timer: 0.0828 sec.
iter 208200 || Loss: 0.9506 || timer: 0.0924 sec.
iter 208210 || Loss: 0.6611 || timer: 0.0928 sec.
iter 208220 || Loss: 0.8215 || timer: 0.0997 sec.
iter 208230 || Loss: 0.7272 || timer: 0.0960 sec.
iter 208240 || Loss: 0.8432 || timer: 0.0899 sec.
iter 208250 || Loss: 0.8733 || timer: 0.0845 sec.
iter 208260 || Loss: 0.6603 || timer: 0.1026 sec.
iter 208270 || Loss: 1.0387 || timer: 0.0896 sec.
iter 208280 || Loss: 1.1437 || timer: 0.1054 sec.
iter 208290 || Loss: 0.8715 || timer: 0.0928 sec.
iter 208300 || Loss: 0.9024 || timer: 0.0829 sec.
iter 208310 || Loss: 0.8200 || timer: 0.0909 sec.
iter 208320 || Loss: 1.4131 || timer: 0.1099 sec.
iter 208330 || Loss: 1.2312 || timer: 0.0900 sec.
iter 208340 || Loss: 0.8621 || timer: 0.0914 sec.
iter 208350 || Loss: 0.8597 || timer: 0.0908 sec.
iter 208360 || Loss: 0.8515 || timer: 0.0825 sec.
iter 208370 || Loss: 0.7630 || timer: 0.0875 sec.
iter 208380 || Loss: 0.9547 || timer: 0.0917 sec.
iter 208390 || Loss: 0.7551 || timer: 0.0958 sec.
iter 208400 || Loss: 0.6592 || timer: 0.0896 sec.
iter 208410 || Loss: 0.8517 || timer: 0.0827 sec.
iter 208420 || Loss: 0.8570 || timer: 0.0865 sec.
iter 208430 || Loss: 1.1637 || timer: 0.0911 sec.
iter 208440 || Loss: 1.6533 || timer: 0.0922 sec.
iter 208450 || Loss: 0.6810 || timer: 0.0250 sec.
iter 208460 || Loss: 1.3319 || timer: 0.1002 sec.
iter 208470 || Loss: 1.0876 || timer: 0.0903 sec.
iter 208480 || Loss: 0.8290 || timer: 0.0890 sec.
iter 208490 || Loss: 0.7347 || timer: 0.0921 sec.
iter 208500 || Loss: 0.9441 || timer: 0.0917 sec.
iter 208510 || Loss: 0.8729 || timer: 0.0899 sec.
iter 208520 || Loss: 1.0304 || timer: 0.0823 sec.
iter 208530 || Loss: 0.8877 || timer: 0.0923 sec.
iter 208540 || Loss: 0.7815 || timer: 0.0836 sec.
iter 208550 || Loss: 1.0916 || timer: 0.1199 sec.
iter 208560 || Loss: 1.2273 || timer: 0.0923 sec.
iter 208570 || Loss: 0.7053 || timer: 0.1087 sec.
iter 208580 || Loss: 1.1227 || timer: 0.0855 sec.
iter 208590 || Loss: 0.9338 || timer: 0.0938 sec.
iter 208600 || Loss: 1.0958 || timer: 0.0828 sec.
iter 208610 || Loss: 0.9210 || timer: 0.0924 sec.
iter 208620 || Loss: 1.3133 || timer: 0.0837 sec.
iter 208630 || Loss: 0.9219 || timer: 0.0841 sec.
iter 208640 || Loss: 0.8605 || timer: 0.1004 sec.
iter 208650 || Loss: 0.9006 || timer: 0.0837 sec.
iter 208660 || Loss: 1.1995 || timer: 0.1410 sec.
iter 208670 || Loss: 0.7743 || timer: 0.0845 sec.
iter 208680 || Loss: 0.9285 || timer: 0.0912 sec.
iter 208690 || Loss: 0.7637 || timer: 0.0939 sec.
iter 208700 || Loss: 0.7642 || timer: 0.0827 sec.
iter 208710 || Loss: 0.9310 || timer: 0.0877 sec.
iter 208720 || Loss: 1.1539 || timer: 0.0860 sec.
iter 208730 || Loss: 1.2341 || timer: 0.0917 sec.
iter 208740 || Loss: 0.8147 || timer: 0.0880 sec.
iter 208750 || Loss: 0.9678 || timer: 0.0829 sec.
iter 208760 || Loss: 0.6967 || timer: 0.0889 sec.
iter 208770 || Loss: 1.0427 || timer: 0.0826 sec.
iter 208780 || Loss: 0.8906 || timer: 0.0175 sec.
iter 208790 || Loss: 0.7243 || timer: 0.1197 sec.
iter 208800 || Loss: 0.7840 || timer: 0.0877 sec.
iter 208810 || Loss: 0.6293 || timer: 0.0927 sec.
iter 208820 || Loss: 0.8333 || timer: 0.0827 sec.
iter 208830 || Loss: 0.8331 || timer: 0.0901 sec.
iter 208840 || Loss: 1.0668 || timer: 0.0917 sec.
iter 208850 || Loss: 0.8411 || timer: 0.0917 sec.
iter 208860 || Loss: 0.8079 || timer: 0.1062 sec.
iter 208870 || Loss: 0.7266 || timer: 0.0873 sec.
iter 208880 || Loss: 0.8777 || timer: 0.0977 sec.
iter 208890 || Loss: 0.6277 || timer: 0.0866 sec.
iter 208900 || Loss: 0.7979 || timer: 0.0904 sec.
iter 208910 || Loss: 1.0608 || timer: 0.0833 sec.
iter 208920 || Loss: 0.6449 || timer: 0.0908 sec.
iter 208930 || Loss: 1.1472 || timer: 0.0905 sec.
iter 208940 || Loss: 0.8788 || timer: 0.0924 sec.
iter 208950 || Loss: 0.9412 || timer: 0.0893 sec.
iter 208960 || Loss: 0.7236 || timer: 0.0922 sec.
iter 208970 || Loss: 1.0878 || timer: 0.0925 sec.
iter 208980 || Loss: 0.8835 || timer: 0.1005 sec.
iter 208990 || Loss: 1.1540 || timer: 0.0795 sec.
iter 209000 || Loss: 0.9182 || timer: 0.0917 sec.
iter 209010 || Loss: 0.8004 || timer: 0.0913 sec.
iter 209020 || Loss: 0.7432 || timer: 0.0897 sec.
iter 209030 || Loss: 0.6337 || timer: 0.1258 sec.
iter 209040 || Loss: 0.9671 || timer: 0.0830 sec.
iter 209050 || Loss: 1.1370 || timer: 0.1058 sec.
iter 209060 || Loss: 0.6938 || timer: 0.0832 sec.
iter 209070 || Loss: 0.8140 || timer: 0.0953 sec.
iter 209080 || Loss: 1.0978 || timer: 0.0904 sec.
iter 209090 || Loss: 0.7625 || timer: 0.0920 sec.
iter 209100 || Loss: 0.7933 || timer: 0.0837 sec.
iter 209110 || Loss: 0.7580 || timer: 0.0230 sec.
iter 209120 || Loss: 0.9609 || timer: 0.1063 sec.
iter 209130 || Loss: 0.7274 || timer: 0.0834 sec.
iter 209140 || Loss: 0.7488 || timer: 0.0941 sec.
iter 209150 || Loss: 1.0880 || timer: 0.0896 sec.
iter 209160 || Loss: 0.9153 || timer: 0.0920 sec.
iter 209170 || Loss: 0.8981 || timer: 0.0911 sec.
iter 209180 || Loss: 1.1094 || timer: 0.0833 sec.
iter 209190 || Loss: 0.8105 || timer: 0.0906 sec.
iter 209200 || Loss: 0.6176 || timer: 0.0910 sec.
iter 209210 || Loss: 0.8192 || timer: 0.0965 sec.
iter 209220 || Loss: 0.7529 || timer: 0.0908 sec.
iter 209230 || Loss: 1.2212 || timer: 0.0895 sec.
iter 209240 || Loss: 0.7171 || timer: 0.1048 sec.
iter 209250 || Loss: 1.1581 || timer: 0.0918 sec.
iter 209260 || Loss: 0.5638 || timer: 0.0827 sec.
iter 209270 || Loss: 0.9697 || timer: 0.0904 sec.
iter 209280 || Loss: 0.9708 || timer: 0.0896 sec.
iter 209290 || Loss: 0.6065 || timer: 0.0923 sec.
iter 209300 || Loss: 0.8977 || timer: 0.0880 sec.
iter 209310 || Loss: 0.6734 || timer: 0.0935 sec.
iter 209320 || Loss: 0.9543 || timer: 0.0854 sec.
iter 209330 || Loss: 1.0842 || timer: 0.0935 sec.
iter 209340 || Loss: 1.2797 || timer: 0.0870 sec.
iter 209350 || Loss: 0.8917 || timer: 0.0889 sec.
iter 209360 || Loss: 1.0113 || timer: 0.0845 sec.
iter 209370 || Loss: 0.9605 || timer: 0.0827 sec.
iter 209380 || Loss: 0.9584 || timer: 0.0951 sec.
iter 209390 || Loss: 0.9438 || timer: 0.1024 sec.
iter 209400 || Loss: 0.6923 || timer: 0.0904 sec.
iter 209410 || Loss: 0.8346 || timer: 0.1152 sec.
iter 209420 || Loss: 0.7065 || timer: 0.0891 sec.
iter 209430 || Loss: 0.8544 || timer: 0.1018 sec.
iter 209440 || Loss: 0.7676 || timer: 0.0259 sec.
iter 209450 || Loss: 0.4878 || timer: 0.0898 sec.
iter 209460 || Loss: 0.9668 || timer: 0.0836 sec.
iter 209470 || Loss: 0.8184 || timer: 0.0951 sec.
iter 209480 || Loss: 0.9600 || timer: 0.0831 sec.
iter 209490 || Loss: 0.7384 || timer: 0.0892 sec.
iter 209500 || Loss: 1.0171 || timer: 0.0866 sec.
iter 209510 || Loss: 0.8092 || timer: 0.0895 sec.
iter 209520 || Loss: 1.0361 || timer: 0.1082 sec.
iter 209530 || Loss: 1.0895 || timer: 0.0917 sec.
iter 209540 || Loss: 1.1104 || timer: 0.1148 sec.
iter 209550 || Loss: 1.1043 || timer: 0.1071 sec.
iter 209560 || Loss: 1.0288 || timer: 0.0891 sec.
iter 209570 || Loss: 0.7738 || timer: 0.0894 sec.
iter 209580 || Loss: 0.9175 || timer: 0.0916 sec.
iter 209590 || Loss: 1.0436 || timer: 0.0827 sec.
iter 209600 || Loss: 0.8145 || timer: 0.1015 sec.
iter 209610 || Loss: 0.9138 || timer: 0.0907 sec.
iter 209620 || Loss: 0.6727 || timer: 0.0961 sec.
iter 209630 || Loss: 0.7864 || timer: 0.0908 sec.
iter 209640 || Loss: 1.1435 || timer: 0.0894 sec.
iter 209650 || Loss: 1.2300 || timer: 0.1056 sec.
iter 209660 || Loss: 1.2617 || timer: 0.0886 sec.
iter 209670 || Loss: 0.8003 || timer: 0.0897 sec.
iter 209680 || Loss: 1.0808 || timer: 0.0960 sec.
iter 209690 || Loss: 0.8857 || timer: 0.1234 sec.
iter 209700 || Loss: 1.1455 || timer: 0.1072 sec.
iter 209710 || Loss: 0.9382 || timer: 0.0822 sec.
iter 209720 || Loss: 0.8285 || timer: 0.0898 sec.
iter 209730 || Loss: 0.7203 || timer: 0.0823 sec.
iter 209740 || Loss: 0.8858 || timer: 0.0922 sec.
iter 209750 || Loss: 0.9798 || timer: 0.1044 sec.
iter 209760 || Loss: 0.7494 || timer: 0.1053 sec.
iter 209770 || Loss: 0.7558 || timer: 0.0231 sec.
iter 209780 || Loss: 0.5249 || timer: 0.1110 sec.
iter 209790 || Loss: 1.1388 || timer: 0.0899 sec.
iter 209800 || Loss: 0.8278 || timer: 0.0894 sec.
iter 209810 || Loss: 0.8907 || timer: 0.0873 sec.
iter 209820 || Loss: 1.0948 || timer: 0.1056 sec.
iter 209830 || Loss: 0.9567 || timer: 0.0912 sec.
iter 209840 || Loss: 0.8208 || timer: 0.0849 sec.
iter 209850 || Loss: 0.6459 || timer: 0.0925 sec.
iter 209860 || Loss: 1.2228 || timer: 0.0850 sec.
iter 209870 || Loss: 0.9723 || timer: 0.1013 sec.
iter 209880 || Loss: 1.3639 || timer: 0.0965 sec.
iter 209890 || Loss: 0.9638 || timer: 0.0918 sec.
iter 209900 || Loss: 0.8617 || timer: 0.0898 sec.
iter 209910 || Loss: 0.9299 || timer: 0.0919 sec.
iter 209920 || Loss: 1.1222 || timer: 0.0906 sec.
iter 209930 || Loss: 0.8073 || timer: 0.0889 sec.
iter 209940 || Loss: 0.7805 || timer: 0.0830 sec.
iter 209950 || Loss: 0.7668 || timer: 0.0879 sec.
iter 209960 || Loss: 1.0589 || timer: 0.0998 sec.
iter 209970 || Loss: 0.9939 || timer: 0.0851 sec.
iter 209980 || Loss: 1.3428 || timer: 0.1124 sec.
iter 209990 || Loss: 0.8552 || timer: 0.0842 sec.
iter 210000 || Loss: 1.2877 || Saving state, iter: 210000
timer: 0.0927 sec.
iter 210010 || Loss: 0.8408 || timer: 0.0951 sec.
iter 210020 || Loss: 0.7413 || timer: 0.0868 sec.
iter 210030 || Loss: 0.8056 || timer: 0.0960 sec.
iter 210040 || Loss: 0.6252 || timer: 0.0837 sec.
iter 210050 || Loss: 1.0612 || timer: 0.0938 sec.
iter 210060 || Loss: 0.6032 || timer: 0.0868 sec.
iter 210070 || Loss: 1.2084 || timer: 0.0927 sec.
iter 210080 || Loss: 0.7912 || timer: 0.0850 sec.
iter 210090 || Loss: 0.9247 || timer: 0.0829 sec.
iter 210100 || Loss: 1.0526 || timer: 0.0204 sec.
iter 210110 || Loss: 1.2565 || timer: 0.0833 sec.
iter 210120 || Loss: 0.8363 || timer: 0.0949 sec.
iter 210130 || Loss: 1.2137 || timer: 0.0893 sec.
iter 210140 || Loss: 0.9535 || timer: 0.1076 sec.
iter 210150 || Loss: 0.8058 || timer: 0.0906 sec.
iter 210160 || Loss: 0.9134 || timer: 0.1083 sec.
iter 210170 || Loss: 1.0168 || timer: 0.0843 sec.
iter 210180 || Loss: 0.7807 || timer: 0.0943 sec.
iter 210190 || Loss: 1.0729 || timer: 0.0902 sec.
iter 210200 || Loss: 1.2538 || timer: 0.1069 sec.
iter 210210 || Loss: 1.1272 || timer: 0.0889 sec.
iter 210220 || Loss: 1.0290 || timer: 0.0868 sec.
iter 210230 || Loss: 1.0438 || timer: 0.0928 sec.
iter 210240 || Loss: 0.7971 || timer: 0.0824 sec.
iter 210250 || Loss: 1.0041 || timer: 0.1119 sec.
iter 210260 || Loss: 0.6731 || timer: 0.0932 sec.
iter 210270 || Loss: 0.9315 || timer: 0.1272 sec.
iter 210280 || Loss: 0.6312 || timer: 0.0829 sec.
iter 210290 || Loss: 1.2374 || timer: 0.0910 sec.
iter 210300 || Loss: 0.7331 || timer: 0.0935 sec.
iter 210310 || Loss: 1.0423 || timer: 0.0910 sec.
iter 210320 || Loss: 1.1484 || timer: 0.1054 sec.
iter 210330 || Loss: 1.2466 || timer: 0.0836 sec.
iter 210340 || Loss: 0.7060 || timer: 0.0913 sec.
iter 210350 || Loss: 0.9713 || timer: 0.0957 sec.
iter 210360 || Loss: 0.6935 || timer: 0.0984 sec.
iter 210370 || Loss: 0.8050 || timer: 0.1038 sec.
iter 210380 || Loss: 0.8522 || timer: 0.0907 sec.
iter 210390 || Loss: 0.9096 || timer: 0.0914 sec.
iter 210400 || Loss: 0.8430 || timer: 0.1302 sec.
iter 210410 || Loss: 1.0789 || timer: 0.0818 sec.
iter 210420 || Loss: 1.1619 || timer: 0.0938 sec.
iter 210430 || Loss: 0.8877 || timer: 0.0246 sec.
iter 210440 || Loss: 1.9688 || timer: 0.0889 sec.
iter 210450 || Loss: 0.9597 || timer: 0.0870 sec.
iter 210460 || Loss: 1.0696 || timer: 0.0919 sec.
iter 210470 || Loss: 1.1350 || timer: 0.0931 sec.
iter 210480 || Loss: 0.9482 || timer: 0.0840 sec.
iter 210490 || Loss: 0.7374 || timer: 0.0840 sec.
iter 210500 || Loss: 0.8117 || timer: 0.1006 sec.
iter 210510 || Loss: 1.1407 || timer: 0.0913 sec.
iter 210520 || Loss: 0.7927 || timer: 0.0914 sec.
iter 210530 || Loss: 0.8447 || timer: 0.1157 sec.
iter 210540 || Loss: 0.6142 || timer: 0.0827 sec.
iter 210550 || Loss: 1.3019 || timer: 0.0895 sec.
iter 210560 || Loss: 0.8227 || timer: 0.0897 sec.
iter 210570 || Loss: 0.6436 || timer: 0.0910 sec.
iter 210580 || Loss: 1.0967 || timer: 0.0988 sec.
iter 210590 || Loss: 0.9957 || timer: 0.0833 sec.
iter 210600 || Loss: 0.8073 || timer: 0.1160 sec.
iter 210610 || Loss: 0.9510 || timer: 0.0827 sec.
iter 210620 || Loss: 0.8852 || timer: 0.1031 sec.
iter 210630 || Loss: 0.5228 || timer: 0.1260 sec.
iter 210640 || Loss: 0.8435 || timer: 0.0889 sec.
iter 210650 || Loss: 0.8547 || timer: 0.0897 sec.
iter 210660 || Loss: 0.6261 || timer: 0.0917 sec.
iter 210670 || Loss: 1.0106 || timer: 0.1047 sec.
iter 210680 || Loss: 0.9319 || timer: 0.0900 sec.
iter 210690 || Loss: 0.9003 || timer: 0.0891 sec.
iter 210700 || Loss: 0.9782 || timer: 0.0911 sec.
iter 210710 || Loss: 0.8104 || timer: 0.0877 sec.
iter 210720 || Loss: 0.8377 || timer: 0.0846 sec.
iter 210730 || Loss: 0.6900 || timer: 0.1077 sec.
iter 210740 || Loss: 0.6570 || timer: 0.0865 sec.
iter 210750 || Loss: 0.8581 || timer: 0.0912 sec.
iter 210760 || Loss: 0.7932 || timer: 0.0210 sec.
iter 210770 || Loss: 0.6430 || timer: 0.0830 sec.
iter 210780 || Loss: 1.0043 || timer: 0.1039 sec.
iter 210790 || Loss: 1.1926 || timer: 0.1044 sec.
iter 210800 || Loss: 1.1346 || timer: 0.0902 sec.
iter 210810 || Loss: 0.9334 || timer: 0.0885 sec.
iter 210820 || Loss: 1.0484 || timer: 0.0867 sec.
iter 210830 || Loss: 0.8485 || timer: 0.0923 sec.
iter 210840 || Loss: 0.6929 || timer: 0.0897 sec.
iter 210850 || Loss: 0.8288 || timer: 0.0840 sec.
iter 210860 || Loss: 0.7049 || timer: 0.1201 sec.
iter 210870 || Loss: 1.2174 || timer: 0.0892 sec.
iter 210880 || Loss: 1.2876 || timer: 0.0826 sec.
iter 210890 || Loss: 0.8658 || timer: 0.0864 sec.
iter 210900 || Loss: 0.6036 || timer: 0.0912 sec.
iter 210910 || Loss: 0.9643 || timer: 0.0822 sec.
iter 210920 || Loss: 0.7679 || timer: 0.0830 sec.
iter 210930 || Loss: 0.6916 || timer: 0.0828 sec.
iter 210940 || Loss: 0.9785 || timer: 0.1041 sec.
iter 210950 || Loss: 0.6911 || timer: 0.1074 sec.
iter 210960 || Loss: 0.7341 || timer: 0.0924 sec.
iter 210970 || Loss: 0.8344 || timer: 0.1040 sec.
iter 210980 || Loss: 0.7141 || timer: 0.0908 sec.
iter 210990 || Loss: 0.6002 || timer: 0.0831 sec.
iter 211000 || Loss: 0.9505 || timer: 0.0904 sec.
iter 211010 || Loss: 0.8353 || timer: 0.0864 sec.
iter 211020 || Loss: 0.9605 || timer: 0.0846 sec.
iter 211030 || Loss: 1.1925 || timer: 0.0886 sec.
iter 211040 || Loss: 0.7248 || timer: 0.0900 sec.
iter 211050 || Loss: 0.9388 || timer: 0.0859 sec.
iter 211060 || Loss: 0.7384 || timer: 0.0893 sec.
iter 211070 || Loss: 0.7320 || timer: 0.0986 sec.
iter 211080 || Loss: 0.8051 || timer: 0.0900 sec.
iter 211090 || Loss: 0.9365 || timer: 0.0268 sec.
iter 211100 || Loss: 0.4263 || timer: 0.0877 sec.
iter 211110 || Loss: 0.6937 || timer: 0.1037 sec.
iter 211120 || Loss: 0.8535 || timer: 0.0915 sec.
iter 211130 || Loss: 0.9181 || timer: 0.0964 sec.
iter 211140 || Loss: 0.9460 || timer: 0.0910 sec.
iter 211150 || Loss: 0.8025 || timer: 0.0922 sec.
iter 211160 || Loss: 1.0404 || timer: 0.0829 sec.
iter 211170 || Loss: 0.8901 || timer: 0.0913 sec.
iter 211180 || Loss: 0.8277 || timer: 0.0903 sec.
iter 211190 || Loss: 1.2499 || timer: 0.1394 sec.
iter 211200 || Loss: 1.1556 || timer: 0.1108 sec.
iter 211210 || Loss: 0.6225 || timer: 0.0824 sec.
iter 211220 || Loss: 1.2489 || timer: 0.0911 sec.
iter 211230 || Loss: 0.8340 || timer: 0.0921 sec.
iter 211240 || Loss: 0.8445 || timer: 0.0927 sec.
iter 211250 || Loss: 1.0430 || timer: 0.1355 sec.
iter 211260 || Loss: 0.8701 || timer: 0.1021 sec.
iter 211270 || Loss: 1.1058 || timer: 0.0894 sec.
iter 211280 || Loss: 0.9206 || timer: 0.1083 sec.
iter 211290 || Loss: 0.9372 || timer: 0.0846 sec.
iter 211300 || Loss: 0.6848 || timer: 0.0916 sec.
iter 211310 || Loss: 1.2912 || timer: 0.0897 sec.
iter 211320 || Loss: 1.2284 || timer: 0.0837 sec.
iter 211330 || Loss: 0.9891 || timer: 0.0902 sec.
iter 211340 || Loss: 0.9840 || timer: 0.0873 sec.
iter 211350 || Loss: 0.9586 || timer: 0.0920 sec.
iter 211360 || Loss: 0.9553 || timer: 0.0949 sec.
iter 211370 || Loss: 0.6175 || timer: 0.0920 sec.
iter 211380 || Loss: 1.2211 || timer: 0.0913 sec.
iter 211390 || Loss: 1.2810 || timer: 0.0834 sec.
iter 211400 || Loss: 0.9067 || timer: 0.0863 sec.
iter 211410 || Loss: 0.6293 || timer: 0.0828 sec.
iter 211420 || Loss: 0.7876 || timer: 0.0278 sec.
iter 211430 || Loss: 2.2641 || timer: 0.1042 sec.
iter 211440 || Loss: 1.0365 || timer: 0.0903 sec.
iter 211450 || Loss: 1.1706 || timer: 0.0942 sec.
iter 211460 || Loss: 0.9381 || timer: 0.0903 sec.
iter 211470 || Loss: 1.1066 || timer: 0.0842 sec.
iter 211480 || Loss: 0.9445 || timer: 0.0941 sec.
iter 211490 || Loss: 0.8504 || timer: 0.0850 sec.
iter 211500 || Loss: 1.0990 || timer: 0.0938 sec.
iter 211510 || Loss: 0.5006 || timer: 0.1101 sec.
iter 211520 || Loss: 0.8074 || timer: 0.1021 sec.
iter 211530 || Loss: 0.7586 || timer: 0.0877 sec.
iter 211540 || Loss: 0.8501 || timer: 0.0842 sec.
iter 211550 || Loss: 1.1159 || timer: 0.0834 sec.
iter 211560 || Loss: 0.7230 || timer: 0.0969 sec.
iter 211570 || Loss: 0.8626 || timer: 0.1034 sec.
iter 211580 || Loss: 0.9309 || timer: 0.0874 sec.
iter 211590 || Loss: 0.9469 || timer: 0.1054 sec.
iter 211600 || Loss: 1.2505 || timer: 0.0887 sec.
iter 211610 || Loss: 0.8517 || timer: 0.1052 sec.
iter 211620 || Loss: 0.6274 || timer: 0.0913 sec.
iter 211630 || Loss: 1.0200 || timer: 0.0915 sec.
iter 211640 || Loss: 0.7751 || timer: 0.0946 sec.
iter 211650 || Loss: 0.7721 || timer: 0.0913 sec.
iter 211660 || Loss: 0.8681 || timer: 0.0924 sec.
iter 211670 || Loss: 0.5723 || timer: 0.0986 sec.
iter 211680 || Loss: 1.2830 || timer: 0.0844 sec.
iter 211690 || Loss: 0.6546 || timer: 0.0914 sec.
iter 211700 || Loss: 0.9829 || timer: 0.1077 sec.
iter 211710 || Loss: 0.7373 || timer: 0.0935 sec.
iter 211720 || Loss: 0.8959 || timer: 0.0917 sec.
iter 211730 || Loss: 0.9266 || timer: 0.0900 sec.
iter 211740 || Loss: 1.1210 || timer: 0.0915 sec.
iter 211750 || Loss: 0.9299 || timer: 0.0236 sec.
iter 211760 || Loss: 0.8843 || timer: 0.0886 sec.
iter 211770 || Loss: 0.7812 || timer: 0.0940 sec.
iter 211780 || Loss: 0.8739 || timer: 0.0842 sec.
iter 211790 || Loss: 0.8873 || timer: 0.1041 sec.
iter 211800 || Loss: 1.0916 || timer: 0.0895 sec.
iter 211810 || Loss: 0.8786 || timer: 0.0890 sec.
iter 211820 || Loss: 0.7345 || timer: 0.0880 sec.
iter 211830 || Loss: 1.2691 || timer: 0.0932 sec.
iter 211840 || Loss: 0.8881 || timer: 0.0897 sec.
iter 211850 || Loss: 0.6999 || timer: 0.1188 sec.
iter 211860 || Loss: 0.7410 || timer: 0.0890 sec.
iter 211870 || Loss: 1.0421 || timer: 0.0911 sec.
iter 211880 || Loss: 0.9321 || timer: 0.0925 sec.
iter 211890 || Loss: 0.9159 || timer: 0.0947 sec.
iter 211900 || Loss: 1.0187 || timer: 0.1111 sec.
iter 211910 || Loss: 1.0697 || timer: 0.0871 sec.
iter 211920 || Loss: 0.7814 || timer: 0.0906 sec.
iter 211930 || Loss: 1.0455 || timer: 0.0897 sec.
iter 211940 || Loss: 0.6891 || timer: 0.0894 sec.
iter 211950 || Loss: 0.9292 || timer: 0.0869 sec.
iter 211960 || Loss: 1.0246 || timer: 0.0827 sec.
iter 211970 || Loss: 0.8518 || timer: 0.0844 sec.
iter 211980 || Loss: 0.9266 || timer: 0.0898 sec.
iter 211990 || Loss: 0.7509 || timer: 0.0955 sec.
iter 212000 || Loss: 0.9700 || timer: 0.0908 sec.
iter 212010 || Loss: 1.1251 || timer: 0.1078 sec.
iter 212020 || Loss: 0.9770 || timer: 0.0839 sec.
iter 212030 || Loss: 1.1522 || timer: 0.1050 sec.
iter 212040 || Loss: 0.9308 || timer: 0.0925 sec.
iter 212050 || Loss: 1.1355 || timer: 0.0876 sec.
iter 212060 || Loss: 1.1123 || timer: 0.1017 sec.
iter 212070 || Loss: 1.1197 || timer: 0.0837 sec.
iter 212080 || Loss: 0.7637 || timer: 0.0251 sec.
iter 212090 || Loss: 0.6136 || timer: 0.0835 sec.
iter 212100 || Loss: 1.0230 || timer: 0.0913 sec.
iter 212110 || Loss: 1.1322 || timer: 0.0921 sec.
iter 212120 || Loss: 1.0412 || timer: 0.0904 sec.
iter 212130 || Loss: 0.6229 || timer: 0.0943 sec.
iter 212140 || Loss: 0.6763 || timer: 0.0900 sec.
iter 212150 || Loss: 0.7091 || timer: 0.0897 sec.
iter 212160 || Loss: 0.8764 || timer: 0.0991 sec.
iter 212170 || Loss: 0.6280 || timer: 0.0849 sec.
iter 212180 || Loss: 0.6637 || timer: 0.0977 sec.
iter 212190 || Loss: 0.8407 || timer: 0.0899 sec.
iter 212200 || Loss: 0.8671 || timer: 0.0923 sec.
iter 212210 || Loss: 0.6758 || timer: 0.0836 sec.
iter 212220 || Loss: 1.0533 || timer: 0.0909 sec.
iter 212230 || Loss: 0.7704 || timer: 0.0884 sec.
iter 212240 || Loss: 1.0168 || timer: 0.0834 sec.
iter 212250 || Loss: 0.8790 || timer: 0.0908 sec.
iter 212260 || Loss: 1.1050 || timer: 0.0902 sec.
iter 212270 || Loss: 0.8948 || timer: 0.0900 sec.
iter 212280 || Loss: 0.9750 || timer: 0.0877 sec.
iter 212290 || Loss: 0.7711 || timer: 0.0913 sec.
iter 212300 || Loss: 0.8769 || timer: 0.0922 sec.
iter 212310 || Loss: 0.7656 || timer: 0.0878 sec.
iter 212320 || Loss: 0.9317 || timer: 0.0992 sec.
iter 212330 || Loss: 0.8163 || timer: 0.0883 sec.
iter 212340 || Loss: 0.8579 || timer: 0.0839 sec.
iter 212350 || Loss: 1.0289 || timer: 0.0823 sec.
iter 212360 || Loss: 1.0800 || timer: 0.0925 sec.
iter 212370 || Loss: 0.9772 || timer: 0.0823 sec.
iter 212380 || Loss: 1.3951 || timer: 0.0931 sec.
iter 212390 || Loss: 0.8718 || timer: 0.0830 sec.
iter 212400 || Loss: 0.7126 || timer: 0.0831 sec.
iter 212410 || Loss: 1.1087 || timer: 0.0198 sec.
iter 212420 || Loss: 2.1622 || timer: 0.0823 sec.
iter 212430 || Loss: 0.7376 || timer: 0.1020 sec.
iter 212440 || Loss: 1.6182 || timer: 0.1089 sec.
iter 212450 || Loss: 0.9445 || timer: 0.0890 sec.
iter 212460 || Loss: 0.6253 || timer: 0.0838 sec.
iter 212470 || Loss: 0.8828 || timer: 0.0903 sec.
iter 212480 || Loss: 0.9870 || timer: 0.0920 sec.
iter 212490 || Loss: 0.8486 || timer: 0.0917 sec.
iter 212500 || Loss: 0.8849 || timer: 0.0826 sec.
iter 212510 || Loss: 0.9526 || timer: 0.1156 sec.
iter 212520 || Loss: 1.0538 || timer: 0.0920 sec.
iter 212530 || Loss: 0.8288 || timer: 0.0912 sec.
iter 212540 || Loss: 1.0401 || timer: 0.1078 sec.
iter 212550 || Loss: 0.9327 || timer: 0.0915 sec.
iter 212560 || Loss: 0.8612 || timer: 0.0842 sec.
iter 212570 || Loss: 0.9290 || timer: 0.0843 sec.
iter 212580 || Loss: 1.2001 || timer: 0.0909 sec.
iter 212590 || Loss: 0.9963 || timer: 0.0899 sec.
iter 212600 || Loss: 0.9914 || timer: 0.1055 sec.
iter 212610 || Loss: 1.1923 || timer: 0.0836 sec.
iter 212620 || Loss: 0.8153 || timer: 0.1119 sec.
iter 212630 || Loss: 0.9478 || timer: 0.1032 sec.
iter 212640 || Loss: 1.1685 || timer: 0.0897 sec.
iter 212650 || Loss: 0.7799 || timer: 0.0919 sec.
iter 212660 || Loss: 0.6052 || timer: 0.0895 sec.
iter 212670 || Loss: 1.0081 || timer: 0.0890 sec.
iter 212680 || Loss: 0.7479 || timer: 0.0922 sec.
iter 212690 || Loss: 0.9972 || timer: 0.1003 sec.
iter 212700 || Loss: 0.9781 || timer: 0.0906 sec.
iter 212710 || Loss: 0.8489 || timer: 0.0840 sec.
iter 212720 || Loss: 0.8825 || timer: 0.0917 sec.
iter 212730 || Loss: 0.8664 || timer: 0.0887 sec.
iter 212740 || Loss: 0.8553 || timer: 0.0209 sec.
iter 212750 || Loss: 1.0089 || timer: 0.1177 sec.
iter 212760 || Loss: 1.2339 || timer: 0.0825 sec.
iter 212770 || Loss: 1.1668 || timer: 0.0849 sec.
iter 212780 || Loss: 1.0816 || timer: 0.0899 sec.
iter 212790 || Loss: 0.7135 || timer: 0.0910 sec.
iter 212800 || Loss: 0.9281 || timer: 0.0960 sec.
iter 212810 || Loss: 1.0317 || timer: 0.0861 sec.
iter 212820 || Loss: 0.7354 || timer: 0.0919 sec.
iter 212830 || Loss: 1.0758 || timer: 0.1046 sec.
iter 212840 || Loss: 0.8653 || timer: 0.1008 sec.
iter 212850 || Loss: 0.9057 || timer: 0.0909 sec.
iter 212860 || Loss: 0.9451 || timer: 0.0931 sec.
iter 212870 || Loss: 0.8773 || timer: 0.0911 sec.
iter 212880 || Loss: 0.7500 || timer: 0.0878 sec.
iter 212890 || Loss: 0.6355 || timer: 0.1097 sec.
iter 212900 || Loss: 0.9899 || timer: 0.0886 sec.
iter 212910 || Loss: 0.8886 || timer: 0.0921 sec.
iter 212920 || Loss: 0.8926 || timer: 0.0896 sec.
iter 212930 || Loss: 0.8749 || timer: 0.0909 sec.
iter 212940 || Loss: 0.8460 || timer: 0.0943 sec.
iter 212950 || Loss: 0.7448 || timer: 0.1023 sec.
iter 212960 || Loss: 0.8010 || timer: 0.0755 sec.
iter 212970 || Loss: 1.1827 || timer: 0.0841 sec.
iter 212980 || Loss: 0.7997 || timer: 0.0918 sec.
iter 212990 || Loss: 1.3750 || timer: 0.0825 sec.
iter 213000 || Loss: 1.3578 || timer: 0.0763 sec.
iter 213010 || Loss: 1.2241 || timer: 0.0857 sec.
iter 213020 || Loss: 1.1421 || timer: 0.0925 sec.
iter 213030 || Loss: 1.2087 || timer: 0.0891 sec.
iter 213040 || Loss: 1.0334 || timer: 0.0839 sec.
iter 213050 || Loss: 1.0893 || timer: 0.0949 sec.
iter 213060 || Loss: 0.6706 || timer: 0.0881 sec.
iter 213070 || Loss: 0.9522 || timer: 0.0274 sec.
iter 213080 || Loss: 0.8968 || timer: 0.0848 sec.
iter 213090 || Loss: 0.7844 || timer: 0.1028 sec.
iter 213100 || Loss: 0.7056 || timer: 0.0920 sec.
iter 213110 || Loss: 0.9360 || timer: 0.1176 sec.
iter 213120 || Loss: 1.1031 || timer: 0.0914 sec.
iter 213130 || Loss: 0.9347 || timer: 0.0908 sec.
iter 213140 || Loss: 0.9378 || timer: 0.0935 sec.
iter 213150 || Loss: 0.6904 || timer: 0.0823 sec.
iter 213160 || Loss: 0.9760 || timer: 0.0956 sec.
iter 213170 || Loss: 1.0785 || timer: 0.0953 sec.
iter 213180 || Loss: 1.2262 || timer: 0.0818 sec.
iter 213190 || Loss: 0.7981 || timer: 0.0916 sec.
iter 213200 || Loss: 1.1231 || timer: 0.0892 sec.
iter 213210 || Loss: 0.8898 || timer: 0.1072 sec.
iter 213220 || Loss: 0.5420 || timer: 0.0909 sec.
iter 213230 || Loss: 0.8210 || timer: 0.0851 sec.
iter 213240 || Loss: 1.1408 || timer: 0.0889 sec.
iter 213250 || Loss: 1.1029 || timer: 0.1174 sec.
iter 213260 || Loss: 0.8915 || timer: 0.0914 sec.
iter 213270 || Loss: 0.8461 || timer: 0.0930 sec.
iter 213280 || Loss: 0.8367 || timer: 0.0902 sec.
iter 213290 || Loss: 0.8870 || timer: 0.0854 sec.
iter 213300 || Loss: 1.1129 || timer: 0.0937 sec.
iter 213310 || Loss: 0.6084 || timer: 0.0819 sec.
iter 213320 || Loss: 0.7544 || timer: 0.1143 sec.
iter 213330 || Loss: 0.9011 || timer: 0.0852 sec.
iter 213340 || Loss: 1.2889 || timer: 0.0846 sec.
iter 213350 || Loss: 0.9870 || timer: 0.0913 sec.
iter 213360 || Loss: 0.8036 || timer: 0.0999 sec.
iter 213370 || Loss: 0.7624 || timer: 0.0827 sec.
iter 213380 || Loss: 0.6702 || timer: 0.1145 sec.
iter 213390 || Loss: 1.0111 || timer: 0.0827 sec.
iter 213400 || Loss: 0.7280 || timer: 0.0242 sec.
iter 213410 || Loss: 0.3015 || timer: 0.0849 sec.
iter 213420 || Loss: 1.0023 || timer: 0.1007 sec.
iter 213430 || Loss: 0.9462 || timer: 0.1067 sec.
iter 213440 || Loss: 1.2593 || timer: 0.0898 sec.
iter 213450 || Loss: 0.8201 || timer: 0.1115 sec.
iter 213460 || Loss: 0.7440 || timer: 0.0897 sec.
iter 213470 || Loss: 1.1186 || timer: 0.0839 sec.
iter 213480 || Loss: 0.7275 || timer: 0.0824 sec.
iter 213490 || Loss: 0.8549 || timer: 0.1217 sec.
iter 213500 || Loss: 1.0588 || timer: 0.1129 sec.
iter 213510 || Loss: 0.8697 || timer: 0.1033 sec.
iter 213520 || Loss: 1.5402 || timer: 0.1001 sec.
iter 213530 || Loss: 0.8673 || timer: 0.0905 sec.
iter 213540 || Loss: 1.0530 || timer: 0.1095 sec.
iter 213550 || Loss: 1.1273 || timer: 0.0818 sec.
iter 213560 || Loss: 0.8380 || timer: 0.0906 sec.
iter 213570 || Loss: 1.2045 || timer: 0.1098 sec.
iter 213580 || Loss: 1.0579 || timer: 0.0923 sec.
iter 213590 || Loss: 0.7421 || timer: 0.0880 sec.
iter 213600 || Loss: 1.2987 || timer: 0.0888 sec.
iter 213610 || Loss: 0.9430 || timer: 0.0940 sec.
iter 213620 || Loss: 1.2166 || timer: 0.0902 sec.
iter 213630 || Loss: 0.9901 || timer: 0.0906 sec.
iter 213640 || Loss: 0.9020 || timer: 0.0920 sec.
iter 213650 || Loss: 1.1990 || timer: 0.0920 sec.
iter 213660 || Loss: 0.8983 || timer: 0.0913 sec.
iter 213670 || Loss: 0.8547 || timer: 0.0922 sec.
iter 213680 || Loss: 1.1284 || timer: 0.1159 sec.
iter 213690 || Loss: 1.3202 || timer: 0.1245 sec.
iter 213700 || Loss: 0.9210 || timer: 0.0886 sec.
iter 213710 || Loss: 0.7210 || timer: 0.0826 sec.
iter 213720 || Loss: 0.9849 || timer: 0.0883 sec.
iter 213730 || Loss: 0.8456 || timer: 0.0269 sec.
iter 213740 || Loss: 1.0607 || timer: 0.1109 sec.
iter 213750 || Loss: 1.1848 || timer: 0.0867 sec.
iter 213760 || Loss: 0.8685 || timer: 0.0900 sec.
iter 213770 || Loss: 0.7378 || timer: 0.1000 sec.
iter 213780 || Loss: 0.8950 || timer: 0.0920 sec.
iter 213790 || Loss: 0.8015 || timer: 0.0903 sec.
iter 213800 || Loss: 0.7237 || timer: 0.0887 sec.
iter 213810 || Loss: 1.0997 || timer: 0.1052 sec.
iter 213820 || Loss: 1.0610 || timer: 0.0899 sec.
iter 213830 || Loss: 0.9338 || timer: 0.0999 sec.
iter 213840 || Loss: 1.3208 || timer: 0.1100 sec.
iter 213850 || Loss: 0.8421 || timer: 0.0828 sec.
iter 213860 || Loss: 0.8157 || timer: 0.0928 sec.
iter 213870 || Loss: 1.0954 || timer: 0.0877 sec.
iter 213880 || Loss: 0.9128 || timer: 0.0895 sec.
iter 213890 || Loss: 0.7052 || timer: 0.0991 sec.
iter 213900 || Loss: 0.9454 || timer: 0.0826 sec.
iter 213910 || Loss: 0.6662 || timer: 0.0921 sec.
iter 213920 || Loss: 0.9242 || timer: 0.0907 sec.
iter 213930 || Loss: 0.6128 || timer: 0.0945 sec.
iter 213940 || Loss: 0.8921 || timer: 0.0900 sec.
iter 213950 || Loss: 0.9797 || timer: 0.0881 sec.
iter 213960 || Loss: 0.5987 || timer: 0.0908 sec.
iter 213970 || Loss: 0.7845 || timer: 0.1094 sec.
iter 213980 || Loss: 0.6130 || timer: 0.1101 sec.
iter 213990 || Loss: 0.8603 || timer: 0.1069 sec.
iter 214000 || Loss: 0.7232 || timer: 0.0939 sec.
iter 214010 || Loss: 1.5051 || timer: 0.1114 sec.
iter 214020 || Loss: 0.6346 || timer: 0.0887 sec.
iter 214030 || Loss: 0.8637 || timer: 0.0927 sec.
iter 214040 || Loss: 1.0765 || timer: 0.0882 sec.
iter 214050 || Loss: 1.0612 || timer: 0.1027 sec.
iter 214060 || Loss: 0.8000 || timer: 0.0205 sec.
iter 214070 || Loss: 1.3830 || timer: 0.0920 sec.
iter 214080 || Loss: 1.2975 || timer: 0.0880 sec.
iter 214090 || Loss: 0.7650 || timer: 0.0825 sec.
iter 214100 || Loss: 1.0185 || timer: 0.0873 sec.
iter 214110 || Loss: 0.8403 || timer: 0.0897 sec.
iter 214120 || Loss: 0.8763 || timer: 0.0867 sec.
iter 214130 || Loss: 0.9443 || timer: 0.0909 sec.
iter 214140 || Loss: 0.7043 || timer: 0.0841 sec.
iter 214150 || Loss: 1.0783 || timer: 0.1079 sec.
iter 214160 || Loss: 0.9388 || timer: 0.0959 sec.
iter 214170 || Loss: 0.6383 || timer: 0.0891 sec.
iter 214180 || Loss: 1.1920 || timer: 0.0898 sec.
iter 214190 || Loss: 0.8545 || timer: 0.0866 sec.
iter 214200 || Loss: 0.8700 || timer: 0.0886 sec.
iter 214210 || Loss: 0.8611 || timer: 0.0906 sec.
iter 214220 || Loss: 0.9808 || timer: 0.0912 sec.
iter 214230 || Loss: 0.9704 || timer: 0.0932 sec.
iter 214240 || Loss: 0.8172 || timer: 0.0995 sec.
iter 214250 || Loss: 0.7857 || timer: 0.0907 sec.
iter 214260 || Loss: 1.0854 || timer: 0.0830 sec.
iter 214270 || Loss: 0.8254 || timer: 0.0923 sec.
iter 214280 || Loss: 0.8348 || timer: 0.0809 sec.
iter 214290 || Loss: 0.7879 || timer: 0.0832 sec.
iter 214300 || Loss: 1.0420 || timer: 0.0958 sec.
iter 214310 || Loss: 0.9241 || timer: 0.1108 sec.
iter 214320 || Loss: 0.8023 || timer: 0.0975 sec.
iter 214330 || Loss: 0.8071 || timer: 0.1114 sec.
iter 214340 || Loss: 0.9145 || timer: 0.0920 sec.
iter 214350 || Loss: 1.0881 || timer: 0.0914 sec.
iter 214360 || Loss: 0.8136 || timer: 0.0925 sec.
iter 214370 || Loss: 1.1409 || timer: 0.1181 sec.
iter 214380 || Loss: 0.9071 || timer: 0.0875 sec.
iter 214390 || Loss: 0.9602 || timer: 0.0262 sec.
iter 214400 || Loss: 0.7889 || timer: 0.0829 sec.
iter 214410 || Loss: 0.9409 || timer: 0.1098 sec.
iter 214420 || Loss: 0.8863 || timer: 0.0851 sec.
iter 214430 || Loss: 0.8062 || timer: 0.0959 sec.
iter 214440 || Loss: 0.9088 || timer: 0.0947 sec.
iter 214450 || Loss: 0.7819 || timer: 0.0903 sec.
iter 214460 || Loss: 1.2990 || timer: 0.0852 sec.
iter 214470 || Loss: 0.8001 || timer: 0.0925 sec.
iter 214480 || Loss: 0.7505 || timer: 0.0920 sec.
iter 214490 || Loss: 0.8667 || timer: 0.0954 sec.
iter 214500 || Loss: 0.6829 || timer: 0.0895 sec.
iter 214510 || Loss: 0.9583 || timer: 0.0907 sec.
iter 214520 || Loss: 0.8219 || timer: 0.0930 sec.
iter 214530 || Loss: 0.8285 || timer: 0.0857 sec.
iter 214540 || Loss: 1.2185 || timer: 0.1012 sec.
iter 214550 || Loss: 0.7370 || timer: 0.0836 sec.
iter 214560 || Loss: 0.8820 || timer: 0.1134 sec.
iter 214570 || Loss: 1.0340 || timer: 0.0906 sec.
iter 214580 || Loss: 0.7389 || timer: 0.0840 sec.
iter 214590 || Loss: 1.1455 || timer: 0.0890 sec.
iter 214600 || Loss: 1.0420 || timer: 0.0829 sec.
iter 214610 || Loss: 1.1218 || timer: 0.0887 sec.
iter 214620 || Loss: 1.2049 || timer: 0.0835 sec.
iter 214630 || Loss: 1.1519 || timer: 0.1092 sec.
iter 214640 || Loss: 0.9199 || timer: 0.0908 sec.
iter 214650 || Loss: 1.4509 || timer: 0.0830 sec.
iter 214660 || Loss: 0.7894 || timer: 0.0901 sec.
iter 214670 || Loss: 0.9259 || timer: 0.0907 sec.
iter 214680 || Loss: 1.1866 || timer: 0.0908 sec.
iter 214690 || Loss: 0.8877 || timer: 0.0912 sec.
iter 214700 || Loss: 0.9547 || timer: 0.0819 sec.
iter 214710 || Loss: 0.8442 || timer: 0.0983 sec.
iter 214720 || Loss: 0.7614 || timer: 0.0192 sec.
iter 214730 || Loss: 0.2115 || timer: 0.0890 sec.
iter 214740 || Loss: 0.8012 || timer: 0.1181 sec.
iter 214750 || Loss: 1.1443 || timer: 0.0924 sec.
iter 214760 || Loss: 0.8866 || timer: 0.0911 sec.
iter 214770 || Loss: 1.2950 || timer: 0.0932 sec.
iter 214780 || Loss: 0.9211 || timer: 0.0913 sec.
iter 214790 || Loss: 0.7493 || timer: 0.0873 sec.
iter 214800 || Loss: 0.6881 || timer: 0.0874 sec.
iter 214810 || Loss: 1.2280 || timer: 0.1104 sec.
iter 214820 || Loss: 0.7613 || timer: 0.0942 sec.
iter 214830 || Loss: 0.7070 || timer: 0.0909 sec.
iter 214840 || Loss: 0.8264 || timer: 0.0899 sec.
iter 214850 || Loss: 0.6751 || timer: 0.0925 sec.
iter 214860 || Loss: 0.9411 || timer: 0.0900 sec.
iter 214870 || Loss: 0.7968 || timer: 0.1070 sec.
iter 214880 || Loss: 0.7531 || timer: 0.1150 sec.
iter 214890 || Loss: 1.0443 || timer: 0.0893 sec.
iter 214900 || Loss: 0.9575 || timer: 0.0916 sec.
iter 214910 || Loss: 0.8331 || timer: 0.0896 sec.
iter 214920 || Loss: 0.7548 || timer: 0.0902 sec.
iter 214930 || Loss: 0.7364 || timer: 0.0897 sec.
iter 214940 || Loss: 1.2495 || timer: 0.0840 sec.
iter 214950 || Loss: 0.7293 || timer: 0.0923 sec.
iter 214960 || Loss: 1.1917 || timer: 0.1188 sec.
iter 214970 || Loss: 0.7756 || timer: 0.0827 sec.
iter 214980 || Loss: 1.0028 || timer: 0.0843 sec.
iter 214990 || Loss: 0.9936 || timer: 0.0893 sec.
iter 215000 || Loss: 0.9917 || Saving state, iter: 215000
timer: 0.0841 sec.
iter 215010 || Loss: 0.8062 || timer: 0.0957 sec.
iter 215020 || Loss: 1.3467 || timer: 0.0828 sec.
iter 215030 || Loss: 0.5912 || timer: 0.1098 sec.
iter 215040 || Loss: 0.7180 || timer: 0.0892 sec.
iter 215050 || Loss: 1.1101 || timer: 0.0175 sec.
iter 215060 || Loss: 1.4791 || timer: 0.1060 sec.
iter 215070 || Loss: 0.9758 || timer: 0.0878 sec.
iter 215080 || Loss: 0.7825 || timer: 0.0983 sec.
iter 215090 || Loss: 0.7276 || timer: 0.0909 sec.
iter 215100 || Loss: 1.2011 || timer: 0.0904 sec.
iter 215110 || Loss: 0.9579 || timer: 0.0902 sec.
iter 215120 || Loss: 0.8231 || timer: 0.0926 sec.
iter 215130 || Loss: 0.9752 || timer: 0.0828 sec.
iter 215140 || Loss: 0.8031 || timer: 0.0851 sec.
iter 215150 || Loss: 0.9043 || timer: 0.1158 sec.
iter 215160 || Loss: 1.1202 || timer: 0.0931 sec.
iter 215170 || Loss: 0.7609 || timer: 0.1015 sec.
iter 215180 || Loss: 1.3677 || timer: 0.0843 sec.
iter 215190 || Loss: 1.0958 || timer: 0.1123 sec.
iter 215200 || Loss: 0.9851 || timer: 0.0916 sec.
iter 215210 || Loss: 1.0572 || timer: 0.0923 sec.
iter 215220 || Loss: 0.8597 || timer: 0.0902 sec.
iter 215230 || Loss: 0.7937 || timer: 0.0962 sec.
iter 215240 || Loss: 1.0273 || timer: 0.0917 sec.
iter 215250 || Loss: 0.9899 || timer: 0.0926 sec.
iter 215260 || Loss: 0.9954 || timer: 0.1003 sec.
iter 215270 || Loss: 0.8714 || timer: 0.0884 sec.
iter 215280 || Loss: 0.7591 || timer: 0.0918 sec.
iter 215290 || Loss: 1.0932 || timer: 0.1076 sec.
iter 215300 || Loss: 0.8764 || timer: 0.0832 sec.
iter 215310 || Loss: 0.9357 || timer: 0.0896 sec.
iter 215320 || Loss: 1.0998 || timer: 0.0829 sec.
iter 215330 || Loss: 0.8646 || timer: 0.1032 sec.
iter 215340 || Loss: 0.8594 || timer: 0.0927 sec.
iter 215350 || Loss: 0.6811 || timer: 0.0918 sec.
iter 215360 || Loss: 1.0361 || timer: 0.0904 sec.
iter 215370 || Loss: 0.7983 || timer: 0.0888 sec.
iter 215380 || Loss: 0.8665 || timer: 0.0203 sec.
iter 215390 || Loss: 1.1906 || timer: 0.0829 sec.
iter 215400 || Loss: 0.9194 || timer: 0.1048 sec.
iter 215410 || Loss: 0.9307 || timer: 0.0820 sec.
iter 215420 || Loss: 0.8725 || timer: 0.0897 sec.
iter 215430 || Loss: 0.8019 || timer: 0.1213 sec.
iter 215440 || Loss: 1.1819 || timer: 0.0916 sec.
iter 215450 || Loss: 1.1428 || timer: 0.0846 sec.
iter 215460 || Loss: 0.8807 || timer: 0.0890 sec.
iter 215470 || Loss: 0.8473 || timer: 0.0889 sec.
iter 215480 || Loss: 0.9059 || timer: 0.0958 sec.
iter 215490 || Loss: 1.1784 || timer: 0.0944 sec.
iter 215500 || Loss: 1.3008 || timer: 0.1112 sec.
iter 215510 || Loss: 1.0010 || timer: 0.0902 sec.
iter 215520 || Loss: 1.1060 || timer: 0.0897 sec.
iter 215530 || Loss: 1.2517 || timer: 0.1047 sec.
iter 215540 || Loss: 0.9146 || timer: 0.0918 sec.
iter 215550 || Loss: 1.1468 || timer: 0.0896 sec.
iter 215560 || Loss: 0.8528 || timer: 0.0923 sec.
iter 215570 || Loss: 0.9885 || timer: 0.1000 sec.
iter 215580 || Loss: 0.9057 || timer: 0.0846 sec.
iter 215590 || Loss: 0.6654 || timer: 0.0910 sec.
iter 215600 || Loss: 1.0868 || timer: 0.0896 sec.
iter 215610 || Loss: 1.2011 || timer: 0.1114 sec.
iter 215620 || Loss: 0.7653 || timer: 0.0923 sec.
iter 215630 || Loss: 1.1554 || timer: 0.0927 sec.
iter 215640 || Loss: 0.4505 || timer: 0.0897 sec.
iter 215650 || Loss: 1.0886 || timer: 0.0817 sec.
iter 215660 || Loss: 0.6979 || timer: 0.1096 sec.
iter 215670 || Loss: 0.9646 || timer: 0.0894 sec.
iter 215680 || Loss: 0.8121 || timer: 0.0857 sec.
iter 215690 || Loss: 0.8230 || timer: 0.0926 sec.
iter 215700 || Loss: 1.0436 || timer: 0.0947 sec.
iter 215710 || Loss: 0.9509 || timer: 0.0205 sec.
iter 215720 || Loss: 0.2761 || timer: 0.0912 sec.
iter 215730 || Loss: 0.9979 || timer: 0.0839 sec.
iter 215740 || Loss: 0.8408 || timer: 0.0920 sec.
iter 215750 || Loss: 1.1301 || timer: 0.0832 sec.
iter 215760 || Loss: 0.9462 || timer: 0.0901 sec.
iter 215770 || Loss: 0.9010 || timer: 0.0913 sec.
iter 215780 || Loss: 1.2736 || timer: 0.0924 sec.
iter 215790 || Loss: 0.5924 || timer: 0.0922 sec.
iter 215800 || Loss: 0.7892 || timer: 0.0840 sec.
iter 215810 || Loss: 1.3113 || timer: 0.1275 sec.
iter 215820 || Loss: 0.8082 || timer: 0.0839 sec.
iter 215830 || Loss: 0.9855 || timer: 0.0916 sec.
iter 215840 || Loss: 0.8263 || timer: 0.0916 sec.
iter 215850 || Loss: 0.9941 || timer: 0.0964 sec.
iter 215860 || Loss: 1.4086 || timer: 0.0906 sec.
iter 215870 || Loss: 1.3094 || timer: 0.1055 sec.
iter 215880 || Loss: 0.9057 || timer: 0.0955 sec.
iter 215890 || Loss: 0.9533 || timer: 0.0851 sec.
iter 215900 || Loss: 0.8476 || timer: 0.0878 sec.
iter 215910 || Loss: 0.8054 || timer: 0.0950 sec.
iter 215920 || Loss: 0.9824 || timer: 0.0828 sec.
iter 215930 || Loss: 1.0665 || timer: 0.0899 sec.
iter 215940 || Loss: 0.9718 || timer: 0.0891 sec.
iter 215950 || Loss: 0.7878 || timer: 0.1039 sec.
iter 215960 || Loss: 0.9784 || timer: 0.0840 sec.
iter 215970 || Loss: 0.9968 || timer: 0.0962 sec.
iter 215980 || Loss: 0.7248 || timer: 0.1074 sec.
iter 215990 || Loss: 0.9914 || timer: 0.0837 sec.
iter 216000 || Loss: 1.1968 || timer: 0.0892 sec.
iter 216010 || Loss: 0.9576 || timer: 0.0897 sec.
iter 216020 || Loss: 0.7855 || timer: 0.0755 sec.
iter 216030 || Loss: 2.3511 || timer: 0.0904 sec.
iter 216040 || Loss: 1.1402 || timer: 0.0323 sec.
iter 216050 || Loss: 1.1139 || timer: 0.1089 sec.
iter 216060 || Loss: 1.4337 || timer: 0.0908 sec.
iter 216070 || Loss: 0.6004 || timer: 0.0822 sec.
iter 216080 || Loss: 0.7786 || timer: 0.0879 sec.
iter 216090 || Loss: 1.1776 || timer: 0.0926 sec.
iter 216100 || Loss: 1.0785 || timer: 0.0880 sec.
iter 216110 || Loss: 0.9344 || timer: 0.0906 sec.
iter 216120 || Loss: 0.7862 || timer: 0.0934 sec.
iter 216130 || Loss: 0.8663 || timer: 0.1088 sec.
iter 216140 || Loss: 1.3612 || timer: 0.0946 sec.
iter 216150 || Loss: 0.8896 || timer: 0.0900 sec.
iter 216160 || Loss: 0.9772 || timer: 0.0845 sec.
iter 216170 || Loss: 0.7568 || timer: 0.0926 sec.
iter 216180 || Loss: 1.4603 || timer: 0.0895 sec.
iter 216190 || Loss: 0.9621 || timer: 0.0835 sec.
iter 216200 || Loss: 1.0831 || timer: 0.0847 sec.
iter 216210 || Loss: 0.8867 || timer: 0.1060 sec.
iter 216220 || Loss: 1.0718 || timer: 0.1000 sec.
iter 216230 || Loss: 0.9778 || timer: 0.0911 sec.
iter 216240 || Loss: 0.8481 || timer: 0.0904 sec.
iter 216250 || Loss: 0.9224 || timer: 0.0830 sec.
iter 216260 || Loss: 1.0492 || timer: 0.1182 sec.
iter 216270 || Loss: 0.9233 || timer: 0.0920 sec.
iter 216280 || Loss: 0.9258 || timer: 0.0934 sec.
iter 216290 || Loss: 0.8485 || timer: 0.0894 sec.
iter 216300 || Loss: 1.1979 || timer: 0.0927 sec.
iter 216310 || Loss: 1.3617 || timer: 0.1091 sec.
iter 216320 || Loss: 0.7330 || timer: 0.0949 sec.
iter 216330 || Loss: 1.1434 || timer: 0.1083 sec.
iter 216340 || Loss: 0.8959 || timer: 0.0907 sec.
iter 216350 || Loss: 0.7315 || timer: 0.0820 sec.
iter 216360 || Loss: 0.8591 || timer: 0.0845 sec.
iter 216370 || Loss: 0.8256 || timer: 0.0151 sec.
iter 216380 || Loss: 1.6093 || timer: 0.1035 sec.
iter 216390 || Loss: 0.8791 || timer: 0.0894 sec.
iter 216400 || Loss: 0.9111 || timer: 0.0914 sec.
iter 216410 || Loss: 0.8044 || timer: 0.1133 sec.
iter 216420 || Loss: 0.9742 || timer: 0.1199 sec.
iter 216430 || Loss: 0.9486 || timer: 0.0897 sec.
iter 216440 || Loss: 0.9547 || timer: 0.0939 sec.
iter 216450 || Loss: 0.8051 || timer: 0.0826 sec.
iter 216460 || Loss: 1.0590 || timer: 0.1084 sec.
iter 216470 || Loss: 0.8051 || timer: 0.1180 sec.
iter 216480 || Loss: 0.9614 || timer: 0.0893 sec.
iter 216490 || Loss: 1.0046 || timer: 0.1070 sec.
iter 216500 || Loss: 1.1283 || timer: 0.0881 sec.
iter 216510 || Loss: 0.9393 || timer: 0.0833 sec.
iter 216520 || Loss: 0.8917 || timer: 0.0880 sec.
iter 216530 || Loss: 1.1059 || timer: 0.0866 sec.
iter 216540 || Loss: 1.1290 || timer: 0.0905 sec.
iter 216550 || Loss: 0.8188 || timer: 0.0840 sec.
iter 216560 || Loss: 0.8035 || timer: 0.0871 sec.
iter 216570 || Loss: 0.6781 || timer: 0.0913 sec.
iter 216580 || Loss: 0.5815 || timer: 0.0891 sec.
iter 216590 || Loss: 0.9825 || timer: 0.0836 sec.
iter 216600 || Loss: 0.8202 || timer: 0.0836 sec.
iter 216610 || Loss: 0.8094 || timer: 0.0947 sec.
iter 216620 || Loss: 0.8206 || timer: 0.1178 sec.
iter 216630 || Loss: 0.9582 || timer: 0.0965 sec.
iter 216640 || Loss: 0.9562 || timer: 0.1009 sec.
iter 216650 || Loss: 0.7014 || timer: 0.0892 sec.
iter 216660 || Loss: 0.8491 || timer: 0.0913 sec.
iter 216670 || Loss: 0.9542 || timer: 0.0879 sec.
iter 216680 || Loss: 0.7840 || timer: 0.1083 sec.
iter 216690 || Loss: 0.7393 || timer: 0.0911 sec.
iter 216700 || Loss: 0.8376 || timer: 0.0256 sec.
iter 216710 || Loss: 0.2999 || timer: 0.0829 sec.
iter 216720 || Loss: 0.7605 || timer: 0.1242 sec.
iter 216730 || Loss: 0.8937 || timer: 0.1118 sec.
iter 216740 || Loss: 0.8335 || timer: 0.0865 sec.
iter 216750 || Loss: 0.9431 || timer: 0.0892 sec.
iter 216760 || Loss: 0.8209 || timer: 0.0971 sec.
iter 216770 || Loss: 1.0419 || timer: 0.0832 sec.
iter 216780 || Loss: 0.8562 || timer: 0.0899 sec.
iter 216790 || Loss: 0.7569 || timer: 0.0904 sec.
iter 216800 || Loss: 1.1034 || timer: 0.1350 sec.
iter 216810 || Loss: 1.1762 || timer: 0.0900 sec.
iter 216820 || Loss: 1.0209 || timer: 0.0885 sec.
iter 216830 || Loss: 1.0613 || timer: 0.0907 sec.
iter 216840 || Loss: 0.7866 || timer: 0.0916 sec.
iter 216850 || Loss: 0.9451 || timer: 0.0835 sec.
iter 216860 || Loss: 0.9996 || timer: 0.0851 sec.
iter 216870 || Loss: 0.9874 || timer: 0.0911 sec.
iter 216880 || Loss: 1.0134 || timer: 0.0889 sec.
iter 216890 || Loss: 0.6840 || timer: 0.0911 sec.
iter 216900 || Loss: 0.8395 || timer: 0.0870 sec.
iter 216910 || Loss: 0.6458 || timer: 0.0907 sec.
iter 216920 || Loss: 0.8386 || timer: 0.0896 sec.
iter 216930 || Loss: 1.0029 || timer: 0.0761 sec.
iter 216940 || Loss: 0.9765 || timer: 0.0958 sec.
iter 216950 || Loss: 0.7587 || timer: 0.1060 sec.
iter 216960 || Loss: 1.1250 || timer: 0.1040 sec.
iter 216970 || Loss: 0.8401 || timer: 0.0896 sec.
iter 216980 || Loss: 1.0338 || timer: 0.0901 sec.
iter 216990 || Loss: 0.8933 || timer: 0.1236 sec.
iter 217000 || Loss: 0.8288 || timer: 0.0900 sec.
iter 217010 || Loss: 1.0795 || timer: 0.1047 sec.
iter 217020 || Loss: 0.6753 || timer: 0.0925 sec.
iter 217030 || Loss: 1.5626 || timer: 0.0240 sec.
iter 217040 || Loss: 1.9185 || timer: 0.0955 sec.
iter 217050 || Loss: 0.8725 || timer: 0.0888 sec.
iter 217060 || Loss: 0.7593 || timer: 0.0906 sec.
iter 217070 || Loss: 0.9793 || timer: 0.0864 sec.
iter 217080 || Loss: 0.8335 || timer: 0.0979 sec.
iter 217090 || Loss: 0.9728 || timer: 0.0836 sec.
iter 217100 || Loss: 0.9465 || timer: 0.0880 sec.
iter 217110 || Loss: 0.7892 || timer: 0.0920 sec.
iter 217120 || Loss: 0.8816 || timer: 0.0839 sec.
iter 217130 || Loss: 0.7897 || timer: 0.0947 sec.
iter 217140 || Loss: 0.8360 || timer: 0.0872 sec.
iter 217150 || Loss: 0.9428 || timer: 0.0904 sec.
iter 217160 || Loss: 1.0641 || timer: 0.0841 sec.
iter 217170 || Loss: 1.0683 || timer: 0.0859 sec.
iter 217180 || Loss: 0.8483 || timer: 0.0973 sec.
iter 217190 || Loss: 0.9878 || timer: 0.0908 sec.
iter 217200 || Loss: 0.9870 || timer: 0.0995 sec.
iter 217210 || Loss: 0.7117 || timer: 0.0906 sec.
iter 217220 || Loss: 0.9948 || timer: 0.0904 sec.
iter 217230 || Loss: 0.9347 || timer: 0.0832 sec.
iter 217240 || Loss: 0.9706 || timer: 0.0925 sec.
iter 217250 || Loss: 1.0652 || timer: 0.0909 sec.
iter 217260 || Loss: 0.6766 || timer: 0.0892 sec.
iter 217270 || Loss: 0.9449 || timer: 0.1089 sec.
iter 217280 || Loss: 1.2415 || timer: 0.0906 sec.
iter 217290 || Loss: 2.2838 || timer: 0.0839 sec.
iter 217300 || Loss: 1.2720 || timer: 0.0828 sec.
iter 217310 || Loss: 1.1346 || timer: 0.0927 sec.
iter 217320 || Loss: 1.0291 || timer: 0.1076 sec.
iter 217330 || Loss: 1.1771 || timer: 0.0913 sec.
iter 217340 || Loss: 1.1142 || timer: 0.0888 sec.
iter 217350 || Loss: 1.0051 || timer: 0.0913 sec.
iter 217360 || Loss: 1.1742 || timer: 0.0268 sec.
iter 217370 || Loss: 0.3552 || timer: 0.0894 sec.
iter 217380 || Loss: 1.0472 || timer: 0.0908 sec.
iter 217390 || Loss: 1.0580 || timer: 0.0835 sec.
iter 217400 || Loss: 0.6569 || timer: 0.0900 sec.
iter 217410 || Loss: 0.9381 || timer: 0.1031 sec.
iter 217420 || Loss: 0.7874 || timer: 0.1041 sec.
iter 217430 || Loss: 0.6821 || timer: 0.1102 sec.
iter 217440 || Loss: 1.0436 || timer: 0.0888 sec.
iter 217450 || Loss: 0.9436 || timer: 0.0925 sec.
iter 217460 || Loss: 1.1390 || timer: 0.1123 sec.
iter 217470 || Loss: 0.8945 || timer: 0.0896 sec.
iter 217480 || Loss: 1.5005 || timer: 0.1028 sec.
iter 217490 || Loss: 1.8934 || timer: 0.0840 sec.
iter 217500 || Loss: 1.0428 || timer: 0.0828 sec.
iter 217510 || Loss: 1.3351 || timer: 0.1019 sec.
iter 217520 || Loss: 1.2114 || timer: 0.0890 sec.
iter 217530 || Loss: 0.8954 || timer: 0.0932 sec.
iter 217540 || Loss: 0.7453 || timer: 0.0880 sec.
iter 217550 || Loss: 1.4183 || timer: 0.0831 sec.
iter 217560 || Loss: 0.9816 || timer: 0.0828 sec.
iter 217570 || Loss: 1.1800 || timer: 0.0861 sec.
iter 217580 || Loss: 0.8525 || timer: 0.0863 sec.
iter 217590 || Loss: 0.8628 || timer: 0.0920 sec.
iter 217600 || Loss: 1.1358 || timer: 0.0910 sec.
iter 217610 || Loss: 0.9703 || timer: 0.0910 sec.
iter 217620 || Loss: 1.0916 || timer: 0.0865 sec.
iter 217630 || Loss: 0.9229 || timer: 0.0826 sec.
iter 217640 || Loss: 0.9768 || timer: 0.1230 sec.
iter 217650 || Loss: 1.3952 || timer: 0.1015 sec.
iter 217660 || Loss: 1.2857 || timer: 0.0905 sec.
iter 217670 || Loss: 1.1715 || timer: 0.0902 sec.
iter 217680 || Loss: 0.9491 || timer: 0.0904 sec.
iter 217690 || Loss: 0.9902 || timer: 0.0181 sec.
iter 217700 || Loss: 0.8846 || timer: 0.0890 sec.
iter 217710 || Loss: 0.9778 || timer: 0.0920 sec.
iter 217720 || Loss: 1.1574 || timer: 0.0898 sec.
iter 217730 || Loss: 1.1446 || timer: 0.0822 sec.
iter 217740 || Loss: 0.9358 || timer: 0.0925 sec.
iter 217750 || Loss: 1.1248 || timer: 0.0908 sec.
iter 217760 || Loss: 0.9742 || timer: 0.0911 sec.
iter 217770 || Loss: 0.8431 || timer: 0.0901 sec.
iter 217780 || Loss: 0.8159 || timer: 0.1162 sec.
iter 217790 || Loss: 1.4023 || timer: 0.1133 sec.
iter 217800 || Loss: 0.9677 || timer: 0.0895 sec.
iter 217810 || Loss: 1.2703 || timer: 0.0919 sec.
iter 217820 || Loss: 0.8762 || timer: 0.0837 sec.
iter 217830 || Loss: 1.1166 || timer: 0.0896 sec.
iter 217840 || Loss: 1.3169 || timer: 0.0881 sec.
iter 217850 || Loss: 0.7356 || timer: 0.0899 sec.
iter 217860 || Loss: 0.9685 || timer: 0.0914 sec.
iter 217870 || Loss: 1.1346 || timer: 0.1087 sec.
iter 217880 || Loss: 0.8819 || timer: 0.0844 sec.
iter 217890 || Loss: 0.7837 || timer: 0.0841 sec.
iter 217900 || Loss: 1.0633 || timer: 0.1091 sec.
iter 217910 || Loss: 0.9752 || timer: 0.0836 sec.
iter 217920 || Loss: 1.3354 || timer: 0.0902 sec.
iter 217930 || Loss: 1.0473 || timer: 0.0890 sec.
iter 217940 || Loss: 1.2295 || timer: 0.0834 sec.
iter 217950 || Loss: 0.9813 || timer: 0.0911 sec.
iter 217960 || Loss: 0.9206 || timer: 0.1155 sec.
iter 217970 || Loss: 1.5748 || timer: 0.0829 sec.
iter 217980 || Loss: 1.1770 || timer: 0.1003 sec.
iter 217990 || Loss: 1.0662 || timer: 0.0902 sec.
iter 218000 || Loss: 1.1884 || timer: 0.0836 sec.
iter 218010 || Loss: 1.0645 || timer: 0.0901 sec.
iter 218020 || Loss: 0.8904 || timer: 0.0174 sec.
iter 218030 || Loss: 0.2909 || timer: 0.0932 sec.
iter 218040 || Loss: 0.8761 || timer: 0.0863 sec.
iter 218050 || Loss: 1.0376 || timer: 0.0914 sec.
iter 218060 || Loss: 0.8875 || timer: 0.0907 sec.
iter 218070 || Loss: 1.1379 || timer: 0.0908 sec.
iter 218080 || Loss: 0.9977 || timer: 0.0907 sec.
iter 218090 || Loss: 0.7196 || timer: 0.0920 sec.
iter 218100 || Loss: 0.9914 || timer: 0.0893 sec.
iter 218110 || Loss: 0.8313 || timer: 0.1071 sec.
iter 218120 || Loss: 0.9399 || timer: 0.1063 sec.
iter 218130 || Loss: 1.1361 || timer: 0.0834 sec.
iter 218140 || Loss: 0.9221 || timer: 0.0894 sec.
iter 218150 || Loss: 0.6772 || timer: 0.0844 sec.
iter 218160 || Loss: 1.4526 || timer: 0.0897 sec.
iter 218170 || Loss: 0.6409 || timer: 0.0844 sec.
iter 218180 || Loss: 1.0990 || timer: 0.0896 sec.
iter 218190 || Loss: 0.6377 || timer: 0.1110 sec.
iter 218200 || Loss: 0.6031 || timer: 0.0830 sec.
iter 218210 || Loss: 0.9896 || timer: 0.0832 sec.
iter 218220 || Loss: 0.7541 || timer: 0.0835 sec.
iter 218230 || Loss: 0.8894 || timer: 0.0901 sec.
iter 218240 || Loss: 0.7490 || timer: 0.0831 sec.
iter 218250 || Loss: 0.9418 || timer: 0.0918 sec.
iter 218260 || Loss: 0.9666 || timer: 0.0910 sec.
iter 218270 || Loss: 1.0812 || timer: 0.0910 sec.
iter 218280 || Loss: 0.9839 || timer: 0.0893 sec.
iter 218290 || Loss: 1.0754 || timer: 0.0907 sec.
iter 218300 || Loss: 0.7070 || timer: 0.0898 sec.
iter 218310 || Loss: 0.9624 || timer: 0.0937 sec.
iter 218320 || Loss: 0.8141 || timer: 0.0838 sec.
iter 218330 || Loss: 0.7565 || timer: 0.1037 sec.
iter 218340 || Loss: 0.7752 || timer: 0.0840 sec.
iter 218350 || Loss: 0.9600 || timer: 0.0166 sec.
iter 218360 || Loss: 0.6072 || timer: 0.0836 sec.
iter 218370 || Loss: 1.0043 || timer: 0.0895 sec.
iter 218380 || Loss: 0.5544 || timer: 0.0849 sec.
iter 218390 || Loss: 1.0278 || timer: 0.0932 sec.
iter 218400 || Loss: 0.9016 || timer: 0.0961 sec.
iter 218410 || Loss: 0.7018 || timer: 0.0905 sec.
iter 218420 || Loss: 0.8681 || timer: 0.0853 sec.
iter 218430 || Loss: 0.9545 || timer: 0.0831 sec.
iter 218440 || Loss: 1.1519 || timer: 0.0906 sec.
iter 218450 || Loss: 1.1082 || timer: 0.0967 sec.
iter 218460 || Loss: 0.9320 || timer: 0.0831 sec.
iter 218470 || Loss: 1.1282 || timer: 0.1029 sec.
iter 218480 || Loss: 0.7362 || timer: 0.0864 sec.
iter 218490 || Loss: 0.8544 || timer: 0.0871 sec.
iter 218500 || Loss: 0.8206 || timer: 0.0831 sec.
iter 218510 || Loss: 0.7916 || timer: 0.0915 sec.
iter 218520 || Loss: 0.8471 || timer: 0.0984 sec.
iter 218530 || Loss: 1.6961 || timer: 0.0824 sec.
iter 218540 || Loss: 1.3933 || timer: 0.1050 sec.
iter 218550 || Loss: 1.0984 || timer: 0.0833 sec.
iter 218560 || Loss: 0.9241 || timer: 0.0917 sec.
iter 218570 || Loss: 1.3440 || timer: 0.1023 sec.
iter 218580 || Loss: 0.9920 || timer: 0.0914 sec.
iter 218590 || Loss: 1.0332 || timer: 0.0918 sec.
iter 218600 || Loss: 1.2347 || timer: 0.0832 sec.
iter 218610 || Loss: 0.8973 || timer: 0.0872 sec.
iter 218620 || Loss: 0.8998 || timer: 0.0865 sec.
iter 218630 || Loss: 0.9501 || timer: 0.1279 sec.
iter 218640 || Loss: 1.0458 || timer: 0.0908 sec.
iter 218650 || Loss: 0.9346 || timer: 0.0856 sec.
iter 218660 || Loss: 1.3112 || timer: 0.0871 sec.
iter 218670 || Loss: 1.3162 || timer: 0.0901 sec.
iter 218680 || Loss: 0.8000 || timer: 0.0277 sec.
iter 218690 || Loss: 0.3320 || timer: 0.0933 sec.
iter 218700 || Loss: 0.8602 || timer: 0.0822 sec.
iter 218710 || Loss: 1.0951 || timer: 0.0892 sec.
iter 218720 || Loss: 0.9218 || timer: 0.0830 sec.
iter 218730 || Loss: 0.8406 || timer: 0.1085 sec.
iter 218740 || Loss: 0.7784 || timer: 0.0846 sec.
iter 218750 || Loss: 0.8716 || timer: 0.0883 sec.
iter 218760 || Loss: 0.6375 || timer: 0.0977 sec.
iter 218770 || Loss: 0.8856 || timer: 0.0900 sec.
iter 218780 || Loss: 0.8388 || timer: 0.1325 sec.
iter 218790 || Loss: 1.0203 || timer: 0.0903 sec.
iter 218800 || Loss: 0.9291 || timer: 0.0918 sec.
iter 218810 || Loss: 0.7840 || timer: 0.0916 sec.
iter 218820 || Loss: 0.7240 || timer: 0.0854 sec.
iter 218830 || Loss: 0.6852 || timer: 0.0909 sec.
iter 218840 || Loss: 0.5300 || timer: 0.1016 sec.
iter 218850 || Loss: 0.8459 || timer: 0.0895 sec.
iter 218860 || Loss: 1.1840 || timer: 0.0971 sec.
iter 218870 || Loss: 1.1524 || timer: 0.0901 sec.
iter 218880 || Loss: 0.9205 || timer: 0.0893 sec.
iter 218890 || Loss: 1.0908 || timer: 0.0895 sec.
iter 218900 || Loss: 0.8299 || timer: 0.0904 sec.
iter 218910 || Loss: 0.7461 || timer: 0.0854 sec.
iter 218920 || Loss: 0.7948 || timer: 0.0990 sec.
iter 218930 || Loss: 0.7668 || timer: 0.0896 sec.
iter 218940 || Loss: 1.1504 || timer: 0.0899 sec.
iter 218950 || Loss: 0.8840 || timer: 0.1048 sec.
iter 218960 || Loss: 0.8069 || timer: 0.0900 sec.
iter 218970 || Loss: 0.9552 || timer: 0.0831 sec.
iter 218980 || Loss: 1.0701 || timer: 0.0941 sec.
iter 218990 || Loss: 0.8046 || timer: 0.0905 sec.
iter 219000 || Loss: 0.9235 || timer: 0.0955 sec.
iter 219010 || Loss: 0.6961 || timer: 0.0247 sec.
iter 219020 || Loss: 0.8110 || timer: 0.0995 sec.
iter 219030 || Loss: 0.8444 || timer: 0.0861 sec.
iter 219040 || Loss: 0.7007 || timer: 0.0915 sec.
iter 219050 || Loss: 0.6897 || timer: 0.0988 sec.
iter 219060 || Loss: 0.9485 || timer: 0.0912 sec.
iter 219070 || Loss: 1.0061 || timer: 0.0898 sec.
iter 219080 || Loss: 0.7727 || timer: 0.0830 sec.
iter 219090 || Loss: 0.9471 || timer: 0.0928 sec.
iter 219100 || Loss: 1.0929 || timer: 0.1165 sec.
iter 219110 || Loss: 0.8736 || timer: 0.1041 sec.
iter 219120 || Loss: 0.9065 || timer: 0.0835 sec.
iter 219130 || Loss: 0.9527 || timer: 0.0896 sec.
iter 219140 || Loss: 0.7947 || timer: 0.0826 sec.
iter 219150 || Loss: 0.9820 || timer: 0.0906 sec.
iter 219160 || Loss: 1.0769 || timer: 0.1050 sec.
iter 219170 || Loss: 1.0493 || timer: 0.0913 sec.
iter 219180 || Loss: 0.9258 || timer: 0.1014 sec.
iter 219190 || Loss: 1.1104 || timer: 0.0895 sec.
iter 219200 || Loss: 1.0877 || timer: 0.0913 sec.
iter 219210 || Loss: 0.8487 || timer: 0.1073 sec.
iter 219220 || Loss: 0.7031 || timer: 0.1044 sec.
iter 219230 || Loss: 0.7475 || timer: 0.1155 sec.
iter 219240 || Loss: 0.8801 || timer: 0.0947 sec.
iter 219250 || Loss: 0.8829 || timer: 0.0913 sec.
iter 219260 || Loss: 0.9230 || timer: 0.0826 sec.
iter 219270 || Loss: 1.0930 || timer: 0.1102 sec.
iter 219280 || Loss: 0.4700 || timer: 0.0921 sec.
iter 219290 || Loss: 0.7554 || timer: 0.0880 sec.
iter 219300 || Loss: 0.7862 || timer: 0.0899 sec.
iter 219310 || Loss: 0.8785 || timer: 0.0918 sec.
iter 219320 || Loss: 0.9790 || timer: 0.0836 sec.
iter 219330 || Loss: 0.8309 || timer: 0.0882 sec.
iter 219340 || Loss: 0.7091 || timer: 0.0246 sec.
iter 219350 || Loss: 1.0933 || timer: 0.1004 sec.
iter 219360 || Loss: 0.9657 || timer: 0.0828 sec.
iter 219370 || Loss: 0.7889 || timer: 0.0917 sec.
iter 219380 || Loss: 0.9369 || timer: 0.0902 sec.
iter 219390 || Loss: 0.9111 || timer: 0.0881 sec.
iter 219400 || Loss: 0.7659 || timer: 0.0980 sec.
iter 219410 || Loss: 0.7307 || timer: 0.1011 sec.
iter 219420 || Loss: 0.9784 || timer: 0.1014 sec.
iter 219430 || Loss: 0.9112 || timer: 0.0886 sec.
iter 219440 || Loss: 0.8084 || timer: 0.0978 sec.
iter 219450 || Loss: 0.8087 || timer: 0.0893 sec.
iter 219460 || Loss: 0.8937 || timer: 0.0893 sec.
iter 219470 || Loss: 0.7826 || timer: 0.0913 sec.
iter 219480 || Loss: 1.2025 || timer: 0.0827 sec.
iter 219490 || Loss: 0.7853 || timer: 0.0880 sec.
iter 219500 || Loss: 1.0101 || timer: 0.0827 sec.
iter 219510 || Loss: 0.8776 || timer: 0.1070 sec.
iter 219520 || Loss: 1.8044 || timer: 0.0907 sec.
iter 219530 || Loss: 1.1533 || timer: 0.0839 sec.
iter 219540 || Loss: 1.1645 || timer: 0.0925 sec.
iter 219550 || Loss: 0.7723 || timer: 0.0869 sec.
iter 219560 || Loss: 0.9960 || timer: 0.0921 sec.
iter 219570 || Loss: 0.7349 || timer: 0.0887 sec.
iter 219580 || Loss: 0.9700 || timer: 0.0913 sec.
iter 219590 || Loss: 0.8736 || timer: 0.0839 sec.
iter 219600 || Loss: 1.2594 || timer: 0.0908 sec.
iter 219610 || Loss: 0.8444 || timer: 0.0928 sec.
iter 219620 || Loss: 0.8687 || timer: 0.0965 sec.
iter 219630 || Loss: 0.7943 || timer: 0.0920 sec.
iter 219640 || Loss: 0.9006 || timer: 0.0895 sec.
iter 219650 || Loss: 0.8555 || timer: 0.0890 sec.
iter 219660 || Loss: 1.0501 || timer: 0.0924 sec.
iter 219670 || Loss: 1.0902 || timer: 0.0290 sec.
iter 219680 || Loss: 0.5549 || timer: 0.1095 sec.
iter 219690 || Loss: 0.7312 || timer: 0.0923 sec.
iter 219700 || Loss: 0.9564 || timer: 0.0830 sec.
iter 219710 || Loss: 1.0885 || timer: 0.1070 sec.
iter 219720 || Loss: 0.8082 || timer: 0.0900 sec.
iter 219730 || Loss: 0.9297 || timer: 0.0905 sec.
iter 219740 || Loss: 1.0124 || timer: 0.0918 sec.
iter 219750 || Loss: 0.6925 || timer: 0.0943 sec.
iter 219760 || Loss: 0.8439 || timer: 0.0905 sec.
iter 219770 || Loss: 0.9413 || timer: 0.1046 sec.
iter 219780 || Loss: 0.9855 || timer: 0.0897 sec.
iter 219790 || Loss: 0.6391 || timer: 0.0887 sec.
iter 219800 || Loss: 1.0107 || timer: 0.1068 sec.
iter 219810 || Loss: 1.3369 || timer: 0.0899 sec.
iter 219820 || Loss: 0.8619 || timer: 0.0831 sec.
iter 219830 || Loss: 1.1362 || timer: 0.1044 sec.
iter 219840 || Loss: 0.7664 || timer: 0.0899 sec.
iter 219850 || Loss: 0.8191 || timer: 0.1050 sec.
iter 219860 || Loss: 0.6446 || timer: 0.0920 sec.
iter 219870 || Loss: 0.9865 || timer: 0.0961 sec.
iter 219880 || Loss: 1.0123 || timer: 0.0835 sec.
iter 219890 || Loss: 0.9897 || timer: 0.1051 sec.
iter 219900 || Loss: 0.6335 || timer: 0.0857 sec.
iter 219910 || Loss: 1.0373 || timer: 0.0907 sec.
iter 219920 || Loss: 1.1480 || timer: 0.0896 sec.
iter 219930 || Loss: 1.3990 || timer: 0.0867 sec.
iter 219940 || Loss: 0.7962 || timer: 0.0913 sec.
iter 219950 || Loss: 0.8294 || timer: 0.0884 sec.
iter 219960 || Loss: 0.6626 || timer: 0.0921 sec.
iter 219970 || Loss: 1.1615 || timer: 0.0903 sec.
iter 219980 || Loss: 0.8125 || timer: 0.0899 sec.
iter 219990 || Loss: 0.9261 || timer: 0.0841 sec.
iter 220000 || Loss: 1.1326 || Saving state, iter: 220000
timer: 0.0287 sec.
iter 220010 || Loss: 2.2447 || timer: 0.1059 sec.
iter 220020 || Loss: 1.1384 || timer: 0.0835 sec.
iter 220030 || Loss: 0.8947 || timer: 0.0915 sec.
iter 220040 || Loss: 0.9172 || timer: 0.1006 sec.
iter 220050 || Loss: 1.0946 || timer: 0.0961 sec.
iter 220060 || Loss: 0.8179 || timer: 0.0911 sec.
iter 220070 || Loss: 0.9273 || timer: 0.0927 sec.
iter 220080 || Loss: 0.8528 || timer: 0.0926 sec.
iter 220090 || Loss: 1.0371 || timer: 0.0917 sec.
iter 220100 || Loss: 0.8674 || timer: 0.1061 sec.
iter 220110 || Loss: 1.1692 || timer: 0.0900 sec.
iter 220120 || Loss: 0.7798 || timer: 0.0972 sec.
iter 220130 || Loss: 0.9281 || timer: 0.0908 sec.
iter 220140 || Loss: 0.7904 || timer: 0.0915 sec.
iter 220150 || Loss: 0.9980 || timer: 0.0944 sec.
iter 220160 || Loss: 0.8443 || timer: 0.0924 sec.
iter 220170 || Loss: 1.0895 || timer: 0.0877 sec.
iter 220180 || Loss: 0.8640 || timer: 0.0845 sec.
iter 220190 || Loss: 0.8733 || timer: 0.0865 sec.
iter 220200 || Loss: 0.9462 || timer: 0.0851 sec.
iter 220210 || Loss: 1.2747 || timer: 0.0835 sec.
iter 220220 || Loss: 0.9083 || timer: 0.0834 sec.
iter 220230 || Loss: 1.0428 || timer: 0.0977 sec.
iter 220240 || Loss: 0.8691 || timer: 0.1050 sec.
iter 220250 || Loss: 0.6910 || timer: 0.1078 sec.
iter 220260 || Loss: 0.6829 || timer: 0.1053 sec.
iter 220270 || Loss: 1.3085 || timer: 0.0843 sec.
iter 220280 || Loss: 0.9911 || timer: 0.0952 sec.
iter 220290 || Loss: 0.8610 || timer: 0.1139 sec.
iter 220300 || Loss: 1.0784 || timer: 0.0803 sec.
iter 220310 || Loss: 0.9848 || timer: 0.0894 sec.
iter 220320 || Loss: 1.2571 || timer: 0.0862 sec.
iter 220330 || Loss: 0.8064 || timer: 0.0172 sec.
iter 220340 || Loss: 1.4754 || timer: 0.0916 sec.
iter 220350 || Loss: 1.0538 || timer: 0.1330 sec.
iter 220360 || Loss: 0.7994 || timer: 0.1101 sec.
iter 220370 || Loss: 0.6918 || timer: 0.0873 sec.
iter 220380 || Loss: 1.3124 || timer: 0.0881 sec.
iter 220390 || Loss: 0.6999 || timer: 0.0913 sec.
iter 220400 || Loss: 1.1016 || timer: 0.1015 sec.
iter 220410 || Loss: 0.8349 || timer: 0.0941 sec.
iter 220420 || Loss: 0.8171 || timer: 0.1330 sec.
iter 220430 || Loss: 0.9336 || timer: 0.1290 sec.
iter 220440 || Loss: 0.7665 || timer: 0.0958 sec.
iter 220450 || Loss: 1.0419 || timer: 0.0879 sec.
iter 220460 || Loss: 0.7664 || timer: 0.0870 sec.
iter 220470 || Loss: 0.8534 || timer: 0.1272 sec.
iter 220480 || Loss: 1.0203 || timer: 0.0898 sec.
iter 220490 || Loss: 1.2952 || timer: 0.0996 sec.
iter 220500 || Loss: 0.8064 || timer: 0.0837 sec.
iter 220510 || Loss: 0.9273 || timer: 0.0847 sec.
iter 220520 || Loss: 1.0399 || timer: 0.1047 sec.
iter 220530 || Loss: 0.7805 || timer: 0.0892 sec.
iter 220540 || Loss: 1.0008 || timer: 0.1344 sec.
iter 220550 || Loss: 0.8982 || timer: 0.0890 sec.
iter 220560 || Loss: 0.9233 || timer: 0.0902 sec.
iter 220570 || Loss: 1.2485 || timer: 0.1153 sec.
iter 220580 || Loss: 0.7853 || timer: 0.0928 sec.
iter 220590 || Loss: 1.1922 || timer: 0.0752 sec.
iter 220600 || Loss: 0.9340 || timer: 0.0830 sec.
iter 220610 || Loss: 0.8190 || timer: 0.0931 sec.
iter 220620 || Loss: 0.8704 || timer: 0.1019 sec.
iter 220630 || Loss: 1.1910 || timer: 0.0773 sec.
iter 220640 || Loss: 1.5935 || timer: 0.0906 sec.
iter 220650 || Loss: 1.0967 || timer: 0.1028 sec.
iter 220660 || Loss: 0.8308 || timer: 0.0183 sec.
iter 220670 || Loss: 1.0896 || timer: 0.0899 sec.
iter 220680 || Loss: 1.0622 || timer: 0.0907 sec.
iter 220690 || Loss: 0.8053 || timer: 0.0888 sec.
iter 220700 || Loss: 1.2678 || timer: 0.0925 sec.
iter 220710 || Loss: 1.0352 || timer: 0.0958 sec.
iter 220720 || Loss: 0.9427 || timer: 0.0965 sec.
iter 220730 || Loss: 1.1279 || timer: 0.0910 sec.
iter 220740 || Loss: 0.9508 || timer: 0.0889 sec.
iter 220750 || Loss: 0.8295 || timer: 0.0867 sec.
iter 220760 || Loss: 0.8187 || timer: 0.0961 sec.
iter 220770 || Loss: 1.1643 || timer: 0.0878 sec.
iter 220780 || Loss: 0.9840 || timer: 0.0849 sec.
iter 220790 || Loss: 0.9975 || timer: 0.1221 sec.
iter 220800 || Loss: 1.0332 || timer: 0.0929 sec.
iter 220810 || Loss: 0.8683 || timer: 0.0895 sec.
iter 220820 || Loss: 0.7791 || timer: 0.1168 sec.
iter 220830 || Loss: 0.9748 || timer: 0.1085 sec.
iter 220840 || Loss: 0.7589 || timer: 0.0830 sec.
iter 220850 || Loss: 0.7719 || timer: 0.0898 sec.
iter 220860 || Loss: 0.5245 || timer: 0.0809 sec.
iter 220870 || Loss: 1.3058 || timer: 0.0816 sec.
iter 220880 || Loss: 2.1165 || timer: 0.0812 sec.
iter 220890 || Loss: 0.9014 || timer: 0.0836 sec.
iter 220900 || Loss: 0.8066 || timer: 0.1368 sec.
iter 220910 || Loss: 1.2885 || timer: 0.0842 sec.
iter 220920 || Loss: 1.0758 || timer: 0.0823 sec.
iter 220930 || Loss: 0.8381 || timer: 0.0811 sec.
iter 220940 || Loss: 0.8657 || timer: 0.0881 sec.
iter 220950 || Loss: 0.8231 || timer: 0.1054 sec.
iter 220960 || Loss: 1.1355 || timer: 0.0818 sec.
iter 220970 || Loss: 1.3840 || timer: 0.0905 sec.
iter 220980 || Loss: 0.9377 || timer: 0.0867 sec.
iter 220990 || Loss: 1.0057 || timer: 0.0291 sec.
iter 221000 || Loss: 1.0877 || timer: 0.0892 sec.
iter 221010 || Loss: 0.9686 || timer: 0.0892 sec.
iter 221020 || Loss: 0.9312 || timer: 0.0898 sec.
iter 221030 || Loss: 1.0493 || timer: 0.0890 sec.
iter 221040 || Loss: 1.2229 || timer: 0.0897 sec.
iter 221050 || Loss: 1.0033 || timer: 0.0903 sec.
iter 221060 || Loss: 0.9384 || timer: 0.0833 sec.
iter 221070 || Loss: 1.1444 || timer: 0.0997 sec.
iter 221080 || Loss: 1.2333 || timer: 0.0817 sec.
iter 221090 || Loss: 0.8881 || timer: 0.1164 sec.
iter 221100 || Loss: 0.7771 || timer: 0.1299 sec.
iter 221110 || Loss: 1.2668 || timer: 0.0880 sec.
iter 221120 || Loss: 0.9662 || timer: 0.0911 sec.
iter 221130 || Loss: 0.7772 || timer: 0.1022 sec.
iter 221140 || Loss: 1.0788 || timer: 0.0820 sec.
iter 221150 || Loss: 1.1083 || timer: 0.0883 sec.
iter 221160 || Loss: 0.7182 || timer: 0.0888 sec.
iter 221170 || Loss: 1.0400 || timer: 0.0832 sec.
iter 221180 || Loss: 0.9227 || timer: 0.0834 sec.
iter 221190 || Loss: 0.9281 || timer: 0.0900 sec.
iter 221200 || Loss: 0.9856 || timer: 0.0825 sec.
iter 221210 || Loss: 0.8387 || timer: 0.0843 sec.
iter 221220 || Loss: 0.7248 || timer: 0.0868 sec.
iter 221230 || Loss: 1.0734 || timer: 0.0806 sec.
iter 221240 || Loss: 0.8150 || timer: 0.1008 sec.
iter 221250 || Loss: 0.7194 || timer: 0.0892 sec.
iter 221260 || Loss: 1.3804 || timer: 0.0821 sec.
iter 221270 || Loss: 0.7440 || timer: 0.0876 sec.
iter 221280 || Loss: 0.6843 || timer: 0.0896 sec.
iter 221290 || Loss: 0.8523 || timer: 0.0828 sec.
iter 221300 || Loss: 0.9248 || timer: 0.0884 sec.
iter 221310 || Loss: 1.0523 || timer: 0.0928 sec.
iter 221320 || Loss: 0.9280 || timer: 0.0261 sec.
iter 221330 || Loss: 0.7000 || timer: 0.0911 sec.
iter 221340 || Loss: 0.7819 || timer: 0.0868 sec.
iter 221350 || Loss: 0.8757 || timer: 0.0885 sec.
iter 221360 || Loss: 0.5408 || timer: 0.0920 sec.
iter 221370 || Loss: 1.1890 || timer: 0.0813 sec.
iter 221380 || Loss: 0.9239 || timer: 0.0879 sec.
iter 221390 || Loss: 1.0999 || timer: 0.0913 sec.
iter 221400 || Loss: 0.6292 || timer: 0.0910 sec.
iter 221410 || Loss: 0.7742 || timer: 0.0855 sec.
iter 221420 || Loss: 0.8424 || timer: 0.1076 sec.
iter 221430 || Loss: 0.9263 || timer: 0.0887 sec.
iter 221440 || Loss: 1.1366 || timer: 0.0907 sec.
iter 221450 || Loss: 0.8572 || timer: 0.0881 sec.
iter 221460 || Loss: 1.0170 || timer: 0.0885 sec.
iter 221470 || Loss: 0.9081 || timer: 0.1108 sec.
iter 221480 || Loss: 0.7462 || timer: 0.0821 sec.
iter 221490 || Loss: 0.8442 || timer: 0.0897 sec.
iter 221500 || Loss: 0.7566 || timer: 0.0830 sec.
iter 221510 || Loss: 0.5952 || timer: 0.0905 sec.
iter 221520 || Loss: 0.8328 || timer: 0.0990 sec.
iter 221530 || Loss: 1.0365 || timer: 0.1012 sec.
iter 221540 || Loss: 0.9728 || timer: 0.1000 sec.
iter 221550 || Loss: 0.8923 || timer: 0.0903 sec.
iter 221560 || Loss: 0.8881 || timer: 0.1128 sec.
iter 221570 || Loss: 0.9557 || timer: 0.0897 sec.
iter 221580 || Loss: 0.9258 || timer: 0.0815 sec.
iter 221590 || Loss: 0.8008 || timer: 0.0823 sec.
iter 221600 || Loss: 0.9334 || timer: 0.0818 sec.
iter 221610 || Loss: 0.8730 || timer: 0.0873 sec.
iter 221620 || Loss: 0.7858 || timer: 0.0899 sec.
iter 221630 || Loss: 0.8753 || timer: 0.0849 sec.
iter 221640 || Loss: 0.9418 || timer: 0.0915 sec.
iter 221650 || Loss: 1.0468 || timer: 0.0302 sec.
iter 221660 || Loss: 0.8524 || timer: 0.0920 sec.
iter 221670 || Loss: 0.9682 || timer: 0.0903 sec.
iter 221680 || Loss: 1.1676 || timer: 0.0886 sec.
iter 221690 || Loss: 0.6707 || timer: 0.0836 sec.
iter 221700 || Loss: 0.7455 || timer: 0.0875 sec.
iter 221710 || Loss: 0.9649 || timer: 0.0823 sec.
iter 221720 || Loss: 0.8634 || timer: 0.0935 sec.
iter 221730 || Loss: 0.7914 || timer: 0.0853 sec.
iter 221740 || Loss: 0.7329 || timer: 0.0862 sec.
iter 221750 || Loss: 0.9416 || timer: 0.1144 sec.
iter 221760 || Loss: 1.8062 || timer: 0.1008 sec.
iter 221770 || Loss: 0.8371 || timer: 0.0956 sec.
iter 221780 || Loss: 0.8142 || timer: 0.0829 sec.
iter 221790 || Loss: 0.7663 || timer: 0.0852 sec.
iter 221800 || Loss: 0.9510 || timer: 0.0919 sec.
iter 221810 || Loss: 1.0241 || timer: 0.0916 sec.
iter 221820 || Loss: 0.9149 || timer: 0.0805 sec.
iter 221830 || Loss: 1.0689 || timer: 0.1038 sec.
iter 221840 || Loss: 0.9321 || timer: 0.0819 sec.
iter 221850 || Loss: 0.7617 || timer: 0.1046 sec.
iter 221860 || Loss: 0.8398 || timer: 0.0928 sec.
iter 221870 || Loss: 0.9330 || timer: 0.0935 sec.
iter 221880 || Loss: 0.7903 || timer: 0.0931 sec.
iter 221890 || Loss: 0.9487 || timer: 0.0883 sec.
iter 221900 || Loss: 0.8113 || timer: 0.0905 sec.
iter 221910 || Loss: 0.9595 || timer: 0.0828 sec.
iter 221920 || Loss: 0.9942 || timer: 0.0829 sec.
iter 221930 || Loss: 0.8359 || timer: 0.0816 sec.
iter 221940 || Loss: 1.2631 || timer: 0.0879 sec.
iter 221950 || Loss: 0.9554 || timer: 0.0894 sec.
iter 221960 || Loss: 1.0306 || timer: 0.0847 sec.
iter 221970 || Loss: 1.1153 || timer: 0.0825 sec.
iter 221980 || Loss: 1.0670 || timer: 0.0195 sec.
iter 221990 || Loss: 0.4390 || timer: 0.0826 sec.
iter 222000 || Loss: 1.0948 || timer: 0.1030 sec.
iter 222010 || Loss: 1.2094 || timer: 0.0879 sec.
iter 222020 || Loss: 0.8581 || timer: 0.1029 sec.
iter 222030 || Loss: 1.0095 || timer: 0.0808 sec.
iter 222040 || Loss: 0.5825 || timer: 0.1266 sec.
iter 222050 || Loss: 0.9141 || timer: 0.0894 sec.
iter 222060 || Loss: 1.1641 || timer: 0.0877 sec.
iter 222070 || Loss: 0.7464 || timer: 0.0889 sec.
iter 222080 || Loss: 0.9610 || timer: 0.0911 sec.
iter 222090 || Loss: 0.9891 || timer: 0.0870 sec.
iter 222100 || Loss: 1.0410 || timer: 0.0920 sec.
iter 222110 || Loss: 0.8917 || timer: 0.0900 sec.
iter 222120 || Loss: 0.9655 || timer: 0.0816 sec.
iter 222130 || Loss: 0.6751 || timer: 0.0894 sec.
iter 222140 || Loss: 1.0296 || timer: 0.0812 sec.
iter 222150 || Loss: 0.8195 || timer: 0.0908 sec.
iter 222160 || Loss: 0.8426 || timer: 0.1086 sec.
iter 222170 || Loss: 1.0517 || timer: 0.1068 sec.
iter 222180 || Loss: 1.1391 || timer: 0.0913 sec.
iter 222190 || Loss: 0.8479 || timer: 0.0871 sec.
iter 222200 || Loss: 1.0627 || timer: 0.0813 sec.
iter 222210 || Loss: 0.6723 || timer: 0.0819 sec.
iter 222220 || Loss: 0.7965 || timer: 0.0900 sec.
iter 222230 || Loss: 1.0131 || timer: 0.1006 sec.
iter 222240 || Loss: 0.7953 || timer: 0.0921 sec.
iter 222250 || Loss: 0.8249 || timer: 0.1141 sec.
iter 222260 || Loss: 1.2719 || timer: 0.1108 sec.
iter 222270 || Loss: 0.8404 || timer: 0.0897 sec.
iter 222280 || Loss: 0.6627 || timer: 0.0853 sec.
iter 222290 || Loss: 1.2967 || timer: 0.1084 sec.
iter 222300 || Loss: 0.8041 || timer: 0.0949 sec.
iter 222310 || Loss: 1.0256 || timer: 0.0176 sec.
iter 222320 || Loss: 2.2802 || timer: 0.0880 sec.
iter 222330 || Loss: 0.8306 || timer: 0.1068 sec.
iter 222340 || Loss: 1.0377 || timer: 0.0962 sec.
iter 222350 || Loss: 0.7977 || timer: 0.1031 sec.
iter 222360 || Loss: 1.0034 || timer: 0.1036 sec.
iter 222370 || Loss: 0.7931 || timer: 0.0897 sec.
iter 222380 || Loss: 0.9191 || timer: 0.0935 sec.
iter 222390 || Loss: 1.0636 || timer: 0.0833 sec.
iter 222400 || Loss: 0.9423 || timer: 0.0819 sec.
iter 222410 || Loss: 0.6398 || timer: 0.0963 sec.
iter 222420 || Loss: 0.9884 || timer: 0.1043 sec.
iter 222430 || Loss: 1.4477 || timer: 0.0935 sec.
iter 222440 || Loss: 1.2259 || timer: 0.0857 sec.
iter 222450 || Loss: 1.0618 || timer: 0.0866 sec.
iter 222460 || Loss: 0.7085 || timer: 0.0818 sec.
iter 222470 || Loss: 0.9269 || timer: 0.1093 sec.
iter 222480 || Loss: 0.8772 || timer: 0.1136 sec.
iter 222490 || Loss: 0.8799 || timer: 0.0816 sec.
iter 222500 || Loss: 0.6446 || timer: 0.0869 sec.
iter 222510 || Loss: 0.9397 || timer: 0.1115 sec.
iter 222520 || Loss: 1.1608 || timer: 0.1073 sec.
iter 222530 || Loss: 1.2047 || timer: 0.0860 sec.
iter 222540 || Loss: 0.8544 || timer: 0.0814 sec.
iter 222550 || Loss: 0.9811 || timer: 0.0823 sec.
iter 222560 || Loss: 1.1558 || timer: 0.0884 sec.
iter 222570 || Loss: 1.0237 || timer: 0.0901 sec.
iter 222580 || Loss: 1.0360 || timer: 0.0820 sec.
iter 222590 || Loss: 1.2507 || timer: 0.0902 sec.
iter 222600 || Loss: 1.1011 || timer: 0.0870 sec.
iter 222610 || Loss: 0.9446 || timer: 0.0895 sec.
iter 222620 || Loss: 0.8337 || timer: 0.0920 sec.
iter 222630 || Loss: 0.6987 || timer: 0.0908 sec.
iter 222640 || Loss: 0.9192 || timer: 0.0268 sec.
iter 222650 || Loss: 0.3023 || timer: 0.0833 sec.
iter 222660 || Loss: 1.1392 || timer: 0.0911 sec.
iter 222670 || Loss: 0.9336 || timer: 0.0909 sec.
iter 222680 || Loss: 0.7914 || timer: 0.0937 sec.
iter 222690 || Loss: 0.8706 || timer: 0.0892 sec.
iter 222700 || Loss: 1.2671 || timer: 0.0917 sec.
iter 222710 || Loss: 0.8312 || timer: 0.0829 sec.
iter 222720 || Loss: 0.8901 || timer: 0.0905 sec.
iter 222730 || Loss: 0.8065 || timer: 0.0957 sec.
iter 222740 || Loss: 1.0299 || timer: 0.1227 sec.
iter 222750 || Loss: 0.9784 || timer: 0.0891 sec.
iter 222760 || Loss: 0.9456 || timer: 0.1260 sec.
iter 222770 || Loss: 0.6900 || timer: 0.0883 sec.
iter 222780 || Loss: 0.6987 || timer: 0.1052 sec.
iter 222790 || Loss: 0.9929 || timer: 0.1049 sec.
iter 222800 || Loss: 0.7535 || timer: 0.0879 sec.
iter 222810 || Loss: 1.1115 || timer: 0.1109 sec.
iter 222820 || Loss: 0.8789 || timer: 0.0886 sec.
iter 222830 || Loss: 0.9808 || timer: 0.1035 sec.
iter 222840 || Loss: 0.6518 || timer: 0.0839 sec.
iter 222850 || Loss: 1.0359 || timer: 0.0884 sec.
iter 222860 || Loss: 1.0366 || timer: 0.1062 sec.
iter 222870 || Loss: 0.9945 || timer: 0.0861 sec.
iter 222880 || Loss: 0.9404 || timer: 0.1070 sec.
iter 222890 || Loss: 0.4988 || timer: 0.0905 sec.
iter 222900 || Loss: 0.9558 || timer: 0.0879 sec.
iter 222910 || Loss: 0.6887 || timer: 0.0822 sec.
iter 222920 || Loss: 1.2629 || timer: 0.1129 sec.
iter 222930 || Loss: 0.9026 || timer: 0.0892 sec.
iter 222940 || Loss: 1.2821 || timer: 0.0756 sec.
iter 222950 || Loss: 0.8167 || timer: 0.0811 sec.
iter 222960 || Loss: 0.7206 || timer: 0.0754 sec.
iter 222970 || Loss: 0.7874 || timer: 0.0229 sec.
iter 222980 || Loss: 1.0305 || timer: 0.0913 sec.
iter 222990 || Loss: 0.7580 || timer: 0.1019 sec.
iter 223000 || Loss: 0.7808 || timer: 0.1069 sec.
iter 223010 || Loss: 0.5291 || timer: 0.0962 sec.
iter 223020 || Loss: 0.8048 || timer: 0.0906 sec.
iter 223030 || Loss: 0.8575 || timer: 0.1017 sec.
iter 223040 || Loss: 0.7565 || timer: 0.1149 sec.
iter 223050 || Loss: 0.8642 || timer: 0.0948 sec.
iter 223060 || Loss: 0.5585 || timer: 0.0842 sec.
iter 223070 || Loss: 0.9375 || timer: 0.0980 sec.
iter 223080 || Loss: 0.7138 || timer: 0.0928 sec.
iter 223090 || Loss: 1.0499 || timer: 0.0934 sec.
iter 223100 || Loss: 0.7102 || timer: 0.0922 sec.
iter 223110 || Loss: 0.7964 || timer: 0.1022 sec.
iter 223120 || Loss: 0.8239 || timer: 0.1141 sec.
iter 223130 || Loss: 0.8822 || timer: 0.0918 sec.
iter 223140 || Loss: 0.9545 || timer: 0.0752 sec.
iter 223150 || Loss: 0.5952 || timer: 0.0840 sec.
iter 223160 || Loss: 0.8545 || timer: 0.0957 sec.
iter 223170 || Loss: 0.7543 || timer: 0.0880 sec.
iter 223180 || Loss: 0.8774 || timer: 0.0833 sec.
iter 223190 || Loss: 1.0728 || timer: 0.0887 sec.
iter 223200 || Loss: 1.1540 || timer: 0.0805 sec.
iter 223210 || Loss: 0.8228 || timer: 0.0850 sec.
iter 223220 || Loss: 0.7088 || timer: 0.0932 sec.
iter 223230 || Loss: 0.8628 || timer: 0.0950 sec.
iter 223240 || Loss: 1.0102 || timer: 0.1177 sec.
iter 223250 || Loss: 1.2292 || timer: 0.0915 sec.
iter 223260 || Loss: 0.9024 || timer: 0.0910 sec.
iter 223270 || Loss: 0.9892 || timer: 0.0824 sec.
iter 223280 || Loss: 0.8700 || timer: 0.0900 sec.
iter 223290 || Loss: 0.7380 || timer: 0.0912 sec.
iter 223300 || Loss: 1.1280 || timer: 0.0213 sec.
iter 223310 || Loss: 0.6793 || timer: 0.1054 sec.
iter 223320 || Loss: 0.7291 || timer: 0.1032 sec.
iter 223330 || Loss: 0.6685 || timer: 0.0910 sec.
iter 223340 || Loss: 1.1111 || timer: 0.0951 sec.
iter 223350 || Loss: 0.7230 || timer: 0.0919 sec.
iter 223360 || Loss: 0.7270 || timer: 0.0838 sec.
iter 223370 || Loss: 1.1138 || timer: 0.0902 sec.
iter 223380 || Loss: 0.8275 || timer: 0.0918 sec.
iter 223390 || Loss: 0.8199 || timer: 0.1048 sec.
iter 223400 || Loss: 0.7541 || timer: 0.1139 sec.
iter 223410 || Loss: 0.6817 || timer: 0.1365 sec.
iter 223420 || Loss: 0.7953 || timer: 0.1051 sec.
iter 223430 || Loss: 0.7017 || timer: 0.0865 sec.
iter 223440 || Loss: 1.0971 || timer: 0.0921 sec.
iter 223450 || Loss: 0.7975 || timer: 0.0817 sec.
iter 223460 || Loss: 1.0232 || timer: 0.0859 sec.
iter 223470 || Loss: 0.7067 || timer: 0.1056 sec.
iter 223480 || Loss: 0.9644 || timer: 0.0828 sec.
iter 223490 || Loss: 0.9081 || timer: 0.1067 sec.
iter 223500 || Loss: 1.0477 || timer: 0.0826 sec.
iter 223510 || Loss: 0.9554 || timer: 0.0903 sec.
iter 223520 || Loss: 1.4238 || timer: 0.1046 sec.
iter 223530 || Loss: 1.1227 || timer: 0.0843 sec.
iter 223540 || Loss: 1.0495 || timer: 0.0843 sec.
iter 223550 || Loss: 0.9685 || timer: 0.1096 sec.
iter 223560 || Loss: 0.8965 || timer: 0.1016 sec.
iter 223570 || Loss: 0.8189 || timer: 0.0824 sec.
iter 223580 || Loss: 0.8542 || timer: 0.0822 sec.
iter 223590 || Loss: 0.7956 || timer: 0.0826 sec.
iter 223600 || Loss: 1.2972 || timer: 0.0919 sec.
iter 223610 || Loss: 1.0284 || timer: 0.0973 sec.
iter 223620 || Loss: 1.0483 || timer: 0.1110 sec.
iter 223630 || Loss: 0.9179 || timer: 0.0148 sec.
iter 223640 || Loss: 3.7081 || timer: 0.0896 sec.
iter 223650 || Loss: 0.7043 || timer: 0.1083 sec.
iter 223660 || Loss: 1.1943 || timer: 0.1012 sec.
iter 223670 || Loss: 0.7922 || timer: 0.0837 sec.
iter 223680 || Loss: 0.8663 || timer: 0.1249 sec.
iter 223690 || Loss: 0.9979 || timer: 0.1077 sec.
iter 223700 || Loss: 0.8253 || timer: 0.0901 sec.
iter 223710 || Loss: 0.8564 || timer: 0.1366 sec.
iter 223720 || Loss: 0.9338 || timer: 0.0892 sec.
iter 223730 || Loss: 0.8086 || timer: 0.0969 sec.
iter 223740 || Loss: 0.7839 || timer: 0.0901 sec.
iter 223750 || Loss: 1.0034 || timer: 0.0902 sec.
iter 223760 || Loss: 0.6537 || timer: 0.0927 sec.
iter 223770 || Loss: 0.8357 || timer: 0.0908 sec.
iter 223780 || Loss: 0.8173 || timer: 0.1024 sec.
iter 223790 || Loss: 1.1765 || timer: 0.0955 sec.
iter 223800 || Loss: 0.6279 || timer: 0.1020 sec.
iter 223810 || Loss: 1.0973 || timer: 0.0925 sec.
iter 223820 || Loss: 1.0756 || timer: 0.0820 sec.
iter 223830 || Loss: 0.8627 || timer: 0.0836 sec.
iter 223840 || Loss: 0.8062 || timer: 0.0919 sec.
iter 223850 || Loss: 1.3393 || timer: 0.0920 sec.
iter 223860 || Loss: 0.8511 || timer: 0.0829 sec.
iter 223870 || Loss: 1.0525 || timer: 0.0937 sec.
iter 223880 || Loss: 1.1595 || timer: 0.1066 sec.
iter 223890 || Loss: 1.1195 || timer: 0.0840 sec.
iter 223900 || Loss: 0.8800 || timer: 0.0864 sec.
iter 223910 || Loss: 1.0720 || timer: 0.1050 sec.
iter 223920 || Loss: 0.8789 || timer: 0.0826 sec.
iter 223930 || Loss: 0.6195 || timer: 0.1110 sec.
iter 223940 || Loss: 1.2107 || timer: 0.0900 sec.
iter 223950 || Loss: 0.7678 || timer: 0.1128 sec.
iter 223960 || Loss: 0.9899 || timer: 0.0222 sec.
iter 223970 || Loss: 1.9032 || timer: 0.0818 sec.
iter 223980 || Loss: 1.0914 || timer: 0.1115 sec.
iter 223990 || Loss: 1.2752 || timer: 0.0885 sec.
iter 224000 || Loss: 0.8994 || timer: 0.1024 sec.
iter 224010 || Loss: 0.9495 || timer: 0.1036 sec.
iter 224020 || Loss: 0.8548 || timer: 0.0954 sec.
iter 224030 || Loss: 0.7922 || timer: 0.0833 sec.
iter 224040 || Loss: 0.6219 || timer: 0.0894 sec.
iter 224050 || Loss: 0.8553 || timer: 0.0888 sec.
iter 224060 || Loss: 0.9460 || timer: 0.0945 sec.
iter 224070 || Loss: 0.6545 || timer: 0.0977 sec.
iter 224080 || Loss: 0.7482 || timer: 0.1028 sec.
iter 224090 || Loss: 0.9092 || timer: 0.0913 sec.
iter 224100 || Loss: 1.1088 || timer: 0.0996 sec.
iter 224110 || Loss: 0.8808 || timer: 0.0885 sec.
iter 224120 || Loss: 1.2947 || timer: 0.0962 sec.
iter 224130 || Loss: 0.7735 || timer: 0.0947 sec.
iter 224140 || Loss: 0.7990 || timer: 0.0902 sec.
iter 224150 || Loss: 0.9031 || timer: 0.0837 sec.
iter 224160 || Loss: 0.9066 || timer: 0.1083 sec.
iter 224170 || Loss: 1.0416 || timer: 0.1214 sec.
iter 224180 || Loss: 1.1216 || timer: 0.0887 sec.
iter 224190 || Loss: 1.1499 || timer: 0.0832 sec.
iter 224200 || Loss: 1.2201 || timer: 0.0908 sec.
iter 224210 || Loss: 0.8973 || timer: 0.0852 sec.
iter 224220 || Loss: 0.9552 || timer: 0.0987 sec.
iter 224230 || Loss: 1.1455 || timer: 0.0848 sec.
iter 224240 || Loss: 0.6223 || timer: 0.0920 sec.
iter 224250 || Loss: 0.7457 || timer: 0.1025 sec.
iter 224260 || Loss: 0.9256 || timer: 0.0840 sec.
iter 224270 || Loss: 0.6601 || timer: 0.0921 sec.
iter 224280 || Loss: 0.9819 || timer: 0.0907 sec.
iter 224290 || Loss: 0.6272 || timer: 0.0287 sec.
iter 224300 || Loss: 0.3153 || timer: 0.0943 sec.
iter 224310 || Loss: 1.0156 || timer: 0.0916 sec.
iter 224320 || Loss: 1.0323 || timer: 0.0904 sec.
iter 224330 || Loss: 1.0314 || timer: 0.0873 sec.
iter 224340 || Loss: 0.9430 || timer: 0.0929 sec.
iter 224350 || Loss: 1.1138 || timer: 0.0890 sec.
iter 224360 || Loss: 0.8927 || timer: 0.0837 sec.
iter 224370 || Loss: 0.7875 || timer: 0.1022 sec.
iter 224380 || Loss: 0.6690 || timer: 0.0851 sec.
iter 224390 || Loss: 0.9675 || timer: 0.0974 sec.
iter 224400 || Loss: 1.0136 || timer: 0.0828 sec.
iter 224410 || Loss: 0.6392 || timer: 0.0839 sec.
iter 224420 || Loss: 1.0122 || timer: 0.0956 sec.
iter 224430 || Loss: 0.8515 || timer: 0.1013 sec.
iter 224440 || Loss: 0.7884 || timer: 0.1038 sec.
iter 224450 || Loss: 0.8895 || timer: 0.1096 sec.
iter 224460 || Loss: 1.0233 || timer: 0.0875 sec.
iter 224470 || Loss: 0.6693 || timer: 0.0990 sec.
iter 224480 || Loss: 1.0379 || timer: 0.0909 sec.
iter 224490 || Loss: 0.8456 || timer: 0.0894 sec.
iter 224500 || Loss: 0.7709 || timer: 0.1072 sec.
iter 224510 || Loss: 0.8829 || timer: 0.0849 sec.
iter 224520 || Loss: 0.8932 || timer: 0.0919 sec.
iter 224530 || Loss: 1.1080 || timer: 0.0878 sec.
iter 224540 || Loss: 1.3267 || timer: 0.0839 sec.
iter 224550 || Loss: 0.7940 || timer: 0.0829 sec.
iter 224560 || Loss: 0.8779 || timer: 0.0888 sec.
iter 224570 || Loss: 1.4041 || timer: 0.0916 sec.
iter 224580 || Loss: 0.9253 || timer: 0.1035 sec.
iter 224590 || Loss: 1.0728 || timer: 0.0918 sec.
iter 224600 || Loss: 0.9444 || timer: 0.0897 sec.
iter 224610 || Loss: 0.6734 || timer: 0.0758 sec.
iter 224620 || Loss: 0.8458 || timer: 0.0135 sec.
iter 224630 || Loss: 1.3029 || timer: 0.0905 sec.
iter 224640 || Loss: 0.8972 || timer: 0.0882 sec.
iter 224650 || Loss: 0.9255 || timer: 0.0904 sec.
iter 224660 || Loss: 0.7679 || timer: 0.0964 sec.
iter 224670 || Loss: 1.1395 || timer: 0.0886 sec.
iter 224680 || Loss: 0.7927 || timer: 0.0887 sec.
iter 224690 || Loss: 0.9726 || timer: 0.0900 sec.
iter 224700 || Loss: 0.9219 || timer: 0.1041 sec.
iter 224710 || Loss: 0.8117 || timer: 0.0984 sec.
iter 224720 || Loss: 0.8655 || timer: 0.0992 sec.
iter 224730 || Loss: 0.7654 || timer: 0.1123 sec.
iter 224740 || Loss: 0.5550 || timer: 0.0862 sec.
iter 224750 || Loss: 0.6356 || timer: 0.0883 sec.
iter 224760 || Loss: 1.0127 || timer: 0.0882 sec.
iter 224770 || Loss: 0.8390 || timer: 0.0759 sec.
iter 224780 || Loss: 1.2241 || timer: 0.0826 sec.
iter 224790 || Loss: 1.1363 || timer: 0.0902 sec.
iter 224800 || Loss: 0.8082 || timer: 0.0918 sec.
iter 224810 || Loss: 0.6034 || timer: 0.1019 sec.
iter 224820 || Loss: 0.8840 || timer: 0.0968 sec.
iter 224830 || Loss: 0.8936 || timer: 0.0906 sec.
iter 224840 || Loss: 1.0448 || timer: 0.0923 sec.
iter 224850 || Loss: 0.8561 || timer: 0.0825 sec.
iter 224860 || Loss: 1.0906 || timer: 0.1019 sec.
iter 224870 || Loss: 1.2286 || timer: 0.1029 sec.
iter 224880 || Loss: 0.9581 || timer: 0.0823 sec.
iter 224890 || Loss: 0.8784 || timer: 0.1092 sec.
iter 224900 || Loss: 0.8764 || timer: 0.0912 sec.
iter 224910 || Loss: 0.8150 || timer: 0.0911 sec.
iter 224920 || Loss: 0.8012 || timer: 0.0814 sec.
iter 224930 || Loss: 0.6952 || timer: 0.0942 sec.
iter 224940 || Loss: 0.9455 || timer: 0.1287 sec.
iter 224950 || Loss: 0.7557 || timer: 0.0240 sec.
iter 224960 || Loss: 0.1868 || timer: 0.0904 sec.
iter 224970 || Loss: 1.0075 || timer: 0.0896 sec.
iter 224980 || Loss: 0.7166 || timer: 0.0847 sec.
iter 224990 || Loss: 0.5702 || timer: 0.0940 sec.
iter 225000 || Loss: 1.2403 || Saving state, iter: 225000
timer: 0.0997 sec.
iter 225010 || Loss: 0.7396 || timer: 0.0887 sec.
iter 225020 || Loss: 0.9173 || timer: 0.1324 sec.
iter 225030 || Loss: 1.1008 || timer: 0.0893 sec.
iter 225040 || Loss: 0.8329 || timer: 0.0827 sec.
iter 225050 || Loss: 0.7104 || timer: 0.1097 sec.
iter 225060 || Loss: 0.9447 || timer: 0.0831 sec.
iter 225070 || Loss: 0.7457 || timer: 0.0754 sec.
iter 225080 || Loss: 1.0332 || timer: 0.0750 sec.
iter 225090 || Loss: 0.8150 || timer: 0.0834 sec.
iter 225100 || Loss: 0.7856 || timer: 0.0740 sec.
iter 225110 || Loss: 1.0907 || timer: 0.0823 sec.
iter 225120 || Loss: 0.8686 || timer: 0.0886 sec.
iter 225130 || Loss: 0.8972 || timer: 0.0874 sec.
iter 225140 || Loss: 0.5882 || timer: 0.0943 sec.
iter 225150 || Loss: 0.6906 || timer: 0.0818 sec.
iter 225160 || Loss: 0.7280 || timer: 0.0934 sec.
iter 225170 || Loss: 0.7890 || timer: 0.0890 sec.
iter 225180 || Loss: 0.7127 || timer: 0.0729 sec.
iter 225190 || Loss: 0.8161 || timer: 0.0910 sec.
iter 225200 || Loss: 0.9676 || timer: 0.0907 sec.
iter 225210 || Loss: 0.9628 || timer: 0.0982 sec.
iter 225220 || Loss: 0.6391 || timer: 0.0896 sec.
iter 225230 || Loss: 0.6450 || timer: 0.0825 sec.
iter 225240 || Loss: 1.3851 || timer: 0.0939 sec.
iter 225250 || Loss: 0.9384 || timer: 0.0842 sec.
iter 225260 || Loss: 0.8410 || timer: 0.0925 sec.
iter 225270 || Loss: 0.7033 || timer: 0.0905 sec.
iter 225280 || Loss: 0.9942 || timer: 0.0213 sec.
iter 225290 || Loss: 1.0822 || timer: 0.0916 sec.
iter 225300 || Loss: 0.7340 || timer: 0.0923 sec.
iter 225310 || Loss: 0.8817 || timer: 0.0915 sec.
iter 225320 || Loss: 0.8824 || timer: 0.0901 sec.
iter 225330 || Loss: 0.9382 || timer: 0.0827 sec.
iter 225340 || Loss: 0.7601 || timer: 0.0904 sec.
iter 225350 || Loss: 0.9755 || timer: 0.0901 sec.
iter 225360 || Loss: 0.7794 || timer: 0.1028 sec.
iter 225370 || Loss: 0.8812 || timer: 0.0888 sec.
iter 225380 || Loss: 0.7799 || timer: 0.0955 sec.
iter 225390 || Loss: 0.8381 || timer: 0.0866 sec.
iter 225400 || Loss: 0.8973 || timer: 0.0911 sec.
iter 225410 || Loss: 0.9771 || timer: 0.0903 sec.
iter 225420 || Loss: 0.6818 || timer: 0.1124 sec.
iter 225430 || Loss: 0.8774 || timer: 0.0871 sec.
iter 225440 || Loss: 0.7574 || timer: 0.0821 sec.
iter 225450 || Loss: 0.7157 || timer: 0.0911 sec.
iter 225460 || Loss: 1.0183 || timer: 0.0872 sec.
iter 225470 || Loss: 0.7964 || timer: 0.0822 sec.
iter 225480 || Loss: 0.7592 || timer: 0.0755 sec.
iter 225490 || Loss: 1.3994 || timer: 0.0833 sec.
iter 225500 || Loss: 1.1372 || timer: 0.0832 sec.
iter 225510 || Loss: 1.1774 || timer: 0.0725 sec.
iter 225520 || Loss: 0.9315 || timer: 0.0854 sec.
iter 225530 || Loss: 1.1044 || timer: 0.0987 sec.
iter 225540 || Loss: 1.0976 || timer: 0.0815 sec.
iter 225550 || Loss: 0.8697 || timer: 0.0959 sec.
iter 225560 || Loss: 0.6480 || timer: 0.0829 sec.
iter 225570 || Loss: 1.1529 || timer: 0.0964 sec.
iter 225580 || Loss: 1.3828 || timer: 0.0952 sec.
iter 225590 || Loss: 1.0162 || timer: 0.1042 sec.
iter 225600 || Loss: 0.8272 || timer: 0.0915 sec.
iter 225610 || Loss: 0.6644 || timer: 0.0216 sec.
iter 225620 || Loss: 1.0320 || timer: 0.0982 sec.
iter 225630 || Loss: 0.7222 || timer: 0.1322 sec.
iter 225640 || Loss: 0.8955 || timer: 0.0945 sec.
iter 225650 || Loss: 1.5390 || timer: 0.0863 sec.
iter 225660 || Loss: 0.7680 || timer: 0.0953 sec.
iter 225670 || Loss: 1.0233 || timer: 0.0917 sec.
iter 225680 || Loss: 0.5722 || timer: 0.0942 sec.
iter 225690 || Loss: 0.8722 || timer: 0.1061 sec.
iter 225700 || Loss: 1.0605 || timer: 0.0882 sec.
iter 225710 || Loss: 0.8364 || timer: 0.1116 sec.
iter 225720 || Loss: 0.9499 || timer: 0.0909 sec.
iter 225730 || Loss: 0.6889 || timer: 0.1100 sec.
iter 225740 || Loss: 0.9189 || timer: 0.0877 sec.
iter 225750 || Loss: 0.9402 || timer: 0.0837 sec.
iter 225760 || Loss: 0.9408 || timer: 0.1100 sec.
iter 225770 || Loss: 1.0837 || timer: 0.1029 sec.
iter 225780 || Loss: 0.9396 || timer: 0.0842 sec.
iter 225790 || Loss: 0.9387 || timer: 0.0829 sec.
iter 225800 || Loss: 0.7835 || timer: 0.0966 sec.
iter 225810 || Loss: 0.8457 || timer: 0.0893 sec.
iter 225820 || Loss: 1.1088 || timer: 0.1029 sec.
iter 225830 || Loss: 0.6864 || timer: 0.0984 sec.
iter 225840 || Loss: 0.9111 || timer: 0.0946 sec.
iter 225850 || Loss: 0.6072 || timer: 0.0918 sec.
iter 225860 || Loss: 0.9916 || timer: 0.0849 sec.
iter 225870 || Loss: 1.0439 || timer: 0.0909 sec.
iter 225880 || Loss: 0.6427 || timer: 0.1047 sec.
iter 225890 || Loss: 0.9331 || timer: 0.0828 sec.
iter 225900 || Loss: 0.9998 || timer: 0.0872 sec.
iter 225910 || Loss: 0.7198 || timer: 0.0887 sec.
iter 225920 || Loss: 0.9969 || timer: 0.0823 sec.
iter 225930 || Loss: 0.8756 || timer: 0.0838 sec.
iter 225940 || Loss: 0.7502 || timer: 0.0262 sec.
iter 225950 || Loss: 0.5109 || timer: 0.0828 sec.
iter 225960 || Loss: 0.9055 || timer: 0.1031 sec.
iter 225970 || Loss: 0.8189 || timer: 0.0921 sec.
iter 225980 || Loss: 1.0057 || timer: 0.0871 sec.
iter 225990 || Loss: 0.9533 || timer: 0.1035 sec.
iter 226000 || Loss: 0.9312 || timer: 0.0828 sec.
iter 226010 || Loss: 0.7223 || timer: 0.1123 sec.
iter 226020 || Loss: 0.7260 || timer: 0.0925 sec.
iter 226030 || Loss: 1.3955 || timer: 0.0840 sec.
iter 226040 || Loss: 1.0647 || timer: 0.1023 sec.
iter 226050 || Loss: 1.3560 || timer: 0.0837 sec.
iter 226060 || Loss: 0.8997 || timer: 0.0894 sec.
iter 226070 || Loss: 1.0883 || timer: 0.0897 sec.
iter 226080 || Loss: 1.1498 || timer: 0.0923 sec.
iter 226090 || Loss: 1.0208 || timer: 0.0918 sec.
iter 226100 || Loss: 0.8701 || timer: 0.0854 sec.
iter 226110 || Loss: 0.6369 || timer: 0.1209 sec.
iter 226120 || Loss: 1.0883 || timer: 0.1292 sec.
iter 226130 || Loss: 0.9003 || timer: 0.0887 sec.
iter 226140 || Loss: 0.9181 || timer: 0.0929 sec.
iter 226150 || Loss: 0.7823 || timer: 0.0927 sec.
iter 226160 || Loss: 1.0691 || timer: 0.0930 sec.
iter 226170 || Loss: 0.7428 || timer: 0.0829 sec.
iter 226180 || Loss: 1.0749 || timer: 0.0879 sec.
iter 226190 || Loss: 0.9370 || timer: 0.0912 sec.
iter 226200 || Loss: 0.6019 || timer: 0.0871 sec.
iter 226210 || Loss: 0.8193 || timer: 0.0824 sec.
iter 226220 || Loss: 0.8646 || timer: 0.0901 sec.
iter 226230 || Loss: 1.0994 || timer: 0.0914 sec.
iter 226240 || Loss: 1.0884 || timer: 0.0921 sec.
iter 226250 || Loss: 0.8422 || timer: 0.0881 sec.
iter 226260 || Loss: 1.2235 || timer: 0.0888 sec.
iter 226270 || Loss: 0.8613 || timer: 0.0259 sec.
iter 226280 || Loss: 0.4774 || timer: 0.0914 sec.
iter 226290 || Loss: 0.8404 || timer: 0.1012 sec.
iter 226300 || Loss: 0.6679 || timer: 0.0992 sec.
iter 226310 || Loss: 1.1348 || timer: 0.0897 sec.
iter 226320 || Loss: 0.6534 || timer: 0.0922 sec.
iter 226330 || Loss: 0.9608 || timer: 0.0844 sec.
iter 226340 || Loss: 0.9411 || timer: 0.0867 sec.
iter 226350 || Loss: 1.0496 || timer: 0.1136 sec.
iter 226360 || Loss: 1.2197 || timer: 0.0906 sec.
iter 226370 || Loss: 0.8735 || timer: 0.0963 sec.
iter 226380 || Loss: 0.8543 || timer: 0.0896 sec.
iter 226390 || Loss: 0.8050 || timer: 0.0916 sec.
iter 226400 || Loss: 0.8741 || timer: 0.0899 sec.
iter 226410 || Loss: 0.8616 || timer: 0.1021 sec.
iter 226420 || Loss: 1.1767 || timer: 0.0828 sec.
iter 226430 || Loss: 1.2111 || timer: 0.0900 sec.
iter 226440 || Loss: 1.3286 || timer: 0.1105 sec.
iter 226450 || Loss: 1.0801 || timer: 0.0843 sec.
iter 226460 || Loss: 0.7509 || timer: 0.0948 sec.
iter 226470 || Loss: 0.9362 || timer: 0.0930 sec.
iter 226480 || Loss: 0.9813 || timer: 0.0891 sec.
iter 226490 || Loss: 0.9528 || timer: 0.0926 sec.
iter 226500 || Loss: 0.6961 || timer: 0.0824 sec.
iter 226510 || Loss: 0.7301 || timer: 0.0927 sec.
iter 226520 || Loss: 0.9798 || timer: 0.0905 sec.
iter 226530 || Loss: 0.8374 || timer: 0.0904 sec.
iter 226540 || Loss: 0.9994 || timer: 0.0833 sec.
iter 226550 || Loss: 1.1811 || timer: 0.0846 sec.
iter 226560 || Loss: 0.9211 || timer: 0.0845 sec.
iter 226570 || Loss: 0.9030 || timer: 0.0900 sec.
iter 226580 || Loss: 0.8926 || timer: 0.0825 sec.
iter 226590 || Loss: 1.0612 || timer: 0.0924 sec.
iter 226600 || Loss: 0.9795 || timer: 0.0189 sec.
iter 226610 || Loss: 1.3734 || timer: 0.0827 sec.
iter 226620 || Loss: 0.8107 || timer: 0.0932 sec.
iter 226630 || Loss: 0.9133 || timer: 0.0921 sec.
iter 226640 || Loss: 0.7570 || timer: 0.0839 sec.
iter 226650 || Loss: 0.8976 || timer: 0.0902 sec.
iter 226660 || Loss: 1.0070 || timer: 0.0840 sec.
iter 226670 || Loss: 0.7738 || timer: 0.0901 sec.
iter 226680 || Loss: 0.7368 || timer: 0.0828 sec.
iter 226690 || Loss: 0.9575 || timer: 0.0809 sec.
iter 226700 || Loss: 0.8522 || timer: 0.0889 sec.
iter 226710 || Loss: 0.7136 || timer: 0.0901 sec.
iter 226720 || Loss: 1.0859 || timer: 0.1065 sec.
iter 226730 || Loss: 1.1847 || timer: 0.0916 sec.
iter 226740 || Loss: 0.9081 || timer: 0.0851 sec.
iter 226750 || Loss: 0.9123 || timer: 0.0901 sec.
iter 226760 || Loss: 0.7238 || timer: 0.0943 sec.
iter 226770 || Loss: 1.0911 || timer: 0.0938 sec.
iter 226780 || Loss: 1.1514 || timer: 0.0900 sec.
iter 226790 || Loss: 0.7455 || timer: 0.0918 sec.
iter 226800 || Loss: 1.0135 || timer: 0.0949 sec.
iter 226810 || Loss: 0.8773 || timer: 0.0929 sec.
iter 226820 || Loss: 0.7522 || timer: 0.0907 sec.
iter 226830 || Loss: 1.1252 || timer: 0.0827 sec.
iter 226840 || Loss: 0.8969 || timer: 0.0895 sec.
iter 226850 || Loss: 1.0521 || timer: 0.0911 sec.
iter 226860 || Loss: 0.7835 || timer: 0.0986 sec.
iter 226870 || Loss: 0.7622 || timer: 0.0834 sec.
iter 226880 || Loss: 1.0320 || timer: 0.0901 sec.
iter 226890 || Loss: 0.8851 || timer: 0.0898 sec.
iter 226900 || Loss: 0.8375 || timer: 0.0829 sec.
iter 226910 || Loss: 0.9070 || timer: 0.0898 sec.
iter 226920 || Loss: 0.7801 || timer: 0.0875 sec.
iter 226930 || Loss: 0.7137 || timer: 0.0201 sec.
iter 226940 || Loss: 0.2881 || timer: 0.0824 sec.
iter 226950 || Loss: 0.9738 || timer: 0.0915 sec.
iter 226960 || Loss: 0.9141 || timer: 0.0922 sec.
iter 226970 || Loss: 0.7626 || timer: 0.0905 sec.
iter 226980 || Loss: 0.9497 || timer: 0.0867 sec.
iter 226990 || Loss: 0.9454 || timer: 0.0882 sec.
iter 227000 || Loss: 0.8236 || timer: 0.1030 sec.
iter 227010 || Loss: 0.8956 || timer: 0.0901 sec.
iter 227020 || Loss: 0.7648 || timer: 0.0825 sec.
iter 227030 || Loss: 0.7215 || timer: 0.1059 sec.
iter 227040 || Loss: 1.0200 || timer: 0.0830 sec.
iter 227050 || Loss: 0.6730 || timer: 0.0895 sec.
iter 227060 || Loss: 0.6219 || timer: 0.0925 sec.
iter 227070 || Loss: 0.7413 || timer: 0.0873 sec.
iter 227080 || Loss: 1.2070 || timer: 0.0908 sec.
iter 227090 || Loss: 1.0545 || timer: 0.0936 sec.
iter 227100 || Loss: 0.8849 || timer: 0.0925 sec.
iter 227110 || Loss: 1.2255 || timer: 0.0911 sec.
iter 227120 || Loss: 0.9894 || timer: 0.1016 sec.
iter 227130 || Loss: 0.7014 || timer: 0.0834 sec.
iter 227140 || Loss: 0.9981 || timer: 0.0899 sec.
iter 227150 || Loss: 1.0581 || timer: 0.0922 sec.
iter 227160 || Loss: 1.0101 || timer: 0.0807 sec.
iter 227170 || Loss: 0.8701 || timer: 0.1071 sec.
iter 227180 || Loss: 0.8696 || timer: 0.0917 sec.
iter 227190 || Loss: 0.6855 || timer: 0.1014 sec.
iter 227200 || Loss: 0.8058 || timer: 0.0843 sec.
iter 227210 || Loss: 1.2006 || timer: 0.0919 sec.
iter 227220 || Loss: 0.8607 || timer: 0.0833 sec.
iter 227230 || Loss: 0.7997 || timer: 0.0872 sec.
iter 227240 || Loss: 0.9208 || timer: 0.1082 sec.
iter 227250 || Loss: 0.7677 || timer: 0.0948 sec.
iter 227260 || Loss: 0.8595 || timer: 0.0290 sec.
iter 227270 || Loss: 1.2280 || timer: 0.0843 sec.
iter 227280 || Loss: 0.6743 || timer: 0.0876 sec.
iter 227290 || Loss: 1.4163 || timer: 0.0852 sec.
iter 227300 || Loss: 0.8485 || timer: 0.0842 sec.
iter 227310 || Loss: 0.8915 || timer: 0.0899 sec.
iter 227320 || Loss: 1.0995 || timer: 0.0832 sec.
iter 227330 || Loss: 0.7379 || timer: 0.0845 sec.
iter 227340 || Loss: 0.8739 || timer: 0.0904 sec.
iter 227350 || Loss: 1.1789 || timer: 0.0918 sec.
iter 227360 || Loss: 0.8069 || timer: 0.1228 sec.
iter 227370 || Loss: 0.8271 || timer: 0.0908 sec.
iter 227380 || Loss: 0.7299 || timer: 0.0915 sec.
iter 227390 || Loss: 0.7945 || timer: 0.0911 sec.
iter 227400 || Loss: 0.8259 || timer: 0.0868 sec.
iter 227410 || Loss: 1.0948 || timer: 0.0922 sec.
iter 227420 || Loss: 0.7767 || timer: 0.0901 sec.
iter 227430 || Loss: 0.9560 || timer: 0.0893 sec.
iter 227440 || Loss: 1.1900 || timer: 0.0920 sec.
iter 227450 || Loss: 0.9592 || timer: 0.0852 sec.
iter 227460 || Loss: 0.7940 || timer: 0.0831 sec.
iter 227470 || Loss: 0.7700 || timer: 0.0928 sec.
iter 227480 || Loss: 0.7807 || timer: 0.0839 sec.
iter 227490 || Loss: 1.0017 || timer: 0.0927 sec.
iter 227500 || Loss: 1.0768 || timer: 0.0914 sec.
iter 227510 || Loss: 0.7024 || timer: 0.0916 sec.
iter 227520 || Loss: 0.8364 || timer: 0.0863 sec.
iter 227530 || Loss: 1.0907 || timer: 0.0937 sec.
iter 227540 || Loss: 0.6124 || timer: 0.0938 sec.
iter 227550 || Loss: 0.8404 || timer: 0.0812 sec.
iter 227560 || Loss: 0.7589 || timer: 0.0883 sec.
iter 227570 || Loss: 1.1739 || timer: 0.1129 sec.
iter 227580 || Loss: 1.0247 || timer: 0.0873 sec.
iter 227590 || Loss: 1.0789 || timer: 0.0234 sec.
iter 227600 || Loss: 1.3808 || timer: 0.0824 sec.
iter 227610 || Loss: 0.7874 || timer: 0.0915 sec.
iter 227620 || Loss: 0.7975 || timer: 0.0857 sec.
iter 227630 || Loss: 1.0612 || timer: 0.0956 sec.
iter 227640 || Loss: 0.8223 || timer: 0.0812 sec.
iter 227650 || Loss: 1.1204 || timer: 0.0819 sec.
iter 227660 || Loss: 0.9756 || timer: 0.0853 sec.
iter 227670 || Loss: 1.1495 || timer: 0.0885 sec.
iter 227680 || Loss: 1.1220 || timer: 0.1200 sec.
iter 227690 || Loss: 0.7242 || timer: 0.0981 sec.
iter 227700 || Loss: 0.8625 || timer: 0.0942 sec.
iter 227710 || Loss: 0.9735 || timer: 0.0807 sec.
iter 227720 || Loss: 1.0199 || timer: 0.0863 sec.
iter 227730 || Loss: 0.8361 || timer: 0.0888 sec.
iter 227740 || Loss: 1.0249 || timer: 0.0915 sec.
iter 227750 || Loss: 0.7879 || timer: 0.0912 sec.
iter 227760 || Loss: 0.7252 || timer: 0.0898 sec.
iter 227770 || Loss: 0.7164 || timer: 0.0878 sec.
iter 227780 || Loss: 0.7618 || timer: 0.0918 sec.
iter 227790 || Loss: 1.0469 || timer: 0.0889 sec.
iter 227800 || Loss: 0.9502 || timer: 0.0830 sec.
iter 227810 || Loss: 0.8755 || timer: 0.0912 sec.
iter 227820 || Loss: 1.2210 || timer: 0.0877 sec.
iter 227830 || Loss: 0.7373 || timer: 0.0889 sec.
iter 227840 || Loss: 0.8836 || timer: 0.0883 sec.
iter 227850 || Loss: 0.8325 || timer: 0.0818 sec.
iter 227860 || Loss: 0.7960 || timer: 0.0879 sec.
iter 227870 || Loss: 0.7296 || timer: 0.1094 sec.
iter 227880 || Loss: 0.9100 || timer: 0.0822 sec.
iter 227890 || Loss: 1.2064 || timer: 0.0824 sec.
iter 227900 || Loss: 1.2718 || timer: 0.0841 sec.
iter 227910 || Loss: 0.8174 || timer: 0.0922 sec.
iter 227920 || Loss: 0.9538 || timer: 0.0163 sec.
iter 227930 || Loss: 0.5221 || timer: 0.0873 sec.
iter 227940 || Loss: 0.8705 || timer: 0.0886 sec.
iter 227950 || Loss: 0.7916 || timer: 0.0878 sec.
iter 227960 || Loss: 0.7903 || timer: 0.0898 sec.
iter 227970 || Loss: 0.9941 || timer: 0.0826 sec.
iter 227980 || Loss: 0.8460 || timer: 0.1241 sec.
iter 227990 || Loss: 0.6060 || timer: 0.0893 sec.
iter 228000 || Loss: 0.7349 || timer: 0.0895 sec.
iter 228010 || Loss: 1.1256 || timer: 0.1084 sec.
iter 228020 || Loss: 1.0893 || timer: 0.1027 sec.
iter 228030 || Loss: 0.8436 || timer: 0.0953 sec.
iter 228040 || Loss: 1.0392 || timer: 0.0878 sec.
iter 228050 || Loss: 0.9443 || timer: 0.0881 sec.
iter 228060 || Loss: 0.8786 || timer: 0.1078 sec.
iter 228070 || Loss: 1.0738 || timer: 0.0879 sec.
iter 228080 || Loss: 0.7569 || timer: 0.0862 sec.
iter 228090 || Loss: 0.8313 || timer: 0.0811 sec.
iter 228100 || Loss: 0.9404 || timer: 0.1078 sec.
iter 228110 || Loss: 1.0563 || timer: 0.1041 sec.
iter 228120 || Loss: 1.2437 || timer: 0.0896 sec.
iter 228130 || Loss: 0.8155 || timer: 0.0813 sec.
iter 228140 || Loss: 0.9147 || timer: 0.0838 sec.
iter 228150 || Loss: 1.1271 || timer: 0.0813 sec.
iter 228160 || Loss: 0.7173 || timer: 0.0929 sec.
iter 228170 || Loss: 0.9019 || timer: 0.0906 sec.
iter 228180 || Loss: 0.7673 || timer: 0.1119 sec.
iter 228190 || Loss: 0.7968 || timer: 0.0907 sec.
iter 228200 || Loss: 1.0434 || timer: 0.0963 sec.
iter 228210 || Loss: 1.0472 || timer: 0.1023 sec.
iter 228220 || Loss: 0.8210 || timer: 0.0859 sec.
iter 228230 || Loss: 1.0150 || timer: 0.0918 sec.
iter 228240 || Loss: 0.7923 || timer: 0.0812 sec.
iter 228250 || Loss: 0.8686 || timer: 0.0180 sec.
iter 228260 || Loss: 1.0780 || timer: 0.0830 sec.
iter 228270 || Loss: 0.9604 || timer: 0.1052 sec.
iter 228280 || Loss: 0.7940 || timer: 0.0896 sec.
iter 228290 || Loss: 1.0024 || timer: 0.0843 sec.
iter 228300 || Loss: 1.0639 || timer: 0.0885 sec.
iter 228310 || Loss: 0.8699 || timer: 0.0890 sec.
iter 228320 || Loss: 0.9417 || timer: 0.0886 sec.
iter 228330 || Loss: 0.7673 || timer: 0.0879 sec.
iter 228340 || Loss: 0.8056 || timer: 0.0910 sec.
iter 228350 || Loss: 0.7923 || timer: 0.0991 sec.
iter 228360 || Loss: 0.9227 || timer: 0.0815 sec.
iter 228370 || Loss: 0.8396 || timer: 0.0898 sec.
iter 228380 || Loss: 0.9891 || timer: 0.0817 sec.
iter 228390 || Loss: 0.6250 || timer: 0.0977 sec.
iter 228400 || Loss: 1.3555 || timer: 0.1092 sec.
iter 228410 || Loss: 1.4085 || timer: 0.1011 sec.
iter 228420 || Loss: 0.7937 || timer: 0.0985 sec.
iter 228430 || Loss: 1.0248 || timer: 0.0915 sec.
iter 228440 || Loss: 0.9647 || timer: 0.1000 sec.
iter 228450 || Loss: 0.8191 || timer: 0.0811 sec.
iter 228460 || Loss: 0.8052 || timer: 0.0879 sec.
iter 228470 || Loss: 0.7834 || timer: 0.0809 sec.
iter 228480 || Loss: 0.8795 || timer: 0.0814 sec.
iter 228490 || Loss: 1.0722 || timer: 0.0954 sec.
iter 228500 || Loss: 0.5484 || timer: 0.0883 sec.
iter 228510 || Loss: 0.9357 || timer: 0.0805 sec.
iter 228520 || Loss: 0.8134 || timer: 0.0883 sec.
iter 228530 || Loss: 0.6877 || timer: 0.0809 sec.
iter 228540 || Loss: 0.7934 || timer: 0.0847 sec.
iter 228550 || Loss: 1.0011 || timer: 0.0900 sec.
iter 228560 || Loss: 1.0439 || timer: 0.0891 sec.
iter 228570 || Loss: 0.7888 || timer: 0.0794 sec.
iter 228580 || Loss: 0.7718 || timer: 0.0244 sec.
iter 228590 || Loss: 0.2807 || timer: 0.0826 sec.
iter 228600 || Loss: 0.8924 || timer: 0.0812 sec.
iter 228610 || Loss: 0.9632 || timer: 0.0829 sec.
iter 228620 || Loss: 1.0878 || timer: 0.0845 sec.
iter 228630 || Loss: 0.7937 || timer: 0.1076 sec.
iter 228640 || Loss: 0.9236 || timer: 0.0884 sec.
iter 228650 || Loss: 1.0221 || timer: 0.0848 sec.
iter 228660 || Loss: 1.4191 || timer: 0.0877 sec.
iter 228670 || Loss: 0.8047 || timer: 0.0825 sec.
iter 228680 || Loss: 0.9498 || timer: 0.1213 sec.
iter 228690 || Loss: 0.9298 || timer: 0.0861 sec.
iter 228700 || Loss: 1.0553 || timer: 0.0868 sec.
iter 228710 || Loss: 0.9362 || timer: 0.0846 sec.
iter 228720 || Loss: 0.8339 || timer: 0.0910 sec.
iter 228730 || Loss: 1.0579 || timer: 0.0906 sec.
iter 228740 || Loss: 0.9351 || timer: 0.0896 sec.
iter 228750 || Loss: 0.6217 || timer: 0.1020 sec.
iter 228760 || Loss: 0.7523 || timer: 0.0900 sec.
iter 228770 || Loss: 0.8316 || timer: 0.1042 sec.
iter 228780 || Loss: 0.9550 || timer: 0.0853 sec.
iter 228790 || Loss: 0.7040 || timer: 0.0823 sec.
iter 228800 || Loss: 0.8007 || timer: 0.0875 sec.
iter 228810 || Loss: 1.3579 || timer: 0.0888 sec.
iter 228820 || Loss: 0.8080 || timer: 0.0921 sec.
iter 228830 || Loss: 0.7663 || timer: 0.0837 sec.
iter 228840 || Loss: 1.1791 || timer: 0.0927 sec.
iter 228850 || Loss: 0.9345 || timer: 0.0905 sec.
iter 228860 || Loss: 0.9459 || timer: 0.0972 sec.
iter 228870 || Loss: 0.8807 || timer: 0.0888 sec.
iter 228880 || Loss: 0.8180 || timer: 0.0911 sec.
iter 228890 || Loss: 1.0137 || timer: 0.0880 sec.
iter 228900 || Loss: 0.6351 || timer: 0.0848 sec.
iter 228910 || Loss: 0.7168 || timer: 0.0178 sec.
iter 228920 || Loss: 1.2782 || timer: 0.0888 sec.
iter 228930 || Loss: 0.7392 || timer: 0.1250 sec.
iter 228940 || Loss: 0.8610 || timer: 0.0914 sec.
iter 228950 || Loss: 1.0204 || timer: 0.0810 sec.
iter 228960 || Loss: 0.8866 || timer: 0.0875 sec.
iter 228970 || Loss: 0.8026 || timer: 0.0826 sec.
iter 228980 || Loss: 0.9080 || timer: 0.1030 sec.
iter 228990 || Loss: 0.9124 || timer: 0.1046 sec.
iter 229000 || Loss: 0.7269 || timer: 0.0821 sec.
iter 229010 || Loss: 0.8223 || timer: 0.1256 sec.
iter 229020 || Loss: 0.7436 || timer: 0.1012 sec.
iter 229030 || Loss: 0.7976 || timer: 0.0900 sec.
iter 229040 || Loss: 0.7918 || timer: 0.0818 sec.
iter 229050 || Loss: 0.7953 || timer: 0.1014 sec.
iter 229060 || Loss: 0.9879 || timer: 0.0895 sec.
iter 229070 || Loss: 0.8264 || timer: 0.0921 sec.
iter 229080 || Loss: 1.0053 || timer: 0.0912 sec.
iter 229090 || Loss: 0.8151 || timer: 0.0900 sec.
iter 229100 || Loss: 0.8681 || timer: 0.0907 sec.
iter 229110 || Loss: 0.6896 || timer: 0.0818 sec.
iter 229120 || Loss: 0.8090 || timer: 0.0952 sec.
iter 229130 || Loss: 0.7145 || timer: 0.0866 sec.
iter 229140 || Loss: 1.0571 || timer: 0.1017 sec.
iter 229150 || Loss: 0.6239 || timer: 0.1071 sec.
iter 229160 || Loss: 1.2089 || timer: 0.0815 sec.
iter 229170 || Loss: 0.7943 || timer: 0.1049 sec.
iter 229180 || Loss: 0.8393 || timer: 0.0857 sec.
iter 229190 || Loss: 0.5361 || timer: 0.0886 sec.
iter 229200 || Loss: 0.8148 || timer: 0.1017 sec.
iter 229210 || Loss: 0.9556 || timer: 0.0857 sec.
iter 229220 || Loss: 0.6221 || timer: 0.0991 sec.
iter 229230 || Loss: 0.9888 || timer: 0.1065 sec.
iter 229240 || Loss: 0.9080 || timer: 0.0231 sec.
iter 229250 || Loss: 2.0757 || timer: 0.0813 sec.
iter 229260 || Loss: 1.0263 || timer: 0.0895 sec.
iter 229270 || Loss: 0.8670 || timer: 0.0831 sec.
iter 229280 || Loss: 1.3499 || timer: 0.0927 sec.
iter 229290 || Loss: 0.7477 || timer: 0.1054 sec.
iter 229300 || Loss: 0.7996 || timer: 0.0896 sec.
iter 229310 || Loss: 0.7896 || timer: 0.1072 sec.
iter 229320 || Loss: 1.1876 || timer: 0.0915 sec.
iter 229330 || Loss: 1.0300 || timer: 0.0810 sec.
iter 229340 || Loss: 0.8539 || timer: 0.0937 sec.
iter 229350 || Loss: 0.6700 || timer: 0.0909 sec.
iter 229360 || Loss: 0.8249 || timer: 0.0822 sec.
iter 229370 || Loss: 0.8943 || timer: 0.0892 sec.
iter 229380 || Loss: 0.6742 || timer: 0.0882 sec.
iter 229390 || Loss: 1.0215 || timer: 0.0810 sec.
iter 229400 || Loss: 0.7632 || timer: 0.0868 sec.
iter 229410 || Loss: 0.8625 || timer: 0.0879 sec.
iter 229420 || Loss: 0.6731 || timer: 0.1033 sec.
iter 229430 || Loss: 1.1324 || timer: 0.1205 sec.
iter 229440 || Loss: 1.0062 || timer: 0.0816 sec.
iter 229450 || Loss: 0.8009 || timer: 0.0860 sec.
iter 229460 || Loss: 0.9287 || timer: 0.0817 sec.
iter 229470 || Loss: 0.7120 || timer: 0.0911 sec.
iter 229480 || Loss: 0.8991 || timer: 0.1076 sec.
iter 229490 || Loss: 0.9136 || timer: 0.1110 sec.
iter 229500 || Loss: 0.8846 || timer: 0.0894 sec.
iter 229510 || Loss: 0.8554 || timer: 0.0864 sec.
iter 229520 || Loss: 1.1844 || timer: 0.1010 sec.
iter 229530 || Loss: 1.0096 || timer: 0.0880 sec.
iter 229540 || Loss: 0.7849 || timer: 0.0864 sec.
iter 229550 || Loss: 0.7877 || timer: 0.0923 sec.
iter 229560 || Loss: 1.0381 || timer: 0.0935 sec.
iter 229570 || Loss: 1.2347 || timer: 0.0258 sec.
iter 229580 || Loss: 0.3483 || timer: 0.0873 sec.
iter 229590 || Loss: 0.7414 || timer: 0.0851 sec.
iter 229600 || Loss: 0.6413 || timer: 0.0905 sec.
iter 229610 || Loss: 0.6689 || timer: 0.0870 sec.
iter 229620 || Loss: 0.7112 || timer: 0.1134 sec.
iter 229630 || Loss: 0.6103 || timer: 0.1091 sec.
iter 229640 || Loss: 1.1460 || timer: 0.1224 sec.
iter 229650 || Loss: 0.8271 || timer: 0.0904 sec.
iter 229660 || Loss: 0.7571 || timer: 0.0855 sec.
iter 229670 || Loss: 0.5477 || timer: 0.1032 sec.
iter 229680 || Loss: 0.8927 || timer: 0.0828 sec.
iter 229690 || Loss: 0.9964 || timer: 0.0911 sec.
iter 229700 || Loss: 0.9572 || timer: 0.0831 sec.
iter 229710 || Loss: 1.0534 || timer: 0.0810 sec.
iter 229720 || Loss: 0.6868 || timer: 0.0825 sec.
iter 229730 || Loss: 1.0473 || timer: 0.0894 sec.
iter 229740 || Loss: 0.8421 || timer: 0.0950 sec.
iter 229750 || Loss: 0.9428 || timer: 0.0980 sec.
iter 229760 || Loss: 0.8743 || timer: 0.0904 sec.
iter 229770 || Loss: 0.7775 || timer: 0.0882 sec.
iter 229780 || Loss: 0.9845 || timer: 0.0953 sec.
iter 229790 || Loss: 0.9343 || timer: 0.0822 sec.
iter 229800 || Loss: 0.6741 || timer: 0.0912 sec.
iter 229810 || Loss: 0.7991 || timer: 0.1122 sec.
iter 229820 || Loss: 1.5586 || timer: 0.0871 sec.
iter 229830 || Loss: 0.8948 || timer: 0.0822 sec.
iter 229840 || Loss: 0.9725 || timer: 0.0902 sec.
iter 229850 || Loss: 0.9503 || timer: 0.0880 sec.
iter 229860 || Loss: 0.9256 || timer: 0.0880 sec.
iter 229870 || Loss: 1.0080 || timer: 0.0860 sec.
iter 229880 || Loss: 0.6765 || timer: 0.0922 sec.
iter 229890 || Loss: 1.1672 || timer: 0.1068 sec.
iter 229900 || Loss: 0.7637 || timer: 0.0167 sec.
iter 229910 || Loss: 0.4311 || timer: 0.1086 sec.
iter 229920 || Loss: 0.5868 || timer: 0.0884 sec.
iter 229930 || Loss: 1.0999 || timer: 0.0948 sec.
iter 229940 || Loss: 1.0096 || timer: 0.0858 sec.
iter 229950 || Loss: 1.1102 || timer: 0.0880 sec.
iter 229960 || Loss: 1.0331 || timer: 0.0877 sec.
iter 229970 || Loss: 1.0668 || timer: 0.0815 sec.
iter 229980 || Loss: 0.6795 || timer: 0.0862 sec.
iter 229990 || Loss: 0.7499 || timer: 0.0891 sec.
iter 230000 || Loss: 0.7665 || Saving state, iter: 230000
timer: 0.0987 sec.
iter 230010 || Loss: 0.8315 || timer: 0.0934 sec.
iter 230020 || Loss: 0.9229 || timer: 0.0881 sec.
iter 230030 || Loss: 0.9653 || timer: 0.0882 sec.
iter 230040 || Loss: 1.0302 || timer: 0.0998 sec.
iter 230050 || Loss: 0.9083 || timer: 0.0864 sec.
iter 230060 || Loss: 0.8734 || timer: 0.0811 sec.
iter 230070 || Loss: 0.8885 || timer: 0.1037 sec.
iter 230080 || Loss: 0.6892 || timer: 0.0930 sec.
iter 230090 || Loss: 0.6955 || timer: 0.0929 sec.
iter 230100 || Loss: 0.7960 || timer: 0.0891 sec.
iter 230110 || Loss: 1.1642 || timer: 0.0908 sec.
iter 230120 || Loss: 0.8966 || timer: 0.0809 sec.
iter 230130 || Loss: 0.6580 || timer: 0.0812 sec.
iter 230140 || Loss: 0.8864 || timer: 0.1194 sec.
iter 230150 || Loss: 0.7663 || timer: 0.1068 sec.
iter 230160 || Loss: 1.1165 || timer: 0.0898 sec.
iter 230170 || Loss: 0.9055 || timer: 0.0823 sec.
iter 230180 || Loss: 0.7202 || timer: 0.1072 sec.
iter 230190 || Loss: 0.9666 || timer: 0.0881 sec.
iter 230200 || Loss: 0.8230 || timer: 0.1014 sec.
iter 230210 || Loss: 1.0649 || timer: 0.0914 sec.
iter 230220 || Loss: 1.5403 || timer: 0.0900 sec.
iter 230230 || Loss: 1.0677 || timer: 0.0249 sec.
iter 230240 || Loss: 5.5453 || timer: 0.1112 sec.
iter 230250 || Loss: 1.5624 || timer: 0.1153 sec.
iter 230260 || Loss: 1.5495 || timer: 0.0919 sec.
iter 230270 || Loss: 0.9904 || timer: 0.0823 sec.
iter 230280 || Loss: 0.9136 || timer: 0.0816 sec.
iter 230290 || Loss: 1.6588 || timer: 0.0935 sec.
iter 230300 || Loss: 1.0603 || timer: 0.0867 sec.
iter 230310 || Loss: 1.1423 || timer: 0.0880 sec.
iter 230320 || Loss: 0.8370 || timer: 0.0903 sec.
iter 230330 || Loss: 1.0437 || timer: 0.0964 sec.
iter 230340 || Loss: 0.9107 || timer: 0.0825 sec.
iter 230350 || Loss: 1.2988 || timer: 0.1014 sec.
iter 230360 || Loss: 1.2780 || timer: 0.0828 sec.
iter 230370 || Loss: 1.1039 || timer: 0.0929 sec.
iter 230380 || Loss: 0.7189 || timer: 0.0860 sec.
iter 230390 || Loss: 1.2063 || timer: 0.1048 sec.
iter 230400 || Loss: 0.9957 || timer: 0.0973 sec.
iter 230410 || Loss: 0.9254 || timer: 0.0860 sec.
iter 230420 || Loss: 0.9199 || timer: 0.0872 sec.
iter 230430 || Loss: 1.0120 || timer: 0.0814 sec.
iter 230440 || Loss: 0.7169 || timer: 0.0939 sec.
iter 230450 || Loss: 1.0041 || timer: 0.0908 sec.
iter 230460 || Loss: 1.3062 || timer: 0.0885 sec.
iter 230470 || Loss: 0.6370 || timer: 0.0892 sec.
iter 230480 || Loss: 0.7678 || timer: 0.0895 sec.
iter 230490 || Loss: 1.0318 || timer: 0.0826 sec.
iter 230500 || Loss: 0.8171 || timer: 0.0819 sec.
iter 230510 || Loss: 0.9210 || timer: 0.0886 sec.
iter 230520 || Loss: 1.3635 || timer: 0.0877 sec.
iter 230530 || Loss: 1.0062 || timer: 0.0883 sec.
iter 230540 || Loss: 0.8991 || timer: 0.0910 sec.
iter 230550 || Loss: 1.2033 || timer: 0.0875 sec.
iter 230560 || Loss: 1.2073 || timer: 0.0164 sec.
iter 230570 || Loss: 4.0064 || timer: 0.0933 sec.
iter 230580 || Loss: 0.9615 || timer: 0.0881 sec.
iter 230590 || Loss: 1.3786 || timer: 0.0841 sec.
iter 230600 || Loss: 0.8855 || timer: 0.0887 sec.
iter 230610 || Loss: 0.8015 || timer: 0.0820 sec.
iter 230620 || Loss: 1.0828 || timer: 0.0813 sec.
iter 230630 || Loss: 1.0329 || timer: 0.0902 sec.
iter 230640 || Loss: 0.7157 || timer: 0.0879 sec.
iter 230650 || Loss: 0.6914 || timer: 0.0890 sec.
iter 230660 || Loss: 0.9902 || timer: 0.0947 sec.
iter 230670 || Loss: 1.3476 || timer: 0.0896 sec.
iter 230680 || Loss: 0.7335 || timer: 0.1018 sec.
iter 230690 || Loss: 0.7542 || timer: 0.0904 sec.
iter 230700 || Loss: 1.2207 || timer: 0.0905 sec.
iter 230710 || Loss: 1.0762 || timer: 0.1239 sec.
iter 230720 || Loss: 1.0874 || timer: 0.0821 sec.
iter 230730 || Loss: 1.0538 || timer: 0.0812 sec.
iter 230740 || Loss: 1.5888 || timer: 0.1143 sec.
iter 230750 || Loss: 0.9194 || timer: 0.0814 sec.
iter 230760 || Loss: 0.9850 || timer: 0.1122 sec.
iter 230770 || Loss: 0.8516 || timer: 0.0817 sec.
iter 230780 || Loss: 0.9917 || timer: 0.1013 sec.
iter 230790 || Loss: 0.8935 || timer: 0.0879 sec.
iter 230800 || Loss: 1.2253 || timer: 0.0884 sec.
iter 230810 || Loss: 0.8369 || timer: 0.0921 sec.
iter 230820 || Loss: 1.0622 || timer: 0.0894 sec.
iter 230830 || Loss: 1.1421 || timer: 0.0917 sec.
iter 230840 || Loss: 0.9959 || timer: 0.0817 sec.
iter 230850 || Loss: 0.9003 || timer: 0.0815 sec.
iter 230860 || Loss: 0.8466 || timer: 0.0857 sec.
iter 230870 || Loss: 0.9918 || timer: 0.0886 sec.
iter 230880 || Loss: 0.9013 || timer: 0.0820 sec.
iter 230890 || Loss: 1.0043 || timer: 0.0164 sec.
iter 230900 || Loss: 0.3546 || timer: 0.0901 sec.
iter 230910 || Loss: 0.7746 || timer: 0.0824 sec.
iter 230920 || Loss: 0.8266 || timer: 0.0819 sec.
iter 230930 || Loss: 0.9567 || timer: 0.0877 sec.
iter 230940 || Loss: 0.6685 || timer: 0.0891 sec.
iter 230950 || Loss: 0.9485 || timer: 0.0809 sec.
iter 230960 || Loss: 1.0245 || timer: 0.0815 sec.
iter 230970 || Loss: 0.9367 || timer: 0.1326 sec.
iter 230980 || Loss: 0.9487 || timer: 0.1018 sec.
iter 230990 || Loss: 0.9958 || timer: 0.0955 sec.
iter 231000 || Loss: 1.0660 || timer: 0.0812 sec.
iter 231010 || Loss: 0.5896 || timer: 0.0884 sec.
iter 231020 || Loss: 0.8700 || timer: 0.0898 sec.
iter 231030 || Loss: 0.9562 || timer: 0.0987 sec.
iter 231040 || Loss: 0.6179 || timer: 0.0927 sec.
iter 231050 || Loss: 0.8468 || timer: 0.0824 sec.
iter 231060 || Loss: 0.7131 || timer: 0.0967 sec.
iter 231070 || Loss: 0.8142 || timer: 0.0987 sec.
iter 231080 || Loss: 1.0656 || timer: 0.0879 sec.
iter 231090 || Loss: 0.9706 || timer: 0.0870 sec.
iter 231100 || Loss: 1.0319 || timer: 0.0902 sec.
iter 231110 || Loss: 1.1460 || timer: 0.0870 sec.
iter 231120 || Loss: 1.1069 || timer: 0.0837 sec.
iter 231130 || Loss: 0.9302 || timer: 0.0921 sec.
iter 231140 || Loss: 0.9730 || timer: 0.1030 sec.
iter 231150 || Loss: 0.9043 || timer: 0.0824 sec.
iter 231160 || Loss: 0.8851 || timer: 0.0886 sec.
iter 231170 || Loss: 0.9141 || timer: 0.1015 sec.
iter 231180 || Loss: 0.6820 || timer: 0.1376 sec.
iter 231190 || Loss: 0.8048 || timer: 0.1146 sec.
iter 231200 || Loss: 0.7268 || timer: 0.1244 sec.
iter 231210 || Loss: 0.7449 || timer: 0.0892 sec.
iter 231220 || Loss: 1.0893 || timer: 0.0202 sec.
iter 231230 || Loss: 0.4972 || timer: 0.0820 sec.
iter 231240 || Loss: 0.8075 || timer: 0.0872 sec.
iter 231250 || Loss: 0.9529 || timer: 0.0869 sec.
iter 231260 || Loss: 0.9645 || timer: 0.0885 sec.
iter 231270 || Loss: 0.7283 || timer: 0.0867 sec.
iter 231280 || Loss: 0.6179 || timer: 0.1151 sec.
iter 231290 || Loss: 0.9089 || timer: 0.0819 sec.
iter 231300 || Loss: 1.2346 || timer: 0.0886 sec.
iter 231310 || Loss: 0.9577 || timer: 0.0905 sec.
iter 231320 || Loss: 0.7882 || timer: 0.0962 sec.
iter 231330 || Loss: 0.6828 || timer: 0.0896 sec.
iter 231340 || Loss: 1.0243 || timer: 0.0899 sec.
iter 231350 || Loss: 1.2091 || timer: 0.0875 sec.
iter 231360 || Loss: 0.8862 || timer: 0.0862 sec.
iter 231370 || Loss: 0.8405 || timer: 0.0899 sec.
iter 231380 || Loss: 0.7990 || timer: 0.0850 sec.
iter 231390 || Loss: 1.1183 || timer: 0.0865 sec.
iter 231400 || Loss: 0.7742 || timer: 0.0905 sec.
iter 231410 || Loss: 0.9555 || timer: 0.0947 sec.
iter 231420 || Loss: 0.8000 || timer: 0.0811 sec.
iter 231430 || Loss: 1.0451 || timer: 0.0907 sec.
iter 231440 || Loss: 1.4729 || timer: 0.0885 sec.
iter 231450 || Loss: 0.8612 || timer: 0.0810 sec.
iter 231460 || Loss: 0.6959 || timer: 0.0985 sec.
iter 231470 || Loss: 1.1388 || timer: 0.0895 sec.
iter 231480 || Loss: 0.6963 || timer: 0.0921 sec.
iter 231490 || Loss: 0.7865 || timer: 0.0917 sec.
iter 231500 || Loss: 1.1381 || timer: 0.0836 sec.
iter 231510 || Loss: 0.8882 || timer: 0.1292 sec.
iter 231520 || Loss: 0.8210 || timer: 0.0935 sec.
iter 231530 || Loss: 1.1137 || timer: 0.0927 sec.
iter 231540 || Loss: 0.8683 || timer: 0.0941 sec.
iter 231550 || Loss: 1.1599 || timer: 0.0213 sec.
iter 231560 || Loss: 0.6744 || timer: 0.0949 sec.
iter 231570 || Loss: 0.9173 || timer: 0.0941 sec.
iter 231580 || Loss: 0.8545 || timer: 0.1047 sec.
iter 231590 || Loss: 1.0291 || timer: 0.0897 sec.
iter 231600 || Loss: 0.7790 || timer: 0.0927 sec.
iter 231610 || Loss: 0.9540 || timer: 0.0836 sec.
iter 231620 || Loss: 0.7448 || timer: 0.0896 sec.
iter 231630 || Loss: 0.8530 || timer: 0.0827 sec.
iter 231640 || Loss: 0.7851 || timer: 0.1077 sec.
iter 231650 || Loss: 0.7631 || timer: 0.1006 sec.
iter 231660 || Loss: 0.9211 || timer: 0.0847 sec.
iter 231670 || Loss: 0.8517 || timer: 0.0827 sec.
iter 231680 || Loss: 0.8954 || timer: 0.0855 sec.
iter 231690 || Loss: 0.6776 || timer: 0.1036 sec.
iter 231700 || Loss: 0.5087 || timer: 0.0911 sec.
iter 231710 || Loss: 0.8658 || timer: 0.0825 sec.
iter 231720 || Loss: 0.7812 || timer: 0.1023 sec.
iter 231730 || Loss: 0.9846 || timer: 0.1172 sec.
iter 231740 || Loss: 0.7616 || timer: 0.0845 sec.
iter 231750 || Loss: 0.8468 || timer: 0.0963 sec.
iter 231760 || Loss: 1.4029 || timer: 0.0947 sec.
iter 231770 || Loss: 0.6357 || timer: 0.0916 sec.
iter 231780 || Loss: 0.9095 || timer: 0.0936 sec.
iter 231790 || Loss: 0.8160 || timer: 0.0919 sec.
iter 231800 || Loss: 0.9198 || timer: 0.0936 sec.
iter 231810 || Loss: 1.0833 || timer: 0.0915 sec.
iter 231820 || Loss: 1.0034 || timer: 0.0921 sec.
iter 231830 || Loss: 1.1559 || timer: 0.1020 sec.
iter 231840 || Loss: 0.9429 || timer: 0.1019 sec.
iter 231850 || Loss: 0.9190 || timer: 0.0849 sec.
iter 231860 || Loss: 0.7032 || timer: 0.1050 sec.
iter 231870 || Loss: 0.8914 || timer: 0.0893 sec.
iter 231880 || Loss: 0.7931 || timer: 0.0263 sec.
iter 231890 || Loss: 0.7256 || timer: 0.0966 sec.
iter 231900 || Loss: 0.8322 || timer: 0.0874 sec.
iter 231910 || Loss: 1.0243 || timer: 0.0906 sec.
iter 231920 || Loss: 0.5869 || timer: 0.0850 sec.
iter 231930 || Loss: 0.9578 || timer: 0.0912 sec.
iter 231940 || Loss: 0.8506 || timer: 0.1250 sec.
iter 231950 || Loss: 1.2414 || timer: 0.0935 sec.
iter 231960 || Loss: 0.7123 || timer: 0.0921 sec.
iter 231970 || Loss: 1.1199 || timer: 0.0771 sec.
iter 231980 || Loss: 0.7408 || timer: 0.1097 sec.
iter 231990 || Loss: 0.6653 || timer: 0.1040 sec.
iter 232000 || Loss: 0.9104 || timer: 0.0892 sec.
iter 232010 || Loss: 1.1421 || timer: 0.1013 sec.
iter 232020 || Loss: 1.1272 || timer: 0.0881 sec.
iter 232030 || Loss: 0.9517 || timer: 0.1193 sec.
iter 232040 || Loss: 1.0064 || timer: 0.1020 sec.
iter 232050 || Loss: 0.8904 || timer: 0.0886 sec.
iter 232060 || Loss: 1.0979 || timer: 0.0893 sec.
iter 232070 || Loss: 0.9906 || timer: 0.0898 sec.
iter 232080 || Loss: 0.6482 || timer: 0.0867 sec.
iter 232090 || Loss: 0.8563 || timer: 0.0809 sec.
iter 232100 || Loss: 0.8668 || timer: 0.0910 sec.
iter 232110 || Loss: 1.1176 || timer: 0.0814 sec.
iter 232120 || Loss: 0.9367 || timer: 0.0810 sec.
iter 232130 || Loss: 0.8132 || timer: 0.0796 sec.
iter 232140 || Loss: 0.8117 || timer: 0.1112 sec.
iter 232150 || Loss: 0.6900 || timer: 0.0888 sec.
iter 232160 || Loss: 0.9926 || timer: 0.0810 sec.
iter 232170 || Loss: 1.0160 || timer: 0.0877 sec.
iter 232180 || Loss: 0.7574 || timer: 0.0898 sec.
iter 232190 || Loss: 1.1115 || timer: 0.0809 sec.
iter 232200 || Loss: 0.8035 || timer: 0.0966 sec.
iter 232210 || Loss: 0.9280 || timer: 0.0161 sec.
iter 232220 || Loss: 0.5745 || timer: 0.0808 sec.
iter 232230 || Loss: 0.7114 || timer: 0.0889 sec.
iter 232240 || Loss: 1.0157 || timer: 0.0823 sec.
iter 232250 || Loss: 0.6807 || timer: 0.0873 sec.
iter 232260 || Loss: 0.9975 || timer: 0.0898 sec.
iter 232270 || Loss: 0.9895 || timer: 0.0807 sec.
iter 232280 || Loss: 0.7393 || timer: 0.0835 sec.
iter 232290 || Loss: 0.6745 || timer: 0.0826 sec.
iter 232300 || Loss: 1.3749 || timer: 0.0965 sec.
iter 232310 || Loss: 0.8576 || timer: 0.0979 sec.
iter 232320 || Loss: 0.7863 || timer: 0.0878 sec.
iter 232330 || Loss: 0.7833 || timer: 0.0823 sec.
iter 232340 || Loss: 0.8595 || timer: 0.0842 sec.
iter 232350 || Loss: 0.7925 || timer: 0.0844 sec.
iter 232360 || Loss: 0.9663 || timer: 0.0865 sec.
iter 232370 || Loss: 0.8054 || timer: 0.0887 sec.
iter 232380 || Loss: 0.5822 || timer: 0.0857 sec.
iter 232390 || Loss: 0.6824 || timer: 0.0878 sec.
iter 232400 || Loss: 0.7981 || timer: 0.0986 sec.
iter 232410 || Loss: 0.6036 || timer: 0.0901 sec.
iter 232420 || Loss: 0.7457 || timer: 0.1058 sec.
iter 232430 || Loss: 0.7222 || timer: 0.0856 sec.
iter 232440 || Loss: 1.0006 || timer: 0.0898 sec.
iter 232450 || Loss: 0.6865 || timer: 0.1013 sec.
iter 232460 || Loss: 0.9285 || timer: 0.0879 sec.
iter 232470 || Loss: 0.9483 || timer: 0.0901 sec.
iter 232480 || Loss: 1.4219 || timer: 0.0844 sec.
iter 232490 || Loss: 0.8779 || timer: 0.0917 sec.
iter 232500 || Loss: 1.0712 || timer: 0.1072 sec.
iter 232510 || Loss: 1.0276 || timer: 0.0902 sec.
iter 232520 || Loss: 0.7986 || timer: 0.0926 sec.
iter 232530 || Loss: 0.9209 || timer: 0.0899 sec.
iter 232540 || Loss: 0.8386 || timer: 0.0159 sec.
iter 232550 || Loss: 2.3332 || timer: 0.0820 sec.
iter 232560 || Loss: 0.9821 || timer: 0.0977 sec.
iter 232570 || Loss: 0.5389 || timer: 0.0898 sec.
iter 232580 || Loss: 0.8569 || timer: 0.0904 sec.
iter 232590 || Loss: 0.8431 || timer: 0.0877 sec.
iter 232600 || Loss: 0.8750 || timer: 0.1025 sec.
iter 232610 || Loss: 0.8753 || timer: 0.0952 sec.
iter 232620 || Loss: 0.8899 || timer: 0.0910 sec.
iter 232630 || Loss: 0.9286 || timer: 0.0810 sec.
iter 232640 || Loss: 1.1858 || timer: 0.1345 sec.
iter 232650 || Loss: 0.9201 || timer: 0.0857 sec.
iter 232660 || Loss: 0.9401 || timer: 0.0963 sec.
iter 232670 || Loss: 0.6699 || timer: 0.0810 sec.
iter 232680 || Loss: 0.8021 || timer: 0.0904 sec.
iter 232690 || Loss: 1.0550 || timer: 0.1006 sec.
iter 232700 || Loss: 0.6817 || timer: 0.0810 sec.
iter 232710 || Loss: 0.7159 || timer: 0.0815 sec.
iter 232720 || Loss: 0.8701 || timer: 0.1061 sec.
iter 232730 || Loss: 0.7356 || timer: 0.0818 sec.
iter 232740 || Loss: 1.0014 || timer: 0.0896 sec.
iter 232750 || Loss: 0.8927 || timer: 0.0918 sec.
iter 232760 || Loss: 1.0848 || timer: 0.0825 sec.
iter 232770 || Loss: 1.2771 || timer: 0.0911 sec.
iter 232780 || Loss: 1.0203 || timer: 0.0882 sec.
iter 232790 || Loss: 0.6658 || timer: 0.0812 sec.
iter 232800 || Loss: 0.6994 || timer: 0.1091 sec.
iter 232810 || Loss: 0.7725 || timer: 0.0905 sec.
iter 232820 || Loss: 0.7810 || timer: 0.0936 sec.
iter 232830 || Loss: 0.9334 || timer: 0.0821 sec.
iter 232840 || Loss: 0.8312 || timer: 0.0869 sec.
iter 232850 || Loss: 0.8263 || timer: 0.0766 sec.
iter 232860 || Loss: 0.4840 || timer: 0.0918 sec.
iter 232870 || Loss: 0.9141 || timer: 0.0161 sec.
iter 232880 || Loss: 0.2527 || timer: 0.0815 sec.
iter 232890 || Loss: 0.5345 || timer: 0.0808 sec.
iter 232900 || Loss: 0.9395 || timer: 0.0813 sec.
iter 232910 || Loss: 0.6652 || timer: 0.0907 sec.
iter 232920 || Loss: 0.9374 || timer: 0.0880 sec.
iter 232930 || Loss: 1.0042 || timer: 0.0840 sec.
iter 232940 || Loss: 0.9465 || timer: 0.0981 sec.
iter 232950 || Loss: 0.9399 || timer: 0.0816 sec.
iter 232960 || Loss: 0.9012 || timer: 0.1038 sec.
iter 232970 || Loss: 0.8408 || timer: 0.0943 sec.
iter 232980 || Loss: 0.7569 || timer: 0.0888 sec.
iter 232990 || Loss: 0.9418 || timer: 0.0878 sec.
iter 233000 || Loss: 0.6999 || timer: 0.0888 sec.
iter 233010 || Loss: 0.6988 || timer: 0.1023 sec.
iter 233020 || Loss: 0.8563 || timer: 0.0828 sec.
iter 233030 || Loss: 0.9248 || timer: 0.0858 sec.
iter 233040 || Loss: 0.9915 || timer: 0.0890 sec.
iter 233050 || Loss: 1.6280 || timer: 0.0904 sec.
iter 233060 || Loss: 0.7586 || timer: 0.0901 sec.
iter 233070 || Loss: 0.7028 || timer: 0.0912 sec.
iter 233080 || Loss: 1.0146 || timer: 0.0888 sec.
iter 233090 || Loss: 0.8352 || timer: 0.0892 sec.
iter 233100 || Loss: 0.8820 || timer: 0.0821 sec.
iter 233110 || Loss: 0.9230 || timer: 0.0815 sec.
iter 233120 || Loss: 1.0770 || timer: 0.0877 sec.
iter 233130 || Loss: 0.7342 || timer: 0.0825 sec.
iter 233140 || Loss: 0.8761 || timer: 0.1071 sec.
iter 233150 || Loss: 0.8398 || timer: 0.0860 sec.
iter 233160 || Loss: 0.9033 || timer: 0.0935 sec.
iter 233170 || Loss: 1.0815 || timer: 0.1069 sec.
iter 233180 || Loss: 0.7149 || timer: 0.0889 sec.
iter 233190 || Loss: 1.0825 || timer: 0.1032 sec.
iter 233200 || Loss: 0.7950 || timer: 0.0266 sec.
iter 233210 || Loss: 0.7843 || timer: 0.0820 sec.
iter 233220 || Loss: 0.8800 || timer: 0.0818 sec.
iter 233230 || Loss: 0.7859 || timer: 0.0886 sec.
iter 233240 || Loss: 0.8865 || timer: 0.0808 sec.
iter 233250 || Loss: 0.6613 || timer: 0.0803 sec.
iter 233260 || Loss: 1.0072 || timer: 0.0881 sec.
iter 233270 || Loss: 1.0762 || timer: 0.1064 sec.
iter 233280 || Loss: 0.9827 || timer: 0.1008 sec.
iter 233290 || Loss: 1.0354 || timer: 0.0908 sec.
iter 233300 || Loss: 0.8937 || timer: 0.0995 sec.
iter 233310 || Loss: 0.8878 || timer: 0.0859 sec.
iter 233320 || Loss: 1.1374 || timer: 0.1229 sec.
iter 233330 || Loss: 1.0565 || timer: 0.0825 sec.
iter 233340 || Loss: 0.9378 || timer: 0.0822 sec.
iter 233350 || Loss: 1.0037 || timer: 0.0749 sec.
iter 233360 || Loss: 0.8414 || timer: 0.1003 sec.
iter 233370 || Loss: 0.9455 || timer: 0.0962 sec.
iter 233380 || Loss: 1.0779 || timer: 0.0889 sec.
iter 233390 || Loss: 0.8169 || timer: 0.1011 sec.
iter 233400 || Loss: 0.9919 || timer: 0.0816 sec.
iter 233410 || Loss: 0.6830 || timer: 0.0912 sec.
iter 233420 || Loss: 0.9156 || timer: 0.0926 sec.
iter 233430 || Loss: 0.6418 || timer: 0.0811 sec.
iter 233440 || Loss: 0.9115 || timer: 0.0888 sec.
iter 233450 || Loss: 1.1678 || timer: 0.0859 sec.
iter 233460 || Loss: 0.8964 || timer: 0.0882 sec.
iter 233470 || Loss: 0.7047 || timer: 0.0811 sec.
iter 233480 || Loss: 0.6204 || timer: 0.1141 sec.
iter 233490 || Loss: 0.9539 || timer: 0.0969 sec.
iter 233500 || Loss: 1.2811 || timer: 0.0931 sec.
iter 233510 || Loss: 0.9650 || timer: 0.1197 sec.
iter 233520 || Loss: 1.2367 || timer: 0.0938 sec.
iter 233530 || Loss: 1.0370 || timer: 0.0176 sec.
iter 233540 || Loss: 0.8502 || timer: 0.0906 sec.
iter 233550 || Loss: 0.8141 || timer: 0.0909 sec.
iter 233560 || Loss: 0.7352 || timer: 0.0911 sec.
iter 233570 || Loss: 0.9509 || timer: 0.0900 sec.
iter 233580 || Loss: 0.7792 || timer: 0.0884 sec.
iter 233590 || Loss: 0.8183 || timer: 0.1000 sec.
iter 233600 || Loss: 0.8593 || timer: 0.0821 sec.
iter 233610 || Loss: 0.8785 || timer: 0.1272 sec.
iter 233620 || Loss: 1.0366 || timer: 0.1004 sec.
iter 233630 || Loss: 1.0370 || timer: 0.0968 sec.
iter 233640 || Loss: 0.8360 || timer: 0.0912 sec.
iter 233650 || Loss: 0.8242 || timer: 0.0914 sec.
iter 233660 || Loss: 1.0367 || timer: 0.0811 sec.
iter 233670 || Loss: 0.8078 || timer: 0.0820 sec.
iter 233680 || Loss: 1.0211 || timer: 0.0908 sec.
iter 233690 || Loss: 1.0104 || timer: 0.1062 sec.
iter 233700 || Loss: 1.0628 || timer: 0.0886 sec.
iter 233710 || Loss: 0.9133 || timer: 0.0867 sec.
iter 233720 || Loss: 0.6912 || timer: 0.0826 sec.
iter 233730 || Loss: 0.8628 || timer: 0.1013 sec.
iter 233740 || Loss: 0.7050 || timer: 0.0928 sec.
iter 233750 || Loss: 0.7897 || timer: 0.0919 sec.
iter 233760 || Loss: 0.8120 || timer: 0.0814 sec.
iter 233770 || Loss: 0.9728 || timer: 0.0913 sec.
iter 233780 || Loss: 0.8034 || timer: 0.0942 sec.
iter 233790 || Loss: 0.7908 || timer: 0.0901 sec.
iter 233800 || Loss: 1.3237 || timer: 0.0811 sec.
iter 233810 || Loss: 0.8134 || timer: 0.0865 sec.
iter 233820 || Loss: 0.8590 || timer: 0.1072 sec.
iter 233830 || Loss: 0.9037 || timer: 0.1007 sec.
iter 233840 || Loss: 0.7144 || timer: 0.0899 sec.
iter 233850 || Loss: 0.7512 || timer: 0.0823 sec.
iter 233860 || Loss: 0.7027 || timer: 0.0233 sec.
iter 233870 || Loss: 0.3968 || timer: 0.1264 sec.
iter 233880 || Loss: 1.1130 || timer: 0.0808 sec.
iter 233890 || Loss: 0.7665 || timer: 0.1037 sec.
iter 233900 || Loss: 0.8501 || timer: 0.0886 sec.
iter 233910 || Loss: 0.8891 || timer: 0.0869 sec.
iter 233920 || Loss: 0.7116 || timer: 0.0809 sec.
iter 233930 || Loss: 0.7600 || timer: 0.0909 sec.
iter 233940 || Loss: 0.8445 || timer: 0.0853 sec.
iter 233950 || Loss: 0.7623 || timer: 0.1032 sec.
iter 233960 || Loss: 0.7423 || timer: 0.1114 sec.
iter 233970 || Loss: 0.7910 || timer: 0.0886 sec.
iter 233980 || Loss: 0.7829 || timer: 0.0803 sec.
iter 233990 || Loss: 0.7520 || timer: 0.0894 sec.
iter 234000 || Loss: 0.9790 || timer: 0.0836 sec.
iter 234010 || Loss: 1.0477 || timer: 0.1042 sec.
iter 234020 || Loss: 1.1637 || timer: 0.0894 sec.
iter 234030 || Loss: 1.1828 || timer: 0.0817 sec.
iter 234040 || Loss: 0.7918 || timer: 0.0912 sec.
iter 234050 || Loss: 0.6415 || timer: 0.0897 sec.
iter 234060 || Loss: 0.7264 || timer: 0.0911 sec.
iter 234070 || Loss: 0.9307 || timer: 0.0819 sec.
iter 234080 || Loss: 0.6914 || timer: 0.0874 sec.
iter 234090 || Loss: 0.8028 || timer: 0.0817 sec.
iter 234100 || Loss: 0.4860 || timer: 0.0935 sec.
iter 234110 || Loss: 0.6711 || timer: 0.0810 sec.
iter 234120 || Loss: 0.6875 || timer: 0.0903 sec.
iter 234130 || Loss: 0.7706 || timer: 0.0821 sec.
iter 234140 || Loss: 0.5569 || timer: 0.1019 sec.
iter 234150 || Loss: 0.8174 || timer: 0.0903 sec.
iter 234160 || Loss: 1.0044 || timer: 0.1061 sec.
iter 234170 || Loss: 0.9239 || timer: 0.0851 sec.
iter 234180 || Loss: 0.7078 || timer: 0.0878 sec.
iter 234190 || Loss: 0.8220 || timer: 0.0161 sec.
iter 234200 || Loss: 0.3966 || timer: 0.0811 sec.
iter 234210 || Loss: 0.9897 || timer: 0.1019 sec.
iter 234220 || Loss: 0.7537 || timer: 0.0902 sec.
iter 234230 || Loss: 0.7773 || timer: 0.0904 sec.
iter 234240 || Loss: 2.3793 || timer: 0.0880 sec.
iter 234250 || Loss: 1.0979 || timer: 0.0868 sec.
iter 234260 || Loss: 1.1345 || timer: 0.0871 sec.
iter 234270 || Loss: 1.2419 || timer: 0.0827 sec.
iter 234280 || Loss: 0.8912 || timer: 0.1091 sec.
iter 234290 || Loss: 0.9429 || timer: 0.1281 sec.
iter 234300 || Loss: 1.0037 || timer: 0.0849 sec.
iter 234310 || Loss: 0.6830 || timer: 0.0863 sec.
iter 234320 || Loss: 1.0096 || timer: 0.0906 sec.
iter 234330 || Loss: 1.0944 || timer: 0.0928 sec.
iter 234340 || Loss: 1.1703 || timer: 0.0949 sec.
iter 234350 || Loss: 0.9372 || timer: 0.0854 sec.
iter 234360 || Loss: 0.8157 || timer: 0.0918 sec.
iter 234370 || Loss: 1.1955 || timer: 0.1025 sec.
iter 234380 || Loss: 1.9806 || timer: 0.0814 sec.
iter 234390 || Loss: 0.6950 || timer: 0.0779 sec.
iter 234400 || Loss: 1.0101 || timer: 0.0864 sec.
iter 234410 || Loss: 0.9702 || timer: 0.0898 sec.
iter 234420 || Loss: 1.0177 || timer: 0.0887 sec.
iter 234430 || Loss: 0.8726 || timer: 0.0812 sec.
iter 234440 || Loss: 0.9468 || timer: 0.0831 sec.
iter 234450 || Loss: 0.8883 || timer: 0.0818 sec.
iter 234460 || Loss: 0.9600 || timer: 0.0906 sec.
iter 234470 || Loss: 0.5056 || timer: 0.0891 sec.
iter 234480 || Loss: 1.0005 || timer: 0.0890 sec.
iter 234490 || Loss: 0.6981 || timer: 0.0868 sec.
iter 234500 || Loss: 1.1845 || timer: 0.0875 sec.
iter 234510 || Loss: 0.7689 || timer: 0.0891 sec.
iter 234520 || Loss: 0.7241 || timer: 0.0228 sec.
iter 234530 || Loss: 1.1518 || timer: 0.1037 sec.
iter 234540 || Loss: 0.8539 || timer: 0.0843 sec.
iter 234550 || Loss: 1.4011 || timer: 0.0880 sec.
iter 234560 || Loss: 0.9939 || timer: 0.0902 sec.
iter 234570 || Loss: 0.9005 || timer: 0.0890 sec.
iter 234580 || Loss: 1.1610 || timer: 0.0893 sec.
iter 234590 || Loss: 0.7361 || timer: 0.0908 sec.
iter 234600 || Loss: 0.8855 || timer: 0.0889 sec.
iter 234610 || Loss: 1.2463 || timer: 0.0792 sec.
iter 234620 || Loss: 0.8476 || timer: 0.1238 sec.
iter 234630 || Loss: 0.9496 || timer: 0.0894 sec.
iter 234640 || Loss: 1.1644 || timer: 0.0903 sec.
iter 234650 || Loss: 0.9726 || timer: 0.0887 sec.
iter 234660 || Loss: 0.8134 || timer: 0.0820 sec.
iter 234670 || Loss: 0.7415 || timer: 0.0883 sec.
iter 234680 || Loss: 0.9892 || timer: 0.0818 sec.
iter 234690 || Loss: 0.8822 || timer: 0.1290 sec.
iter 234700 || Loss: 0.9117 || timer: 0.0898 sec.
iter 234710 || Loss: 0.8666 || timer: 0.0856 sec.
iter 234720 || Loss: 0.7858 || timer: 0.0901 sec.
iter 234730 || Loss: 1.0232 || timer: 0.0869 sec.
iter 234740 || Loss: 0.9366 || timer: 0.1150 sec.
iter 234750 || Loss: 1.3965 || timer: 0.0884 sec.
iter 234760 || Loss: 0.9506 || timer: 0.0813 sec.
iter 234770 || Loss: 0.9400 || timer: 0.0800 sec.
iter 234780 || Loss: 1.0938 || timer: 0.0891 sec.
iter 234790 || Loss: 1.2908 || timer: 0.0845 sec.
iter 234800 || Loss: 0.7879 || timer: 0.0904 sec.
iter 234810 || Loss: 1.1156 || timer: 0.0886 sec.
iter 234820 || Loss: 1.1383 || timer: 0.0816 sec.
iter 234830 || Loss: 0.6877 || timer: 0.0864 sec.
iter 234840 || Loss: 0.9004 || timer: 0.0818 sec.
iter 234850 || Loss: 0.9763 || timer: 0.0256 sec.
iter 234860 || Loss: 0.3511 || timer: 0.0940 sec.
iter 234870 || Loss: 1.2431 || timer: 0.0826 sec.
iter 234880 || Loss: 0.5921 || timer: 0.1135 sec.
iter 234890 || Loss: 0.8147 || timer: 0.0885 sec.
iter 234900 || Loss: 1.0510 || timer: 0.1127 sec.
iter 234910 || Loss: 0.9195 || timer: 0.0992 sec.
iter 234920 || Loss: 0.8248 || timer: 0.1059 sec.
iter 234930 || Loss: 0.8013 || timer: 0.0979 sec.
iter 234940 || Loss: 1.0135 || timer: 0.0865 sec.
iter 234950 || Loss: 1.0136 || timer: 0.0992 sec.
iter 234960 || Loss: 0.7390 || timer: 0.1200 sec.
iter 234970 || Loss: 0.9338 || timer: 0.0909 sec.
iter 234980 || Loss: 0.9133 || timer: 0.0895 sec.
iter 234990 || Loss: 1.0726 || timer: 0.0906 sec.
iter 235000 || Loss: 1.0160 || Saving state, iter: 235000
timer: 0.1105 sec.
iter 235010 || Loss: 0.8044 || timer: 0.0897 sec.
iter 235020 || Loss: 0.5597 || timer: 0.0923 sec.
iter 235030 || Loss: 0.6783 || timer: 0.0909 sec.
iter 235040 || Loss: 1.1439 || timer: 0.0886 sec.
iter 235050 || Loss: 0.9144 || timer: 0.0828 sec.
iter 235060 || Loss: 1.1040 || timer: 0.0891 sec.
iter 235070 || Loss: 0.8554 || timer: 0.0906 sec.
iter 235080 || Loss: 0.8559 || timer: 0.0898 sec.
iter 235090 || Loss: 0.9789 || timer: 0.0900 sec.
iter 235100 || Loss: 0.9554 || timer: 0.0902 sec.
iter 235110 || Loss: 0.6825 || timer: 0.0933 sec.
iter 235120 || Loss: 1.2928 || timer: 0.0929 sec.
iter 235130 || Loss: 0.8823 || timer: 0.0861 sec.
iter 235140 || Loss: 0.7634 || timer: 0.0913 sec.
iter 235150 || Loss: 0.6929 || timer: 0.0903 sec.
iter 235160 || Loss: 0.6102 || timer: 0.0883 sec.
iter 235170 || Loss: 0.6968 || timer: 0.0821 sec.
iter 235180 || Loss: 0.8297 || timer: 0.0161 sec.
iter 235190 || Loss: 0.3885 || timer: 0.0856 sec.
iter 235200 || Loss: 0.7769 || timer: 0.0885 sec.
iter 235210 || Loss: 0.9923 || timer: 0.0900 sec.
iter 235220 || Loss: 0.7368 || timer: 0.0901 sec.
iter 235230 || Loss: 0.9822 || timer: 0.0898 sec.
iter 235240 || Loss: 0.6959 || timer: 0.0924 sec.
iter 235250 || Loss: 0.9810 || timer: 0.1078 sec.
iter 235260 || Loss: 1.0337 || timer: 0.0905 sec.
iter 235270 || Loss: 1.2449 || timer: 0.0828 sec.
iter 235280 || Loss: 0.8937 || timer: 0.1100 sec.
iter 235290 || Loss: 0.8954 || timer: 0.0816 sec.
iter 235300 || Loss: 0.8783 || timer: 0.0925 sec.
iter 235310 || Loss: 0.8687 || timer: 0.0868 sec.
iter 235320 || Loss: 0.7519 || timer: 0.0899 sec.
iter 235330 || Loss: 0.9680 || timer: 0.0877 sec.
iter 235340 || Loss: 1.0855 || timer: 0.0922 sec.
iter 235350 || Loss: 1.0483 || timer: 0.0882 sec.
iter 235360 || Loss: 1.0870 || timer: 0.0807 sec.
iter 235370 || Loss: 0.9642 || timer: 0.0910 sec.
iter 235380 || Loss: 0.9070 || timer: 0.0807 sec.
iter 235390 || Loss: 0.8841 || timer: 0.1092 sec.
iter 235400 || Loss: 0.9967 || timer: 0.0873 sec.
iter 235410 || Loss: 0.8633 || timer: 0.0810 sec.
iter 235420 || Loss: 0.5392 || timer: 0.1074 sec.
iter 235430 || Loss: 1.4816 || timer: 0.0883 sec.
iter 235440 || Loss: 0.7857 || timer: 0.0790 sec.
iter 235450 || Loss: 1.0408 || timer: 0.0896 sec.
iter 235460 || Loss: 0.9510 || timer: 0.1061 sec.
iter 235470 || Loss: 0.8982 || timer: 0.0856 sec.
iter 235480 || Loss: 0.8819 || timer: 0.0891 sec.
iter 235490 || Loss: 0.8142 || timer: 0.0877 sec.
iter 235500 || Loss: 1.1036 || timer: 0.1058 sec.
iter 235510 || Loss: 1.1199 || timer: 0.0244 sec.
iter 235520 || Loss: 1.0710 || timer: 0.0976 sec.
iter 235530 || Loss: 0.9439 || timer: 0.0899 sec.
iter 235540 || Loss: 0.8432 || timer: 0.0823 sec.
iter 235550 || Loss: 0.5595 || timer: 0.0858 sec.
iter 235560 || Loss: 1.1664 || timer: 0.0818 sec.
iter 235570 || Loss: 0.7653 || timer: 0.0881 sec.
iter 235580 || Loss: 0.8931 || timer: 0.0910 sec.
iter 235590 || Loss: 0.7341 || timer: 0.0904 sec.
iter 235600 || Loss: 0.8764 || timer: 0.0908 sec.
iter 235610 || Loss: 0.6523 || timer: 0.0977 sec.
iter 235620 || Loss: 0.7328 || timer: 0.0814 sec.
iter 235630 || Loss: 1.2800 || timer: 0.0876 sec.
iter 235640 || Loss: 0.9516 || timer: 0.0906 sec.
iter 235650 || Loss: 0.9077 || timer: 0.1042 sec.
iter 235660 || Loss: 0.8458 || timer: 0.0881 sec.
iter 235670 || Loss: 1.1007 || timer: 0.0867 sec.
iter 235680 || Loss: 1.0674 || timer: 0.1086 sec.
iter 235690 || Loss: 0.7996 || timer: 0.0864 sec.
iter 235700 || Loss: 0.9322 || timer: 0.0859 sec.
iter 235710 || Loss: 0.7827 || timer: 0.0935 sec.
iter 235720 || Loss: 0.9588 || timer: 0.0901 sec.
iter 235730 || Loss: 1.0674 || timer: 0.0881 sec.
iter 235740 || Loss: 1.2776 || timer: 0.0904 sec.
iter 235750 || Loss: 0.8779 || timer: 0.0896 sec.
iter 235760 || Loss: 0.8895 || timer: 0.0880 sec.
iter 235770 || Loss: 0.7462 || timer: 0.0890 sec.
iter 235780 || Loss: 0.9289 || timer: 0.0865 sec.
iter 235790 || Loss: 0.8098 || timer: 0.0851 sec.
iter 235800 || Loss: 0.7751 || timer: 0.0916 sec.
iter 235810 || Loss: 0.9909 || timer: 0.0854 sec.
iter 235820 || Loss: 0.7558 || timer: 0.1224 sec.
iter 235830 || Loss: 0.8846 || timer: 0.0899 sec.
iter 235840 || Loss: 1.1282 || timer: 0.0190 sec.
iter 235850 || Loss: 0.8593 || timer: 0.1072 sec.
iter 235860 || Loss: 1.1732 || timer: 0.1010 sec.
iter 235870 || Loss: 1.0584 || timer: 0.1099 sec.
iter 235880 || Loss: 0.7603 || timer: 0.0918 sec.
iter 235890 || Loss: 0.7839 || timer: 0.0842 sec.
iter 235900 || Loss: 0.7083 || timer: 0.1034 sec.
iter 235910 || Loss: 0.8659 || timer: 0.0903 sec.
iter 235920 || Loss: 0.9434 || timer: 0.0921 sec.
iter 235930 || Loss: 0.9976 || timer: 0.0853 sec.
iter 235940 || Loss: 0.9809 || timer: 0.1206 sec.
iter 235950 || Loss: 0.8252 || timer: 0.0925 sec.
iter 235960 || Loss: 0.5321 || timer: 0.0897 sec.
iter 235970 || Loss: 0.6066 || timer: 0.0911 sec.
iter 235980 || Loss: 0.8688 || timer: 0.0822 sec.
iter 235990 || Loss: 1.0636 || timer: 0.0923 sec.
iter 236000 || Loss: 0.7854 || timer: 0.0876 sec.
iter 236010 || Loss: 0.8443 || timer: 0.0887 sec.
iter 236020 || Loss: 1.0576 || timer: 0.0829 sec.
iter 236030 || Loss: 1.0339 || timer: 0.0886 sec.
iter 236040 || Loss: 0.8797 || timer: 0.0828 sec.
iter 236050 || Loss: 1.0351 || timer: 0.0884 sec.
iter 236060 || Loss: 0.6954 || timer: 0.0852 sec.
iter 236070 || Loss: 1.0872 || timer: 0.0825 sec.
iter 236080 || Loss: 0.8578 || timer: 0.0886 sec.
iter 236090 || Loss: 0.9151 || timer: 0.0901 sec.
iter 236100 || Loss: 0.8511 || timer: 0.0829 sec.
iter 236110 || Loss: 1.2960 || timer: 0.0912 sec.
iter 236120 || Loss: 1.0527 || timer: 0.0903 sec.
iter 236130 || Loss: 0.7964 || timer: 0.0805 sec.
iter 236140 || Loss: 0.6950 || timer: 0.1133 sec.
iter 236150 || Loss: 0.8930 || timer: 0.0869 sec.
iter 236160 || Loss: 0.9954 || timer: 0.0820 sec.
iter 236170 || Loss: 0.8187 || timer: 0.0237 sec.
iter 236180 || Loss: 2.2948 || timer: 0.0815 sec.
iter 236190 || Loss: 0.6559 || timer: 0.0889 sec.
iter 236200 || Loss: 1.2387 || timer: 0.0827 sec.
iter 236210 || Loss: 1.1078 || timer: 0.0825 sec.
iter 236220 || Loss: 1.0203 || timer: 0.0874 sec.
iter 236230 || Loss: 0.7764 || timer: 0.0967 sec.
iter 236240 || Loss: 1.0464 || timer: 0.0895 sec.
iter 236250 || Loss: 1.0947 || timer: 0.0817 sec.
iter 236260 || Loss: 0.6345 || timer: 0.0918 sec.
iter 236270 || Loss: 0.9133 || timer: 0.1310 sec.
iter 236280 || Loss: 0.8124 || timer: 0.0909 sec.
iter 236290 || Loss: 1.0748 || timer: 0.0895 sec.
iter 236300 || Loss: 0.6848 || timer: 0.0883 sec.
iter 236310 || Loss: 1.1629 || timer: 0.0907 sec.
iter 236320 || Loss: 1.0776 || timer: 0.0846 sec.
iter 236330 || Loss: 0.7965 || timer: 0.0809 sec.
iter 236340 || Loss: 0.9097 || timer: 0.0917 sec.
iter 236350 || Loss: 1.0021 || timer: 0.0889 sec.
iter 236360 || Loss: 0.6443 || timer: 0.0896 sec.
iter 236370 || Loss: 0.8117 || timer: 0.0842 sec.
iter 236380 || Loss: 0.9271 || timer: 0.0815 sec.
iter 236390 || Loss: 0.8332 || timer: 0.0882 sec.
iter 236400 || Loss: 0.9087 || timer: 0.0881 sec.
iter 236410 || Loss: 0.7902 || timer: 0.0813 sec.
iter 236420 || Loss: 1.0454 || timer: 0.0890 sec.
iter 236430 || Loss: 0.8564 || timer: 0.0880 sec.
iter 236440 || Loss: 0.7481 || timer: 0.1073 sec.
iter 236450 || Loss: 0.8882 || timer: 0.0811 sec.
iter 236460 || Loss: 0.6645 || timer: 0.0887 sec.
iter 236470 || Loss: 0.9375 || timer: 0.0900 sec.
iter 236480 || Loss: 1.0643 || timer: 0.0904 sec.
iter 236490 || Loss: 0.7272 || timer: 0.0874 sec.
iter 236500 || Loss: 0.8223 || timer: 0.0170 sec.
iter 236510 || Loss: 2.4163 || timer: 0.0873 sec.
iter 236520 || Loss: 1.4417 || timer: 0.0864 sec.
iter 236530 || Loss: 1.0106 || timer: 0.0879 sec.
iter 236540 || Loss: 2.0725 || timer: 0.1177 sec.
iter 236550 || Loss: 1.3993 || timer: 0.0921 sec.
iter 236560 || Loss: 0.8635 || timer: 0.0889 sec.
iter 236570 || Loss: 0.8249 || timer: 0.0900 sec.
iter 236580 || Loss: 0.9584 || timer: 0.0896 sec.
iter 236590 || Loss: 0.8523 || timer: 0.0889 sec.
iter 236600 || Loss: 0.7687 || timer: 0.1297 sec.
iter 236610 || Loss: 0.5878 || timer: 0.0897 sec.
iter 236620 || Loss: 1.0309 || timer: 0.0900 sec.
iter 236630 || Loss: 0.8176 || timer: 0.0827 sec.
iter 236640 || Loss: 0.9215 || timer: 0.0932 sec.
iter 236650 || Loss: 0.9107 || timer: 0.0853 sec.
iter 236660 || Loss: 0.9210 || timer: 0.0983 sec.
iter 236670 || Loss: 0.7366 || timer: 0.1052 sec.
iter 236680 || Loss: 1.2101 || timer: 0.0879 sec.
iter 236690 || Loss: 0.9096 || timer: 0.0968 sec.
iter 236700 || Loss: 0.7299 || timer: 0.0824 sec.
iter 236710 || Loss: 0.9223 || timer: 0.0900 sec.
iter 236720 || Loss: 0.9001 || timer: 0.0927 sec.
iter 236730 || Loss: 1.0405 || timer: 0.0903 sec.
iter 236740 || Loss: 0.8437 || timer: 0.0816 sec.
iter 236750 || Loss: 0.9522 || timer: 0.0809 sec.
iter 236760 || Loss: 0.5770 || timer: 0.0910 sec.
iter 236770 || Loss: 0.9280 || timer: 0.0833 sec.
iter 236780 || Loss: 1.0382 || timer: 0.0886 sec.
iter 236790 || Loss: 0.9684 || timer: 0.1039 sec.
iter 236800 || Loss: 0.7563 || timer: 0.0818 sec.
iter 236810 || Loss: 1.0085 || timer: 0.1060 sec.
iter 236820 || Loss: 0.8603 || timer: 0.0882 sec.
iter 236830 || Loss: 1.0081 || timer: 0.0154 sec.
iter 236840 || Loss: 1.7153 || timer: 0.1032 sec.
iter 236850 || Loss: 0.8517 || timer: 0.0887 sec.
iter 236860 || Loss: 0.6279 || timer: 0.0828 sec.
iter 236870 || Loss: 0.9278 || timer: 0.1055 sec.
iter 236880 || Loss: 0.6469 || timer: 0.0835 sec.
iter 236890 || Loss: 0.8072 || timer: 0.0948 sec.
iter 236900 || Loss: 0.6935 || timer: 0.0971 sec.
iter 236910 || Loss: 0.8084 || timer: 0.1187 sec.
iter 236920 || Loss: 0.7940 || timer: 0.0819 sec.
iter 236930 || Loss: 0.6457 || timer: 0.1206 sec.
iter 236940 || Loss: 0.8688 || timer: 0.0812 sec.
iter 236950 || Loss: 0.8176 || timer: 0.0900 sec.
iter 236960 || Loss: 0.7670 || timer: 0.0899 sec.
iter 236970 || Loss: 0.7704 || timer: 0.0976 sec.
iter 236980 || Loss: 1.1595 || timer: 0.0824 sec.
iter 236990 || Loss: 1.2579 || timer: 0.0945 sec.
iter 237000 || Loss: 1.3533 || timer: 0.0825 sec.
iter 237010 || Loss: 1.0950 || timer: 0.0924 sec.
iter 237020 || Loss: 0.6339 || timer: 0.0907 sec.
iter 237030 || Loss: 1.0197 || timer: 0.0882 sec.
iter 237040 || Loss: 0.6891 || timer: 0.0898 sec.
iter 237050 || Loss: 0.8721 || timer: 0.0903 sec.
iter 237060 || Loss: 0.8351 || timer: 0.0891 sec.
iter 237070 || Loss: 0.8348 || timer: 0.0901 sec.
iter 237080 || Loss: 0.8070 || timer: 0.0826 sec.
iter 237090 || Loss: 1.0191 || timer: 0.0982 sec.
iter 237100 || Loss: 1.7785 || timer: 0.0985 sec.
iter 237110 || Loss: 1.2701 || timer: 0.0912 sec.
iter 237120 || Loss: 0.9967 || timer: 0.0900 sec.
iter 237130 || Loss: 0.8892 || timer: 0.0844 sec.
iter 237140 || Loss: 0.8341 || timer: 0.1081 sec.
iter 237150 || Loss: 1.1197 || timer: 0.0982 sec.
iter 237160 || Loss: 1.2203 || timer: 0.0164 sec.
iter 237170 || Loss: 0.8419 || timer: 0.0814 sec.
iter 237180 || Loss: 1.0525 || timer: 0.0885 sec.
iter 237190 || Loss: 0.7281 || timer: 0.0945 sec.
iter 237200 || Loss: 0.6606 || timer: 0.0920 sec.
iter 237210 || Loss: 0.7685 || timer: 0.0830 sec.
iter 237220 || Loss: 0.8634 || timer: 0.0898 sec.
iter 237230 || Loss: 0.8623 || timer: 0.0814 sec.
iter 237240 || Loss: 0.9357 || timer: 0.0820 sec.
iter 237250 || Loss: 0.6690 || timer: 0.0889 sec.
iter 237260 || Loss: 0.7315 || timer: 0.1227 sec.
iter 237270 || Loss: 0.7293 || timer: 0.0815 sec.
iter 237280 || Loss: 0.8656 || timer: 0.0891 sec.
iter 237290 || Loss: 1.2169 || timer: 0.0832 sec.
iter 237300 || Loss: 1.3830 || timer: 0.0820 sec.
iter 237310 || Loss: 0.7272 || timer: 0.0827 sec.
iter 237320 || Loss: 0.8804 || timer: 0.1107 sec.
iter 237330 || Loss: 0.7279 || timer: 0.0902 sec.
iter 237340 || Loss: 0.9092 || timer: 0.0921 sec.
iter 237350 || Loss: 0.7321 || timer: 0.0884 sec.
iter 237360 || Loss: 0.9562 || timer: 0.0849 sec.
iter 237370 || Loss: 1.1582 || timer: 0.1068 sec.
iter 237380 || Loss: 0.6904 || timer: 0.0825 sec.
iter 237390 || Loss: 0.8329 || timer: 0.0839 sec.
iter 237400 || Loss: 1.0750 || timer: 0.0903 sec.
iter 237410 || Loss: 0.9469 || timer: 0.0903 sec.
iter 237420 || Loss: 1.0185 || timer: 0.0977 sec.
iter 237430 || Loss: 1.1785 || timer: 0.0916 sec.
iter 237440 || Loss: 0.9675 || timer: 0.0909 sec.
iter 237450 || Loss: 1.0995 || timer: 0.0912 sec.
iter 237460 || Loss: 0.9531 || timer: 0.0930 sec.
iter 237470 || Loss: 0.8147 || timer: 0.0820 sec.
iter 237480 || Loss: 0.7386 || timer: 0.0908 sec.
iter 237490 || Loss: 0.6758 || timer: 0.0167 sec.
iter 237500 || Loss: 0.4201 || timer: 0.0801 sec.
iter 237510 || Loss: 0.9855 || timer: 0.0922 sec.
iter 237520 || Loss: 0.9132 || timer: 0.0816 sec.
iter 237530 || Loss: 0.6971 || timer: 0.0913 sec.
iter 237540 || Loss: 0.8408 || timer: 0.0892 sec.
iter 237550 || Loss: 0.9212 || timer: 0.0872 sec.
iter 237560 || Loss: 0.8355 || timer: 0.0827 sec.
iter 237570 || Loss: 1.4062 || timer: 0.0824 sec.
iter 237580 || Loss: 0.9560 || timer: 0.0822 sec.
iter 237590 || Loss: 1.1259 || timer: 0.0994 sec.
iter 237600 || Loss: 0.8585 || timer: 0.0888 sec.
iter 237610 || Loss: 1.1775 || timer: 0.0822 sec.
iter 237620 || Loss: 0.6886 || timer: 0.0903 sec.
iter 237630 || Loss: 0.8837 || timer: 0.0907 sec.
iter 237640 || Loss: 0.9967 || timer: 0.0843 sec.
iter 237650 || Loss: 1.0318 || timer: 0.0918 sec.
iter 237660 || Loss: 1.1025 || timer: 0.0902 sec.
iter 237670 || Loss: 0.8613 || timer: 0.0808 sec.
iter 237680 || Loss: 1.0453 || timer: 0.0903 sec.
iter 237690 || Loss: 0.9121 || timer: 0.0814 sec.
iter 237700 || Loss: 0.6833 || timer: 0.0878 sec.
iter 237710 || Loss: 1.0967 || timer: 0.0884 sec.
iter 237720 || Loss: 0.6990 || timer: 0.0854 sec.
iter 237730 || Loss: 0.9753 || timer: 0.0912 sec.
iter 237740 || Loss: 0.5652 || timer: 0.0905 sec.
iter 237750 || Loss: 0.7674 || timer: 0.0911 sec.
iter 237760 || Loss: 1.2686 || timer: 0.0942 sec.
iter 237770 || Loss: 0.7876 || timer: 0.1082 sec.
iter 237780 || Loss: 0.7135 || timer: 0.0870 sec.
iter 237790 || Loss: 0.8713 || timer: 0.0831 sec.
iter 237800 || Loss: 1.1989 || timer: 0.0814 sec.
iter 237810 || Loss: 0.8567 || timer: 0.0819 sec.
iter 237820 || Loss: 0.8796 || timer: 0.0241 sec.
iter 237830 || Loss: 0.5385 || timer: 0.1039 sec.
iter 237840 || Loss: 0.7800 || timer: 0.1015 sec.
iter 237850 || Loss: 0.9483 || timer: 0.0816 sec.
iter 237860 || Loss: 0.9183 || timer: 0.0820 sec.
iter 237870 || Loss: 1.1413 || timer: 0.1186 sec.
iter 237880 || Loss: 0.9123 || timer: 0.0832 sec.
iter 237890 || Loss: 0.9197 || timer: 0.0820 sec.
iter 237900 || Loss: 0.9775 || timer: 0.1225 sec.
iter 237910 || Loss: 0.7916 || timer: 0.0962 sec.
iter 237920 || Loss: 0.8073 || timer: 0.1252 sec.
iter 237930 || Loss: 1.1793 || timer: 0.0889 sec.
iter 237940 || Loss: 1.0834 || timer: 0.0862 sec.
iter 237950 || Loss: 0.9639 || timer: 0.0888 sec.
iter 237960 || Loss: 0.7748 || timer: 0.0906 sec.
iter 237970 || Loss: 0.8884 || timer: 0.0986 sec.
iter 237980 || Loss: 0.8314 || timer: 0.0907 sec.
iter 237990 || Loss: 0.8958 || timer: 0.0925 sec.
iter 238000 || Loss: 0.8569 || timer: 0.0919 sec.
iter 238010 || Loss: 0.8476 || timer: 0.0896 sec.
iter 238020 || Loss: 0.7837 || timer: 0.0803 sec.
iter 238030 || Loss: 0.8240 || timer: 0.0823 sec.
iter 238040 || Loss: 1.1420 || timer: 0.0890 sec.
iter 238050 || Loss: 0.8307 || timer: 0.0910 sec.
iter 238060 || Loss: 0.9175 || timer: 0.0919 sec.
iter 238070 || Loss: 0.7255 || timer: 0.0871 sec.
iter 238080 || Loss: 0.8009 || timer: 0.0840 sec.
iter 238090 || Loss: 0.7176 || timer: 0.0816 sec.
iter 238100 || Loss: 0.9033 || timer: 0.1069 sec.
iter 238110 || Loss: 0.6387 || timer: 0.1068 sec.
iter 238120 || Loss: 0.9638 || timer: 0.0877 sec.
iter 238130 || Loss: 1.0215 || timer: 0.1319 sec.
iter 238140 || Loss: 0.8094 || timer: 0.0872 sec.
iter 238150 || Loss: 0.7825 || timer: 0.0241 sec.
iter 238160 || Loss: 0.8159 || timer: 0.1071 sec.
iter 238170 || Loss: 0.9387 || timer: 0.1051 sec.
iter 238180 || Loss: 1.1738 || timer: 0.0896 sec.
iter 238190 || Loss: 0.8014 || timer: 0.0922 sec.
iter 238200 || Loss: 1.0438 || timer: 0.0909 sec.
iter 238210 || Loss: 0.9455 || timer: 0.0912 sec.
iter 238220 || Loss: 0.8676 || timer: 0.0811 sec.
iter 238230 || Loss: 1.0058 || timer: 0.1201 sec.
iter 238240 || Loss: 0.7755 || timer: 0.1221 sec.
iter 238250 || Loss: 0.8627 || timer: 0.0965 sec.
iter 238260 || Loss: 0.8093 || timer: 0.0806 sec.
iter 238270 || Loss: 0.9411 || timer: 0.0915 sec.
iter 238280 || Loss: 0.8233 || timer: 0.0865 sec.
iter 238290 || Loss: 0.8243 || timer: 0.0908 sec.
iter 238300 || Loss: 0.8316 || timer: 0.0903 sec.
iter 238310 || Loss: 1.0643 || timer: 0.1042 sec.
iter 238320 || Loss: 0.7452 || timer: 0.0889 sec.
iter 238330 || Loss: 0.6899 || timer: 0.0975 sec.
iter 238340 || Loss: 0.7354 || timer: 0.0933 sec.
iter 238350 || Loss: 1.0435 || timer: 0.0878 sec.
iter 238360 || Loss: 0.6909 || timer: 0.0900 sec.
iter 238370 || Loss: 0.7574 || timer: 0.0854 sec.
iter 238380 || Loss: 1.1889 || timer: 0.0822 sec.
iter 238390 || Loss: 1.2288 || timer: 0.0885 sec.
iter 238400 || Loss: 0.9517 || timer: 0.1355 sec.
iter 238410 || Loss: 0.8072 || timer: 0.0910 sec.
iter 238420 || Loss: 0.8619 || timer: 0.0913 sec.
iter 238430 || Loss: 0.7485 || timer: 0.0912 sec.
iter 238440 || Loss: 0.8304 || timer: 0.0872 sec.
iter 238450 || Loss: 0.7429 || timer: 0.0887 sec.
iter 238460 || Loss: 0.8102 || timer: 0.0908 sec.
iter 238470 || Loss: 1.0700 || timer: 0.0813 sec.
iter 238480 || Loss: 1.3252 || timer: 0.0195 sec.
iter 238490 || Loss: 0.1085 || timer: 0.0893 sec.
iter 238500 || Loss: 0.7146 || timer: 0.0823 sec.
iter 238510 || Loss: 1.0083 || timer: 0.0811 sec.
iter 238520 || Loss: 0.7067 || timer: 0.0813 sec.
iter 238530 || Loss: 0.7685 || timer: 0.0900 sec.
iter 238540 || Loss: 0.7721 || timer: 0.0890 sec.
iter 238550 || Loss: 0.7753 || timer: 0.0841 sec.
iter 238560 || Loss: 0.4790 || timer: 0.0902 sec.
iter 238570 || Loss: 0.9504 || timer: 0.1048 sec.
iter 238580 || Loss: 1.2553 || timer: 0.1342 sec.
iter 238590 || Loss: 1.2056 || timer: 0.0805 sec.
iter 238600 || Loss: 0.8988 || timer: 0.0872 sec.
iter 238610 || Loss: 0.7007 || timer: 0.0815 sec.
iter 238620 || Loss: 1.2033 || timer: 0.0836 sec.
iter 238630 || Loss: 0.8245 || timer: 0.0824 sec.
iter 238640 || Loss: 0.6972 || timer: 0.0825 sec.
iter 238650 || Loss: 0.9929 || timer: 0.0910 sec.
iter 238660 || Loss: 0.9862 || timer: 0.0804 sec.
iter 238670 || Loss: 0.7913 || timer: 0.0892 sec.
iter 238680 || Loss: 0.6944 || timer: 0.0931 sec.
iter 238690 || Loss: 0.9416 || timer: 0.0818 sec.
iter 238700 || Loss: 1.1475 || timer: 0.0885 sec.
iter 238710 || Loss: 1.1334 || timer: 0.0863 sec.
iter 238720 || Loss: 0.7972 || timer: 0.0917 sec.
iter 238730 || Loss: 0.8948 || timer: 0.0890 sec.
iter 238740 || Loss: 0.6964 || timer: 0.0949 sec.
iter 238750 || Loss: 1.1209 || timer: 0.0918 sec.
iter 238760 || Loss: 1.1664 || timer: 0.0867 sec.
iter 238770 || Loss: 1.2132 || timer: 0.1041 sec.
iter 238780 || Loss: 0.8006 || timer: 0.0814 sec.
iter 238790 || Loss: 1.1770 || timer: 0.0810 sec.
iter 238800 || Loss: 0.7591 || timer: 0.0837 sec.
iter 238810 || Loss: 0.9080 || timer: 0.0182 sec.
iter 238820 || Loss: 0.7408 || timer: 0.1055 sec.
iter 238830 || Loss: 1.1163 || timer: 0.0822 sec.
iter 238840 || Loss: 1.0481 || timer: 0.0835 sec.
iter 238850 || Loss: 0.8919 || timer: 0.0830 sec.
iter 238860 || Loss: 0.7136 || timer: 0.0814 sec.
iter 238870 || Loss: 0.8552 || timer: 0.0875 sec.
iter 238880 || Loss: 0.7137 || timer: 0.0900 sec.
iter 238890 || Loss: 0.8150 || timer: 0.0886 sec.
iter 238900 || Loss: 0.8206 || timer: 0.0830 sec.
iter 238910 || Loss: 0.7544 || timer: 0.0861 sec.
iter 238920 || Loss: 0.9978 || timer: 0.0805 sec.
iter 238930 || Loss: 1.0066 || timer: 0.0837 sec.
iter 238940 || Loss: 0.8324 || timer: 0.0863 sec.
iter 238950 || Loss: 1.0474 || timer: 0.0914 sec.
iter 238960 || Loss: 0.7157 || timer: 0.0815 sec.
iter 238970 || Loss: 1.1704 || timer: 0.1008 sec.
iter 238980 || Loss: 0.8710 || timer: 0.1046 sec.
iter 238990 || Loss: 1.2003 || timer: 0.0922 sec.
iter 239000 || Loss: 1.0063 || timer: 0.0809 sec.
iter 239010 || Loss: 0.7454 || timer: 0.0816 sec.
iter 239020 || Loss: 1.1071 || timer: 0.0863 sec.
iter 239030 || Loss: 1.0441 || timer: 0.0920 sec.
iter 239040 || Loss: 0.8026 || timer: 0.0891 sec.
iter 239050 || Loss: 0.7281 || timer: 0.0882 sec.
iter 239060 || Loss: 0.9075 || timer: 0.0861 sec.
iter 239070 || Loss: 0.9261 || timer: 0.0926 sec.
iter 239080 || Loss: 0.9024 || timer: 0.0849 sec.
iter 239090 || Loss: 0.7507 || timer: 0.1059 sec.
iter 239100 || Loss: 0.9543 || timer: 0.0844 sec.
iter 239110 || Loss: 0.8674 || timer: 0.0818 sec.
iter 239120 || Loss: 1.0608 || timer: 0.0907 sec.
iter 239130 || Loss: 0.8631 || timer: 0.0903 sec.
iter 239140 || Loss: 0.7621 || timer: 0.0167 sec.
iter 239150 || Loss: 0.6086 || timer: 0.0853 sec.
iter 239160 || Loss: 0.8255 || timer: 0.0883 sec.
iter 239170 || Loss: 1.0181 || timer: 0.0815 sec.
iter 239180 || Loss: 0.8107 || timer: 0.1139 sec.
iter 239190 || Loss: 0.9357 || timer: 0.0862 sec.
iter 239200 || Loss: 0.9960 || timer: 0.0897 sec.
iter 239210 || Loss: 0.7055 || timer: 0.0900 sec.
iter 239220 || Loss: 0.9404 || timer: 0.0902 sec.
iter 239230 || Loss: 0.7967 || timer: 0.0942 sec.
iter 239240 || Loss: 0.7308 || timer: 0.1114 sec.
iter 239250 || Loss: 0.8928 || timer: 0.0900 sec.
iter 239260 || Loss: 0.5817 || timer: 0.1040 sec.
iter 239270 || Loss: 0.6689 || timer: 0.0906 sec.
iter 239280 || Loss: 0.8735 || timer: 0.0905 sec.
iter 239290 || Loss: 0.8991 || timer: 0.0863 sec.
iter 239300 || Loss: 0.8670 || timer: 0.1012 sec.
iter 239310 || Loss: 0.8593 || timer: 0.0884 sec.
iter 239320 || Loss: 0.8213 || timer: 0.0882 sec.
iter 239330 || Loss: 0.9169 || timer: 0.0739 sec.
iter 239340 || Loss: 0.9675 || timer: 0.1001 sec.
iter 239350 || Loss: 0.6448 || timer: 0.1035 sec.
iter 239360 || Loss: 0.7547 || timer: 0.0906 sec.
iter 239370 || Loss: 0.8805 || timer: 0.0813 sec.
iter 239380 || Loss: 0.9309 || timer: 0.0912 sec.
iter 239390 || Loss: 1.0869 || timer: 0.0817 sec.
iter 239400 || Loss: 0.7088 || timer: 0.0899 sec.
iter 239410 || Loss: 0.9977 || timer: 0.0954 sec.
iter 239420 || Loss: 1.0089 || timer: 0.0908 sec.
iter 239430 || Loss: 0.8753 || timer: 0.0896 sec.
iter 239440 || Loss: 0.6681 || timer: 0.0911 sec.
iter 239450 || Loss: 0.6771 || timer: 0.0894 sec.
iter 239460 || Loss: 0.7548 || timer: 0.0886 sec.
iter 239470 || Loss: 0.7918 || timer: 0.0160 sec.
iter 239480 || Loss: 0.9174 || timer: 0.0949 sec.
iter 239490 || Loss: 1.6677 || timer: 0.0828 sec.
iter 239500 || Loss: 0.9563 || timer: 0.0953 sec.
iter 239510 || Loss: 1.0175 || timer: 0.1363 sec.
iter 239520 || Loss: 0.9401 || timer: 0.0905 sec.
iter 239530 || Loss: 0.9233 || timer: 0.0848 sec.
iter 239540 || Loss: 0.7958 || timer: 0.0808 sec.
iter 239550 || Loss: 0.8366 || timer: 0.0733 sec.
iter 239560 || Loss: 0.5432 || timer: 0.0762 sec.
iter 239570 || Loss: 1.0556 || timer: 0.0980 sec.
iter 239580 || Loss: 0.6197 || timer: 0.0826 sec.
iter 239590 || Loss: 0.7766 || timer: 0.0907 sec.
iter 239600 || Loss: 0.8860 || timer: 0.0815 sec.
iter 239610 || Loss: 0.6745 || timer: 0.0832 sec.
iter 239620 || Loss: 0.5952 || timer: 0.0819 sec.
iter 239630 || Loss: 0.8372 || timer: 0.1087 sec.
iter 239640 || Loss: 0.7307 || timer: 0.1048 sec.
iter 239650 || Loss: 1.2832 || timer: 0.1113 sec.
iter 239660 || Loss: 0.6194 || timer: 0.0821 sec.
iter 239670 || Loss: 1.3460 || timer: 0.0927 sec.
iter 239680 || Loss: 0.8678 || timer: 0.0904 sec.
iter 239690 || Loss: 0.6920 || timer: 0.0918 sec.
iter 239700 || Loss: 0.6498 || timer: 0.0905 sec.
iter 239710 || Loss: 0.7011 || timer: 0.0921 sec.
iter 239720 || Loss: 0.5878 || timer: 0.0889 sec.
iter 239730 || Loss: 1.0680 || timer: 0.0914 sec.
iter 239740 || Loss: 0.6584 || timer: 0.0986 sec.
iter 239750 || Loss: 0.6904 || timer: 0.0819 sec.
iter 239760 || Loss: 1.1587 || timer: 0.0915 sec.
iter 239770 || Loss: 0.8780 || timer: 0.0931 sec.
iter 239780 || Loss: 1.4167 || timer: 0.0918 sec.
iter 239790 || Loss: 0.9822 || timer: 0.0884 sec.
iter 239800 || Loss: 0.6844 || timer: 0.0274 sec.
iter 239810 || Loss: 1.9094 || timer: 0.0981 sec.
iter 239820 || Loss: 0.6678 || timer: 0.1060 sec.
iter 239830 || Loss: 1.0983 || timer: 0.1065 sec.
iter 239840 || Loss: 0.7532 || timer: 0.0923 sec.
iter 239850 || Loss: 1.1040 || timer: 0.0908 sec.
iter 239860 || Loss: 0.8592 || timer: 0.0934 sec.
iter 239870 || Loss: 0.8541 || timer: 0.1035 sec.
iter 239880 || Loss: 1.2683 || timer: 0.0918 sec.
iter 239890 || Loss: 1.0837 || timer: 0.0819 sec.
iter 239900 || Loss: 0.9811 || timer: 0.0955 sec.
iter 239910 || Loss: 0.9133 || timer: 0.0890 sec.
iter 239920 || Loss: 0.7949 || timer: 0.1112 sec.
iter 239930 || Loss: 0.7598 || timer: 0.0921 sec.
iter 239940 || Loss: 1.0776 || timer: 0.0812 sec.
iter 239950 || Loss: 1.0539 || timer: 0.0911 sec.
iter 239960 || Loss: 0.8321 || timer: 0.0816 sec.
iter 239970 || Loss: 1.0849 || timer: 0.0837 sec.
iter 239980 || Loss: 0.8089 || timer: 0.1040 sec.
iter 239990 || Loss: 0.9020 || timer: 0.0955 sec.
iter 240000 || Loss: 1.0836 || Saving state, iter: 240000
timer: 0.0859 sec.
iter 240010 || Loss: 0.9073 || timer: 0.0919 sec.
iter 240020 || Loss: 0.6752 || timer: 0.1035 sec.
iter 240030 || Loss: 0.9040 || timer: 0.0954 sec.
iter 240040 || Loss: 1.1866 || timer: 0.0755 sec.
iter 240050 || Loss: 0.8633 || timer: 0.0883 sec.
iter 240060 || Loss: 0.8861 || timer: 0.0918 sec.
iter 240070 || Loss: 0.7483 || timer: 0.0810 sec.
iter 240080 || Loss: 1.0540 || timer: 0.0909 sec.
iter 240090 || Loss: 0.9384 || timer: 0.1054 sec.
iter 240100 || Loss: 0.8002 || timer: 0.0870 sec.
iter 240110 || Loss: 0.7652 || timer: 0.0851 sec.
iter 240120 || Loss: 0.7483 || timer: 0.1105 sec.
iter 240130 || Loss: 0.8950 || timer: 0.0163 sec.
iter 240140 || Loss: 0.6131 || timer: 0.0851 sec.
iter 240150 || Loss: 0.8820 || timer: 0.0822 sec.
iter 240160 || Loss: 0.7851 || timer: 0.0900 sec.
iter 240170 || Loss: 0.8929 || timer: 0.0867 sec.
iter 240180 || Loss: 0.9440 || timer: 0.0803 sec.
iter 240190 || Loss: 0.7269 || timer: 0.0883 sec.
iter 240200 || Loss: 0.8039 || timer: 0.0852 sec.
iter 240210 || Loss: 0.6311 || timer: 0.0793 sec.
iter 240220 || Loss: 0.6560 || timer: 0.0826 sec.
iter 240230 || Loss: 0.7605 || timer: 0.0950 sec.
iter 240240 || Loss: 0.7345 || timer: 0.0889 sec.
iter 240250 || Loss: 0.7763 || timer: 0.0894 sec.
iter 240260 || Loss: 0.5907 || timer: 0.0911 sec.
iter 240270 || Loss: 1.1068 || timer: 0.0817 sec.
iter 240280 || Loss: 0.9761 || timer: 0.1013 sec.
iter 240290 || Loss: 0.6879 || timer: 0.0850 sec.
iter 240300 || Loss: 0.7866 || timer: 0.1071 sec.
iter 240310 || Loss: 1.1271 || timer: 0.0962 sec.
iter 240320 || Loss: 0.7403 || timer: 0.1129 sec.
iter 240330 || Loss: 0.7637 || timer: 0.0814 sec.
iter 240340 || Loss: 0.7905 || timer: 0.0898 sec.
iter 240350 || Loss: 0.8473 || timer: 0.1125 sec.
iter 240360 || Loss: 0.9870 || timer: 0.0811 sec.
iter 240370 || Loss: 0.8137 || timer: 0.1012 sec.
iter 240380 || Loss: 1.1277 || timer: 0.1028 sec.
iter 240390 || Loss: 0.8716 || timer: 0.0908 sec.
iter 240400 || Loss: 0.7347 || timer: 0.0808 sec.
iter 240410 || Loss: 0.8488 || timer: 0.0969 sec.
iter 240420 || Loss: 0.8153 || timer: 0.0886 sec.
iter 240430 || Loss: 0.7626 || timer: 0.0807 sec.
iter 240440 || Loss: 0.8216 || timer: 0.0863 sec.
iter 240450 || Loss: 0.6402 || timer: 0.0809 sec.
iter 240460 || Loss: 0.8268 || timer: 0.0205 sec.
iter 240470 || Loss: 0.3590 || timer: 0.0888 sec.
iter 240480 || Loss: 1.0344 || timer: 0.0898 sec.
iter 240490 || Loss: 0.8314 || timer: 0.0891 sec.
iter 240500 || Loss: 0.7759 || timer: 0.0932 sec.
iter 240510 || Loss: 1.0526 || timer: 0.0889 sec.
iter 240520 || Loss: 0.8562 || timer: 0.0805 sec.
iter 240530 || Loss: 1.0461 || timer: 0.0887 sec.
iter 240540 || Loss: 1.1237 || timer: 0.0813 sec.
iter 240550 || Loss: 1.0217 || timer: 0.0964 sec.
iter 240560 || Loss: 0.7497 || timer: 0.1190 sec.
iter 240570 || Loss: 0.7107 || timer: 0.0795 sec.
iter 240580 || Loss: 1.1064 || timer: 0.0932 sec.
iter 240590 || Loss: 0.9725 || timer: 0.0896 sec.
iter 240600 || Loss: 0.9137 || timer: 0.0877 sec.
iter 240610 || Loss: 0.8393 || timer: 0.0814 sec.
iter 240620 || Loss: 1.0365 || timer: 0.1020 sec.
iter 240630 || Loss: 0.9058 || timer: 0.1027 sec.
iter 240640 || Loss: 0.9631 || timer: 0.1019 sec.
iter 240650 || Loss: 1.3911 || timer: 0.0892 sec.
iter 240660 || Loss: 0.5467 || timer: 0.0818 sec.
iter 240670 || Loss: 0.7377 || timer: 0.0884 sec.
iter 240680 || Loss: 1.1411 || timer: 0.0868 sec.
iter 240690 || Loss: 0.7892 || timer: 0.0890 sec.
iter 240700 || Loss: 0.8791 || timer: 0.0815 sec.
iter 240710 || Loss: 1.0011 || timer: 0.0900 sec.
iter 240720 || Loss: 1.6489 || timer: 0.0813 sec.
iter 240730 || Loss: 1.1273 || timer: 0.0990 sec.
iter 240740 || Loss: 1.1402 || timer: 0.0808 sec.
iter 240750 || Loss: 0.6772 || timer: 0.0861 sec.
iter 240760 || Loss: 0.8962 || timer: 0.0809 sec.
iter 240770 || Loss: 0.9889 || timer: 0.0837 sec.
iter 240780 || Loss: 1.0947 || timer: 0.0895 sec.
iter 240790 || Loss: 0.8488 || timer: 0.0277 sec.
iter 240800 || Loss: 0.5655 || timer: 0.1014 sec.
iter 240810 || Loss: 0.8263 || timer: 0.1054 sec.
iter 240820 || Loss: 0.8488 || timer: 0.0871 sec.
iter 240830 || Loss: 0.7141 || timer: 0.0816 sec.
iter 240840 || Loss: 0.7699 || timer: 0.0872 sec.
iter 240850 || Loss: 0.8752 || timer: 0.1128 sec.
iter 240860 || Loss: 0.9850 || timer: 0.0902 sec.
iter 240870 || Loss: 0.9738 || timer: 0.0983 sec.
iter 240880 || Loss: 1.0428 || timer: 0.0889 sec.
iter 240890 || Loss: 0.9775 || timer: 0.1239 sec.
iter 240900 || Loss: 0.6885 || timer: 0.0828 sec.
iter 240910 || Loss: 0.8859 || timer: 0.0901 sec.
iter 240920 || Loss: 0.8045 || timer: 0.0864 sec.
iter 240930 || Loss: 0.7501 || timer: 0.0905 sec.
iter 240940 || Loss: 1.0637 || timer: 0.1030 sec.
iter 240950 || Loss: 1.4367 || timer: 0.0896 sec.
iter 240960 || Loss: 0.8299 || timer: 0.1363 sec.
iter 240970 || Loss: 1.0601 || timer: 0.0965 sec.
iter 240980 || Loss: 1.2983 || timer: 0.0862 sec.
iter 240990 || Loss: 0.7077 || timer: 0.0993 sec.
iter 241000 || Loss: 0.9872 || timer: 0.1153 sec.
iter 241010 || Loss: 1.1699 || timer: 0.0877 sec.
iter 241020 || Loss: 0.8740 || timer: 0.0826 sec.
iter 241030 || Loss: 0.8746 || timer: 0.0871 sec.
iter 241040 || Loss: 1.1451 || timer: 0.0914 sec.
iter 241050 || Loss: 0.9955 || timer: 0.1032 sec.
iter 241060 || Loss: 0.9571 || timer: 0.0915 sec.
iter 241070 || Loss: 1.1437 || timer: 0.0879 sec.
iter 241080 || Loss: 0.8365 || timer: 0.0817 sec.
iter 241090 || Loss: 1.0937 || timer: 0.0825 sec.
iter 241100 || Loss: 0.8838 || timer: 0.1078 sec.
iter 241110 || Loss: 0.7591 || timer: 0.0915 sec.
iter 241120 || Loss: 1.0801 || timer: 0.0239 sec.
iter 241130 || Loss: 0.7128 || timer: 0.0805 sec.
iter 241140 || Loss: 0.9667 || timer: 0.0870 sec.
iter 241150 || Loss: 1.1341 || timer: 0.0877 sec.
iter 241160 || Loss: 0.7063 || timer: 0.0907 sec.
iter 241170 || Loss: 0.9103 || timer: 0.0803 sec.
iter 241180 || Loss: 0.9141 || timer: 0.0903 sec.
iter 241190 || Loss: 1.0333 || timer: 0.0903 sec.
iter 241200 || Loss: 0.8685 || timer: 0.0806 sec.
iter 241210 || Loss: 0.8260 || timer: 0.0895 sec.
iter 241220 || Loss: 0.9074 || timer: 0.1094 sec.
iter 241230 || Loss: 0.7019 || timer: 0.0820 sec.
iter 241240 || Loss: 0.6636 || timer: 0.1132 sec.
iter 241250 || Loss: 0.7550 || timer: 0.0902 sec.
iter 241260 || Loss: 0.8766 || timer: 0.0847 sec.
iter 241270 || Loss: 0.8784 || timer: 0.0869 sec.
iter 241280 || Loss: 0.8603 || timer: 0.1340 sec.
iter 241290 || Loss: 0.9479 || timer: 0.0897 sec.
iter 241300 || Loss: 1.1840 || timer: 0.0908 sec.
iter 241310 || Loss: 1.0108 || timer: 0.1159 sec.
iter 241320 || Loss: 0.9607 || timer: 0.0853 sec.
iter 241330 || Loss: 1.0843 || timer: 0.0957 sec.
iter 241340 || Loss: 1.4872 || timer: 0.0906 sec.
iter 241350 || Loss: 1.1532 || timer: 0.0907 sec.
iter 241360 || Loss: 0.8535 || timer: 0.0854 sec.
iter 241370 || Loss: 1.3622 || timer: 0.0860 sec.
iter 241380 || Loss: 0.8803 || timer: 0.0914 sec.
iter 241390 || Loss: 1.2145 || timer: 0.0801 sec.
iter 241400 || Loss: 1.1126 || timer: 0.1192 sec.
iter 241410 || Loss: 1.3197 || timer: 0.0869 sec.
iter 241420 || Loss: 1.0613 || timer: 0.0859 sec.
iter 241430 || Loss: 0.9174 || timer: 0.0889 sec.
iter 241440 || Loss: 1.0010 || timer: 0.0923 sec.
iter 241450 || Loss: 0.9058 || timer: 0.0227 sec.
iter 241460 || Loss: 1.7307 || timer: 0.1094 sec.
iter 241470 || Loss: 0.9788 || timer: 0.0924 sec.
iter 241480 || Loss: 0.7365 || timer: 0.0948 sec.
iter 241490 || Loss: 1.1984 || timer: 0.0901 sec.
iter 241500 || Loss: 1.2467 || timer: 0.0913 sec.
iter 241510 || Loss: 0.8649 || timer: 0.0852 sec.
iter 241520 || Loss: 0.8636 || timer: 0.0939 sec.
iter 241530 || Loss: 0.8370 || timer: 0.0827 sec.
iter 241540 || Loss: 1.0464 || timer: 0.0902 sec.
iter 241550 || Loss: 0.7798 || timer: 0.1010 sec.
iter 241560 || Loss: 0.9452 || timer: 0.0929 sec.
iter 241570 || Loss: 1.0277 || timer: 0.0794 sec.
iter 241580 || Loss: 0.6590 || timer: 0.0841 sec.
iter 241590 || Loss: 0.7679 || timer: 0.0941 sec.
iter 241600 || Loss: 1.3984 || timer: 0.0983 sec.
iter 241610 || Loss: 1.4681 || timer: 0.0867 sec.
iter 241620 || Loss: 0.8964 || timer: 0.0836 sec.
iter 241630 || Loss: 0.9147 || timer: 0.0832 sec.
iter 241640 || Loss: 1.0459 || timer: 0.0910 sec.
iter 241650 || Loss: 0.7043 || timer: 0.0884 sec.
iter 241660 || Loss: 0.9913 || timer: 0.0928 sec.
iter 241670 || Loss: 0.9478 || timer: 0.0901 sec.
iter 241680 || Loss: 0.8984 || timer: 0.0918 sec.
iter 241690 || Loss: 1.0831 || timer: 0.0897 sec.
iter 241700 || Loss: 0.7974 || timer: 0.0891 sec.
iter 241710 || Loss: 0.7145 || timer: 0.0922 sec.
iter 241720 || Loss: 0.5810 || timer: 0.0971 sec.
iter 241730 || Loss: 1.0993 || timer: 0.1077 sec.
iter 241740 || Loss: 0.8635 || timer: 0.0910 sec.
iter 241750 || Loss: 0.6843 || timer: 0.0922 sec.
iter 241760 || Loss: 1.1774 || timer: 0.0835 sec.
iter 241770 || Loss: 0.8073 || timer: 0.1261 sec.
iter 241780 || Loss: 1.0648 || timer: 0.0255 sec.
iter 241790 || Loss: 1.2060 || timer: 0.0849 sec.
iter 241800 || Loss: 0.7710 || timer: 0.0919 sec.
iter 241810 || Loss: 0.9703 || timer: 0.0755 sec.
iter 241820 || Loss: 0.5517 || timer: 0.0905 sec.
iter 241830 || Loss: 1.1099 || timer: 0.1049 sec.
iter 241840 || Loss: 0.7851 || timer: 0.0976 sec.
iter 241850 || Loss: 0.7897 || timer: 0.0810 sec.
iter 241860 || Loss: 0.9211 || timer: 0.0873 sec.
iter 241870 || Loss: 0.8559 || timer: 0.0829 sec.
iter 241880 || Loss: 1.0795 || timer: 0.1202 sec.
iter 241890 || Loss: 1.1915 || timer: 0.1351 sec.
iter 241900 || Loss: 1.1743 || timer: 0.0939 sec.
iter 241910 || Loss: 0.7205 || timer: 0.0904 sec.
iter 241920 || Loss: 0.9517 || timer: 0.0868 sec.
iter 241930 || Loss: 0.7078 || timer: 0.1048 sec.
iter 241940 || Loss: 0.7633 || timer: 0.0813 sec.
iter 241950 || Loss: 1.1827 || timer: 0.0816 sec.
iter 241960 || Loss: 0.7815 || timer: 0.0844 sec.
iter 241970 || Loss: 1.8630 || timer: 0.0888 sec.
iter 241980 || Loss: 0.9203 || timer: 0.1228 sec.
iter 241990 || Loss: 1.1603 || timer: 0.0904 sec.
iter 242000 || Loss: 0.8899 || timer: 0.1085 sec.
iter 242010 || Loss: 0.8461 || timer: 0.0819 sec.
iter 242020 || Loss: 0.8418 || timer: 0.0912 sec.
iter 242030 || Loss: 0.6904 || timer: 0.0827 sec.
iter 242040 || Loss: 0.9453 || timer: 0.0804 sec.
iter 242050 || Loss: 0.8016 || timer: 0.0921 sec.
iter 242060 || Loss: 1.1174 || timer: 0.0898 sec.
iter 242070 || Loss: 1.0115 || timer: 0.0880 sec.
iter 242080 || Loss: 1.0245 || timer: 0.0890 sec.
iter 242090 || Loss: 0.8125 || timer: 0.0870 sec.
iter 242100 || Loss: 0.8912 || timer: 0.1081 sec.
iter 242110 || Loss: 0.6813 || timer: 0.0232 sec.
iter 242120 || Loss: 0.5334 || timer: 0.0941 sec.
iter 242130 || Loss: 0.8933 || timer: 0.1230 sec.
iter 242140 || Loss: 0.8979 || timer: 0.0809 sec.
iter 242150 || Loss: 0.8719 || timer: 0.0850 sec.
iter 242160 || Loss: 0.7733 || timer: 0.0826 sec.
iter 242170 || Loss: 1.2669 || timer: 0.0871 sec.
iter 242180 || Loss: 0.8179 || timer: 0.0894 sec.
iter 242190 || Loss: 0.7636 || timer: 0.0877 sec.
iter 242200 || Loss: 0.7288 || timer: 0.1039 sec.
iter 242210 || Loss: 0.8151 || timer: 0.0977 sec.
iter 242220 || Loss: 0.7631 || timer: 0.1028 sec.
iter 242230 || Loss: 0.9170 || timer: 0.0813 sec.
iter 242240 || Loss: 0.6763 || timer: 0.0820 sec.
iter 242250 || Loss: 1.0528 || timer: 0.0814 sec.
iter 242260 || Loss: 1.0780 || timer: 0.0883 sec.
iter 242270 || Loss: 0.6149 || timer: 0.0891 sec.
iter 242280 || Loss: 0.6766 || timer: 0.0902 sec.
iter 242290 || Loss: 0.8754 || timer: 0.0888 sec.
iter 242300 || Loss: 1.1711 || timer: 0.0932 sec.
iter 242310 || Loss: 0.8002 || timer: 0.0913 sec.
iter 242320 || Loss: 0.8299 || timer: 0.0929 sec.
iter 242330 || Loss: 1.1146 || timer: 0.0832 sec.
iter 242340 || Loss: 0.8416 || timer: 0.0868 sec.
iter 242350 || Loss: 0.7597 || timer: 0.0973 sec.
iter 242360 || Loss: 1.0458 || timer: 0.0829 sec.
iter 242370 || Loss: 1.0427 || timer: 0.1069 sec.
iter 242380 || Loss: 0.4772 || timer: 0.0891 sec.
iter 242390 || Loss: 0.7748 || timer: 0.0883 sec.
iter 242400 || Loss: 0.9578 || timer: 0.0873 sec.
iter 242410 || Loss: 0.9999 || timer: 0.0829 sec.
iter 242420 || Loss: 0.9773 || timer: 0.0897 sec.
iter 242430 || Loss: 1.0470 || timer: 0.0890 sec.
iter 242440 || Loss: 0.7739 || timer: 0.0214 sec.
iter 242450 || Loss: 0.8388 || timer: 0.0978 sec.
iter 242460 || Loss: 0.6791 || timer: 0.0822 sec.
iter 242470 || Loss: 0.8301 || timer: 0.1017 sec.
iter 242480 || Loss: 0.7365 || timer: 0.0844 sec.
iter 242490 || Loss: 0.9193 || timer: 0.1014 sec.
iter 242500 || Loss: 0.6915 || timer: 0.1064 sec.
iter 242510 || Loss: 0.7081 || timer: 0.0976 sec.
iter 242520 || Loss: 0.9051 || timer: 0.0926 sec.
iter 242530 || Loss: 0.9681 || timer: 0.0858 sec.
iter 242540 || Loss: 0.7506 || timer: 0.1217 sec.
iter 242550 || Loss: 0.9114 || timer: 0.0861 sec.
iter 242560 || Loss: 0.8149 || timer: 0.0890 sec.
iter 242570 || Loss: 0.7120 || timer: 0.0879 sec.
iter 242580 || Loss: 0.9647 || timer: 0.0829 sec.
iter 242590 || Loss: 0.9803 || timer: 0.0888 sec.
iter 242600 || Loss: 0.9260 || timer: 0.0864 sec.
iter 242610 || Loss: 0.9638 || timer: 0.0910 sec.
iter 242620 || Loss: 0.5631 || timer: 0.0839 sec.
iter 242630 || Loss: 0.6603 || timer: 0.0869 sec.
iter 242640 || Loss: 0.8066 || timer: 0.0827 sec.
iter 242650 || Loss: 0.9504 || timer: 0.0819 sec.
iter 242660 || Loss: 0.6638 || timer: 0.0898 sec.
iter 242670 || Loss: 0.9765 || timer: 0.0891 sec.
iter 242680 || Loss: 0.9917 || timer: 0.0950 sec.
iter 242690 || Loss: 0.6985 || timer: 0.0818 sec.
iter 242700 || Loss: 0.7180 || timer: 0.0875 sec.
iter 242710 || Loss: 1.0217 || timer: 0.1059 sec.
iter 242720 || Loss: 0.9539 || timer: 0.0896 sec.
iter 242730 || Loss: 1.0318 || timer: 0.0887 sec.
iter 242740 || Loss: 1.3449 || timer: 0.0891 sec.
iter 242750 || Loss: 1.2666 || timer: 0.0888 sec.
iter 242760 || Loss: 0.8625 || timer: 0.0895 sec.
iter 242770 || Loss: 0.8339 || timer: 0.0270 sec.
iter 242780 || Loss: 0.2273 || timer: 0.0884 sec.
iter 242790 || Loss: 0.7654 || timer: 0.0812 sec.
iter 242800 || Loss: 0.8364 || timer: 0.0820 sec.
iter 242810 || Loss: 0.7315 || timer: 0.1338 sec.
iter 242820 || Loss: 0.8721 || timer: 0.0900 sec.
iter 242830 || Loss: 1.0928 || timer: 0.0880 sec.
iter 242840 || Loss: 0.8688 || timer: 0.0880 sec.
iter 242850 || Loss: 1.0489 || timer: 0.0887 sec.
iter 242860 || Loss: 0.6414 || timer: 0.0837 sec.
iter 242870 || Loss: 1.0266 || timer: 0.1000 sec.
iter 242880 || Loss: 0.6204 || timer: 0.0819 sec.
iter 242890 || Loss: 0.9106 || timer: 0.1418 sec.
iter 242900 || Loss: 1.0448 || timer: 0.0827 sec.
iter 242910 || Loss: 1.0617 || timer: 0.0910 sec.
iter 242920 || Loss: 0.9293 || timer: 0.0809 sec.
iter 242930 || Loss: 0.7233 || timer: 0.0886 sec.
iter 242940 || Loss: 0.7171 || timer: 0.0906 sec.
iter 242950 || Loss: 0.7713 || timer: 0.0921 sec.
iter 242960 || Loss: 1.0647 || timer: 0.1032 sec.
iter 242970 || Loss: 0.9501 || timer: 0.0987 sec.
iter 242980 || Loss: 1.0031 || timer: 0.0882 sec.
iter 242990 || Loss: 0.8541 || timer: 0.0952 sec.
iter 243000 || Loss: 0.6404 || timer: 0.0807 sec.
iter 243010 || Loss: 0.9231 || timer: 0.1117 sec.
iter 243020 || Loss: 0.8716 || timer: 0.0808 sec.
iter 243030 || Loss: 0.6697 || timer: 0.0810 sec.
iter 243040 || Loss: 0.8124 || timer: 0.0892 sec.
iter 243050 || Loss: 0.8111 || timer: 0.0901 sec.
iter 243060 || Loss: 1.1616 || timer: 0.0892 sec.
iter 243070 || Loss: 0.9657 || timer: 0.0900 sec.
iter 243080 || Loss: 1.1536 || timer: 0.1202 sec.
iter 243090 || Loss: 0.7317 || timer: 0.0808 sec.
iter 243100 || Loss: 0.8433 || timer: 0.0187 sec.
iter 243110 || Loss: 0.6803 || timer: 0.1009 sec.
iter 243120 || Loss: 0.8332 || timer: 0.1044 sec.
iter 243130 || Loss: 0.9794 || timer: 0.1008 sec.
iter 243140 || Loss: 0.9264 || timer: 0.0809 sec.
iter 243150 || Loss: 1.2512 || timer: 0.0991 sec.
iter 243160 || Loss: 0.5757 || timer: 0.0822 sec.
iter 243170 || Loss: 0.7481 || timer: 0.0838 sec.
iter 243180 || Loss: 0.9928 || timer: 0.0857 sec.
iter 243190 || Loss: 0.8724 || timer: 0.0894 sec.
iter 243200 || Loss: 0.9297 || timer: 0.0985 sec.
iter 243210 || Loss: 0.9918 || timer: 0.0889 sec.
iter 243220 || Loss: 1.1151 || timer: 0.0901 sec.
iter 243230 || Loss: 0.9195 || timer: 0.0858 sec.
iter 243240 || Loss: 0.8032 || timer: 0.0837 sec.
iter 243250 || Loss: 0.8671 || timer: 0.0873 sec.
iter 243260 || Loss: 0.6606 || timer: 0.0873 sec.
iter 243270 || Loss: 0.9235 || timer: 0.0901 sec.
iter 243280 || Loss: 1.0935 || timer: 0.0809 sec.
iter 243290 || Loss: 0.9406 || timer: 0.0864 sec.
iter 243300 || Loss: 0.7171 || timer: 0.0963 sec.
iter 243310 || Loss: 1.0336 || timer: 0.0809 sec.
iter 243320 || Loss: 1.2113 || timer: 0.0921 sec.
iter 243330 || Loss: 0.7194 || timer: 0.0850 sec.
iter 243340 || Loss: 0.5689 || timer: 0.0906 sec.
iter 243350 || Loss: 0.9246 || timer: 0.0896 sec.
iter 243360 || Loss: 1.2015 || timer: 0.1102 sec.
iter 243370 || Loss: 0.8803 || timer: 0.0858 sec.
iter 243380 || Loss: 0.6073 || timer: 0.1069 sec.
iter 243390 || Loss: 0.9226 || timer: 0.0972 sec.
iter 243400 || Loss: 1.1620 || timer: 0.0893 sec.
iter 243410 || Loss: 0.9407 || timer: 0.1135 sec.
iter 243420 || Loss: 0.7405 || timer: 0.1025 sec.
iter 243430 || Loss: 0.6693 || timer: 0.0218 sec.
iter 243440 || Loss: 1.0251 || timer: 0.0876 sec.
iter 243450 || Loss: 0.6510 || timer: 0.1055 sec.
iter 243460 || Loss: 0.9370 || timer: 0.1133 sec.
iter 243470 || Loss: 0.6229 || timer: 0.0923 sec.
iter 243480 || Loss: 1.1380 || timer: 0.0882 sec.
iter 243490 || Loss: 0.8334 || timer: 0.1033 sec.
iter 243500 || Loss: 0.9218 || timer: 0.0901 sec.
iter 243510 || Loss: 0.7243 || timer: 0.0978 sec.
iter 243520 || Loss: 0.7978 || timer: 0.0926 sec.
iter 243530 || Loss: 0.7829 || timer: 0.1322 sec.
iter 243540 || Loss: 0.8605 || timer: 0.0895 sec.
iter 243550 || Loss: 0.7965 || timer: 0.0821 sec.
iter 243560 || Loss: 0.9043 || timer: 0.0890 sec.
iter 243570 || Loss: 0.8048 || timer: 0.0958 sec.
iter 243580 || Loss: 0.6063 || timer: 0.1204 sec.
iter 243590 || Loss: 0.8854 || timer: 0.0879 sec.
iter 243600 || Loss: 0.8213 || timer: 0.1002 sec.
iter 243610 || Loss: 1.1089 || timer: 0.0885 sec.
iter 243620 || Loss: 1.0569 || timer: 0.0892 sec.
iter 243630 || Loss: 1.0421 || timer: 0.0910 sec.
iter 243640 || Loss: 0.7811 || timer: 0.0873 sec.
iter 243650 || Loss: 0.8013 || timer: 0.1101 sec.
iter 243660 || Loss: 0.8137 || timer: 0.0885 sec.
iter 243670 || Loss: 0.9505 || timer: 0.0865 sec.
iter 243680 || Loss: 0.8099 || timer: 0.0914 sec.
iter 243690 || Loss: 0.8234 || timer: 0.0909 sec.
iter 243700 || Loss: 0.7648 || timer: 0.0956 sec.
iter 243710 || Loss: 0.6868 || timer: 0.0998 sec.
iter 243720 || Loss: 1.0686 || timer: 0.0909 sec.
iter 243730 || Loss: 0.8626 || timer: 0.0795 sec.
iter 243740 || Loss: 0.6720 || timer: 0.0872 sec.
iter 243750 || Loss: 0.7964 || timer: 0.0965 sec.
iter 243760 || Loss: 0.8787 || timer: 0.0251 sec.
iter 243770 || Loss: 0.2625 || timer: 0.0821 sec.
iter 243780 || Loss: 0.6987 || timer: 0.0882 sec.
iter 243790 || Loss: 0.6244 || timer: 0.0894 sec.
iter 243800 || Loss: 0.5517 || timer: 0.0879 sec.
iter 243810 || Loss: 0.9516 || timer: 0.0899 sec.
iter 243820 || Loss: 0.5634 || timer: 0.0905 sec.
iter 243830 || Loss: 1.0551 || timer: 0.0815 sec.
iter 243840 || Loss: 0.8762 || timer: 0.0919 sec.
iter 243850 || Loss: 0.8661 || timer: 0.1054 sec.
iter 243860 || Loss: 0.5956 || timer: 0.1452 sec.
iter 243870 || Loss: 0.7911 || timer: 0.0830 sec.
iter 243880 || Loss: 0.8198 || timer: 0.1056 sec.
iter 243890 || Loss: 1.0006 || timer: 0.0854 sec.
iter 243900 || Loss: 0.6895 || timer: 0.0989 sec.
iter 243910 || Loss: 0.8167 || timer: 0.0941 sec.
iter 243920 || Loss: 1.0220 || timer: 0.0860 sec.
iter 243930 || Loss: 0.8390 || timer: 0.0988 sec.
iter 243940 || Loss: 0.9367 || timer: 0.0973 sec.
iter 243950 || Loss: 0.6841 || timer: 0.0827 sec.
iter 243960 || Loss: 1.1241 || timer: 0.0918 sec.
iter 243970 || Loss: 1.1301 || timer: 0.0819 sec.
iter 243980 || Loss: 0.9094 || timer: 0.0841 sec.
iter 243990 || Loss: 0.8122 || timer: 0.0875 sec.
iter 244000 || Loss: 1.0675 || timer: 0.0998 sec.
iter 244010 || Loss: 0.9849 || timer: 0.0809 sec.
iter 244020 || Loss: 0.9111 || timer: 0.0804 sec.
iter 244030 || Loss: 1.0411 || timer: 0.0821 sec.
iter 244040 || Loss: 0.7044 || timer: 0.0897 sec.
iter 244050 || Loss: 0.9752 || timer: 0.0860 sec.
iter 244060 || Loss: 0.6487 || timer: 0.0871 sec.
iter 244070 || Loss: 0.8935 || timer: 0.1073 sec.
iter 244080 || Loss: 0.7512 || timer: 0.1009 sec.
iter 244090 || Loss: 0.7201 || timer: 0.0296 sec.
iter 244100 || Loss: 0.3834 || timer: 0.0981 sec.
iter 244110 || Loss: 0.7755 || timer: 0.0914 sec.
iter 244120 || Loss: 0.7956 || timer: 0.0909 sec.
iter 244130 || Loss: 0.9044 || timer: 0.0873 sec.
iter 244140 || Loss: 0.8188 || timer: 0.0956 sec.
iter 244150 || Loss: 0.8228 || timer: 0.0926 sec.
iter 244160 || Loss: 0.9652 || timer: 0.0809 sec.
iter 244170 || Loss: 0.7023 || timer: 0.0888 sec.
iter 244180 || Loss: 0.8105 || timer: 0.0827 sec.
iter 244190 || Loss: 0.7268 || timer: 0.0989 sec.
iter 244200 || Loss: 0.6147 || timer: 0.0909 sec.
iter 244210 || Loss: 0.8582 || timer: 0.1116 sec.
iter 244220 || Loss: 0.9798 || timer: 0.0827 sec.
iter 244230 || Loss: 0.8186 || timer: 0.1040 sec.
iter 244240 || Loss: 0.7817 || timer: 0.0891 sec.
iter 244250 || Loss: 0.7838 || timer: 0.0875 sec.
iter 244260 || Loss: 0.8715 || timer: 0.1235 sec.
iter 244270 || Loss: 1.0466 || timer: 0.0879 sec.
iter 244280 || Loss: 0.8687 || timer: 0.0882 sec.
iter 244290 || Loss: 0.9617 || timer: 0.0812 sec.
iter 244300 || Loss: 0.7308 || timer: 0.0952 sec.
iter 244310 || Loss: 0.7040 || timer: 0.0908 sec.
iter 244320 || Loss: 0.8259 || timer: 0.1033 sec.
iter 244330 || Loss: 0.9845 || timer: 0.1027 sec.
iter 244340 || Loss: 0.8293 || timer: 0.0832 sec.
iter 244350 || Loss: 1.0346 || timer: 0.0908 sec.
iter 244360 || Loss: 0.6194 || timer: 0.1061 sec.
iter 244370 || Loss: 1.0633 || timer: 0.1002 sec.
iter 244380 || Loss: 0.9419 || timer: 0.0849 sec.
iter 244390 || Loss: 0.8245 || timer: 0.1122 sec.
iter 244400 || Loss: 1.0710 || timer: 0.0929 sec.
iter 244410 || Loss: 0.7278 || timer: 0.0854 sec.
iter 244420 || Loss: 0.7674 || timer: 0.0272 sec.
iter 244430 || Loss: 0.3823 || timer: 0.0903 sec.
iter 244440 || Loss: 0.7655 || timer: 0.0914 sec.
iter 244450 || Loss: 0.9133 || timer: 0.0834 sec.
iter 244460 || Loss: 0.7504 || timer: 0.0872 sec.
iter 244470 || Loss: 0.9438 || timer: 0.1016 sec.
iter 244480 || Loss: 0.5945 || timer: 0.0898 sec.
iter 244490 || Loss: 0.8362 || timer: 0.0889 sec.
iter 244500 || Loss: 1.0498 || timer: 0.1252 sec.
iter 244510 || Loss: 0.7751 || timer: 0.1088 sec.
iter 244520 || Loss: 1.0376 || timer: 0.1266 sec.
iter 244530 || Loss: 0.7014 || timer: 0.0813 sec.
iter 244540 || Loss: 0.6279 || timer: 0.0922 sec.
iter 244550 || Loss: 0.6647 || timer: 0.0920 sec.
iter 244560 || Loss: 1.1341 || timer: 0.0831 sec.
iter 244570 || Loss: 0.9296 || timer: 0.0860 sec.
iter 244580 || Loss: 0.6534 || timer: 0.0822 sec.
iter 244590 || Loss: 1.0960 || timer: 0.1034 sec.
iter 244600 || Loss: 0.8015 || timer: 0.0816 sec.
iter 244610 || Loss: 0.7792 || timer: 0.1007 sec.
iter 244620 || Loss: 0.8598 || timer: 0.0836 sec.
iter 244630 || Loss: 0.6192 || timer: 0.0981 sec.
iter 244640 || Loss: 0.8702 || timer: 0.0895 sec.
iter 244650 || Loss: 1.0203 || timer: 0.1243 sec.
iter 244660 || Loss: 0.9233 || timer: 0.1232 sec.
iter 244670 || Loss: 0.8261 || timer: 0.1084 sec.
iter 244680 || Loss: 1.1564 || timer: 0.0892 sec.
iter 244690 || Loss: 0.6328 || timer: 0.0756 sec.
iter 244700 || Loss: 0.6049 || timer: 0.0831 sec.
iter 244710 || Loss: 0.7207 || timer: 0.0904 sec.
iter 244720 || Loss: 0.6681 || timer: 0.0897 sec.
iter 244730 || Loss: 0.6587 || timer: 0.0881 sec.
iter 244740 || Loss: 0.8331 || timer: 0.0821 sec.
iter 244750 || Loss: 0.9733 || timer: 0.0238 sec.
iter 244760 || Loss: 0.6010 || timer: 0.1061 sec.
iter 244770 || Loss: 0.9681 || timer: 0.0888 sec.
iter 244780 || Loss: 0.7895 || timer: 0.0833 sec.
iter 244790 || Loss: 0.8913 || timer: 0.0889 sec.
iter 244800 || Loss: 1.1106 || timer: 0.1126 sec.
iter 244810 || Loss: 0.9799 || timer: 0.0906 sec.
iter 244820 || Loss: 0.9881 || timer: 0.0905 sec.
iter 244830 || Loss: 0.9002 || timer: 0.1107 sec.
iter 244840 || Loss: 0.8572 || timer: 0.0915 sec.
iter 244850 || Loss: 0.8402 || timer: 0.1000 sec.
iter 244860 || Loss: 0.8692 || timer: 0.0927 sec.
iter 244870 || Loss: 0.8103 || timer: 0.0884 sec.
iter 244880 || Loss: 0.8208 || timer: 0.0904 sec.
iter 244890 || Loss: 0.7501 || timer: 0.0836 sec.
iter 244900 || Loss: 0.8313 || timer: 0.0913 sec.
iter 244910 || Loss: 0.8460 || timer: 0.0812 sec.
iter 244920 || Loss: 0.7997 || timer: 0.1076 sec.
iter 244930 || Loss: 1.2977 || timer: 0.0911 sec.
iter 244940 || Loss: 1.0406 || timer: 0.0896 sec.
iter 244950 || Loss: 1.2548 || timer: 0.1052 sec.
iter 244960 || Loss: 0.8593 || timer: 0.0980 sec.
iter 244970 || Loss: 0.7668 || timer: 0.0810 sec.
iter 244980 || Loss: 0.7100 || timer: 0.0931 sec.
iter 244990 || Loss: 0.7023 || timer: 0.0875 sec.
iter 245000 || Loss: 0.5134 || Saving state, iter: 245000
timer: 0.0813 sec.
iter 245010 || Loss: 0.8831 || timer: 0.0871 sec.
iter 245020 || Loss: 0.8355 || timer: 0.0911 sec.
iter 245030 || Loss: 1.1442 || timer: 0.0955 sec.
iter 245040 || Loss: 0.7027 || timer: 0.0914 sec.
iter 245050 || Loss: 1.0039 || timer: 0.1046 sec.
iter 245060 || Loss: 0.6216 || timer: 0.0895 sec.
iter 245070 || Loss: 1.0913 || timer: 0.0891 sec.
iter 245080 || Loss: 1.0649 || timer: 0.0167 sec.
iter 245090 || Loss: 0.4929 || timer: 0.1156 sec.
iter 245100 || Loss: 0.9547 || timer: 0.0915 sec.
iter 245110 || Loss: 1.0373 || timer: 0.0872 sec.
iter 245120 || Loss: 1.0018 || timer: 0.0868 sec.
iter 245130 || Loss: 0.8320 || timer: 0.1243 sec.
iter 245140 || Loss: 0.8029 || timer: 0.1061 sec.
iter 245150 || Loss: 0.9362 || timer: 0.0838 sec.
iter 245160 || Loss: 1.0769 || timer: 0.0807 sec.
iter 245170 || Loss: 0.6834 || timer: 0.1174 sec.
iter 245180 || Loss: 0.7358 || timer: 0.1190 sec.
iter 245190 || Loss: 0.7060 || timer: 0.0825 sec.
iter 245200 || Loss: 1.1260 || timer: 0.0898 sec.
iter 245210 || Loss: 0.7030 || timer: 0.0862 sec.
iter 245220 || Loss: 1.0203 || timer: 0.0828 sec.
iter 245230 || Loss: 0.9638 || timer: 0.0972 sec.
iter 245240 || Loss: 1.0287 || timer: 0.0923 sec.
iter 245250 || Loss: 1.1538 || timer: 0.0913 sec.
iter 245260 || Loss: 0.8584 || timer: 0.0802 sec.
iter 245270 || Loss: 0.9168 || timer: 0.0920 sec.
iter 245280 || Loss: 1.1333 || timer: 0.0935 sec.
iter 245290 || Loss: 1.1398 || timer: 0.0803 sec.
iter 245300 || Loss: 0.8351 || timer: 0.1306 sec.
iter 245310 || Loss: 0.9662 || timer: 0.0833 sec.
iter 245320 || Loss: 1.0638 || timer: 0.0803 sec.
iter 245330 || Loss: 1.0415 || timer: 0.0811 sec.
iter 245340 || Loss: 0.9195 || timer: 0.0866 sec.
iter 245350 || Loss: 0.6908 || timer: 0.0931 sec.
iter 245360 || Loss: 0.6933 || timer: 0.0885 sec.
iter 245370 || Loss: 0.8859 || timer: 0.0898 sec.
iter 245380 || Loss: 0.8823 || timer: 0.0819 sec.
iter 245390 || Loss: 0.7788 || timer: 0.0965 sec.
iter 245400 || Loss: 1.1719 || timer: 0.0826 sec.
iter 245410 || Loss: 0.7504 || timer: 0.0227 sec.
iter 245420 || Loss: 0.5439 || timer: 0.0834 sec.
iter 245430 || Loss: 1.0151 || timer: 0.1087 sec.
iter 245440 || Loss: 1.0167 || timer: 0.0825 sec.
iter 245450 || Loss: 0.7044 || timer: 0.0891 sec.
iter 245460 || Loss: 0.9909 || timer: 0.1010 sec.
iter 245470 || Loss: 0.6487 || timer: 0.0842 sec.
iter 245480 || Loss: 1.0909 || timer: 0.0865 sec.
iter 245490 || Loss: 0.9741 || timer: 0.0873 sec.
iter 245500 || Loss: 0.8035 || timer: 0.0813 sec.
iter 245510 || Loss: 0.7897 || timer: 0.1390 sec.
iter 245520 || Loss: 0.7361 || timer: 0.0822 sec.
iter 245530 || Loss: 1.1435 || timer: 0.0916 sec.
iter 245540 || Loss: 0.6696 || timer: 0.0905 sec.
iter 245550 || Loss: 0.6329 || timer: 0.0893 sec.
iter 245560 || Loss: 0.9587 || timer: 0.0906 sec.
iter 245570 || Loss: 0.6711 || timer: 0.0808 sec.
iter 245580 || Loss: 0.9982 || timer: 0.0895 sec.
iter 245590 || Loss: 0.5580 || timer: 0.0932 sec.
iter 245600 || Loss: 0.6665 || timer: 0.0877 sec.
iter 245610 || Loss: 0.7581 || timer: 0.0833 sec.
iter 245620 || Loss: 0.8502 || timer: 0.0976 sec.
iter 245630 || Loss: 0.7108 || timer: 0.0836 sec.
iter 245640 || Loss: 0.7716 || timer: 0.0895 sec.
iter 245650 || Loss: 1.2114 || timer: 0.0905 sec.
iter 245660 || Loss: 0.7379 || timer: 0.0832 sec.
iter 245670 || Loss: 0.8836 || timer: 0.0961 sec.
iter 245680 || Loss: 0.8041 || timer: 0.0811 sec.
iter 245690 || Loss: 0.8748 || timer: 0.1095 sec.
iter 245700 || Loss: 0.8507 || timer: 0.0811 sec.
iter 245710 || Loss: 0.7079 || timer: 0.0901 sec.
iter 245720 || Loss: 1.0475 || timer: 0.0908 sec.
iter 245730 || Loss: 0.7797 || timer: 0.0864 sec.
iter 245740 || Loss: 0.7728 || timer: 0.0197 sec.
iter 245750 || Loss: 1.2159 || timer: 0.0836 sec.
iter 245760 || Loss: 1.0017 || timer: 0.0822 sec.
iter 245770 || Loss: 0.8783 || timer: 0.0906 sec.
iter 245780 || Loss: 0.9989 || timer: 0.0900 sec.
iter 245790 || Loss: 0.9821 || timer: 0.1205 sec.
iter 245800 || Loss: 0.9543 || timer: 0.0871 sec.
iter 245810 || Loss: 0.8123 || timer: 0.1076 sec.
iter 245820 || Loss: 1.1039 || timer: 0.0826 sec.
iter 245830 || Loss: 0.8807 || timer: 0.1170 sec.
iter 245840 || Loss: 0.8472 || timer: 0.1140 sec.
iter 245850 || Loss: 0.7931 || timer: 0.0979 sec.
iter 245860 || Loss: 0.9071 || timer: 0.1099 sec.
iter 245870 || Loss: 0.7803 || timer: 0.0889 sec.
iter 245880 || Loss: 0.8772 || timer: 0.0883 sec.
iter 245890 || Loss: 0.8109 || timer: 0.1001 sec.
iter 245900 || Loss: 0.8410 || timer: 0.0809 sec.
iter 245910 || Loss: 1.1133 || timer: 0.0887 sec.
iter 245920 || Loss: 0.8442 || timer: 0.0818 sec.
iter 245930 || Loss: 0.7811 || timer: 0.0951 sec.
iter 245940 || Loss: 0.6968 || timer: 0.0922 sec.
iter 245950 || Loss: 1.0531 || timer: 0.0912 sec.
iter 245960 || Loss: 1.0973 || timer: 0.0897 sec.
iter 245970 || Loss: 0.8760 || timer: 0.0799 sec.
iter 245980 || Loss: 0.8964 || timer: 0.0898 sec.
iter 245990 || Loss: 0.6471 || timer: 0.1095 sec.
iter 246000 || Loss: 0.9148 || timer: 0.0912 sec.
iter 246010 || Loss: 1.2009 || timer: 0.0823 sec.
iter 246020 || Loss: 0.6986 || timer: 0.0820 sec.
iter 246030 || Loss: 0.8392 || timer: 0.0916 sec.
iter 246040 || Loss: 0.7636 || timer: 0.0888 sec.
iter 246050 || Loss: 0.9445 || timer: 0.0892 sec.
iter 246060 || Loss: 0.7435 || timer: 0.0803 sec.
iter 246070 || Loss: 0.7341 || timer: 0.0207 sec.
iter 246080 || Loss: 1.2271 || timer: 0.0957 sec.
iter 246090 || Loss: 0.7932 || timer: 0.0894 sec.
iter 246100 || Loss: 0.6320 || timer: 0.0893 sec.
iter 246110 || Loss: 1.2317 || timer: 0.1015 sec.
iter 246120 || Loss: 0.6726 || timer: 0.0890 sec.
iter 246130 || Loss: 0.9840 || timer: 0.0877 sec.
iter 246140 || Loss: 0.7214 || timer: 0.0826 sec.
iter 246150 || Loss: 0.8133 || timer: 0.0892 sec.
iter 246160 || Loss: 0.7758 || timer: 0.0909 sec.
iter 246170 || Loss: 0.7822 || timer: 0.1133 sec.
iter 246180 || Loss: 0.8612 || timer: 0.0876 sec.
iter 246190 || Loss: 0.8301 || timer: 0.0910 sec.
iter 246200 || Loss: 0.8791 || timer: 0.0996 sec.
iter 246210 || Loss: 0.8110 || timer: 0.0943 sec.
iter 246220 || Loss: 1.0152 || timer: 0.0889 sec.
iter 246230 || Loss: 0.7075 || timer: 0.0838 sec.
iter 246240 || Loss: 1.0015 || timer: 0.0866 sec.
iter 246250 || Loss: 0.9247 || timer: 0.0858 sec.
iter 246260 || Loss: 1.0545 || timer: 0.1133 sec.
iter 246270 || Loss: 0.9362 || timer: 0.0866 sec.
iter 246280 || Loss: 0.9347 || timer: 0.0869 sec.
iter 246290 || Loss: 0.9874 || timer: 0.0914 sec.
iter 246300 || Loss: 0.8129 || timer: 0.0887 sec.
iter 246310 || Loss: 0.9340 || timer: 0.0838 sec.
iter 246320 || Loss: 1.3623 || timer: 0.0907 sec.
iter 246330 || Loss: 0.9860 || timer: 0.1088 sec.
iter 246340 || Loss: 1.0685 || timer: 0.1077 sec.
iter 246350 || Loss: 0.9227 || timer: 0.0900 sec.
iter 246360 || Loss: 0.8990 || timer: 0.1013 sec.
iter 246370 || Loss: 0.8205 || timer: 0.0866 sec.
iter 246380 || Loss: 0.6992 || timer: 0.0893 sec.
iter 246390 || Loss: 1.1638 || timer: 0.1024 sec.
iter 246400 || Loss: 0.9215 || timer: 0.0235 sec.
iter 246410 || Loss: 1.9801 || timer: 0.0818 sec.
iter 246420 || Loss: 1.1761 || timer: 0.0810 sec.
iter 246430 || Loss: 0.7276 || timer: 0.0891 sec.
iter 246440 || Loss: 0.6790 || timer: 0.0820 sec.
iter 246450 || Loss: 0.9407 || timer: 0.1091 sec.
iter 246460 || Loss: 0.6985 || timer: 0.0942 sec.
iter 246470 || Loss: 0.8914 || timer: 0.0890 sec.
iter 246480 || Loss: 0.8237 || timer: 0.0814 sec.
iter 246490 || Loss: 0.6715 || timer: 0.0911 sec.
iter 246500 || Loss: 0.9066 || timer: 0.1086 sec.
iter 246510 || Loss: 0.8259 || timer: 0.1120 sec.
iter 246520 || Loss: 0.8330 || timer: 0.0808 sec.
iter 246530 || Loss: 1.0925 || timer: 0.0897 sec.
iter 246540 || Loss: 0.9804 || timer: 0.0892 sec.
iter 246550 || Loss: 0.9036 || timer: 0.0816 sec.
iter 246560 || Loss: 0.9082 || timer: 0.0898 sec.
iter 246570 || Loss: 0.8351 || timer: 0.0969 sec.
iter 246580 || Loss: 0.9364 || timer: 0.0986 sec.
iter 246590 || Loss: 0.8072 || timer: 0.0889 sec.
iter 246600 || Loss: 0.6667 || timer: 0.1124 sec.
iter 246610 || Loss: 0.7737 || timer: 0.0775 sec.
iter 246620 || Loss: 1.0116 || timer: 0.0992 sec.
iter 246630 || Loss: 0.8797 || timer: 0.0886 sec.
iter 246640 || Loss: 0.8632 || timer: 0.0816 sec.
iter 246650 || Loss: 1.3609 || timer: 0.0853 sec.
iter 246660 || Loss: 0.8755 || timer: 0.0985 sec.
iter 246670 || Loss: 0.8938 || timer: 0.0900 sec.
iter 246680 || Loss: 0.9680 || timer: 0.0817 sec.
iter 246690 || Loss: 0.7478 || timer: 0.0887 sec.
iter 246700 || Loss: 0.6644 || timer: 0.0818 sec.
iter 246710 || Loss: 0.8759 || timer: 0.0908 sec.
iter 246720 || Loss: 0.7895 || timer: 0.0809 sec.
iter 246730 || Loss: 0.8304 || timer: 0.0236 sec.
iter 246740 || Loss: 2.2979 || timer: 0.0910 sec.
iter 246750 || Loss: 0.7806 || timer: 0.1100 sec.
iter 246760 || Loss: 0.7625 || timer: 0.0818 sec.
iter 246770 || Loss: 1.0120 || timer: 0.0808 sec.
iter 246780 || Loss: 1.0191 || timer: 0.0858 sec.
iter 246790 || Loss: 0.9015 || timer: 0.0826 sec.
iter 246800 || Loss: 1.3099 || timer: 0.0911 sec.
iter 246810 || Loss: 0.5693 || timer: 0.1000 sec.
iter 246820 || Loss: 0.8463 || timer: 0.0884 sec.
iter 246830 || Loss: 0.7301 || timer: 0.1010 sec.
iter 246840 || Loss: 1.0878 || timer: 0.0884 sec.
iter 246850 || Loss: 0.7371 || timer: 0.0860 sec.
iter 246860 || Loss: 1.1805 || timer: 0.0894 sec.
iter 246870 || Loss: 0.9461 || timer: 0.1026 sec.
iter 246880 || Loss: 0.9575 || timer: 0.0815 sec.
iter 246890 || Loss: 0.7350 || timer: 0.0899 sec.
iter 246900 || Loss: 0.8910 || timer: 0.0896 sec.
iter 246910 || Loss: 1.2966 || timer: 0.0977 sec.
iter 246920 || Loss: 0.7544 || timer: 0.0867 sec.
iter 246930 || Loss: 0.6250 || timer: 0.0823 sec.
iter 246940 || Loss: 0.6548 || timer: 0.1107 sec.
iter 246950 || Loss: 0.9619 || timer: 0.1081 sec.
iter 246960 || Loss: 0.9786 || timer: 0.0812 sec.
iter 246970 || Loss: 0.8122 || timer: 0.0903 sec.
iter 246980 || Loss: 1.1262 || timer: 0.0891 sec.
iter 246990 || Loss: 1.1957 || timer: 0.0898 sec.
iter 247000 || Loss: 0.9050 || timer: 0.0807 sec.
iter 247010 || Loss: 1.2605 || timer: 0.0818 sec.
iter 247020 || Loss: 1.4111 || timer: 0.0920 sec.
iter 247030 || Loss: 1.0919 || timer: 0.0899 sec.
iter 247040 || Loss: 1.0118 || timer: 0.1015 sec.
iter 247050 || Loss: 0.7689 || timer: 0.0995 sec.
iter 247060 || Loss: 0.9163 || timer: 0.0244 sec.
iter 247070 || Loss: 0.3112 || timer: 0.0898 sec.
iter 247080 || Loss: 0.8331 || timer: 0.0885 sec.
iter 247090 || Loss: 0.9250 || timer: 0.0864 sec.
iter 247100 || Loss: 0.8504 || timer: 0.0813 sec.
iter 247110 || Loss: 0.7629 || timer: 0.0847 sec.
iter 247120 || Loss: 1.0905 || timer: 0.1027 sec.
iter 247130 || Loss: 0.9227 || timer: 0.0820 sec.
iter 247140 || Loss: 1.1834 || timer: 0.0889 sec.
iter 247150 || Loss: 0.8712 || timer: 0.0953 sec.
iter 247160 || Loss: 1.2680 || timer: 0.0970 sec.
iter 247170 || Loss: 0.8404 || timer: 0.1017 sec.
iter 247180 || Loss: 0.9249 || timer: 0.1003 sec.
iter 247190 || Loss: 1.0009 || timer: 0.0826 sec.
iter 247200 || Loss: 1.3597 || timer: 0.0831 sec.
iter 247210 || Loss: 0.9256 || timer: 0.0871 sec.
iter 247220 || Loss: 0.9794 || timer: 0.0812 sec.
iter 247230 || Loss: 0.5591 || timer: 0.0898 sec.
iter 247240 || Loss: 2.0487 || timer: 0.0809 sec.
iter 247250 || Loss: 0.8906 || timer: 0.0894 sec.
iter 247260 || Loss: 1.4328 || timer: 0.0905 sec.
iter 247270 || Loss: 1.0488 || timer: 0.0803 sec.
iter 247280 || Loss: 1.0438 || timer: 0.0887 sec.
iter 247290 || Loss: 1.0219 || timer: 0.0818 sec.
iter 247300 || Loss: 1.0060 || timer: 0.0810 sec.
iter 247310 || Loss: 0.7161 || timer: 0.0931 sec.
iter 247320 || Loss: 1.2081 || timer: 0.0813 sec.
iter 247330 || Loss: 1.1551 || timer: 0.1050 sec.
iter 247340 || Loss: 0.8807 || timer: 0.0826 sec.
iter 247350 || Loss: 1.1482 || timer: 0.0919 sec.
iter 247360 || Loss: 0.9350 || timer: 0.0886 sec.
iter 247370 || Loss: 0.7271 || timer: 0.0932 sec.
iter 247380 || Loss: 0.9240 || timer: 0.1019 sec.
iter 247390 || Loss: 0.9499 || timer: 0.0166 sec.
iter 247400 || Loss: 0.6139 || timer: 0.0909 sec.
iter 247410 || Loss: 1.1680 || timer: 0.0829 sec.
iter 247420 || Loss: 0.7416 || timer: 0.0797 sec.
iter 247430 || Loss: 0.8693 || timer: 0.0808 sec.
iter 247440 || Loss: 0.6793 || timer: 0.1021 sec.
iter 247450 || Loss: 0.8751 || timer: 0.0864 sec.
iter 247460 || Loss: 0.7988 || timer: 0.0853 sec.
iter 247470 || Loss: 0.9337 || timer: 0.0808 sec.
iter 247480 || Loss: 0.7356 || timer: 0.0917 sec.
iter 247490 || Loss: 1.1581 || timer: 0.1100 sec.
iter 247500 || Loss: 1.0263 || timer: 0.0914 sec.
iter 247510 || Loss: 0.6998 || timer: 0.0886 sec.
iter 247520 || Loss: 1.0194 || timer: 0.0888 sec.
iter 247530 || Loss: 1.0211 || timer: 0.1032 sec.
iter 247540 || Loss: 0.7627 || timer: 0.0882 sec.
iter 247550 || Loss: 1.4368 || timer: 0.0879 sec.
iter 247560 || Loss: 0.8310 || timer: 0.0918 sec.
iter 247570 || Loss: 0.7192 || timer: 0.1108 sec.
iter 247580 || Loss: 0.8550 || timer: 0.0924 sec.
iter 247590 || Loss: 0.7387 || timer: 0.0924 sec.
iter 247600 || Loss: 0.8393 || timer: 0.0932 sec.
iter 247610 || Loss: 0.8989 || timer: 0.0896 sec.
iter 247620 || Loss: 0.6869 || timer: 0.0895 sec.
iter 247630 || Loss: 0.7543 || timer: 0.0876 sec.
iter 247640 || Loss: 1.0081 || timer: 0.0913 sec.
iter 247650 || Loss: 1.1925 || timer: 0.0891 sec.
iter 247660 || Loss: 0.7742 || timer: 0.0891 sec.
iter 247670 || Loss: 0.6830 || timer: 0.0885 sec.
iter 247680 || Loss: 1.0091 || timer: 0.0866 sec.
iter 247690 || Loss: 0.9313 || timer: 0.0903 sec.
iter 247700 || Loss: 1.0531 || timer: 0.0882 sec.
iter 247710 || Loss: 1.1413 || timer: 0.0880 sec.
iter 247720 || Loss: 0.7272 || timer: 0.0233 sec.
iter 247730 || Loss: 0.3243 || timer: 0.0870 sec.
iter 247740 || Loss: 0.9149 || timer: 0.1125 sec.
iter 247750 || Loss: 0.6743 || timer: 0.0813 sec.
iter 247760 || Loss: 1.0436 || timer: 0.0846 sec.
iter 247770 || Loss: 0.9897 || timer: 0.0842 sec.
iter 247780 || Loss: 0.8532 || timer: 0.0910 sec.
iter 247790 || Loss: 0.9014 || timer: 0.0751 sec.
iter 247800 || Loss: 0.8212 || timer: 0.0829 sec.
iter 247810 || Loss: 1.1474 || timer: 0.0895 sec.
iter 247820 || Loss: 0.7049 || timer: 0.0957 sec.
iter 247830 || Loss: 0.8196 || timer: 0.0813 sec.
iter 247840 || Loss: 0.8759 || timer: 0.0943 sec.
iter 247850 || Loss: 0.8780 || timer: 0.0917 sec.
iter 247860 || Loss: 0.7547 || timer: 0.0876 sec.
iter 247870 || Loss: 1.0893 || timer: 0.0973 sec.
iter 247880 || Loss: 0.7908 || timer: 0.1168 sec.
iter 247890 || Loss: 0.9754 || timer: 0.0967 sec.
iter 247900 || Loss: 0.6601 || timer: 0.0897 sec.
iter 247910 || Loss: 0.9743 || timer: 0.0818 sec.
iter 247920 || Loss: 0.8338 || timer: 0.0844 sec.
iter 247930 || Loss: 0.6117 || timer: 0.0821 sec.
iter 247940 || Loss: 0.6527 || timer: 0.1012 sec.
iter 247950 || Loss: 0.9146 || timer: 0.0906 sec.
iter 247960 || Loss: 0.8093 || timer: 0.0982 sec.
iter 247970 || Loss: 0.9297 || timer: 0.1076 sec.
iter 247980 || Loss: 0.9085 || timer: 0.0849 sec.
iter 247990 || Loss: 0.6159 || timer: 0.0901 sec.
iter 248000 || Loss: 0.9742 || timer: 0.0869 sec.
iter 248010 || Loss: 0.9690 || timer: 0.0815 sec.
iter 248020 || Loss: 1.0053 || timer: 0.0894 sec.
iter 248030 || Loss: 1.0924 || timer: 0.0817 sec.
iter 248040 || Loss: 0.8299 || timer: 0.0830 sec.
iter 248050 || Loss: 0.7767 || timer: 0.0234 sec.
iter 248060 || Loss: 0.5940 || timer: 0.0878 sec.
iter 248070 || Loss: 0.8231 || timer: 0.0995 sec.
iter 248080 || Loss: 1.0399 || timer: 0.0951 sec.
iter 248090 || Loss: 0.6505 || timer: 0.1010 sec.
iter 248100 || Loss: 0.8766 || timer: 0.1197 sec.
iter 248110 || Loss: 0.7806 || timer: 0.1133 sec.
iter 248120 || Loss: 0.8115 || timer: 0.0909 sec.
iter 248130 || Loss: 0.8970 || timer: 0.0882 sec.
iter 248140 || Loss: 0.8223 || timer: 0.0905 sec.
iter 248150 || Loss: 1.0864 || timer: 0.1192 sec.
iter 248160 || Loss: 0.8300 || timer: 0.1046 sec.
iter 248170 || Loss: 0.8666 || timer: 0.0823 sec.
iter 248180 || Loss: 1.0356 || timer: 0.0909 sec.
iter 248190 || Loss: 0.8645 || timer: 0.0810 sec.
iter 248200 || Loss: 1.1328 || timer: 0.1063 sec.
iter 248210 || Loss: 0.7396 || timer: 0.1055 sec.
iter 248220 || Loss: 0.9041 || timer: 0.0858 sec.
iter 248230 || Loss: 0.8419 || timer: 0.0813 sec.
iter 248240 || Loss: 0.9407 || timer: 0.0820 sec.
iter 248250 || Loss: 1.1605 || timer: 0.0927 sec.
iter 248260 || Loss: 0.8689 || timer: 0.0848 sec.
iter 248270 || Loss: 0.8612 || timer: 0.0819 sec.
iter 248280 || Loss: 0.8782 || timer: 0.0816 sec.
iter 248290 || Loss: 1.1605 || timer: 0.0904 sec.
iter 248300 || Loss: 0.8548 || timer: 0.0924 sec.
iter 248310 || Loss: 1.2263 || timer: 0.1001 sec.
iter 248320 || Loss: 0.6205 || timer: 0.0945 sec.
iter 248330 || Loss: 1.3437 || timer: 0.0918 sec.
iter 248340 || Loss: 0.8835 || timer: 0.1115 sec.
iter 248350 || Loss: 0.8135 || timer: 0.0816 sec.
iter 248360 || Loss: 1.2207 || timer: 0.0897 sec.
iter 248370 || Loss: 0.8195 || timer: 0.1015 sec.
iter 248380 || Loss: 0.8348 || timer: 0.0164 sec.
iter 248390 || Loss: 0.3432 || timer: 0.0824 sec.
iter 248400 || Loss: 1.0623 || timer: 0.0835 sec.
iter 248410 || Loss: 0.9518 || timer: 0.0906 sec.
iter 248420 || Loss: 0.6995 || timer: 0.0903 sec.
iter 248430 || Loss: 0.8884 || timer: 0.0826 sec.
iter 248440 || Loss: 0.7170 || timer: 0.0892 sec.
iter 248450 || Loss: 0.8392 || timer: 0.1026 sec.
iter 248460 || Loss: 0.6409 || timer: 0.0826 sec.
iter 248470 || Loss: 0.8749 || timer: 0.0908 sec.
iter 248480 || Loss: 0.8254 || timer: 0.1242 sec.
iter 248490 || Loss: 1.3182 || timer: 0.0836 sec.
iter 248500 || Loss: 1.1445 || timer: 0.1051 sec.
iter 248510 || Loss: 1.6803 || timer: 0.0922 sec.
iter 248520 || Loss: 1.3547 || timer: 0.1211 sec.
iter 248530 || Loss: 0.9175 || timer: 0.0938 sec.
iter 248540 || Loss: 0.9505 || timer: 0.0843 sec.
iter 248550 || Loss: 0.9477 || timer: 0.0890 sec.
iter 248560 || Loss: 1.3769 || timer: 0.0888 sec.
iter 248570 || Loss: 0.7348 || timer: 0.0922 sec.
iter 248580 || Loss: 1.0251 || timer: 0.0885 sec.
iter 248590 || Loss: 1.1262 || timer: 0.1074 sec.
iter 248600 || Loss: 1.0403 || timer: 0.0886 sec.
iter 248610 || Loss: 1.0307 || timer: 0.0859 sec.
iter 248620 || Loss: 0.7803 || timer: 0.0891 sec.
iter 248630 || Loss: 0.9699 || timer: 0.0852 sec.
iter 248640 || Loss: 0.7535 || timer: 0.0870 sec.
iter 248650 || Loss: 0.7419 || timer: 0.0814 sec.
iter 248660 || Loss: 0.7361 || timer: 0.0893 sec.
iter 248670 || Loss: 0.8065 || timer: 0.1103 sec.
iter 248680 || Loss: 0.7461 || timer: 0.0806 sec.
iter 248690 || Loss: 0.7139 || timer: 0.1221 sec.
iter 248700 || Loss: 0.8200 || timer: 0.0814 sec.
iter 248710 || Loss: 0.8844 || timer: 0.0263 sec.
iter 248720 || Loss: 0.8836 || timer: 0.0888 sec.
iter 248730 || Loss: 0.9186 || timer: 0.0886 sec.
iter 248740 || Loss: 0.8843 || timer: 0.0867 sec.
iter 248750 || Loss: 1.2769 || timer: 0.1105 sec.
iter 248760 || Loss: 1.0019 || timer: 0.0898 sec.
iter 248770 || Loss: 0.6807 || timer: 0.0856 sec.
iter 248780 || Loss: 0.7915 || timer: 0.0880 sec.
iter 248790 || Loss: 0.6700 || timer: 0.0805 sec.
iter 248800 || Loss: 0.7511 || timer: 0.0840 sec.
iter 248810 || Loss: 0.9321 || timer: 0.1425 sec.
iter 248820 || Loss: 0.9602 || timer: 0.1080 sec.
iter 248830 || Loss: 0.8589 || timer: 0.1044 sec.
iter 248840 || Loss: 0.9176 || timer: 0.0800 sec.
iter 248850 || Loss: 1.2052 || timer: 0.0978 sec.
iter 248860 || Loss: 0.9846 || timer: 0.0895 sec.
iter 248870 || Loss: 1.0734 || timer: 0.0843 sec.
iter 248880 || Loss: 0.8623 || timer: 0.0813 sec.
iter 248890 || Loss: 0.7993 || timer: 0.0909 sec.
iter 248900 || Loss: 1.0220 || timer: 0.0848 sec.
iter 248910 || Loss: 1.1369 || timer: 0.0935 sec.
iter 248920 || Loss: 0.9354 || timer: 0.0865 sec.
iter 248930 || Loss: 0.8133 || timer: 0.1058 sec.
iter 248940 || Loss: 0.8887 || timer: 0.0808 sec.
iter 248950 || Loss: 0.7692 || timer: 0.0957 sec.
iter 248960 || Loss: 0.7808 || timer: 0.1060 sec.
iter 248970 || Loss: 0.8410 || timer: 0.0910 sec.
iter 248980 || Loss: 0.7816 || timer: 0.0814 sec.
iter 248990 || Loss: 1.0953 || timer: 0.0860 sec.
iter 249000 || Loss: 0.8121 || timer: 0.0802 sec.
iter 249010 || Loss: 0.7657 || timer: 0.0744 sec.
iter 249020 || Loss: 0.6218 || timer: 0.1022 sec.
iter 249030 || Loss: 0.8370 || timer: 0.0896 sec.
iter 249040 || Loss: 0.9435 || timer: 0.0223 sec.
iter 249050 || Loss: 0.5031 || timer: 0.0914 sec.
iter 249060 || Loss: 0.9477 || timer: 0.0821 sec.
iter 249070 || Loss: 0.5648 || timer: 0.0821 sec.
iter 249080 || Loss: 0.8363 || timer: 0.0970 sec.
iter 249090 || Loss: 0.7534 || timer: 0.0799 sec.
iter 249100 || Loss: 0.6934 || timer: 0.0977 sec.
iter 249110 || Loss: 0.7359 || timer: 0.0814 sec.
iter 249120 || Loss: 0.8172 || timer: 0.0816 sec.
iter 249130 || Loss: 0.9011 || timer: 0.0928 sec.
iter 249140 || Loss: 1.3663 || timer: 0.1145 sec.
iter 249150 || Loss: 0.9215 || timer: 0.0897 sec.
iter 249160 || Loss: 0.6823 || timer: 0.0880 sec.
iter 249170 || Loss: 1.0006 || timer: 0.0863 sec.
iter 249180 || Loss: 0.6724 || timer: 0.0918 sec.
iter 249190 || Loss: 0.9175 || timer: 0.0888 sec.
iter 249200 || Loss: 0.9118 || timer: 0.0808 sec.
iter 249210 || Loss: 1.0313 || timer: 0.0806 sec.
iter 249220 || Loss: 0.8262 || timer: 0.0811 sec.
iter 249230 || Loss: 0.9264 || timer: 0.0935 sec.
iter 249240 || Loss: 0.7561 || timer: 0.1115 sec.
iter 249250 || Loss: 0.9223 || timer: 0.0813 sec.
iter 249260 || Loss: 0.6112 || timer: 0.0909 sec.
iter 249270 || Loss: 0.7171 || timer: 0.0824 sec.
iter 249280 || Loss: 0.8729 || timer: 0.0881 sec.
iter 249290 || Loss: 1.2326 || timer: 0.0915 sec.
iter 249300 || Loss: 0.9354 || timer: 0.0815 sec.
iter 249310 || Loss: 0.9553 || timer: 0.0893 sec.
iter 249320 || Loss: 1.0035 || timer: 0.0841 sec.
iter 249330 || Loss: 1.0415 || timer: 0.1086 sec.
iter 249340 || Loss: 0.7476 || timer: 0.0968 sec.
iter 249350 || Loss: 0.7757 || timer: 0.0815 sec.
iter 249360 || Loss: 0.8090 || timer: 0.1051 sec.
iter 249370 || Loss: 1.1053 || timer: 0.0232 sec.
iter 249380 || Loss: 0.4943 || timer: 0.0817 sec.
iter 249390 || Loss: 1.0645 || timer: 0.0880 sec.
iter 249400 || Loss: 0.6116 || timer: 0.0871 sec.
iter 249410 || Loss: 0.7711 || timer: 0.0902 sec.
iter 249420 || Loss: 1.1118 || timer: 0.0875 sec.
iter 249430 || Loss: 0.6648 || timer: 0.1119 sec.
iter 249440 || Loss: 1.1097 || timer: 0.0887 sec.
iter 249450 || Loss: 1.1283 || timer: 0.0928 sec.
iter 249460 || Loss: 1.1429 || timer: 0.0835 sec.
iter 249470 || Loss: 1.0815 || timer: 0.0945 sec.
iter 249480 || Loss: 0.8966 || timer: 0.0906 sec.
iter 249490 || Loss: 0.7341 || timer: 0.1057 sec.
iter 249500 || Loss: 0.6986 || timer: 0.0909 sec.
iter 249510 || Loss: 1.0014 || timer: 0.0887 sec.
iter 249520 || Loss: 0.7076 || timer: 0.1065 sec.
iter 249530 || Loss: 1.1535 || timer: 0.0926 sec.
iter 249540 || Loss: 0.8466 || timer: 0.1043 sec.
iter 249550 || Loss: 0.8537 || timer: 0.1123 sec.
iter 249560 || Loss: 0.6948 || timer: 0.0837 sec.
iter 249570 || Loss: 0.8332 || timer: 0.0835 sec.
iter 249580 || Loss: 0.6475 || timer: 0.0837 sec.
iter 249590 || Loss: 0.8834 || timer: 0.0885 sec.
iter 249600 || Loss: 0.8803 || timer: 0.0847 sec.
iter 249610 || Loss: 0.8253 || timer: 0.0906 sec.
iter 249620 || Loss: 0.8747 || timer: 0.0915 sec.
iter 249630 || Loss: 0.8115 || timer: 0.0873 sec.
iter 249640 || Loss: 0.7571 || timer: 0.0912 sec.
iter 249650 || Loss: 1.1443 || timer: 0.0923 sec.
iter 249660 || Loss: 1.1065 || timer: 0.0920 sec.
iter 249670 || Loss: 1.0173 || timer: 0.0914 sec.
iter 249680 || Loss: 1.0451 || timer: 0.1020 sec.
iter 249690 || Loss: 0.6850 || timer: 0.0898 sec.
iter 249700 || Loss: 0.6432 || timer: 0.0291 sec.
iter 249710 || Loss: 1.1523 || timer: 0.0936 sec.
iter 249720 || Loss: 1.1434 || timer: 0.1025 sec.
iter 249730 || Loss: 1.0313 || timer: 0.0877 sec.
iter 249740 || Loss: 1.2646 || timer: 0.0841 sec.
iter 249750 || Loss: 0.7176 || timer: 0.0865 sec.
iter 249760 || Loss: 0.7594 || timer: 0.0844 sec.
iter 249770 || Loss: 1.1064 || timer: 0.0849 sec.
iter 249780 || Loss: 0.9151 || timer: 0.0900 sec.
iter 249790 || Loss: 0.8447 || timer: 0.1058 sec.
iter 249800 || Loss: 0.7218 || timer: 0.1107 sec.
iter 249810 || Loss: 1.0389 || timer: 0.1083 sec.
iter 249820 || Loss: 0.7564 || timer: 0.0844 sec.
iter 249830 || Loss: 0.9956 || timer: 0.0834 sec.
iter 249840 || Loss: 0.9434 || timer: 0.0914 sec.
iter 249850 || Loss: 1.1197 || timer: 0.0885 sec.
iter 249860 || Loss: 1.0360 || timer: 0.1109 sec.
iter 249870 || Loss: 0.8268 || timer: 0.0903 sec.
iter 249880 || Loss: 1.0315 || timer: 0.0822 sec.
iter 249890 || Loss: 0.6447 || timer: 0.0996 sec.
iter 249900 || Loss: 0.8774 || timer: 0.0828 sec.
iter 249910 || Loss: 0.9788 || timer: 0.1059 sec.
iter 249920 || Loss: 0.7687 || timer: 0.1002 sec.
iter 249930 || Loss: 0.9310 || timer: 0.0880 sec.
iter 249940 || Loss: 0.8488 || timer: 0.1163 sec.
iter 249950 || Loss: 0.9479 || timer: 0.0820 sec.
iter 249960 || Loss: 0.7526 || timer: 0.0967 sec.
iter 249970 || Loss: 0.5935 || timer: 0.0843 sec.
iter 249980 || Loss: 0.5703 || timer: 0.0808 sec.
iter 249990 || Loss: 0.5927 || timer: 0.0900 sec.
iter 250000 || Loss: 0.7017 || Saving state, iter: 250000
timer: 0.0901 sec.
iter 250010 || Loss: 0.7226 || timer: 0.0883 sec.
iter 250020 || Loss: 1.2812 || timer: 0.0916 sec.
iter 250030 || Loss: 0.8615 || timer: 0.0157 sec.
iter 250040 || Loss: 2.3155 || timer: 0.0819 sec.
iter 250050 || Loss: 0.7697 || timer: 0.1035 sec.
iter 250060 || Loss: 0.8251 || timer: 0.0911 sec.
iter 250070 || Loss: 0.7554 || timer: 0.0896 sec.
iter 250080 || Loss: 0.9950 || timer: 0.0911 sec.
iter 250090 || Loss: 0.8731 || timer: 0.0886 sec.
iter 250100 || Loss: 0.7329 || timer: 0.1096 sec.
iter 250110 || Loss: 0.7931 || timer: 0.0878 sec.
iter 250120 || Loss: 1.2581 || timer: 0.0821 sec.
iter 250130 || Loss: 0.8520 || timer: 0.0994 sec.
iter 250140 || Loss: 0.5599 || timer: 0.0820 sec.
iter 250150 || Loss: 0.8338 || timer: 0.0826 sec.
iter 250160 || Loss: 0.9703 || timer: 0.1042 sec.
iter 250170 || Loss: 0.7733 || timer: 0.0922 sec.
iter 250180 || Loss: 0.9942 || timer: 0.0818 sec.
iter 250190 || Loss: 1.1139 || timer: 0.1061 sec.
iter 250200 || Loss: 0.8667 || timer: 0.0813 sec.
iter 250210 || Loss: 0.5590 || timer: 0.0896 sec.
iter 250220 || Loss: 1.1397 || timer: 0.1045 sec.
iter 250230 || Loss: 0.8008 || timer: 0.0899 sec.
iter 250240 || Loss: 0.7894 || timer: 0.1012 sec.
iter 250250 || Loss: 1.1991 || timer: 0.0808 sec.
iter 250260 || Loss: 1.0044 || timer: 0.0884 sec.
iter 250270 || Loss: 0.9347 || timer: 0.0937 sec.
iter 250280 || Loss: 0.7897 || timer: 0.0818 sec.
iter 250290 || Loss: 1.0891 || timer: 0.0814 sec.
iter 250300 || Loss: 0.9042 || timer: 0.0830 sec.
iter 250310 || Loss: 0.8230 || timer: 0.0933 sec.
iter 250320 || Loss: 0.9106 || timer: 0.1315 sec.
iter 250330 || Loss: 0.8540 || timer: 0.0819 sec.
iter 250340 || Loss: 0.6724 || timer: 0.1240 sec.
iter 250350 || Loss: 0.8903 || timer: 0.0810 sec.
iter 250360 || Loss: 0.8773 || timer: 0.0278 sec.
iter 250370 || Loss: 1.6678 || timer: 0.0830 sec.
iter 250380 || Loss: 0.8256 || timer: 0.0937 sec.
iter 250390 || Loss: 1.0105 || timer: 0.1264 sec.
iter 250400 || Loss: 0.7577 || timer: 0.0830 sec.
iter 250410 || Loss: 0.5289 || timer: 0.1096 sec.
iter 250420 || Loss: 0.8322 || timer: 0.1097 sec.
iter 250430 || Loss: 0.9430 || timer: 0.1362 sec.
iter 250440 || Loss: 0.9804 || timer: 0.0812 sec.
iter 250450 || Loss: 1.0275 || timer: 0.1033 sec.
iter 250460 || Loss: 0.5689 || timer: 0.1138 sec.
iter 250470 || Loss: 0.8370 || timer: 0.0896 sec.
iter 250480 || Loss: 0.8547 || timer: 0.0921 sec.
iter 250490 || Loss: 0.7250 || timer: 0.0819 sec.
iter 250500 || Loss: 0.8425 || timer: 0.1111 sec.
iter 250510 || Loss: 0.8850 || timer: 0.0972 sec.
iter 250520 || Loss: 0.7681 || timer: 0.0890 sec.
iter 250530 || Loss: 0.9951 || timer: 0.0914 sec.
iter 250540 || Loss: 0.7810 || timer: 0.0888 sec.
iter 250550 || Loss: 0.7409 || timer: 0.0885 sec.
iter 250560 || Loss: 0.8325 || timer: 0.0927 sec.
iter 250570 || Loss: 1.0446 || timer: 0.0912 sec.
iter 250580 || Loss: 0.7349 || timer: 0.0966 sec.
iter 250590 || Loss: 0.8069 || timer: 0.0892 sec.
iter 250600 || Loss: 1.0674 || timer: 0.0895 sec.
iter 250610 || Loss: 1.1617 || timer: 0.0870 sec.
iter 250620 || Loss: 0.9615 || timer: 0.0884 sec.
iter 250630 || Loss: 0.7165 || timer: 0.0916 sec.
iter 250640 || Loss: 0.9833 || timer: 0.0869 sec.
iter 250650 || Loss: 1.3047 || timer: 0.0936 sec.
iter 250660 || Loss: 0.8944 || timer: 0.0815 sec.
iter 250670 || Loss: 0.7125 || timer: 0.0879 sec.
iter 250680 || Loss: 0.9523 || timer: 0.0884 sec.
iter 250690 || Loss: 0.8828 || timer: 0.0241 sec.
iter 250700 || Loss: 0.4290 || timer: 0.0885 sec.
iter 250710 || Loss: 0.8957 || timer: 0.1051 sec.
iter 250720 || Loss: 0.7910 || timer: 0.0821 sec.
iter 250730 || Loss: 0.8687 || timer: 0.0817 sec.
iter 250740 || Loss: 0.8071 || timer: 0.0980 sec.
iter 250750 || Loss: 0.8935 || timer: 0.0914 sec.
iter 250760 || Loss: 1.0683 || timer: 0.0859 sec.
iter 250770 || Loss: 0.7469 || timer: 0.0868 sec.
iter 250780 || Loss: 0.9356 || timer: 0.0927 sec.
iter 250790 || Loss: 0.7539 || timer: 0.1004 sec.
iter 250800 || Loss: 0.7592 || timer: 0.0832 sec.
iter 250810 || Loss: 0.8103 || timer: 0.0877 sec.
iter 250820 || Loss: 0.9253 || timer: 0.0865 sec.
iter 250830 || Loss: 1.0150 || timer: 0.0822 sec.
iter 250840 || Loss: 0.9081 || timer: 0.0921 sec.
iter 250850 || Loss: 0.7453 || timer: 0.1028 sec.
iter 250860 || Loss: 0.9466 || timer: 0.0901 sec.
iter 250870 || Loss: 0.9323 || timer: 0.0828 sec.
iter 250880 || Loss: 1.0432 || timer: 0.0824 sec.
iter 250890 || Loss: 0.6525 || timer: 0.0906 sec.
iter 250900 || Loss: 0.7904 || timer: 0.0884 sec.
iter 250910 || Loss: 0.8680 || timer: 0.1070 sec.
iter 250920 || Loss: 0.8845 || timer: 0.0871 sec.
iter 250930 || Loss: 0.7761 || timer: 0.0939 sec.
iter 250940 || Loss: 0.8515 || timer: 0.0813 sec.
iter 250950 || Loss: 1.1337 || timer: 0.0890 sec.
iter 250960 || Loss: 0.9938 || timer: 0.0909 sec.
iter 250970 || Loss: 1.1252 || timer: 0.0853 sec.
iter 250980 || Loss: 0.6217 || timer: 0.0809 sec.
iter 250990 || Loss: 0.9907 || timer: 0.0840 sec.
iter 251000 || Loss: 0.9202 || timer: 0.0917 sec.
iter 251010 || Loss: 0.8116 || timer: 0.0901 sec.
iter 251020 || Loss: 0.9447 || timer: 0.0165 sec.
iter 251030 || Loss: 0.5141 || timer: 0.0874 sec.
iter 251040 || Loss: 0.8868 || timer: 0.0887 sec.
iter 251050 || Loss: 0.9808 || timer: 0.1047 sec.
iter 251060 || Loss: 0.8092 || timer: 0.0820 sec.
iter 251070 || Loss: 0.9638 || timer: 0.0906 sec.
iter 251080 || Loss: 0.9369 || timer: 0.0912 sec.
iter 251090 || Loss: 0.6610 || timer: 0.0816 sec.
iter 251100 || Loss: 1.0629 || timer: 0.0891 sec.
iter 251110 || Loss: 0.8121 || timer: 0.0888 sec.
iter 251120 || Loss: 0.6500 || timer: 0.1214 sec.
iter 251130 || Loss: 1.0675 || timer: 0.0886 sec.
iter 251140 || Loss: 0.8381 || timer: 0.0836 sec.
iter 251150 || Loss: 1.0436 || timer: 0.0824 sec.
iter 251160 || Loss: 0.9191 || timer: 0.0891 sec.
iter 251170 || Loss: 0.9024 || timer: 0.0853 sec.
iter 251180 || Loss: 0.8693 || timer: 0.0895 sec.
iter 251190 || Loss: 0.8326 || timer: 0.0885 sec.
iter 251200 || Loss: 0.6976 || timer: 0.0829 sec.
iter 251210 || Loss: 0.9845 || timer: 0.0887 sec.
iter 251220 || Loss: 1.0729 || timer: 0.0874 sec.
iter 251230 || Loss: 0.6764 || timer: 0.0903 sec.
iter 251240 || Loss: 0.8890 || timer: 0.0889 sec.
iter 251250 || Loss: 0.9318 || timer: 0.0892 sec.
iter 251260 || Loss: 0.9586 || timer: 0.0855 sec.
iter 251270 || Loss: 1.2202 || timer: 0.0867 sec.
iter 251280 || Loss: 0.7454 || timer: 0.0932 sec.
iter 251290 || Loss: 0.9921 || timer: 0.0811 sec.
iter 251300 || Loss: 0.8565 || timer: 0.1001 sec.
iter 251310 || Loss: 1.1102 || timer: 0.0835 sec.
iter 251320 || Loss: 0.7796 || timer: 0.0864 sec.
iter 251330 || Loss: 1.0124 || timer: 0.1283 sec.
iter 251340 || Loss: 1.1620 || timer: 0.0822 sec.
iter 251350 || Loss: 0.8596 || timer: 0.0467 sec.
iter 251360 || Loss: 2.7016 || timer: 0.0813 sec.
iter 251370 || Loss: 1.1013 || timer: 0.0908 sec.
iter 251380 || Loss: 1.0417 || timer: 0.0878 sec.
iter 251390 || Loss: 0.7192 || timer: 0.0897 sec.
iter 251400 || Loss: 0.7787 || timer: 0.0925 sec.
iter 251410 || Loss: 1.0618 || timer: 0.1022 sec.
iter 251420 || Loss: 0.9626 || timer: 0.0824 sec.
iter 251430 || Loss: 1.0251 || timer: 0.0820 sec.
iter 251440 || Loss: 0.9871 || timer: 0.0995 sec.
iter 251450 || Loss: 1.1493 || timer: 0.0993 sec.
iter 251460 || Loss: 0.9618 || timer: 0.0825 sec.
iter 251470 || Loss: 2.1527 || timer: 0.0878 sec.
iter 251480 || Loss: 1.7728 || timer: 0.0904 sec.
iter 251490 || Loss: 1.3720 || timer: 0.1041 sec.
iter 251500 || Loss: 0.9734 || timer: 0.0881 sec.
iter 251510 || Loss: 1.4370 || timer: 0.0912 sec.
iter 251520 || Loss: 1.3399 || timer: 0.0846 sec.
iter 251530 || Loss: 1.4764 || timer: 0.0822 sec.
iter 251540 || Loss: 1.1389 || timer: 0.0811 sec.
iter 251550 || Loss: 0.7853 || timer: 0.0838 sec.
iter 251560 || Loss: 0.9383 || timer: 0.0925 sec.
iter 251570 || Loss: 0.7680 || timer: 0.0921 sec.
iter 251580 || Loss: 1.4871 || timer: 0.0822 sec.
iter 251590 || Loss: 0.8369 || timer: 0.0880 sec.
iter 251600 || Loss: 0.8704 || timer: 0.0926 sec.
iter 251610 || Loss: 0.8611 || timer: 0.1042 sec.
iter 251620 || Loss: 0.9038 || timer: 0.0817 sec.
iter 251630 || Loss: 1.4617 || timer: 0.0824 sec.
iter 251640 || Loss: 0.9775 || timer: 0.0936 sec.
iter 251650 || Loss: 1.0394 || timer: 0.0921 sec.
iter 251660 || Loss: 0.5816 || timer: 0.0848 sec.
iter 251670 || Loss: 0.9259 || timer: 0.1083 sec.
iter 251680 || Loss: 1.0477 || timer: 0.0275 sec.
iter 251690 || Loss: 0.5225 || timer: 0.0905 sec.
iter 251700 || Loss: 0.8120 || timer: 0.0845 sec.
iter 251710 || Loss: 0.8085 || timer: 0.0888 sec.
iter 251720 || Loss: 1.1804 || timer: 0.0812 sec.
iter 251730 || Loss: 1.1727 || timer: 0.0926 sec.
iter 251740 || Loss: 0.8100 || timer: 0.0908 sec.
iter 251750 || Loss: 1.0669 || timer: 0.0900 sec.
iter 251760 || Loss: 0.7168 || timer: 0.0872 sec.
iter 251770 || Loss: 0.7607 || timer: 0.1072 sec.
iter 251780 || Loss: 0.7828 || timer: 0.1008 sec.
iter 251790 || Loss: 0.9175 || timer: 0.1260 sec.
iter 251800 || Loss: 0.7478 || timer: 0.0865 sec.
iter 251810 || Loss: 0.8508 || timer: 0.0917 sec.
iter 251820 || Loss: 0.6492 || timer: 0.1021 sec.
iter 251830 || Loss: 1.3221 || timer: 0.0823 sec.
iter 251840 || Loss: 1.0500 || timer: 0.0920 sec.
iter 251850 || Loss: 1.0588 || timer: 0.0811 sec.
iter 251860 || Loss: 1.0045 || timer: 0.0919 sec.
iter 251870 || Loss: 0.6304 || timer: 0.0809 sec.
iter 251880 || Loss: 0.7996 || timer: 0.0918 sec.
iter 251890 || Loss: 0.6401 || timer: 0.0837 sec.
iter 251900 || Loss: 0.9763 || timer: 0.0878 sec.
iter 251910 || Loss: 0.6600 || timer: 0.0904 sec.
iter 251920 || Loss: 0.9636 || timer: 0.0894 sec.
iter 251930 || Loss: 0.8345 || timer: 0.1073 sec.
iter 251940 || Loss: 0.7653 || timer: 0.0873 sec.
iter 251950 || Loss: 0.6858 || timer: 0.0859 sec.
iter 251960 || Loss: 0.7166 || timer: 0.0827 sec.
iter 251970 || Loss: 1.1436 || timer: 0.0959 sec.
iter 251980 || Loss: 1.0065 || timer: 0.0920 sec.
iter 251990 || Loss: 0.8678 || timer: 0.0883 sec.
iter 252000 || Loss: 1.0033 || timer: 0.0881 sec.
iter 252010 || Loss: 0.9303 || timer: 0.0217 sec.
iter 252020 || Loss: 0.2333 || timer: 0.0883 sec.
iter 252030 || Loss: 1.1331 || timer: 0.1139 sec.
iter 252040 || Loss: 0.6485 || timer: 0.0915 sec.
iter 252050 || Loss: 0.7004 || timer: 0.0854 sec.
iter 252060 || Loss: 1.0962 || timer: 0.0828 sec.
iter 252070 || Loss: 0.8620 || timer: 0.0930 sec.
iter 252080 || Loss: 0.9983 || timer: 0.0821 sec.
iter 252090 || Loss: 1.1685 || timer: 0.0890 sec.
iter 252100 || Loss: 0.9012 || timer: 0.0884 sec.
iter 252110 || Loss: 0.9595 || timer: 0.0991 sec.
iter 252120 || Loss: 0.7679 || timer: 0.0983 sec.
iter 252130 || Loss: 1.0449 || timer: 0.0880 sec.
iter 252140 || Loss: 0.6958 || timer: 0.0883 sec.
iter 252150 || Loss: 0.5712 || timer: 0.0861 sec.
iter 252160 || Loss: 0.8321 || timer: 0.0884 sec.
iter 252170 || Loss: 0.7140 || timer: 0.0809 sec.
iter 252180 || Loss: 0.7962 || timer: 0.1166 sec.
iter 252190 || Loss: 0.7596 || timer: 0.0949 sec.
iter 252200 || Loss: 1.0049 || timer: 0.0869 sec.
iter 252210 || Loss: 0.8484 || timer: 0.0926 sec.
iter 252220 || Loss: 0.7049 || timer: 0.0903 sec.
iter 252230 || Loss: 1.0057 || timer: 0.0884 sec.
iter 252240 || Loss: 0.9888 || timer: 0.0893 sec.
iter 252250 || Loss: 0.9949 || timer: 0.0814 sec.
iter 252260 || Loss: 0.6868 || timer: 0.0816 sec.
iter 252270 || Loss: 0.8387 || timer: 0.0923 sec.
iter 252280 || Loss: 0.8398 || timer: 0.0821 sec.
iter 252290 || Loss: 0.7161 || timer: 0.0826 sec.
iter 252300 || Loss: 0.8966 || timer: 0.0926 sec.
iter 252310 || Loss: 0.6727 || timer: 0.0907 sec.
iter 252320 || Loss: 0.8685 || timer: 0.0890 sec.
iter 252330 || Loss: 1.1958 || timer: 0.0909 sec.
iter 252340 || Loss: 0.7819 || timer: 0.0164 sec.
iter 252350 || Loss: 0.9046 || timer: 0.0888 sec.
iter 252360 || Loss: 0.7285 || timer: 0.0905 sec.
iter 252370 || Loss: 0.8356 || timer: 0.1029 sec.
iter 252380 || Loss: 0.7833 || timer: 0.0987 sec.
iter 252390 || Loss: 0.9518 || timer: 0.0802 sec.
iter 252400 || Loss: 0.7905 || timer: 0.0885 sec.
iter 252410 || Loss: 0.8619 || timer: 0.0824 sec.
iter 252420 || Loss: 0.8173 || timer: 0.0924 sec.
iter 252430 || Loss: 0.7411 || timer: 0.0920 sec.
iter 252440 || Loss: 0.6124 || timer: 0.1152 sec.
iter 252450 || Loss: 1.0668 || timer: 0.0916 sec.
iter 252460 || Loss: 0.9284 || timer: 0.0916 sec.
iter 252470 || Loss: 1.0239 || timer: 0.0853 sec.
iter 252480 || Loss: 0.7464 || timer: 0.0897 sec.
iter 252490 || Loss: 1.1708 || timer: 0.0915 sec.
iter 252500 || Loss: 0.9359 || timer: 0.0819 sec.
iter 252510 || Loss: 0.7072 || timer: 0.0916 sec.
iter 252520 || Loss: 0.9357 || timer: 0.0889 sec.
iter 252530 || Loss: 1.2063 || timer: 0.0903 sec.
iter 252540 || Loss: 0.8787 || timer: 0.0962 sec.
iter 252550 || Loss: 0.8445 || timer: 0.0904 sec.
iter 252560 || Loss: 0.8449 || timer: 0.0886 sec.
iter 252570 || Loss: 1.1260 || timer: 0.0888 sec.
iter 252580 || Loss: 0.8996 || timer: 0.0942 sec.
iter 252590 || Loss: 0.9718 || timer: 0.0811 sec.
iter 252600 || Loss: 0.9063 || timer: 0.0867 sec.
iter 252610 || Loss: 0.8367 || timer: 0.0803 sec.
iter 252620 || Loss: 0.5359 || timer: 0.0877 sec.
iter 252630 || Loss: 0.9941 || timer: 0.1082 sec.
iter 252640 || Loss: 1.1021 || timer: 0.0909 sec.
iter 252650 || Loss: 0.7454 || timer: 0.0904 sec.
iter 252660 || Loss: 0.9124 || timer: 0.0863 sec.
iter 252670 || Loss: 1.2237 || timer: 0.0312 sec.
iter 252680 || Loss: 0.3230 || timer: 0.0918 sec.
iter 252690 || Loss: 0.9062 || timer: 0.0891 sec.
iter 252700 || Loss: 0.6573 || timer: 0.0951 sec.
iter 252710 || Loss: 1.1104 || timer: 0.0916 sec.
iter 252720 || Loss: 0.9886 || timer: 0.0884 sec.
iter 252730 || Loss: 0.7948 || timer: 0.0911 sec.
iter 252740 || Loss: 0.8233 || timer: 0.1013 sec.
iter 252750 || Loss: 1.0570 || timer: 0.1066 sec.
iter 252760 || Loss: 0.8030 || timer: 0.0904 sec.
iter 252770 || Loss: 1.0638 || timer: 0.1069 sec.
iter 252780 || Loss: 1.0466 || timer: 0.0943 sec.
iter 252790 || Loss: 0.9432 || timer: 0.0924 sec.
iter 252800 || Loss: 1.1409 || timer: 0.0893 sec.
iter 252810 || Loss: 0.7177 || timer: 0.0924 sec.
iter 252820 || Loss: 0.9703 || timer: 0.0814 sec.
iter 252830 || Loss: 1.2275 || timer: 0.0998 sec.
iter 252840 || Loss: 0.7152 || timer: 0.1086 sec.
iter 252850 || Loss: 1.0816 || timer: 0.0819 sec.
iter 252860 || Loss: 1.4222 || timer: 0.0887 sec.
iter 252870 || Loss: 1.0482 || timer: 0.0811 sec.
iter 252880 || Loss: 0.9325 || timer: 0.0968 sec.
iter 252890 || Loss: 0.9264 || timer: 0.1002 sec.
iter 252900 || Loss: 0.6248 || timer: 0.0844 sec.
iter 252910 || Loss: 0.8527 || timer: 0.1171 sec.
iter 252920 || Loss: 1.0043 || timer: 0.0814 sec.
iter 252930 || Loss: 1.0212 || timer: 0.0856 sec.
iter 252940 || Loss: 0.8621 || timer: 0.0811 sec.
iter 252950 || Loss: 1.2201 || timer: 0.0869 sec.
iter 252960 || Loss: 1.3321 || timer: 0.0903 sec.
iter 252970 || Loss: 1.0225 || timer: 0.0928 sec.
iter 252980 || Loss: 0.9801 || timer: 0.1051 sec.
iter 252990 || Loss: 0.7209 || timer: 0.0882 sec.
iter 253000 || Loss: 1.1579 || timer: 0.0223 sec.
iter 253010 || Loss: 1.2416 || timer: 0.0911 sec.
iter 253020 || Loss: 1.1632 || timer: 0.0943 sec.
iter 253030 || Loss: 0.8792 || timer: 0.0853 sec.
iter 253040 || Loss: 0.8972 || timer: 0.0888 sec.
iter 253050 || Loss: 0.7040 || timer: 0.0900 sec.
iter 253060 || Loss: 0.7758 || timer: 0.0804 sec.
iter 253070 || Loss: 1.1709 || timer: 0.1011 sec.
iter 253080 || Loss: 0.9550 || timer: 0.0968 sec.
iter 253090 || Loss: 0.9289 || timer: 0.0892 sec.
iter 253100 || Loss: 0.8740 || timer: 0.0857 sec.
iter 253110 || Loss: 0.7714 || timer: 0.0903 sec.
iter 253120 || Loss: 0.9326 || timer: 0.0830 sec.
iter 253130 || Loss: 0.8529 || timer: 0.0836 sec.
iter 253140 || Loss: 0.9536 || timer: 0.0828 sec.
iter 253150 || Loss: 1.1533 || timer: 0.1023 sec.
iter 253160 || Loss: 1.1208 || timer: 0.1020 sec.
iter 253170 || Loss: 1.0490 || timer: 0.0825 sec.
iter 253180 || Loss: 0.7209 || timer: 0.0918 sec.
iter 253190 || Loss: 0.7454 || timer: 0.0888 sec.
iter 253200 || Loss: 0.8372 || timer: 0.1046 sec.
iter 253210 || Loss: 0.8539 || timer: 0.0937 sec.
iter 253220 || Loss: 0.6875 || timer: 0.0872 sec.
iter 253230 || Loss: 1.0582 || timer: 0.0812 sec.
iter 253240 || Loss: 1.1499 || timer: 0.0924 sec.
iter 253250 || Loss: 1.0723 || timer: 0.0799 sec.
iter 253260 || Loss: 1.1450 || timer: 0.1006 sec.
iter 253270 || Loss: 0.9476 || timer: 0.0932 sec.
iter 253280 || Loss: 0.8854 || timer: 0.0899 sec.
iter 253290 || Loss: 0.6897 || timer: 0.0868 sec.
iter 253300 || Loss: 1.2076 || timer: 0.0840 sec.
iter 253310 || Loss: 0.8341 || timer: 0.0919 sec.
iter 253320 || Loss: 1.0492 || timer: 0.0857 sec.
iter 253330 || Loss: 0.6558 || timer: 0.0301 sec.
iter 253340 || Loss: 0.7960 || timer: 0.0983 sec.
iter 253350 || Loss: 1.1693 || timer: 0.0845 sec.
iter 253360 || Loss: 0.6570 || timer: 0.0859 sec.
iter 253370 || Loss: 1.3097 || timer: 0.0911 sec.
iter 253380 || Loss: 0.8656 || timer: 0.0886 sec.
iter 253390 || Loss: 0.9844 || timer: 0.0924 sec.
iter 253400 || Loss: 1.3925 || timer: 0.1080 sec.
iter 253410 || Loss: 0.9290 || timer: 0.0838 sec.
iter 253420 || Loss: 0.9743 || timer: 0.0931 sec.
iter 253430 || Loss: 0.7567 || timer: 0.1161 sec.
iter 253440 || Loss: 0.9545 || timer: 0.0824 sec.
iter 253450 || Loss: 0.9522 || timer: 0.0877 sec.
iter 253460 || Loss: 0.7177 || timer: 0.1033 sec.
iter 253470 || Loss: 0.7705 || timer: 0.0885 sec.
iter 253480 || Loss: 0.6396 || timer: 0.0880 sec.
iter 253490 || Loss: 1.0064 || timer: 0.0927 sec.
iter 253500 || Loss: 0.9639 || timer: 0.1010 sec.
iter 253510 || Loss: 0.9292 || timer: 0.0829 sec.
iter 253520 || Loss: 0.7316 || timer: 0.1052 sec.
iter 253530 || Loss: 1.1237 || timer: 0.0915 sec.
iter 253540 || Loss: 0.9998 || timer: 0.1082 sec.
iter 253550 || Loss: 0.7810 || timer: 0.0925 sec.
iter 253560 || Loss: 0.7988 || timer: 0.0941 sec.
iter 253570 || Loss: 1.0987 || timer: 0.0935 sec.
iter 253580 || Loss: 0.9928 || timer: 0.0849 sec.
iter 253590 || Loss: 1.1082 || timer: 0.0823 sec.
iter 253600 || Loss: 1.1546 || timer: 0.0839 sec.
iter 253610 || Loss: 1.0120 || timer: 0.0853 sec.
iter 253620 || Loss: 1.2106 || timer: 0.0827 sec.
iter 253630 || Loss: 0.8266 || timer: 0.0906 sec.
iter 253640 || Loss: 1.2182 || timer: 0.0987 sec.
iter 253650 || Loss: 1.0935 || timer: 0.1223 sec.
iter 253660 || Loss: 0.8728 || timer: 0.0207 sec.
iter 253670 || Loss: 0.6433 || timer: 0.0824 sec.
iter 253680 || Loss: 0.8772 || timer: 0.0886 sec.
iter 253690 || Loss: 0.9433 || timer: 0.0826 sec.
iter 253700 || Loss: 1.2302 || timer: 0.0915 sec.
iter 253710 || Loss: 0.8530 || timer: 0.1095 sec.
iter 253720 || Loss: 0.7535 || timer: 0.0953 sec.
iter 253730 || Loss: 1.0066 || timer: 0.0893 sec.
iter 253740 || Loss: 0.8690 || timer: 0.0817 sec.
iter 253750 || Loss: 1.0671 || timer: 0.1085 sec.
iter 253760 || Loss: 0.8736 || timer: 0.1084 sec.
iter 253770 || Loss: 0.9173 || timer: 0.0894 sec.
iter 253780 || Loss: 0.8085 || timer: 0.0877 sec.
iter 253790 || Loss: 0.6305 || timer: 0.0827 sec.
iter 253800 || Loss: 0.7456 || timer: 0.1041 sec.
iter 253810 || Loss: 1.0005 || timer: 0.0996 sec.
iter 253820 || Loss: 1.2903 || timer: 0.0846 sec.
iter 253830 || Loss: 0.8969 || timer: 0.0915 sec.
iter 253840 || Loss: 0.8774 || timer: 0.0973 sec.
iter 253850 || Loss: 0.8524 || timer: 0.0874 sec.
iter 253860 || Loss: 1.1782 || timer: 0.1052 sec.
iter 253870 || Loss: 1.1303 || timer: 0.0887 sec.
iter 253880 || Loss: 0.8254 || timer: 0.1002 sec.
iter 253890 || Loss: 0.8986 || timer: 0.0850 sec.
iter 253900 || Loss: 0.6360 || timer: 0.1075 sec.
iter 253910 || Loss: 0.7329 || timer: 0.0908 sec.
iter 253920 || Loss: 0.9500 || timer: 0.0833 sec.
iter 253930 || Loss: 1.0930 || timer: 0.0929 sec.
iter 253940 || Loss: 0.8892 || timer: 0.0903 sec.
iter 253950 || Loss: 1.0634 || timer: 0.1058 sec.
iter 253960 || Loss: 0.8350 || timer: 0.0845 sec.
iter 253970 || Loss: 0.8281 || timer: 0.1091 sec.
iter 253980 || Loss: 0.7673 || timer: 0.1023 sec.
iter 253990 || Loss: 0.9057 || timer: 0.0182 sec.
iter 254000 || Loss: 0.2421 || timer: 0.0897 sec.
iter 254010 || Loss: 0.8631 || timer: 0.0837 sec.
iter 254020 || Loss: 0.7131 || timer: 0.0831 sec.
iter 254030 || Loss: 0.8704 || timer: 0.0901 sec.
iter 254040 || Loss: 0.9078 || timer: 0.0913 sec.
iter 254050 || Loss: 0.6426 || timer: 0.0864 sec.
iter 254060 || Loss: 0.9597 || timer: 0.0941 sec.
iter 254070 || Loss: 0.7583 || timer: 0.0903 sec.
iter 254080 || Loss: 0.9415 || timer: 0.0825 sec.
iter 254090 || Loss: 0.7299 || timer: 0.1056 sec.
iter 254100 || Loss: 0.8939 || timer: 0.1086 sec.
iter 254110 || Loss: 0.6704 || timer: 0.1057 sec.
iter 254120 || Loss: 1.1627 || timer: 0.0933 sec.
iter 254130 || Loss: 0.9074 || timer: 0.0937 sec.
iter 254140 || Loss: 0.6867 || timer: 0.0946 sec.
iter 254150 || Loss: 0.8907 || timer: 0.0836 sec.
iter 254160 || Loss: 0.7503 || timer: 0.0907 sec.
iter 254170 || Loss: 0.6380 || timer: 0.0833 sec.
iter 254180 || Loss: 0.7875 || timer: 0.1102 sec.
iter 254190 || Loss: 0.9967 || timer: 0.0835 sec.
iter 254200 || Loss: 0.8767 || timer: 0.0927 sec.
iter 254210 || Loss: 0.6779 || timer: 0.0896 sec.
iter 254220 || Loss: 0.8591 || timer: 0.0888 sec.
iter 254230 || Loss: 0.7191 || timer: 0.0911 sec.
iter 254240 || Loss: 1.0683 || timer: 0.0828 sec.
iter 254250 || Loss: 0.7194 || timer: 0.0902 sec.
iter 254260 || Loss: 0.7262 || timer: 0.1059 sec.
iter 254270 || Loss: 0.5983 || timer: 0.0944 sec.
iter 254280 || Loss: 1.0838 || timer: 0.0861 sec.
iter 254290 || Loss: 0.7223 || timer: 0.0969 sec.
iter 254300 || Loss: 0.7827 || timer: 0.0924 sec.
iter 254310 || Loss: 1.1445 || timer: 0.0889 sec.
iter 254320 || Loss: 1.2274 || timer: 0.0242 sec.
iter 254330 || Loss: 0.3802 || timer: 0.1176 sec.
iter 254340 || Loss: 0.9284 || timer: 0.1174 sec.
iter 254350 || Loss: 1.0997 || timer: 0.0916 sec.
iter 254360 || Loss: 1.0965 || timer: 0.0935 sec.
iter 254370 || Loss: 0.9121 || timer: 0.0916 sec.
iter 254380 || Loss: 1.0105 || timer: 0.1103 sec.
iter 254390 || Loss: 0.8310 || timer: 0.0889 sec.
iter 254400 || Loss: 0.7825 || timer: 0.1011 sec.
iter 254410 || Loss: 0.6796 || timer: 0.0898 sec.
iter 254420 || Loss: 0.8130 || timer: 0.0974 sec.
iter 254430 || Loss: 1.1434 || timer: 0.0972 sec.
iter 254440 || Loss: 0.8104 || timer: 0.0910 sec.
iter 254450 || Loss: 0.8835 || timer: 0.0930 sec.
iter 254460 || Loss: 0.7522 || timer: 0.0827 sec.
iter 254470 || Loss: 0.9613 || timer: 0.0828 sec.
iter 254480 || Loss: 1.1253 || timer: 0.0890 sec.
iter 254490 || Loss: 1.0531 || timer: 0.1033 sec.
iter 254500 || Loss: 1.0563 || timer: 0.0826 sec.
iter 254510 || Loss: 0.9244 || timer: 0.0881 sec.
iter 254520 || Loss: 0.7521 || timer: 0.0824 sec.
iter 254530 || Loss: 0.9863 || timer: 0.0883 sec.
iter 254540 || Loss: 1.1836 || timer: 0.1047 sec.
iter 254550 || Loss: 0.9120 || timer: 0.0900 sec.
iter 254560 || Loss: 1.3040 || timer: 0.1115 sec.
iter 254570 || Loss: 0.9574 || timer: 0.0899 sec.
iter 254580 || Loss: 0.9834 || timer: 0.0909 sec.
iter 254590 || Loss: 0.7849 || timer: 0.0908 sec.
iter 254600 || Loss: 0.8178 || timer: 0.1059 sec.
iter 254610 || Loss: 0.8689 || timer: 0.1219 sec.
iter 254620 || Loss: 0.6504 || timer: 0.0917 sec.
iter 254630 || Loss: 0.8365 || timer: 0.0911 sec.
iter 254640 || Loss: 1.2461 || timer: 0.0938 sec.
iter 254650 || Loss: 0.9184 || timer: 0.0168 sec.
iter 254660 || Loss: 1.9219 || timer: 0.0971 sec.
iter 254670 || Loss: 0.6844 || timer: 0.1050 sec.
iter 254680 || Loss: 0.8104 || timer: 0.0850 sec.
iter 254690 || Loss: 0.8546 || timer: 0.0876 sec.
iter 254700 || Loss: 0.9067 || timer: 0.0905 sec.
iter 254710 || Loss: 0.9059 || timer: 0.0858 sec.
iter 254720 || Loss: 0.6627 || timer: 0.0828 sec.
iter 254730 || Loss: 0.6338 || timer: 0.0834 sec.
iter 254740 || Loss: 1.1218 || timer: 0.0914 sec.
iter 254750 || Loss: 0.7278 || timer: 0.1205 sec.
iter 254760 || Loss: 0.7207 || timer: 0.0928 sec.
iter 254770 || Loss: 0.9808 || timer: 0.0818 sec.
iter 254780 || Loss: 0.5873 || timer: 0.0835 sec.
iter 254790 || Loss: 0.7236 || timer: 0.1092 sec.
iter 254800 || Loss: 0.7637 || timer: 0.0997 sec.
iter 254810 || Loss: 0.7960 || timer: 0.0834 sec.
iter 254820 || Loss: 0.7444 || timer: 0.0891 sec.
iter 254830 || Loss: 1.0087 || timer: 0.1111 sec.
iter 254840 || Loss: 0.8236 || timer: 0.1043 sec.
iter 254850 || Loss: 0.8062 || timer: 0.0828 sec.
iter 254860 || Loss: 0.9595 || timer: 0.0823 sec.
iter 254870 || Loss: 0.6345 || timer: 0.0834 sec.
iter 254880 || Loss: 0.6349 || timer: 0.0870 sec.
iter 254890 || Loss: 1.1796 || timer: 0.0867 sec.
iter 254900 || Loss: 0.5875 || timer: 0.1016 sec.
iter 254910 || Loss: 1.2859 || timer: 0.0836 sec.
iter 254920 || Loss: 0.9843 || timer: 0.1038 sec.
iter 254930 || Loss: 1.0207 || timer: 0.0970 sec.
iter 254940 || Loss: 0.7612 || timer: 0.0914 sec.
iter 254950 || Loss: 0.8805 || timer: 0.0968 sec.
iter 254960 || Loss: 1.0440 || timer: 0.0889 sec.
iter 254970 || Loss: 1.3876 || timer: 0.1095 sec.
iter 254980 || Loss: 0.9900 || timer: 0.0235 sec.
iter 254990 || Loss: 1.7591 || timer: 0.0928 sec.
iter 255000 || Loss: 1.2006 || Saving state, iter: 255000
timer: 0.0922 sec.
iter 255010 || Loss: 1.4085 || timer: 0.0949 sec.
iter 255020 || Loss: 0.7605 || timer: 0.0948 sec.
iter 255030 || Loss: 1.1552 || timer: 0.1021 sec.
iter 255040 || Loss: 1.0861 || timer: 0.0861 sec.
iter 255050 || Loss: 0.9605 || timer: 0.0825 sec.
iter 255060 || Loss: 0.6937 || timer: 0.0921 sec.
iter 255070 || Loss: 0.9424 || timer: 0.0920 sec.
iter 255080 || Loss: 0.7290 || timer: 0.1093 sec.
iter 255090 || Loss: 1.0637 || timer: 0.0832 sec.
iter 255100 || Loss: 1.1892 || timer: 0.0880 sec.
iter 255110 || Loss: 0.7857 || timer: 0.0903 sec.
iter 255120 || Loss: 0.6630 || timer: 0.0929 sec.
iter 255130 || Loss: 1.2844 || timer: 0.0907 sec.
iter 255140 || Loss: 0.9454 || timer: 0.0837 sec.
iter 255150 || Loss: 0.7258 || timer: 0.1064 sec.
iter 255160 || Loss: 0.8949 || timer: 0.0933 sec.
iter 255170 || Loss: 0.7198 || timer: 0.0874 sec.
iter 255180 || Loss: 0.8760 || timer: 0.1019 sec.
iter 255190 || Loss: 0.7371 || timer: 0.0906 sec.
iter 255200 || Loss: 0.7495 || timer: 0.0899 sec.
iter 255210 || Loss: 0.9786 || timer: 0.0835 sec.
iter 255220 || Loss: 0.9528 || timer: 0.1086 sec.
iter 255230 || Loss: 0.9018 || timer: 0.0892 sec.
iter 255240 || Loss: 0.9913 || timer: 0.0911 sec.
iter 255250 || Loss: 0.9270 || timer: 0.0901 sec.
iter 255260 || Loss: 0.7476 || timer: 0.0922 sec.
iter 255270 || Loss: 0.7417 || timer: 0.0866 sec.
iter 255280 || Loss: 0.7607 || timer: 0.0929 sec.
iter 255290 || Loss: 0.9300 || timer: 0.0901 sec.
iter 255300 || Loss: 0.7422 || timer: 0.0971 sec.
iter 255310 || Loss: 0.7307 || timer: 0.0200 sec.
iter 255320 || Loss: 1.3277 || timer: 0.0835 sec.
iter 255330 || Loss: 0.9624 || timer: 0.0886 sec.
iter 255340 || Loss: 0.8675 || timer: 0.0832 sec.
iter 255350 || Loss: 0.8628 || timer: 0.1055 sec.
iter 255360 || Loss: 0.9980 || timer: 0.0824 sec.
iter 255370 || Loss: 0.8978 || timer: 0.0902 sec.
iter 255380 || Loss: 0.8475 || timer: 0.0935 sec.
iter 255390 || Loss: 0.7615 || timer: 0.0864 sec.
iter 255400 || Loss: 0.8627 || timer: 0.0911 sec.
iter 255410 || Loss: 1.2464 || timer: 0.0973 sec.
iter 255420 || Loss: 0.6249 || timer: 0.1060 sec.
iter 255430 || Loss: 1.1322 || timer: 0.0868 sec.
iter 255440 || Loss: 1.1250 || timer: 0.0845 sec.
iter 255450 || Loss: 0.8392 || timer: 0.1021 sec.
iter 255460 || Loss: 0.6327 || timer: 0.0962 sec.
iter 255470 || Loss: 0.6839 || timer: 0.0827 sec.
iter 255480 || Loss: 0.9276 || timer: 0.0869 sec.
iter 255490 || Loss: 0.7128 || timer: 0.0875 sec.
iter 255500 || Loss: 1.0360 || timer: 0.1056 sec.
iter 255510 || Loss: 0.7332 || timer: 0.0896 sec.
iter 255520 || Loss: 0.8720 || timer: 0.1120 sec.
iter 255530 || Loss: 0.7690 || timer: 0.0945 sec.
iter 255540 || Loss: 0.6362 || timer: 0.0835 sec.
iter 255550 || Loss: 1.0268 || timer: 0.0896 sec.
iter 255560 || Loss: 0.7915 || timer: 0.0904 sec.
iter 255570 || Loss: 0.8776 || timer: 0.0929 sec.
iter 255580 || Loss: 0.6956 || timer: 0.0898 sec.
iter 255590 || Loss: 0.8105 || timer: 0.1056 sec.
iter 255600 || Loss: 1.2037 || timer: 0.0920 sec.
iter 255610 || Loss: 0.7772 || timer: 0.0822 sec.
iter 255620 || Loss: 0.9230 || timer: 0.0907 sec.
iter 255630 || Loss: 0.6479 || timer: 0.0992 sec.
iter 255640 || Loss: 1.2628 || timer: 0.0199 sec.
iter 255650 || Loss: 0.3333 || timer: 0.1094 sec.
iter 255660 || Loss: 1.3386 || timer: 0.1021 sec.
iter 255670 || Loss: 1.1719 || timer: 0.0922 sec.
iter 255680 || Loss: 0.9241 || timer: 0.1198 sec.
iter 255690 || Loss: 0.8302 || timer: 0.0821 sec.
iter 255700 || Loss: 0.8525 || timer: 0.1200 sec.
iter 255710 || Loss: 0.7699 || timer: 0.0820 sec.
iter 255720 || Loss: 0.6514 || timer: 0.0978 sec.
iter 255730 || Loss: 1.2552 || timer: 0.0899 sec.
iter 255740 || Loss: 1.0627 || timer: 0.0949 sec.
iter 255750 || Loss: 0.8036 || timer: 0.0923 sec.
iter 255760 || Loss: 0.8779 || timer: 0.0821 sec.
iter 255770 || Loss: 0.8439 || timer: 0.0887 sec.
iter 255780 || Loss: 0.7090 || timer: 0.0826 sec.
iter 255790 || Loss: 0.6860 || timer: 0.0851 sec.
iter 255800 || Loss: 1.1503 || timer: 0.0826 sec.
iter 255810 || Loss: 0.7707 || timer: 0.1011 sec.
iter 255820 || Loss: 1.0156 || timer: 0.0823 sec.
iter 255830 || Loss: 0.9935 || timer: 0.0929 sec.
iter 255840 || Loss: 1.0597 || timer: 0.0828 sec.
iter 255850 || Loss: 1.0251 || timer: 0.0937 sec.
iter 255860 || Loss: 0.8780 || timer: 0.1028 sec.
iter 255870 || Loss: 1.0486 || timer: 0.0841 sec.
iter 255880 || Loss: 0.8177 || timer: 0.0893 sec.
iter 255890 || Loss: 0.7875 || timer: 0.0973 sec.
iter 255900 || Loss: 0.6456 || timer: 0.0884 sec.
iter 255910 || Loss: 1.1201 || timer: 0.0993 sec.
iter 255920 || Loss: 1.1370 || timer: 0.0898 sec.
iter 255930 || Loss: 0.8969 || timer: 0.0983 sec.
iter 255940 || Loss: 0.9703 || timer: 0.0894 sec.
iter 255950 || Loss: 1.1152 || timer: 0.1013 sec.
iter 255960 || Loss: 0.6324 || timer: 0.0897 sec.
iter 255970 || Loss: 0.7400 || timer: 0.0243 sec.
iter 255980 || Loss: 0.7559 || timer: 0.0898 sec.
iter 255990 || Loss: 0.7812 || timer: 0.0840 sec.
iter 256000 || Loss: 0.7386 || timer: 0.1015 sec.
iter 256010 || Loss: 0.6796 || timer: 0.0915 sec.
iter 256020 || Loss: 0.9025 || timer: 0.1029 sec.
iter 256030 || Loss: 1.0510 || timer: 0.0925 sec.
iter 256040 || Loss: 1.0354 || timer: 0.0929 sec.
iter 256050 || Loss: 0.9169 || timer: 0.0959 sec.
iter 256060 || Loss: 0.9310 || timer: 0.0914 sec.
iter 256070 || Loss: 0.6381 || timer: 0.0994 sec.
iter 256080 || Loss: 0.7949 || timer: 0.0945 sec.
iter 256090 || Loss: 1.2370 || timer: 0.0875 sec.
iter 256100 || Loss: 0.7833 || timer: 0.0915 sec.
iter 256110 || Loss: 0.8542 || timer: 0.0916 sec.
iter 256120 || Loss: 0.6727 || timer: 0.0889 sec.
iter 256130 || Loss: 1.0973 || timer: 0.0825 sec.
iter 256140 || Loss: 0.7820 || timer: 0.1058 sec.
iter 256150 || Loss: 0.6965 || timer: 0.1063 sec.
iter 256160 || Loss: 0.9966 || timer: 0.0844 sec.
iter 256170 || Loss: 0.6754 || timer: 0.0913 sec.
iter 256180 || Loss: 0.7462 || timer: 0.1093 sec.
iter 256190 || Loss: 1.0758 || timer: 0.0907 sec.
iter 256200 || Loss: 1.0241 || timer: 0.0862 sec.
iter 256210 || Loss: 0.8347 || timer: 0.0847 sec.
iter 256220 || Loss: 0.8597 || timer: 0.0911 sec.
iter 256230 || Loss: 0.9346 || timer: 0.1164 sec.
iter 256240 || Loss: 0.7319 || timer: 0.0911 sec.
iter 256250 || Loss: 0.6428 || timer: 0.0886 sec.
iter 256260 || Loss: 0.7750 || timer: 0.0926 sec.
iter 256270 || Loss: 1.0870 || timer: 0.0968 sec.
iter 256280 || Loss: 0.9246 || timer: 0.0913 sec.
iter 256290 || Loss: 1.1131 || timer: 0.1306 sec.
iter 256300 || Loss: 0.7625 || timer: 0.0210 sec.
iter 256310 || Loss: 1.1225 || timer: 0.0924 sec.
iter 256320 || Loss: 0.9223 || timer: 0.0904 sec.
iter 256330 || Loss: 0.9327 || timer: 0.0850 sec.
iter 256340 || Loss: 0.8773 || timer: 0.0903 sec.
iter 256350 || Loss: 1.2036 || timer: 0.0916 sec.
iter 256360 || Loss: 1.0538 || timer: 0.1105 sec.
iter 256370 || Loss: 0.7699 || timer: 0.1032 sec.
iter 256380 || Loss: 0.8473 || timer: 0.0843 sec.
iter 256390 || Loss: 0.7017 || timer: 0.0861 sec.
iter 256400 || Loss: 0.8369 || timer: 0.0954 sec.
iter 256410 || Loss: 1.0087 || timer: 0.1104 sec.
iter 256420 || Loss: 0.7574 || timer: 0.0943 sec.
iter 256430 || Loss: 0.9208 || timer: 0.0902 sec.
iter 256440 || Loss: 0.8436 || timer: 0.0938 sec.
iter 256450 || Loss: 0.7728 || timer: 0.0911 sec.
iter 256460 || Loss: 0.9292 || timer: 0.0906 sec.
iter 256470 || Loss: 0.8675 || timer: 0.0912 sec.
iter 256480 || Loss: 0.7583 || timer: 0.0959 sec.
iter 256490 || Loss: 0.8454 || timer: 0.0846 sec.
iter 256500 || Loss: 1.5066 || timer: 0.0914 sec.
iter 256510 || Loss: 0.6529 || timer: 0.0999 sec.
iter 256520 || Loss: 0.8076 || timer: 0.0855 sec.
iter 256530 || Loss: 0.9845 || timer: 0.0814 sec.
iter 256540 || Loss: 0.8382 || timer: 0.0923 sec.
iter 256550 || Loss: 0.7899 || timer: 0.0915 sec.
iter 256560 || Loss: 0.9506 || timer: 0.0857 sec.
iter 256570 || Loss: 0.9216 || timer: 0.0919 sec.
iter 256580 || Loss: 0.7330 || timer: 0.0875 sec.
iter 256590 || Loss: 0.7517 || timer: 0.0900 sec.
iter 256600 || Loss: 0.9439 || timer: 0.0940 sec.
iter 256610 || Loss: 0.9490 || timer: 0.1068 sec.
iter 256620 || Loss: 0.9806 || timer: 0.0919 sec.
iter 256630 || Loss: 0.8944 || timer: 0.0153 sec.
iter 256640 || Loss: 0.7596 || timer: 0.0922 sec.
iter 256650 || Loss: 0.8478 || timer: 0.0843 sec.
iter 256660 || Loss: 0.7814 || timer: 0.0902 sec.
iter 256670 || Loss: 0.7988 || timer: 0.0867 sec.
iter 256680 || Loss: 0.8903 || timer: 0.0918 sec.
iter 256690 || Loss: 0.8611 || timer: 0.0863 sec.
iter 256700 || Loss: 0.6976 || timer: 0.0827 sec.
iter 256710 || Loss: 0.7469 || timer: 0.0917 sec.
iter 256720 || Loss: 0.8618 || timer: 0.0885 sec.
iter 256730 || Loss: 0.7847 || timer: 0.1246 sec.
iter 256740 || Loss: 0.7319 || timer: 0.0913 sec.
iter 256750 || Loss: 0.9960 || timer: 0.1081 sec.
iter 256760 || Loss: 0.8622 || timer: 0.1038 sec.
iter 256770 || Loss: 0.6546 || timer: 0.0924 sec.
iter 256780 || Loss: 1.0759 || timer: 0.1050 sec.
iter 256790 || Loss: 0.7188 || timer: 0.0937 sec.
iter 256800 || Loss: 0.8317 || timer: 0.0931 sec.
iter 256810 || Loss: 0.7194 || timer: 0.1040 sec.
iter 256820 || Loss: 0.9137 || timer: 0.0834 sec.
iter 256830 || Loss: 1.0100 || timer: 0.1018 sec.
iter 256840 || Loss: 0.6108 || timer: 0.1058 sec.
iter 256850 || Loss: 0.8315 || timer: 0.0895 sec.
iter 256860 || Loss: 0.7640 || timer: 0.0787 sec.
iter 256870 || Loss: 1.0533 || timer: 0.1043 sec.
iter 256880 || Loss: 1.0409 || timer: 0.1219 sec.
iter 256890 || Loss: 0.9789 || timer: 0.0847 sec.
iter 256900 || Loss: 0.8220 || timer: 0.1005 sec.
iter 256910 || Loss: 0.9168 || timer: 0.0908 sec.
iter 256920 || Loss: 0.7695 || timer: 0.0867 sec.
iter 256930 || Loss: 1.1319 || timer: 0.0905 sec.
iter 256940 || Loss: 0.6939 || timer: 0.0887 sec.
iter 256950 || Loss: 0.9507 || timer: 0.0880 sec.
iter 256960 || Loss: 1.1305 || timer: 0.0230 sec.
iter 256970 || Loss: 1.0004 || timer: 0.0869 sec.
iter 256980 || Loss: 0.8804 || timer: 0.0820 sec.
iter 256990 || Loss: 0.7002 || timer: 0.0813 sec.
iter 257000 || Loss: 0.8787 || timer: 0.1110 sec.
iter 257010 || Loss: 0.9723 || timer: 0.0900 sec.
iter 257020 || Loss: 1.2249 || timer: 0.0910 sec.
iter 257030 || Loss: 0.5988 || timer: 0.0849 sec.
iter 257040 || Loss: 0.7746 || timer: 0.1093 sec.
iter 257050 || Loss: 0.9246 || timer: 0.0915 sec.
iter 257060 || Loss: 0.8766 || timer: 0.1130 sec.
iter 257070 || Loss: 1.1891 || timer: 0.0914 sec.
iter 257080 || Loss: 0.8238 || timer: 0.0838 sec.
iter 257090 || Loss: 1.0278 || timer: 0.0915 sec.
iter 257100 || Loss: 0.8586 || timer: 0.0935 sec.
iter 257110 || Loss: 0.7208 || timer: 0.0909 sec.
iter 257120 || Loss: 1.0322 || timer: 0.0918 sec.
iter 257130 || Loss: 0.9509 || timer: 0.0829 sec.
iter 257140 || Loss: 0.9219 || timer: 0.0950 sec.
iter 257150 || Loss: 0.8305 || timer: 0.0928 sec.
iter 257160 || Loss: 0.7446 || timer: 0.0841 sec.
iter 257170 || Loss: 1.0949 || timer: 0.1198 sec.
iter 257180 || Loss: 0.9675 || timer: 0.0895 sec.
iter 257190 || Loss: 0.8253 || timer: 0.0930 sec.
iter 257200 || Loss: 0.9117 || timer: 0.1055 sec.
iter 257210 || Loss: 0.8163 || timer: 0.0927 sec.
iter 257220 || Loss: 0.9877 || timer: 0.0920 sec.
iter 257230 || Loss: 0.9042 || timer: 0.0911 sec.
iter 257240 || Loss: 1.2577 || timer: 0.0856 sec.
iter 257250 || Loss: 1.2388 || timer: 0.0773 sec.
iter 257260 || Loss: 1.0017 || timer: 0.1070 sec.
iter 257270 || Loss: 0.7229 || timer: 0.0842 sec.
iter 257280 || Loss: 0.9402 || timer: 0.0847 sec.
iter 257290 || Loss: 1.0611 || timer: 0.0221 sec.
iter 257300 || Loss: 0.2471 || timer: 0.0924 sec.
iter 257310 || Loss: 0.6972 || timer: 0.0832 sec.
iter 257320 || Loss: 1.5093 || timer: 0.0984 sec.
iter 257330 || Loss: 0.7361 || timer: 0.0926 sec.
iter 257340 || Loss: 0.8165 || timer: 0.0909 sec.
iter 257350 || Loss: 1.2163 || timer: 0.0920 sec.
iter 257360 || Loss: 0.7533 || timer: 0.0880 sec.
iter 257370 || Loss: 0.6356 || timer: 0.0897 sec.
iter 257380 || Loss: 0.7971 || timer: 0.0971 sec.
iter 257390 || Loss: 0.7032 || timer: 0.1192 sec.
iter 257400 || Loss: 1.0495 || timer: 0.0881 sec.
iter 257410 || Loss: 0.9310 || timer: 0.0850 sec.
iter 257420 || Loss: 0.8815 || timer: 0.0826 sec.
iter 257430 || Loss: 0.6629 || timer: 0.0849 sec.
iter 257440 || Loss: 0.9017 || timer: 0.0989 sec.
iter 257450 || Loss: 0.8738 || timer: 0.0904 sec.
iter 257460 || Loss: 0.9746 || timer: 0.0819 sec.
iter 257470 || Loss: 0.7408 || timer: 0.0767 sec.
iter 257480 || Loss: 0.8128 || timer: 0.1085 sec.
iter 257490 || Loss: 0.8228 || timer: 0.0973 sec.
iter 257500 || Loss: 0.8670 || timer: 0.0867 sec.
iter 257510 || Loss: 0.7831 || timer: 0.0880 sec.
iter 257520 || Loss: 0.6919 || timer: 0.0888 sec.
iter 257530 || Loss: 1.3134 || timer: 0.0909 sec.
iter 257540 || Loss: 0.8343 || timer: 0.1228 sec.
iter 257550 || Loss: 0.6887 || timer: 0.0931 sec.
iter 257560 || Loss: 0.7117 || timer: 0.0911 sec.
iter 257570 || Loss: 0.8336 || timer: 0.0911 sec.
iter 257580 || Loss: 0.9748 || timer: 0.0899 sec.
iter 257590 || Loss: 0.9217 || timer: 0.1029 sec.
iter 257600 || Loss: 0.8758 || timer: 0.0873 sec.
iter 257610 || Loss: 0.6614 || timer: 0.0911 sec.
iter 257620 || Loss: 0.8952 || timer: 0.0221 sec.
iter 257630 || Loss: 0.7643 || timer: 0.0908 sec.
iter 257640 || Loss: 1.0931 || timer: 0.0950 sec.
iter 257650 || Loss: 0.5581 || timer: 0.0835 sec.
iter 257660 || Loss: 0.9362 || timer: 0.1246 sec.
iter 257670 || Loss: 1.2585 || timer: 0.0884 sec.
iter 257680 || Loss: 0.9483 || timer: 0.0909 sec.
iter 257690 || Loss: 0.8599 || timer: 0.0850 sec.
iter 257700 || Loss: 0.7884 || timer: 0.0855 sec.
iter 257710 || Loss: 0.7254 || timer: 0.0905 sec.
iter 257720 || Loss: 0.6971 || timer: 0.1324 sec.
iter 257730 || Loss: 0.5789 || timer: 0.1193 sec.
iter 257740 || Loss: 0.7238 || timer: 0.0827 sec.
iter 257750 || Loss: 0.8354 || timer: 0.0912 sec.
iter 257760 || Loss: 0.9912 || timer: 0.0933 sec.
iter 257770 || Loss: 0.7775 || timer: 0.0915 sec.
iter 257780 || Loss: 0.9989 || timer: 0.1060 sec.
iter 257790 || Loss: 1.0084 || timer: 0.0903 sec.
iter 257800 || Loss: 0.5964 || timer: 0.0853 sec.
iter 257810 || Loss: 0.7224 || timer: 0.0892 sec.
iter 257820 || Loss: 0.8845 || timer: 0.0816 sec.
iter 257830 || Loss: 0.9868 || timer: 0.1058 sec.
iter 257840 || Loss: 0.8898 || timer: 0.1003 sec.
iter 257850 || Loss: 0.8408 || timer: 0.0893 sec.
iter 257860 || Loss: 0.8833 || timer: 0.1051 sec.
iter 257870 || Loss: 0.7604 || timer: 0.0849 sec.
iter 257880 || Loss: 0.9218 || timer: 0.0900 sec.
iter 257890 || Loss: 0.7779 || timer: 0.0880 sec.
iter 257900 || Loss: 0.6466 || timer: 0.0888 sec.
iter 257910 || Loss: 0.8719 || timer: 0.0824 sec.
iter 257920 || Loss: 0.8247 || timer: 0.1073 sec.
iter 257930 || Loss: 0.5003 || timer: 0.0906 sec.
iter 257940 || Loss: 0.6672 || timer: 0.0820 sec.
iter 257950 || Loss: 0.7013 || timer: 0.0279 sec.
iter 257960 || Loss: 0.1527 || timer: 0.1043 sec.
iter 257970 || Loss: 0.7082 || timer: 0.0975 sec.
iter 257980 || Loss: 0.8848 || timer: 0.0830 sec.
iter 257990 || Loss: 0.6926 || timer: 0.0923 sec.
iter 258000 || Loss: 0.9204 || timer: 0.0804 sec.
iter 258010 || Loss: 0.7419 || timer: 0.0825 sec.
iter 258020 || Loss: 1.0955 || timer: 0.1029 sec.
iter 258030 || Loss: 0.7880 || timer: 0.0852 sec.
iter 258040 || Loss: 0.5473 || timer: 0.0892 sec.
iter 258050 || Loss: 0.8131 || timer: 0.1152 sec.
iter 258060 || Loss: 0.8020 || timer: 0.0917 sec.
iter 258070 || Loss: 0.9319 || timer: 0.0857 sec.
iter 258080 || Loss: 0.7538 || timer: 0.0819 sec.
iter 258090 || Loss: 1.1078 || timer: 0.0758 sec.
iter 258100 || Loss: 0.6606 || timer: 0.0897 sec.
iter 258110 || Loss: 1.1493 || timer: 0.0965 sec.
iter 258120 || Loss: 0.7514 || timer: 0.0902 sec.
iter 258130 || Loss: 0.9830 || timer: 0.0819 sec.
iter 258140 || Loss: 0.8032 || timer: 0.0817 sec.
iter 258150 || Loss: 1.1246 || timer: 0.0812 sec.
iter 258160 || Loss: 1.0733 || timer: 0.0894 sec.
iter 258170 || Loss: 0.5969 || timer: 0.0895 sec.
iter 258180 || Loss: 1.1356 || timer: 0.0816 sec.
iter 258190 || Loss: 0.6340 || timer: 0.0825 sec.
iter 258200 || Loss: 0.9325 || timer: 0.0875 sec.
iter 258210 || Loss: 0.5815 || timer: 0.0893 sec.
iter 258220 || Loss: 0.8775 || timer: 0.0917 sec.
iter 258230 || Loss: 0.8102 || timer: 0.1019 sec.
iter 258240 || Loss: 0.6619 || timer: 0.0969 sec.
iter 258250 || Loss: 0.8284 || timer: 0.0819 sec.
iter 258260 || Loss: 0.6412 || timer: 0.0903 sec.
iter 258270 || Loss: 1.0859 || timer: 0.0951 sec.
iter 258280 || Loss: 0.8422 || timer: 0.0165 sec.
iter 258290 || Loss: 0.3467 || timer: 0.0833 sec.
iter 258300 || Loss: 0.7312 || timer: 0.0872 sec.
iter 258310 || Loss: 0.7787 || timer: 0.1075 sec.
iter 258320 || Loss: 0.8164 || timer: 0.1093 sec.
iter 258330 || Loss: 0.7744 || timer: 0.0861 sec.
iter 258340 || Loss: 0.9518 || timer: 0.1200 sec.
iter 258350 || Loss: 1.0066 || timer: 0.0902 sec.
iter 258360 || Loss: 0.7483 || timer: 0.0905 sec.
iter 258370 || Loss: 0.7649 || timer: 0.0883 sec.
iter 258380 || Loss: 1.0387 || timer: 0.0982 sec.
iter 258390 || Loss: 0.9157 || timer: 0.0896 sec.
iter 258400 || Loss: 0.9113 || timer: 0.0901 sec.
iter 258410 || Loss: 0.7864 || timer: 0.0899 sec.
iter 258420 || Loss: 0.8794 || timer: 0.0912 sec.
iter 258430 || Loss: 1.1103 || timer: 0.0998 sec.
iter 258440 || Loss: 0.9658 || timer: 0.1057 sec.
iter 258450 || Loss: 0.7331 || timer: 0.0822 sec.
iter 258460 || Loss: 1.2315 || timer: 0.0831 sec.
iter 258470 || Loss: 0.8794 || timer: 0.0916 sec.
iter 258480 || Loss: 0.7496 || timer: 0.0793 sec.
iter 258490 || Loss: 0.7108 || timer: 0.0873 sec.
iter 258500 || Loss: 0.9488 || timer: 0.1065 sec.
iter 258510 || Loss: 0.9046 || timer: 0.0826 sec.
iter 258520 || Loss: 1.0682 || timer: 0.0912 sec.
iter 258530 || Loss: 0.9329 || timer: 0.0842 sec.
iter 258540 || Loss: 0.8126 || timer: 0.0816 sec.
iter 258550 || Loss: 0.7402 || timer: 0.0889 sec.
iter 258560 || Loss: 0.8595 || timer: 0.0968 sec.
iter 258570 || Loss: 0.9336 || timer: 0.0855 sec.
iter 258580 || Loss: 0.8556 || timer: 0.0858 sec.
iter 258590 || Loss: 0.6762 || timer: 0.0889 sec.
iter 258600 || Loss: 0.5148 || timer: 0.0989 sec.
iter 258610 || Loss: 0.7356 || timer: 0.0236 sec.
iter 258620 || Loss: 1.3620 || timer: 0.0823 sec.
iter 258630 || Loss: 0.8980 || timer: 0.0975 sec.
iter 258640 || Loss: 0.7762 || timer: 0.0924 sec.
iter 258650 || Loss: 0.7611 || timer: 0.0816 sec.
iter 258660 || Loss: 0.8209 || timer: 0.0878 sec.
iter 258670 || Loss: 0.7978 || timer: 0.0882 sec.
iter 258680 || Loss: 1.0822 || timer: 0.0869 sec.
iter 258690 || Loss: 0.9114 || timer: 0.0872 sec.
iter 258700 || Loss: 1.0477 || timer: 0.0901 sec.
iter 258710 || Loss: 0.6703 || timer: 0.0940 sec.
iter 258720 || Loss: 0.8286 || timer: 0.0828 sec.
iter 258730 || Loss: 1.1017 || timer: 0.0899 sec.
iter 258740 || Loss: 0.7724 || timer: 0.0932 sec.
iter 258750 || Loss: 0.8562 || timer: 0.0910 sec.
iter 258760 || Loss: 1.1611 || timer: 0.0897 sec.
iter 258770 || Loss: 0.8638 || timer: 0.0923 sec.
iter 258780 || Loss: 0.6880 || timer: 0.0896 sec.
iter 258790 || Loss: 0.8304 || timer: 0.0974 sec.
iter 258800 || Loss: 0.7409 || timer: 0.0921 sec.
iter 258810 || Loss: 0.7028 || timer: 0.1027 sec.
iter 258820 || Loss: 1.0216 || timer: 0.0894 sec.
iter 258830 || Loss: 0.6252 || timer: 0.0911 sec.
iter 258840 || Loss: 1.3428 || timer: 0.0892 sec.
iter 258850 || Loss: 1.0667 || timer: 0.1269 sec.
iter 258860 || Loss: 0.8127 || timer: 0.0912 sec.
iter 258870 || Loss: 1.1002 || timer: 0.0871 sec.
iter 258880 || Loss: 1.0151 || timer: 0.0870 sec.
iter 258890 || Loss: 0.7346 || timer: 0.0829 sec.
iter 258900 || Loss: 1.2439 || timer: 0.0820 sec.
iter 258910 || Loss: 0.7072 || timer: 0.0910 sec.
iter 258920 || Loss: 1.0531 || timer: 0.0914 sec.
iter 258930 || Loss: 0.7737 || timer: 0.1101 sec.
iter 258940 || Loss: 1.0693 || timer: 0.0147 sec.
iter 258950 || Loss: 0.9484 || timer: 0.0896 sec.
iter 258960 || Loss: 0.6209 || timer: 0.0784 sec.
iter 258970 || Loss: 0.6641 || timer: 0.0832 sec.
iter 258980 || Loss: 0.7786 || timer: 0.0909 sec.
iter 258990 || Loss: 1.1213 || timer: 0.0856 sec.
iter 259000 || Loss: 0.6742 || timer: 0.0835 sec.
iter 259010 || Loss: 0.9845 || timer: 0.1355 sec.
iter 259020 || Loss: 0.9314 || timer: 0.0753 sec.
iter 259030 || Loss: 1.1201 || timer: 0.0911 sec.
iter 259040 || Loss: 1.2546 || timer: 0.1078 sec.
iter 259050 || Loss: 0.8750 || timer: 0.1001 sec.
iter 259060 || Loss: 0.9894 || timer: 0.0891 sec.
iter 259070 || Loss: 1.0424 || timer: 0.0946 sec.
iter 259080 || Loss: 0.9154 || timer: 0.0887 sec.
iter 259090 || Loss: 1.0485 || timer: 0.0768 sec.
iter 259100 || Loss: 1.1191 || timer: 0.1008 sec.
iter 259110 || Loss: 1.1941 || timer: 0.1122 sec.
iter 259120 || Loss: 1.2146 || timer: 0.0841 sec.
iter 259130 || Loss: 1.0310 || timer: 0.0900 sec.
iter 259140 || Loss: 0.7172 || timer: 0.0896 sec.
iter 259150 || Loss: 1.0360 || timer: 0.0758 sec.
iter 259160 || Loss: 0.7879 || timer: 0.0855 sec.
iter 259170 || Loss: 0.9077 || timer: 0.0821 sec.
iter 259180 || Loss: 0.8553 || timer: 0.1154 sec.
iter 259190 || Loss: 0.7350 || timer: 0.0853 sec.
iter 259200 || Loss: 0.8839 || timer: 0.0829 sec.
iter 259210 || Loss: 0.8273 || timer: 0.0903 sec.
iter 259220 || Loss: 0.9221 || timer: 0.1084 sec.
iter 259230 || Loss: 0.9100 || timer: 0.0825 sec.
iter 259240 || Loss: 0.8582 || timer: 0.0889 sec.
iter 259250 || Loss: 0.9751 || timer: 0.1130 sec.
iter 259260 || Loss: 0.8425 || timer: 0.0907 sec.
iter 259270 || Loss: 0.9862 || timer: 0.0237 sec.
iter 259280 || Loss: 0.2783 || timer: 0.0854 sec.
iter 259290 || Loss: 0.8050 || timer: 0.0821 sec.
iter 259300 || Loss: 0.9756 || timer: 0.0888 sec.
iter 259310 || Loss: 1.1467 || timer: 0.1072 sec.
iter 259320 || Loss: 0.9263 || timer: 0.0912 sec.
iter 259330 || Loss: 0.8053 || timer: 0.1149 sec.
iter 259340 || Loss: 0.8750 || timer: 0.1004 sec.
iter 259350 || Loss: 0.6797 || timer: 0.0832 sec.
iter 259360 || Loss: 0.6812 || timer: 0.0968 sec.
iter 259370 || Loss: 1.0056 || timer: 0.1252 sec.
iter 259380 || Loss: 0.8179 || timer: 0.0891 sec.
iter 259390 || Loss: 0.7631 || timer: 0.0893 sec.
iter 259400 || Loss: 0.6073 || timer: 0.0898 sec.
iter 259410 || Loss: 0.7908 || timer: 0.0911 sec.
iter 259420 || Loss: 0.9286 || timer: 0.0942 sec.
iter 259430 || Loss: 0.8296 || timer: 0.0831 sec.
iter 259440 || Loss: 0.9077 || timer: 0.1180 sec.
iter 259450 || Loss: 0.7330 || timer: 0.0910 sec.
iter 259460 || Loss: 1.0037 || timer: 0.0893 sec.
iter 259470 || Loss: 0.8086 || timer: 0.0911 sec.
iter 259480 || Loss: 0.9170 || timer: 0.0903 sec.
iter 259490 || Loss: 0.7520 || timer: 0.0885 sec.
iter 259500 || Loss: 0.7713 || timer: 0.0895 sec.
iter 259510 || Loss: 1.2979 || timer: 0.0947 sec.
iter 259520 || Loss: 1.5105 || timer: 0.0908 sec.
iter 259530 || Loss: 0.8648 || timer: 0.0923 sec.
iter 259540 || Loss: 1.2223 || timer: 0.0835 sec.
iter 259550 || Loss: 0.9743 || timer: 0.0824 sec.
iter 259560 || Loss: 0.7714 || timer: 0.0980 sec.
iter 259570 || Loss: 0.7793 || timer: 0.0896 sec.
iter 259580 || Loss: 0.8313 || timer: 0.1050 sec.
iter 259590 || Loss: 0.7415 || timer: 0.0890 sec.
iter 259600 || Loss: 0.8475 || timer: 0.0239 sec.
iter 259610 || Loss: 2.1061 || timer: 0.0829 sec.
iter 259620 || Loss: 0.9168 || timer: 0.0829 sec.
iter 259630 || Loss: 0.7964 || timer: 0.0919 sec.
iter 259640 || Loss: 0.8382 || timer: 0.0967 sec.
iter 259650 || Loss: 0.7237 || timer: 0.0897 sec.
iter 259660 || Loss: 1.0886 || timer: 0.0910 sec.
iter 259670 || Loss: 1.3244 || timer: 0.0800 sec.
iter 259680 || Loss: 1.1144 || timer: 0.0860 sec.
iter 259690 || Loss: 0.7877 || timer: 0.0833 sec.
iter 259700 || Loss: 0.7375 || timer: 0.1191 sec.
iter 259710 || Loss: 0.8028 || timer: 0.0927 sec.
iter 259720 || Loss: 1.3264 || timer: 0.0896 sec.
iter 259730 || Loss: 0.7644 || timer: 0.0918 sec.
iter 259740 || Loss: 0.7356 || timer: 0.0929 sec.
iter 259750 || Loss: 0.9137 || timer: 0.0923 sec.
iter 259760 || Loss: 0.7797 || timer: 0.0895 sec.
iter 259770 || Loss: 0.6693 || timer: 0.0879 sec.
iter 259780 || Loss: 0.7139 || timer: 0.0842 sec.
iter 259790 || Loss: 0.7810 || timer: 0.0820 sec.
iter 259800 || Loss: 0.9441 || timer: 0.0956 sec.
iter 259810 || Loss: 0.7613 || timer: 0.0930 sec.
iter 259820 || Loss: 0.9093 || timer: 0.0902 sec.
iter 259830 || Loss: 1.0836 || timer: 0.0811 sec.
iter 259840 || Loss: 0.5849 || timer: 0.0903 sec.
iter 259850 || Loss: 0.6001 || timer: 0.0896 sec.
iter 259860 || Loss: 0.8445 || timer: 0.0927 sec.
iter 259870 || Loss: 0.7778 || timer: 0.0926 sec.
iter 259880 || Loss: 0.8072 || timer: 0.0883 sec.
iter 259890 || Loss: 1.0991 || timer: 0.0890 sec.
iter 259900 || Loss: 0.8372 || timer: 0.0838 sec.
iter 259910 || Loss: 0.9219 || timer: 0.0937 sec.
iter 259920 || Loss: 0.6480 || timer: 0.0891 sec.
iter 259930 || Loss: 0.7019 || timer: 0.0275 sec.
iter 259940 || Loss: 0.7040 || timer: 0.0844 sec.
iter 259950 || Loss: 0.7546 || timer: 0.0785 sec.
iter 259960 || Loss: 0.9610 || timer: 0.0852 sec.
iter 259970 || Loss: 0.7528 || timer: 0.0918 sec.
iter 259980 || Loss: 0.9145 || timer: 0.0903 sec.
iter 259990 || Loss: 0.9144 || timer: 0.0913 sec.
iter 260000 || Loss: 0.8214 || Saving state, iter: 260000
timer: 0.0911 sec.
iter 260010 || Loss: 0.7302 || timer: 0.0905 sec.
iter 260020 || Loss: 1.1068 || timer: 0.0915 sec.
iter 260030 || Loss: 0.8232 || timer: 0.0988 sec.
iter 260040 || Loss: 0.5747 || timer: 0.0835 sec.
iter 260050 || Loss: 0.7740 || timer: 0.0937 sec.
iter 260060 || Loss: 0.7559 || timer: 0.0821 sec.
iter 260070 || Loss: 0.7728 || timer: 0.0900 sec.
iter 260080 || Loss: 0.6863 || timer: 0.0828 sec.
iter 260090 || Loss: 1.0516 || timer: 0.0919 sec.
iter 260100 || Loss: 0.9543 || timer: 0.0902 sec.
iter 260110 || Loss: 1.0053 || timer: 0.0920 sec.
iter 260120 || Loss: 0.8668 || timer: 0.0957 sec.
iter 260130 || Loss: 0.7364 || timer: 0.0896 sec.
iter 260140 || Loss: 0.9398 || timer: 0.0844 sec.
iter 260150 || Loss: 0.8629 || timer: 0.0875 sec.
iter 260160 || Loss: 1.1039 || timer: 0.0800 sec.
iter 260170 || Loss: 0.5809 || timer: 0.0908 sec.
iter 260180 || Loss: 0.6021 || timer: 0.0909 sec.
iter 260190 || Loss: 0.9412 || timer: 0.1070 sec.
iter 260200 || Loss: 0.8857 || timer: 0.0921 sec.
iter 260210 || Loss: 0.9767 || timer: 0.0916 sec.
iter 260220 || Loss: 0.9202 || timer: 0.0928 sec.
iter 260230 || Loss: 0.7083 || timer: 0.0843 sec.
iter 260240 || Loss: 0.7409 || timer: 0.0812 sec.
iter 260250 || Loss: 1.0256 || timer: 0.0895 sec.
iter 260260 || Loss: 0.7917 || timer: 0.0217 sec.
iter 260270 || Loss: 1.3246 || timer: 0.0897 sec.
iter 260280 || Loss: 1.0135 || timer: 0.0882 sec.
iter 260290 || Loss: 0.8324 || timer: 0.0907 sec.
iter 260300 || Loss: 0.8070 || timer: 0.0920 sec.
iter 260310 || Loss: 1.3027 || timer: 0.1269 sec.
iter 260320 || Loss: 0.9018 || timer: 0.0965 sec.
iter 260330 || Loss: 0.6177 || timer: 0.1048 sec.
iter 260340 || Loss: 0.8669 || timer: 0.0922 sec.
iter 260350 || Loss: 0.8503 || timer: 0.0894 sec.
iter 260360 || Loss: 0.8396 || timer: 0.1145 sec.
iter 260370 || Loss: 0.8019 || timer: 0.0827 sec.
iter 260380 || Loss: 0.9698 || timer: 0.0906 sec.
iter 260390 || Loss: 0.7889 || timer: 0.1142 sec.
iter 260400 || Loss: 0.9718 || timer: 0.0824 sec.
iter 260410 || Loss: 0.8165 || timer: 0.0826 sec.
iter 260420 || Loss: 1.2722 || timer: 0.0967 sec.
iter 260430 || Loss: 0.8500 || timer: 0.0887 sec.
iter 260440 || Loss: 0.8259 || timer: 0.0887 sec.
iter 260450 || Loss: 0.7066 || timer: 0.0890 sec.
iter 260460 || Loss: 1.0693 || timer: 0.0952 sec.
iter 260470 || Loss: 1.1230 || timer: 0.0831 sec.
iter 260480 || Loss: 1.0798 || timer: 0.0819 sec.
iter 260490 || Loss: 0.8862 || timer: 0.0889 sec.
iter 260500 || Loss: 0.8507 || timer: 0.0911 sec.
iter 260510 || Loss: 0.7678 || timer: 0.0901 sec.
iter 260520 || Loss: 0.5165 || timer: 0.0833 sec.
iter 260530 || Loss: 0.7188 || timer: 0.0848 sec.
iter 260540 || Loss: 1.0136 || timer: 0.0875 sec.
iter 260550 || Loss: 0.8120 || timer: 0.0825 sec.
iter 260560 || Loss: 1.1079 || timer: 0.0934 sec.
iter 260570 || Loss: 0.5670 || timer: 0.0838 sec.
iter 260580 || Loss: 0.7936 || timer: 0.1120 sec.
iter 260590 || Loss: 1.1109 || timer: 0.0202 sec.
iter 260600 || Loss: 0.6312 || timer: 0.0833 sec.
iter 260610 || Loss: 0.8085 || timer: 0.0896 sec.
iter 260620 || Loss: 0.9938 || timer: 0.0835 sec.
iter 260630 || Loss: 0.8484 || timer: 0.0901 sec.
iter 260640 || Loss: 0.7685 || timer: 0.1037 sec.
iter 260650 || Loss: 0.9145 || timer: 0.0831 sec.
iter 260660 || Loss: 1.0728 || timer: 0.0901 sec.
iter 260670 || Loss: 0.8734 || timer: 0.0852 sec.
iter 260680 || Loss: 0.9283 || timer: 0.0839 sec.
iter 260690 || Loss: 1.1709 || timer: 0.1024 sec.
iter 260700 || Loss: 0.7837 || timer: 0.0825 sec.
iter 260710 || Loss: 0.8440 || timer: 0.0866 sec.
iter 260720 || Loss: 1.5773 || timer: 0.0937 sec.
iter 260730 || Loss: 1.1086 || timer: 0.0934 sec.
iter 260740 || Loss: 1.0414 || timer: 0.1083 sec.
iter 260750 || Loss: 0.6262 || timer: 0.1035 sec.
iter 260760 || Loss: 0.6792 || timer: 0.0930 sec.
iter 260770 || Loss: 0.8670 || timer: 0.0824 sec.
iter 260780 || Loss: 0.8162 || timer: 0.0895 sec.
iter 260790 || Loss: 1.0289 || timer: 0.0898 sec.
iter 260800 || Loss: 0.9116 || timer: 0.0840 sec.
iter 260810 || Loss: 0.9043 || timer: 0.0840 sec.
iter 260820 || Loss: 0.9163 || timer: 0.0931 sec.
iter 260830 || Loss: 0.9474 || timer: 0.0937 sec.
iter 260840 || Loss: 0.9573 || timer: 0.0842 sec.
iter 260850 || Loss: 0.7839 || timer: 0.1060 sec.
iter 260860 || Loss: 1.1467 || timer: 0.0974 sec.
iter 260870 || Loss: 0.7374 || timer: 0.0826 sec.
iter 260880 || Loss: 0.8334 || timer: 0.0837 sec.
iter 260890 || Loss: 0.8004 || timer: 0.0819 sec.
iter 260900 || Loss: 1.0333 || timer: 0.1033 sec.
iter 260910 || Loss: 0.8207 || timer: 0.0959 sec.
iter 260920 || Loss: 0.8989 || timer: 0.0167 sec.
iter 260930 || Loss: 1.4856 || timer: 0.1049 sec.
iter 260940 || Loss: 1.0486 || timer: 0.0912 sec.
iter 260950 || Loss: 1.0945 || timer: 0.0891 sec.
iter 260960 || Loss: 1.0894 || timer: 0.0823 sec.
iter 260970 || Loss: 0.8001 || timer: 0.0883 sec.
iter 260980 || Loss: 1.0114 || timer: 0.0901 sec.
iter 260990 || Loss: 0.9375 || timer: 0.0919 sec.
iter 261000 || Loss: 1.0725 || timer: 0.0903 sec.
iter 261010 || Loss: 0.8465 || timer: 0.0881 sec.
iter 261020 || Loss: 1.0815 || timer: 0.1101 sec.
iter 261030 || Loss: 0.7737 || timer: 0.0906 sec.
iter 261040 || Loss: 1.1620 || timer: 0.0901 sec.
iter 261050 || Loss: 1.1367 || timer: 0.0900 sec.
iter 261060 || Loss: 0.6894 || timer: 0.0827 sec.
iter 261070 || Loss: 0.9813 || timer: 0.0996 sec.
iter 261080 || Loss: 0.8563 || timer: 0.0901 sec.
iter 261090 || Loss: 0.9321 || timer: 0.0921 sec.
iter 261100 || Loss: 0.8797 || timer: 0.0903 sec.
iter 261110 || Loss: 0.9050 || timer: 0.0837 sec.
iter 261120 || Loss: 1.0115 || timer: 0.0904 sec.
iter 261130 || Loss: 0.6286 || timer: 0.1044 sec.
iter 261140 || Loss: 1.1225 || timer: 0.0915 sec.
iter 261150 || Loss: 0.7983 || timer: 0.0921 sec.
iter 261160 || Loss: 1.2458 || timer: 0.0880 sec.
iter 261170 || Loss: 0.7066 || timer: 0.0924 sec.
iter 261180 || Loss: 1.2315 || timer: 0.0823 sec.
iter 261190 || Loss: 1.0002 || timer: 0.0910 sec.
iter 261200 || Loss: 0.7066 || timer: 0.0859 sec.
iter 261210 || Loss: 0.7668 || timer: 0.0884 sec.
iter 261220 || Loss: 0.9324 || timer: 0.0900 sec.
iter 261230 || Loss: 0.8104 || timer: 0.0902 sec.
iter 261240 || Loss: 0.8656 || timer: 0.1029 sec.
iter 261250 || Loss: 0.6648 || timer: 0.0275 sec.
iter 261260 || Loss: 1.0750 || timer: 0.0871 sec.
iter 261270 || Loss: 1.2183 || timer: 0.0906 sec.
iter 261280 || Loss: 0.7430 || timer: 0.0917 sec.
iter 261290 || Loss: 0.6236 || timer: 0.0850 sec.
iter 261300 || Loss: 1.0618 || timer: 0.0915 sec.
iter 261310 || Loss: 1.0331 || timer: 0.0902 sec.
iter 261320 || Loss: 0.9606 || timer: 0.1039 sec.
iter 261330 || Loss: 0.8225 || timer: 0.0873 sec.
iter 261340 || Loss: 0.9798 || timer: 0.0901 sec.
iter 261350 || Loss: 0.7353 || timer: 0.0957 sec.
iter 261360 || Loss: 0.7654 || timer: 0.0939 sec.
iter 261370 || Loss: 0.6882 || timer: 0.0835 sec.
iter 261380 || Loss: 0.9463 || timer: 0.0907 sec.
iter 261390 || Loss: 0.7007 || timer: 0.0939 sec.
iter 261400 || Loss: 0.5930 || timer: 0.0904 sec.
iter 261410 || Loss: 0.8359 || timer: 0.0820 sec.
iter 261420 || Loss: 0.8405 || timer: 0.0924 sec.
iter 261430 || Loss: 0.6110 || timer: 0.0904 sec.
iter 261440 || Loss: 1.1836 || timer: 0.0968 sec.
iter 261450 || Loss: 0.9556 || timer: 0.0912 sec.
iter 261460 || Loss: 0.6669 || timer: 0.0901 sec.
iter 261470 || Loss: 0.8073 || timer: 0.0895 sec.
iter 261480 || Loss: 0.8617 || timer: 0.0906 sec.
iter 261490 || Loss: 0.7842 || timer: 0.0901 sec.
iter 261500 || Loss: 0.8781 || timer: 0.0912 sec.
iter 261510 || Loss: 0.7953 || timer: 0.0921 sec.
iter 261520 || Loss: 0.8979 || timer: 0.0824 sec.
iter 261530 || Loss: 0.7528 || timer: 0.1002 sec.
iter 261540 || Loss: 1.0142 || timer: 0.0928 sec.
iter 261550 || Loss: 0.9633 || timer: 0.0915 sec.
iter 261560 || Loss: 0.8393 || timer: 0.1130 sec.
iter 261570 || Loss: 0.6643 || timer: 0.0826 sec.
iter 261580 || Loss: 0.6887 || timer: 0.0181 sec.
iter 261590 || Loss: 0.7609 || timer: 0.0830 sec.
iter 261600 || Loss: 0.6053 || timer: 0.0909 sec.
iter 261610 || Loss: 0.6709 || timer: 0.0876 sec.
iter 261620 || Loss: 0.7277 || timer: 0.0930 sec.
iter 261630 || Loss: 0.8317 || timer: 0.1091 sec.
iter 261640 || Loss: 0.9951 || timer: 0.0836 sec.
iter 261650 || Loss: 0.9205 || timer: 0.0886 sec.
iter 261660 || Loss: 0.8770 || timer: 0.0823 sec.
iter 261670 || Loss: 0.8910 || timer: 0.0910 sec.
iter 261680 || Loss: 0.8965 || timer: 0.1183 sec.
iter 261690 || Loss: 0.8698 || timer: 0.1085 sec.
iter 261700 || Loss: 0.9882 || timer: 0.0874 sec.
iter 261710 || Loss: 1.0082 || timer: 0.0837 sec.
iter 261720 || Loss: 0.9331 || timer: 0.1104 sec.
iter 261730 || Loss: 0.9032 || timer: 0.0956 sec.
iter 261740 || Loss: 0.7933 || timer: 0.0915 sec.
iter 261750 || Loss: 1.1294 || timer: 0.1027 sec.
iter 261760 || Loss: 1.0358 || timer: 0.1245 sec.
iter 261770 || Loss: 0.7966 || timer: 0.0910 sec.
iter 261780 || Loss: 1.1364 || timer: 0.0911 sec.
iter 261790 || Loss: 0.8199 || timer: 0.0890 sec.
iter 261800 || Loss: 0.9356 || timer: 0.0843 sec.
iter 261810 || Loss: 0.6710 || timer: 0.0841 sec.
iter 261820 || Loss: 0.7055 || timer: 0.0928 sec.
iter 261830 || Loss: 0.8032 || timer: 0.0913 sec.
iter 261840 || Loss: 0.8092 || timer: 0.0897 sec.
iter 261850 || Loss: 1.1272 || timer: 0.0834 sec.
iter 261860 || Loss: 0.9048 || timer: 0.0933 sec.
iter 261870 || Loss: 0.7878 || timer: 0.0864 sec.
iter 261880 || Loss: 0.7892 || timer: 0.0823 sec.
iter 261890 || Loss: 0.6322 || timer: 0.0933 sec.
iter 261900 || Loss: 1.1454 || timer: 0.0905 sec.
iter 261910 || Loss: 0.9448 || timer: 0.0170 sec.
iter 261920 || Loss: 1.3725 || timer: 0.0950 sec.
iter 261930 || Loss: 0.7960 || timer: 0.1022 sec.
iter 261940 || Loss: 0.8113 || timer: 0.0888 sec.
iter 261950 || Loss: 0.6741 || timer: 0.0890 sec.
iter 261960 || Loss: 0.7220 || timer: 0.0880 sec.
iter 261970 || Loss: 0.7835 || timer: 0.0883 sec.
iter 261980 || Loss: 1.0045 || timer: 0.0920 sec.
iter 261990 || Loss: 0.7976 || timer: 0.0905 sec.
iter 262000 || Loss: 0.8142 || timer: 0.1180 sec.
iter 262010 || Loss: 0.9068 || timer: 0.1133 sec.
iter 262020 || Loss: 0.8061 || timer: 0.1073 sec.
iter 262030 || Loss: 0.7429 || timer: 0.0895 sec.
iter 262040 || Loss: 0.8721 || timer: 0.0902 sec.
iter 262050 || Loss: 1.0278 || timer: 0.0885 sec.
iter 262060 || Loss: 0.8926 || timer: 0.0902 sec.
iter 262070 || Loss: 1.0266 || timer: 0.0840 sec.
iter 262080 || Loss: 0.8506 || timer: 0.0924 sec.
iter 262090 || Loss: 0.7871 || timer: 0.0809 sec.
iter 262100 || Loss: 0.6721 || timer: 0.1123 sec.
iter 262110 || Loss: 0.6399 || timer: 0.0899 sec.
iter 262120 || Loss: 0.6761 || timer: 0.0821 sec.
iter 262130 || Loss: 0.6802 || timer: 0.0925 sec.
iter 262140 || Loss: 0.9949 || timer: 0.0892 sec.
iter 262150 || Loss: 0.9464 || timer: 0.1099 sec.
iter 262160 || Loss: 0.6492 || timer: 0.0904 sec.
iter 262170 || Loss: 0.8004 || timer: 0.0919 sec.
iter 262180 || Loss: 1.0045 || timer: 0.0912 sec.
iter 262190 || Loss: 0.6806 || timer: 0.1011 sec.
iter 262200 || Loss: 1.1450 || timer: 0.0884 sec.
iter 262210 || Loss: 0.8668 || timer: 0.0826 sec.
iter 262220 || Loss: 1.0123 || timer: 0.0940 sec.
iter 262230 || Loss: 0.6837 || timer: 0.1040 sec.
iter 262240 || Loss: 0.8465 || timer: 0.0244 sec.
iter 262250 || Loss: 0.2043 || timer: 0.0917 sec.
iter 262260 || Loss: 0.8274 || timer: 0.0917 sec.
iter 262270 || Loss: 0.7462 || timer: 0.0919 sec.
iter 262280 || Loss: 0.8289 || timer: 0.0752 sec.
iter 262290 || Loss: 0.8657 || timer: 0.0842 sec.
iter 262300 || Loss: 0.9023 || timer: 0.0855 sec.
iter 262310 || Loss: 1.0918 || timer: 0.0918 sec.
iter 262320 || Loss: 0.9263 || timer: 0.1117 sec.
iter 262330 || Loss: 1.1965 || timer: 0.0833 sec.
iter 262340 || Loss: 1.0321 || timer: 0.1084 sec.
iter 262350 || Loss: 0.8661 || timer: 0.0915 sec.
iter 262360 || Loss: 0.8656 || timer: 0.1117 sec.
iter 262370 || Loss: 0.9611 || timer: 0.0832 sec.
iter 262380 || Loss: 0.7523 || timer: 0.0827 sec.
iter 262390 || Loss: 0.7967 || timer: 0.0841 sec.
iter 262400 || Loss: 1.0863 || timer: 0.1035 sec.
iter 262410 || Loss: 1.0167 || timer: 0.0875 sec.
iter 262420 || Loss: 0.7792 || timer: 0.0919 sec.
iter 262430 || Loss: 0.8541 || timer: 0.0938 sec.
iter 262440 || Loss: 0.9050 || timer: 0.0923 sec.
iter 262450 || Loss: 0.7856 || timer: 0.1064 sec.
iter 262460 || Loss: 1.0427 || timer: 0.0835 sec.
iter 262470 || Loss: 1.2977 || timer: 0.0906 sec.
iter 262480 || Loss: 0.8287 || timer: 0.0913 sec.
iter 262490 || Loss: 0.9998 || timer: 0.0883 sec.
iter 262500 || Loss: 0.7405 || timer: 0.1150 sec.
iter 262510 || Loss: 1.0403 || timer: 0.0914 sec.
iter 262520 || Loss: 0.7240 || timer: 0.0839 sec.
iter 262530 || Loss: 0.8661 || timer: 0.0919 sec.
iter 262540 || Loss: 0.6771 || timer: 0.0945 sec.
iter 262550 || Loss: 0.7002 || timer: 0.1071 sec.
iter 262560 || Loss: 0.8185 || timer: 0.0833 sec.
iter 262570 || Loss: 1.0009 || timer: 0.0250 sec.
iter 262580 || Loss: 0.7267 || timer: 0.0826 sec.
iter 262590 || Loss: 0.8259 || timer: 0.0826 sec.
iter 262600 || Loss: 0.7357 || timer: 0.0893 sec.
iter 262610 || Loss: 0.8213 || timer: 0.0889 sec.
iter 262620 || Loss: 0.9885 || timer: 0.0922 sec.
iter 262630 || Loss: 0.8179 || timer: 0.0840 sec.
iter 262640 || Loss: 1.3600 || timer: 0.0928 sec.
iter 262650 || Loss: 0.7641 || timer: 0.0898 sec.
iter 262660 || Loss: 1.0170 || timer: 0.0907 sec.
iter 262670 || Loss: 0.8861 || timer: 0.0955 sec.
iter 262680 || Loss: 0.8392 || timer: 0.0924 sec.
iter 262690 || Loss: 0.7081 || timer: 0.0982 sec.
iter 262700 || Loss: 0.8547 || timer: 0.0880 sec.
iter 262710 || Loss: 0.6158 || timer: 0.1086 sec.
iter 262720 || Loss: 0.9729 || timer: 0.0909 sec.
iter 262730 || Loss: 0.7470 || timer: 0.0910 sec.
iter 262740 || Loss: 0.7991 || timer: 0.0951 sec.
iter 262750 || Loss: 0.9196 || timer: 0.0914 sec.
iter 262760 || Loss: 0.8058 || timer: 0.0938 sec.
iter 262770 || Loss: 0.8685 || timer: 0.1018 sec.
iter 262780 || Loss: 0.5987 || timer: 0.0832 sec.
iter 262790 || Loss: 1.0887 || timer: 0.0916 sec.
iter 262800 || Loss: 0.9644 || timer: 0.0836 sec.
iter 262810 || Loss: 0.7636 || timer: 0.0819 sec.
iter 262820 || Loss: 0.8200 || timer: 0.0917 sec.
iter 262830 || Loss: 0.9493 || timer: 0.0873 sec.
iter 262840 || Loss: 0.5161 || timer: 0.0879 sec.
iter 262850 || Loss: 0.6640 || timer: 0.0826 sec.
iter 262860 || Loss: 1.1651 || timer: 0.1105 sec.
iter 262870 || Loss: 0.9487 || timer: 0.0848 sec.
iter 262880 || Loss: 0.7599 || timer: 0.0894 sec.
iter 262890 || Loss: 0.9988 || timer: 0.0834 sec.
iter 262900 || Loss: 0.7094 || timer: 0.0271 sec.
iter 262910 || Loss: 0.6319 || timer: 0.0928 sec.
iter 262920 || Loss: 0.6511 || timer: 0.0916 sec.
iter 262930 || Loss: 0.7047 || timer: 0.0899 sec.
iter 262940 || Loss: 1.0587 || timer: 0.0961 sec.
iter 262950 || Loss: 0.7852 || timer: 0.0930 sec.
iter 262960 || Loss: 0.9430 || timer: 0.0863 sec.
iter 262970 || Loss: 0.5806 || timer: 0.0836 sec.
iter 262980 || Loss: 0.5855 || timer: 0.0840 sec.
iter 262990 || Loss: 0.6169 || timer: 0.0968 sec.
iter 263000 || Loss: 0.9648 || timer: 0.0976 sec.
iter 263010 || Loss: 0.9134 || timer: 0.0835 sec.
iter 263020 || Loss: 0.8784 || timer: 0.1110 sec.
iter 263030 || Loss: 0.8591 || timer: 0.0919 sec.
iter 263040 || Loss: 0.8065 || timer: 0.0835 sec.
iter 263050 || Loss: 1.1392 || timer: 0.0910 sec.
iter 263060 || Loss: 1.1557 || timer: 0.0920 sec.
iter 263070 || Loss: 0.8673 || timer: 0.0922 sec.
iter 263080 || Loss: 0.7621 || timer: 0.0983 sec.
iter 263090 || Loss: 0.8118 || timer: 0.0847 sec.
iter 263100 || Loss: 0.7547 || timer: 0.0836 sec.
iter 263110 || Loss: 0.6631 || timer: 0.0915 sec.
iter 263120 || Loss: 0.9343 || timer: 0.0937 sec.
iter 263130 || Loss: 0.6753 || timer: 0.0883 sec.
iter 263140 || Loss: 0.7439 || timer: 0.1016 sec.
iter 263150 || Loss: 0.9813 || timer: 0.0913 sec.
iter 263160 || Loss: 0.6982 || timer: 0.0896 sec.
iter 263170 || Loss: 0.8754 || timer: 0.1139 sec.
iter 263180 || Loss: 0.8696 || timer: 0.0928 sec.
iter 263190 || Loss: 0.9165 || timer: 0.1031 sec.
iter 263200 || Loss: 0.7283 || timer: 0.0904 sec.
iter 263210 || Loss: 0.6769 || timer: 0.0905 sec.
iter 263220 || Loss: 1.0344 || timer: 0.0904 sec.
iter 263230 || Loss: 0.8719 || timer: 0.0228 sec.
iter 263240 || Loss: 0.1921 || timer: 0.0839 sec.
iter 263250 || Loss: 0.6107 || timer: 0.0993 sec.
iter 263260 || Loss: 1.2257 || timer: 0.0873 sec.
iter 263270 || Loss: 0.8801 || timer: 0.1185 sec.
iter 263280 || Loss: 1.1588 || timer: 0.0907 sec.
iter 263290 || Loss: 0.9160 || timer: 0.1034 sec.
iter 263300 || Loss: 0.9128 || timer: 0.0922 sec.
iter 263310 || Loss: 0.8111 || timer: 0.0901 sec.
iter 263320 || Loss: 0.9468 || timer: 0.0836 sec.
iter 263330 || Loss: 1.0480 || timer: 0.1104 sec.
iter 263340 || Loss: 0.7359 || timer: 0.0905 sec.
iter 263350 || Loss: 0.9232 || timer: 0.1054 sec.
iter 263360 || Loss: 1.0033 || timer: 0.0929 sec.
iter 263370 || Loss: 0.7890 || timer: 0.0832 sec.
iter 263380 || Loss: 0.9984 || timer: 0.0823 sec.
iter 263390 || Loss: 0.9726 || timer: 0.0918 sec.
iter 263400 || Loss: 0.9836 || timer: 0.0982 sec.
iter 263410 || Loss: 0.9748 || timer: 0.0929 sec.
iter 263420 || Loss: 0.7648 || timer: 0.0891 sec.
iter 263430 || Loss: 1.0484 || timer: 0.0905 sec.
iter 263440 || Loss: 0.6583 || timer: 0.1192 sec.
iter 263450 || Loss: 1.0451 || timer: 0.0910 sec.
iter 263460 || Loss: 0.6773 || timer: 0.0907 sec.
iter 263470 || Loss: 0.9382 || timer: 0.0907 sec.
iter 263480 || Loss: 0.9810 || timer: 0.0911 sec.
iter 263490 || Loss: 0.9698 || timer: 0.0833 sec.
iter 263500 || Loss: 0.7530 || timer: 0.0926 sec.
iter 263510 || Loss: 0.9498 || timer: 0.0907 sec.
iter 263520 || Loss: 1.0307 || timer: 0.1202 sec.
iter 263530 || Loss: 1.1706 || timer: 0.0898 sec.
iter 263540 || Loss: 1.0046 || timer: 0.0960 sec.
iter 263550 || Loss: 0.8822 || timer: 0.1065 sec.
iter 263560 || Loss: 1.2834 || timer: 0.0165 sec.
iter 263570 || Loss: 1.5194 || timer: 0.0870 sec.
iter 263580 || Loss: 0.7780 || timer: 0.0876 sec.
iter 263590 || Loss: 0.9682 || timer: 0.0887 sec.
iter 263600 || Loss: 1.0127 || timer: 0.1242 sec.
iter 263610 || Loss: 1.3654 || timer: 0.0905 sec.
iter 263620 || Loss: 0.9297 || timer: 0.0905 sec.
iter 263630 || Loss: 1.4387 || timer: 0.0879 sec.
iter 263640 || Loss: 0.7369 || timer: 0.1240 sec.
iter 263650 || Loss: 1.0459 || timer: 0.1112 sec.
iter 263660 || Loss: 0.9348 || timer: 0.1247 sec.
iter 263670 || Loss: 0.7430 || timer: 0.0902 sec.
iter 263680 || Loss: 0.9382 || timer: 0.0909 sec.
iter 263690 || Loss: 0.7455 || timer: 0.0896 sec.
iter 263700 || Loss: 0.8135 || timer: 0.1044 sec.
iter 263710 || Loss: 0.6060 || timer: 0.0827 sec.
iter 263720 || Loss: 0.9099 || timer: 0.0821 sec.
iter 263730 || Loss: 0.9268 || timer: 0.1121 sec.
iter 263740 || Loss: 0.7380 || timer: 0.0892 sec.
iter 263750 || Loss: 1.0390 || timer: 0.1006 sec.
iter 263760 || Loss: 0.9642 || timer: 0.0869 sec.
iter 263770 || Loss: 0.9063 || timer: 0.1049 sec.
iter 263780 || Loss: 1.0077 || timer: 0.0819 sec.
iter 263790 || Loss: 0.9576 || timer: 0.0892 sec.
iter 263800 || Loss: 1.0199 || timer: 0.0878 sec.
iter 263810 || Loss: 0.8024 || timer: 0.0828 sec.
iter 263820 || Loss: 0.7358 || timer: 0.1018 sec.
iter 263830 || Loss: 1.1744 || timer: 0.0843 sec.
iter 263840 || Loss: 0.9757 || timer: 0.0918 sec.
iter 263850 || Loss: 1.3643 || timer: 0.0886 sec.
iter 263860 || Loss: 0.6152 || timer: 0.0907 sec.
iter 263870 || Loss: 0.8374 || timer: 0.0906 sec.
iter 263880 || Loss: 0.9799 || timer: 0.0900 sec.
iter 263890 || Loss: 0.6697 || timer: 0.0241 sec.
iter 263900 || Loss: 0.6537 || timer: 0.0918 sec.
iter 263910 || Loss: 1.0270 || timer: 0.0935 sec.
iter 263920 || Loss: 0.9492 || timer: 0.1022 sec.
iter 263930 || Loss: 0.7710 || timer: 0.1426 sec.
iter 263940 || Loss: 0.9644 || timer: 0.0924 sec.
iter 263950 || Loss: 1.0833 || timer: 0.0824 sec.
iter 263960 || Loss: 0.8886 || timer: 0.0830 sec.
iter 263970 || Loss: 1.1257 || timer: 0.0990 sec.
iter 263980 || Loss: 0.8191 || timer: 0.0908 sec.
iter 263990 || Loss: 0.7785 || timer: 0.1153 sec.
iter 264000 || Loss: 0.5965 || timer: 0.0895 sec.
iter 264010 || Loss: 1.0223 || timer: 0.0831 sec.
iter 264020 || Loss: 0.8704 || timer: 0.0816 sec.
iter 264030 || Loss: 0.5889 || timer: 0.0820 sec.
iter 264040 || Loss: 1.0906 || timer: 0.0822 sec.
iter 264050 || Loss: 0.7370 || timer: 0.0833 sec.
iter 264060 || Loss: 1.0210 || timer: 0.0895 sec.
iter 264070 || Loss: 0.9667 || timer: 0.0840 sec.
iter 264080 || Loss: 0.9809 || timer: 0.1062 sec.
iter 264090 || Loss: 1.0894 || timer: 0.0842 sec.
iter 264100 || Loss: 1.3065 || timer: 0.1007 sec.
iter 264110 || Loss: 0.8945 || timer: 0.0875 sec.
iter 264120 || Loss: 0.9670 || timer: 0.0889 sec.
iter 264130 || Loss: 1.0154 || timer: 0.0906 sec.
iter 264140 || Loss: 0.7282 || timer: 0.0850 sec.
iter 264150 || Loss: 0.7324 || timer: 0.0903 sec.
iter 264160 || Loss: 0.9358 || timer: 0.0880 sec.
iter 264170 || Loss: 0.8162 || timer: 0.0904 sec.
iter 264180 || Loss: 0.8917 || timer: 0.0998 sec.
iter 264190 || Loss: 0.7032 || timer: 0.0876 sec.
iter 264200 || Loss: 0.6346 || timer: 0.0825 sec.
iter 264210 || Loss: 1.2387 || timer: 0.0898 sec.
iter 264220 || Loss: 0.8390 || timer: 0.0279 sec.
iter 264230 || Loss: 0.1806 || timer: 0.0827 sec.
iter 264240 || Loss: 0.8301 || timer: 0.1044 sec.
iter 264250 || Loss: 1.0176 || timer: 0.0830 sec.
iter 264260 || Loss: 0.9077 || timer: 0.0880 sec.
iter 264270 || Loss: 1.4528 || timer: 0.1040 sec.
iter 264280 || Loss: 1.1877 || timer: 0.0823 sec.
iter 264290 || Loss: 0.8648 || timer: 0.1030 sec.
iter 264300 || Loss: 1.0472 || timer: 0.0824 sec.
iter 264310 || Loss: 0.7068 || timer: 0.0837 sec.
iter 264320 || Loss: 0.6663 || timer: 0.0995 sec.
iter 264330 || Loss: 1.0826 || timer: 0.0898 sec.
iter 264340 || Loss: 1.2519 || timer: 0.0826 sec.
iter 264350 || Loss: 0.7775 || timer: 0.0819 sec.
iter 264360 || Loss: 0.9150 || timer: 0.0914 sec.
iter 264370 || Loss: 1.5290 || timer: 0.0927 sec.
iter 264380 || Loss: 1.0241 || timer: 0.1022 sec.
iter 264390 || Loss: 0.8297 || timer: 0.0896 sec.
iter 264400 || Loss: 0.9173 || timer: 0.0825 sec.
iter 264410 || Loss: 1.1530 || timer: 0.1130 sec.
iter 264420 || Loss: 0.9857 || timer: 0.0826 sec.
iter 264430 || Loss: 0.7820 || timer: 0.0825 sec.
iter 264440 || Loss: 0.9409 || timer: 0.0919 sec.
iter 264450 || Loss: 0.6081 || timer: 0.0724 sec.
iter 264460 || Loss: 0.9547 || timer: 0.0892 sec.
iter 264470 || Loss: 1.0102 || timer: 0.1004 sec.
iter 264480 || Loss: 1.2634 || timer: 0.0890 sec.
iter 264490 || Loss: 0.7243 || timer: 0.0850 sec.
iter 264500 || Loss: 1.0871 || timer: 0.0840 sec.
iter 264510 || Loss: 0.8446 || timer: 0.0766 sec.
iter 264520 || Loss: 1.0278 || timer: 0.0915 sec.
iter 264530 || Loss: 0.9746 || timer: 0.0978 sec.
iter 264540 || Loss: 0.5019 || timer: 0.0895 sec.
iter 264550 || Loss: 0.9308 || timer: 0.0258 sec.
iter 264560 || Loss: 2.5204 || timer: 0.1058 sec.
iter 264570 || Loss: 1.2673 || timer: 0.0791 sec.
iter 264580 || Loss: 0.6864 || timer: 0.0814 sec.
iter 264590 || Loss: 0.8682 || timer: 0.0750 sec.
iter 264600 || Loss: 0.7871 || timer: 0.0835 sec.
iter 264610 || Loss: 0.5585 || timer: 0.0923 sec.
iter 264620 || Loss: 0.7571 || timer: 0.0866 sec.
iter 264630 || Loss: 0.8034 || timer: 0.1035 sec.
iter 264640 || Loss: 1.0457 || timer: 0.0905 sec.
iter 264650 || Loss: 0.9929 || timer: 0.0942 sec.
iter 264660 || Loss: 0.7097 || timer: 0.0910 sec.
iter 264670 || Loss: 0.8165 || timer: 0.0906 sec.
iter 264680 || Loss: 0.7917 || timer: 0.0958 sec.
iter 264690 || Loss: 0.9479 || timer: 0.0930 sec.
iter 264700 || Loss: 0.7615 || timer: 0.0906 sec.
iter 264710 || Loss: 0.8362 || timer: 0.1051 sec.
iter 264720 || Loss: 0.7361 || timer: 0.0885 sec.
iter 264730 || Loss: 2.3359 || timer: 0.0825 sec.
iter 264740 || Loss: 1.7852 || timer: 0.0816 sec.
iter 264750 || Loss: 1.2615 || timer: 0.0878 sec.
iter 264760 || Loss: 1.0753 || timer: 0.0927 sec.
iter 264770 || Loss: 1.0267 || timer: 0.0896 sec.
iter 264780 || Loss: 1.0120 || timer: 0.0825 sec.
iter 264790 || Loss: 0.9733 || timer: 0.0824 sec.
iter 264800 || Loss: 0.8752 || timer: 0.1080 sec.
iter 264810 || Loss: 0.7694 || timer: 0.0823 sec.
iter 264820 || Loss: 0.6290 || timer: 0.0905 sec.
iter 264830 || Loss: 1.1628 || timer: 0.0890 sec.
iter 264840 || Loss: 0.9773 || timer: 0.0893 sec.
iter 264850 || Loss: 0.7346 || timer: 0.0835 sec.
iter 264860 || Loss: 0.9323 || timer: 0.0906 sec.
iter 264870 || Loss: 0.7280 || timer: 0.0901 sec.
iter 264880 || Loss: 0.7755 || timer: 0.0236 sec.
iter 264890 || Loss: 1.3265 || timer: 0.0911 sec.
iter 264900 || Loss: 0.8625 || timer: 0.0826 sec.
iter 264910 || Loss: 1.0940 || timer: 0.0912 sec.
iter 264920 || Loss: 0.5957 || timer: 0.0848 sec.
iter 264930 || Loss: 0.8971 || timer: 0.0912 sec.
iter 264940 || Loss: 1.3448 || timer: 0.1100 sec.
iter 264950 || Loss: 0.6480 || timer: 0.0892 sec.
iter 264960 || Loss: 1.4325 || timer: 0.0926 sec.
iter 264970 || Loss: 1.3340 || timer: 0.0832 sec.
iter 264980 || Loss: 1.0295 || timer: 0.1240 sec.
iter 264990 || Loss: 0.9839 || timer: 0.0867 sec.
iter 265000 || Loss: 0.9833 || Saving state, iter: 265000
timer: 0.1030 sec.
iter 265010 || Loss: 0.8015 || timer: 0.0887 sec.
iter 265020 || Loss: 0.7889 || timer: 0.0762 sec.
iter 265030 || Loss: 0.6944 || timer: 0.0820 sec.
iter 265040 || Loss: 0.9483 || timer: 0.0840 sec.
iter 265050 || Loss: 0.8157 || timer: 0.0905 sec.
iter 265060 || Loss: 0.8421 || timer: 0.1066 sec.
iter 265070 || Loss: 1.0183 || timer: 0.1100 sec.
iter 265080 || Loss: 0.8478 || timer: 0.0941 sec.
iter 265090 || Loss: 0.8278 || timer: 0.0897 sec.
iter 265100 || Loss: 1.1433 || timer: 0.0833 sec.
iter 265110 || Loss: 0.8983 || timer: 0.0799 sec.
iter 265120 || Loss: 0.7843 || timer: 0.0931 sec.
iter 265130 || Loss: 0.9914 || timer: 0.0826 sec.
iter 265140 || Loss: 0.9296 || timer: 0.0823 sec.
iter 265150 || Loss: 1.0535 || timer: 0.0903 sec.
iter 265160 || Loss: 0.7692 || timer: 0.0998 sec.
iter 265170 || Loss: 0.9195 || timer: 0.0814 sec.
iter 265180 || Loss: 0.9877 || timer: 0.0912 sec.
iter 265190 || Loss: 0.9774 || timer: 0.1047 sec.
iter 265200 || Loss: 0.7239 || timer: 0.1131 sec.
iter 265210 || Loss: 0.8845 || timer: 0.0159 sec.
iter 265220 || Loss: 0.4326 || timer: 0.0804 sec.
iter 265230 || Loss: 1.1494 || timer: 0.0878 sec.
iter 265240 || Loss: 0.7178 || timer: 0.0903 sec.
iter 265250 || Loss: 0.7463 || timer: 0.0876 sec.
iter 265260 || Loss: 0.7953 || timer: 0.0936 sec.
iter 265270 || Loss: 0.6368 || timer: 0.0894 sec.
iter 265280 || Loss: 0.9570 || timer: 0.0913 sec.
iter 265290 || Loss: 0.8506 || timer: 0.0928 sec.
iter 265300 || Loss: 0.7874 || timer: 0.1063 sec.
iter 265310 || Loss: 0.7795 || timer: 0.1200 sec.
iter 265320 || Loss: 0.9863 || timer: 0.1118 sec.
iter 265330 || Loss: 0.8804 || timer: 0.0822 sec.
iter 265340 || Loss: 0.9283 || timer: 0.0903 sec.
iter 265350 || Loss: 1.1690 || timer: 0.0925 sec.
iter 265360 || Loss: 0.8701 || timer: 0.0878 sec.
iter 265370 || Loss: 0.6773 || timer: 0.0836 sec.
iter 265380 || Loss: 0.5803 || timer: 0.0902 sec.
iter 265390 || Loss: 0.8772 || timer: 0.0908 sec.
iter 265400 || Loss: 0.7687 || timer: 0.1146 sec.
iter 265410 || Loss: 1.0806 || timer: 0.0758 sec.
iter 265420 || Loss: 0.8180 || timer: 0.1112 sec.
iter 265430 || Loss: 0.8181 || timer: 0.0893 sec.
iter 265440 || Loss: 0.9242 || timer: 0.0877 sec.
iter 265450 || Loss: 0.6420 || timer: 0.0826 sec.
iter 265460 || Loss: 0.6321 || timer: 0.0755 sec.
iter 265470 || Loss: 0.7057 || timer: 0.0937 sec.
iter 265480 || Loss: 0.9266 || timer: 0.1155 sec.
iter 265490 || Loss: 0.8932 || timer: 0.0897 sec.
iter 265500 || Loss: 0.9279 || timer: 0.1053 sec.
iter 265510 || Loss: 0.7547 || timer: 0.0835 sec.
iter 265520 || Loss: 0.8470 || timer: 0.0917 sec.
iter 265530 || Loss: 0.7411 || timer: 0.0946 sec.
iter 265540 || Loss: 0.5550 || timer: 0.0221 sec.
iter 265550 || Loss: 0.5474 || timer: 0.1047 sec.
iter 265560 || Loss: 0.8592 || timer: 0.1021 sec.
iter 265570 || Loss: 0.7539 || timer: 0.0844 sec.
iter 265580 || Loss: 1.0439 || timer: 0.0924 sec.
iter 265590 || Loss: 0.8258 || timer: 0.0952 sec.
iter 265600 || Loss: 0.7864 || timer: 0.0904 sec.
iter 265610 || Loss: 1.2567 || timer: 0.0825 sec.
iter 265620 || Loss: 1.1002 || timer: 0.1022 sec.
iter 265630 || Loss: 1.7267 || timer: 0.0879 sec.
iter 265640 || Loss: 1.0401 || timer: 0.1245 sec.
iter 265650 || Loss: 0.7805 || timer: 0.0876 sec.
iter 265660 || Loss: 0.9174 || timer: 0.0938 sec.
iter 265670 || Loss: 1.2727 || timer: 0.0920 sec.
iter 265680 || Loss: 1.0803 || timer: 0.0907 sec.
iter 265690 || Loss: 0.8974 || timer: 0.0890 sec.
iter 265700 || Loss: 0.7407 || timer: 0.0893 sec.
iter 265710 || Loss: 0.8394 || timer: 0.0897 sec.
iter 265720 || Loss: 0.9052 || timer: 0.0901 sec.
iter 265730 || Loss: 0.9420 || timer: 0.0880 sec.
iter 265740 || Loss: 1.0994 || timer: 0.0853 sec.
iter 265750 || Loss: 0.8494 || timer: 0.0909 sec.
iter 265760 || Loss: 0.8505 || timer: 0.0898 sec.
iter 265770 || Loss: 0.8981 || timer: 0.0900 sec.
iter 265780 || Loss: 1.0216 || timer: 0.0949 sec.
iter 265790 || Loss: 1.0828 || timer: 0.0831 sec.
iter 265800 || Loss: 1.3190 || timer: 0.0813 sec.
iter 265810 || Loss: 0.7840 || timer: 0.0881 sec.
iter 265820 || Loss: 0.7430 || timer: 0.0898 sec.
iter 265830 || Loss: 0.9752 || timer: 0.0904 sec.
iter 265840 || Loss: 0.7697 || timer: 0.0904 sec.
iter 265850 || Loss: 0.7989 || timer: 0.0848 sec.
iter 265860 || Loss: 0.9224 || timer: 0.0826 sec.
iter 265870 || Loss: 0.7173 || timer: 0.0225 sec.
iter 265880 || Loss: 1.5957 || timer: 0.0913 sec.
iter 265890 || Loss: 0.9054 || timer: 0.0840 sec.
iter 265900 || Loss: 0.7673 || timer: 0.0837 sec.
iter 265910 || Loss: 0.7121 || timer: 0.0918 sec.
iter 265920 || Loss: 0.7702 || timer: 0.0895 sec.
iter 265930 || Loss: 0.9894 || timer: 0.0996 sec.
iter 265940 || Loss: 0.6056 || timer: 0.1235 sec.
iter 265950 || Loss: 0.9085 || timer: 0.0969 sec.
iter 265960 || Loss: 0.9058 || timer: 0.0911 sec.
iter 265970 || Loss: 0.8507 || timer: 0.1007 sec.
iter 265980 || Loss: 0.9662 || timer: 0.0823 sec.
iter 265990 || Loss: 0.8442 || timer: 0.1060 sec.
iter 266000 || Loss: 0.8156 || timer: 0.0894 sec.
iter 266010 || Loss: 1.0505 || timer: 0.0899 sec.
iter 266020 || Loss: 0.7590 || timer: 0.0835 sec.
iter 266030 || Loss: 1.0610 || timer: 0.1019 sec.
iter 266040 || Loss: 0.9872 || timer: 0.0913 sec.
iter 266050 || Loss: 1.1569 || timer: 0.1088 sec.
iter 266060 || Loss: 0.5635 || timer: 0.0745 sec.
iter 266070 || Loss: 0.8575 || timer: 0.0959 sec.
iter 266080 || Loss: 1.1617 || timer: 0.0902 sec.
iter 266090 || Loss: 0.7874 || timer: 0.0761 sec.
iter 266100 || Loss: 0.8882 || timer: 0.0889 sec.
iter 266110 || Loss: 1.1820 || timer: 0.0828 sec.
iter 266120 || Loss: 0.8389 || timer: 0.0872 sec.
iter 266130 || Loss: 1.0372 || timer: 0.0959 sec.
iter 266140 || Loss: 1.0002 || timer: 0.0929 sec.
iter 266150 || Loss: 0.9563 || timer: 0.1018 sec.
iter 266160 || Loss: 0.7564 || timer: 0.0933 sec.
iter 266170 || Loss: 1.0778 || timer: 0.0874 sec.
iter 266180 || Loss: 0.8044 || timer: 0.0826 sec.
iter 266190 || Loss: 1.0851 || timer: 0.0953 sec.
iter 266200 || Loss: 0.8202 || timer: 0.0154 sec.
iter 266210 || Loss: 0.3686 || timer: 0.1058 sec.
iter 266220 || Loss: 0.8059 || timer: 0.0968 sec.
iter 266230 || Loss: 0.9342 || timer: 0.0894 sec.
iter 266240 || Loss: 0.6006 || timer: 0.0997 sec.
iter 266250 || Loss: 0.8767 || timer: 0.0995 sec.
iter 266260 || Loss: 0.9589 || timer: 0.0935 sec.
iter 266270 || Loss: 0.8428 || timer: 0.0826 sec.
iter 266280 || Loss: 0.7135 || timer: 0.0902 sec.
iter 266290 || Loss: 0.9214 || timer: 0.0902 sec.
iter 266300 || Loss: 1.0605 || timer: 0.0925 sec.
iter 266310 || Loss: 0.7193 || timer: 0.0828 sec.
iter 266320 || Loss: 0.6795 || timer: 0.0893 sec.
iter 266330 || Loss: 1.0544 || timer: 0.0875 sec.
iter 266340 || Loss: 0.8783 || timer: 0.0988 sec.
iter 266350 || Loss: 0.5996 || timer: 0.1026 sec.
iter 266360 || Loss: 0.9961 || timer: 0.0993 sec.
iter 266370 || Loss: 0.9472 || timer: 0.0840 sec.
iter 266380 || Loss: 1.1001 || timer: 0.1087 sec.
iter 266390 || Loss: 0.7914 || timer: 0.1040 sec.
iter 266400 || Loss: 1.0585 || timer: 0.0920 sec.
iter 266410 || Loss: 1.1496 || timer: 0.0896 sec.
iter 266420 || Loss: 0.9874 || timer: 0.1121 sec.
iter 266430 || Loss: 0.6711 || timer: 0.0907 sec.
iter 266440 || Loss: 0.9203 || timer: 0.0995 sec.
iter 266450 || Loss: 0.8705 || timer: 0.0904 sec.
iter 266460 || Loss: 0.8620 || timer: 0.0898 sec.
iter 266470 || Loss: 0.8539 || timer: 0.0918 sec.
iter 266480 || Loss: 0.7479 || timer: 0.0918 sec.
iter 266490 || Loss: 0.8864 || timer: 0.0893 sec.
iter 266500 || Loss: 0.7715 || timer: 0.0955 sec.
iter 266510 || Loss: 0.8643 || timer: 0.0892 sec.
iter 266520 || Loss: 1.0028 || timer: 0.0826 sec.
iter 266530 || Loss: 0.8057 || timer: 0.0294 sec.
iter 266540 || Loss: 0.8247 || timer: 0.0834 sec.
iter 266550 || Loss: 0.8697 || timer: 0.0825 sec.
iter 266560 || Loss: 0.7562 || timer: 0.0893 sec.
iter 266570 || Loss: 0.8994 || timer: 0.0905 sec.
iter 266580 || Loss: 0.7924 || timer: 0.0906 sec.
iter 266590 || Loss: 0.8187 || timer: 0.1003 sec.
iter 266600 || Loss: 1.0273 || timer: 0.0830 sec.
iter 266610 || Loss: 0.6056 || timer: 0.0820 sec.
iter 266620 || Loss: 0.6230 || timer: 0.0993 sec.
iter 266630 || Loss: 0.6668 || timer: 0.0971 sec.
iter 266640 || Loss: 0.8405 || timer: 0.0834 sec.
iter 266650 || Loss: 0.9110 || timer: 0.0893 sec.
iter 266660 || Loss: 0.8563 || timer: 0.0929 sec.
iter 266670 || Loss: 0.7734 || timer: 0.0893 sec.
iter 266680 || Loss: 1.1479 || timer: 0.1043 sec.
iter 266690 || Loss: 1.0101 || timer: 0.1040 sec.
iter 266700 || Loss: 0.9914 || timer: 0.0894 sec.
iter 266710 || Loss: 0.9228 || timer: 0.0922 sec.
iter 266720 || Loss: 0.7593 || timer: 0.0936 sec.
iter 266730 || Loss: 0.5960 || timer: 0.1003 sec.
iter 266740 || Loss: 0.8323 || timer: 0.1063 sec.
iter 266750 || Loss: 1.1538 || timer: 0.0866 sec.
iter 266760 || Loss: 0.8592 || timer: 0.0880 sec.
iter 266770 || Loss: 0.7483 || timer: 0.0932 sec.
iter 266780 || Loss: 0.6519 || timer: 0.0850 sec.
iter 266790 || Loss: 0.7764 || timer: 0.1075 sec.
iter 266800 || Loss: 0.7283 || timer: 0.0829 sec.
iter 266810 || Loss: 0.9507 || timer: 0.0992 sec.
iter 266820 || Loss: 1.0909 || timer: 0.0894 sec.
iter 266830 || Loss: 0.6844 || timer: 0.0897 sec.
iter 266840 || Loss: 0.8275 || timer: 0.0887 sec.
iter 266850 || Loss: 0.8592 || timer: 0.0902 sec.
iter 266860 || Loss: 0.7934 || timer: 0.0224 sec.
iter 266870 || Loss: 0.8818 || timer: 0.1182 sec.
iter 266880 || Loss: 0.9728 || timer: 0.0868 sec.
iter 266890 || Loss: 0.9683 || timer: 0.0799 sec.
iter 266900 || Loss: 0.8899 || timer: 0.1007 sec.
iter 266910 || Loss: 0.9605 || timer: 0.0940 sec.
iter 266920 || Loss: 0.9801 || timer: 0.0900 sec.
iter 266930 || Loss: 0.9912 || timer: 0.0906 sec.
iter 266940 || Loss: 0.7859 || timer: 0.1294 sec.
iter 266950 || Loss: 0.8134 || timer: 0.0823 sec.
iter 266960 || Loss: 0.8878 || timer: 0.1117 sec.
iter 266970 || Loss: 0.7943 || timer: 0.0834 sec.
iter 266980 || Loss: 0.9573 || timer: 0.0946 sec.
iter 266990 || Loss: 0.9872 || timer: 0.0807 sec.
iter 267000 || Loss: 0.8213 || timer: 0.0828 sec.
iter 267010 || Loss: 0.7597 || timer: 0.0833 sec.
iter 267020 || Loss: 0.9547 || timer: 0.0845 sec.
iter 267030 || Loss: 0.8792 || timer: 0.0813 sec.
iter 267040 || Loss: 1.0026 || timer: 0.0824 sec.
iter 267050 || Loss: 0.7802 || timer: 0.0956 sec.
iter 267060 || Loss: 0.6356 || timer: 0.0902 sec.
iter 267070 || Loss: 0.8064 || timer: 0.0832 sec.
iter 267080 || Loss: 0.7162 || timer: 0.0814 sec.
iter 267090 || Loss: 1.0393 || timer: 0.0823 sec.
iter 267100 || Loss: 0.7187 || timer: 0.0899 sec.
iter 267110 || Loss: 0.9936 || timer: 0.0887 sec.
iter 267120 || Loss: 1.1364 || timer: 0.0875 sec.
iter 267130 || Loss: 0.7916 || timer: 0.0809 sec.
iter 267140 || Loss: 1.0158 || timer: 0.0874 sec.
iter 267150 || Loss: 1.0334 || timer: 0.0827 sec.
iter 267160 || Loss: 0.9038 || timer: 0.0830 sec.
iter 267170 || Loss: 0.7910 || timer: 0.0819 sec.
iter 267180 || Loss: 0.9820 || timer: 0.1020 sec.
iter 267190 || Loss: 0.6722 || timer: 0.0180 sec.
iter 267200 || Loss: 0.5225 || timer: 0.0882 sec.
iter 267210 || Loss: 0.8010 || timer: 0.0909 sec.
iter 267220 || Loss: 0.7141 || timer: 0.0876 sec.
iter 267230 || Loss: 0.8647 || timer: 0.0909 sec.
iter 267240 || Loss: 0.7190 || timer: 0.0756 sec.
iter 267250 || Loss: 0.9486 || timer: 0.0875 sec.
iter 267260 || Loss: 0.9733 || timer: 0.0907 sec.
iter 267270 || Loss: 0.7398 || timer: 0.0834 sec.
iter 267280 || Loss: 0.7626 || timer: 0.1128 sec.
iter 267290 || Loss: 1.0501 || timer: 0.0874 sec.
iter 267300 || Loss: 0.9994 || timer: 0.1038 sec.
iter 267310 || Loss: 1.0499 || timer: 0.0832 sec.
iter 267320 || Loss: 0.9191 || timer: 0.0833 sec.
iter 267330 || Loss: 0.7550 || timer: 0.0905 sec.
iter 267340 || Loss: 0.7215 || timer: 0.0903 sec.
iter 267350 || Loss: 0.8173 || timer: 0.1150 sec.
iter 267360 || Loss: 0.7919 || timer: 0.0946 sec.
iter 267370 || Loss: 0.9814 || timer: 0.0962 sec.
iter 267380 || Loss: 0.8637 || timer: 0.0877 sec.
iter 267390 || Loss: 0.8545 || timer: 0.0841 sec.
iter 267400 || Loss: 0.7891 || timer: 0.1331 sec.
iter 267410 || Loss: 0.8469 || timer: 0.0890 sec.
iter 267420 || Loss: 0.8405 || timer: 0.0894 sec.
iter 267430 || Loss: 0.8070 || timer: 0.0917 sec.
iter 267440 || Loss: 0.9313 || timer: 0.0833 sec.
iter 267450 || Loss: 1.0082 || timer: 0.0816 sec.
iter 267460 || Loss: 0.9144 || timer: 0.0898 sec.
iter 267470 || Loss: 0.7396 || timer: 0.0906 sec.
iter 267480 || Loss: 1.0475 || timer: 0.0915 sec.
iter 267490 || Loss: 1.1685 || timer: 0.1115 sec.
iter 267500 || Loss: 0.9584 || timer: 0.0835 sec.
iter 267510 || Loss: 0.8287 || timer: 0.0828 sec.
iter 267520 || Loss: 1.2273 || timer: 0.0225 sec.
iter 267530 || Loss: 0.3114 || timer: 0.0894 sec.
iter 267540 || Loss: 0.7896 || timer: 0.0834 sec.
iter 267550 || Loss: 0.6471 || timer: 0.0879 sec.
iter 267560 || Loss: 0.6403 || timer: 0.0895 sec.
iter 267570 || Loss: 1.0382 || timer: 0.1019 sec.
iter 267580 || Loss: 0.8798 || timer: 0.0897 sec.
iter 267590 || Loss: 0.9696 || timer: 0.0823 sec.
iter 267600 || Loss: 0.6682 || timer: 0.0739 sec.
iter 267610 || Loss: 0.7332 || timer: 0.0811 sec.
iter 267620 || Loss: 0.7848 || timer: 0.1003 sec.
iter 267630 || Loss: 0.6780 || timer: 0.0830 sec.
iter 267640 || Loss: 0.9301 || timer: 0.0830 sec.
iter 267650 || Loss: 0.6609 || timer: 0.0863 sec.
iter 267660 || Loss: 0.6892 || timer: 0.0755 sec.
iter 267670 || Loss: 0.5907 || timer: 0.0901 sec.
iter 267680 || Loss: 0.7540 || timer: 0.1029 sec.
iter 267690 || Loss: 0.8365 || timer: 0.0904 sec.
iter 267700 || Loss: 0.8545 || timer: 0.0756 sec.
iter 267710 || Loss: 0.7321 || timer: 0.0744 sec.
iter 267720 || Loss: 0.7714 || timer: 0.0819 sec.
iter 267730 || Loss: 1.0366 || timer: 0.0828 sec.
iter 267740 || Loss: 1.0317 || timer: 0.0904 sec.
iter 267750 || Loss: 0.9977 || timer: 0.0810 sec.
iter 267760 || Loss: 0.8212 || timer: 0.1045 sec.
iter 267770 || Loss: 0.6442 || timer: 0.0896 sec.
iter 267780 || Loss: 1.0386 || timer: 0.1050 sec.
iter 267790 || Loss: 0.8650 || timer: 0.0898 sec.
iter 267800 || Loss: 0.5723 || timer: 0.0845 sec.
iter 267810 || Loss: 0.8488 || timer: 0.0912 sec.
iter 267820 || Loss: 0.7021 || timer: 0.0832 sec.
iter 267830 || Loss: 0.5682 || timer: 0.0832 sec.
iter 267840 || Loss: 0.6260 || timer: 0.1220 sec.
iter 267850 || Loss: 0.7502 || timer: 0.0231 sec.
iter 267860 || Loss: 0.3223 || timer: 0.0926 sec.
iter 267870 || Loss: 0.9316 || timer: 0.1012 sec.
iter 267880 || Loss: 0.9214 || timer: 0.0897 sec.
iter 267890 || Loss: 0.6730 || timer: 0.0763 sec.
iter 267900 || Loss: 0.7214 || timer: 0.0990 sec.
iter 267910 || Loss: 0.9480 || timer: 0.0828 sec.
iter 267920 || Loss: 0.7820 || timer: 0.0878 sec.
iter 267930 || Loss: 0.8595 || timer: 0.0895 sec.
iter 267940 || Loss: 0.9622 || timer: 0.1019 sec.
iter 267950 || Loss: 0.8969 || timer: 0.0875 sec.
iter 267960 || Loss: 0.9207 || timer: 0.0894 sec.
iter 267970 || Loss: 0.8877 || timer: 0.0832 sec.
iter 267980 || Loss: 0.9675 || timer: 0.0831 sec.
iter 267990 || Loss: 0.7636 || timer: 0.0824 sec.
iter 268000 || Loss: 0.7031 || timer: 0.0828 sec.
iter 268010 || Loss: 1.0770 || timer: 0.1011 sec.
iter 268020 || Loss: 0.6535 || timer: 0.0900 sec.
iter 268030 || Loss: 0.8019 || timer: 0.0865 sec.
iter 268040 || Loss: 0.7680 || timer: 0.1061 sec.
iter 268050 || Loss: 1.1607 || timer: 0.0755 sec.
iter 268060 || Loss: 1.1317 || timer: 0.0976 sec.
iter 268070 || Loss: 0.7439 || timer: 0.0915 sec.
iter 268080 || Loss: 0.8991 || timer: 0.0826 sec.
iter 268090 || Loss: 0.9192 || timer: 0.1145 sec.
iter 268100 || Loss: 0.9470 || timer: 0.1108 sec.
iter 268110 || Loss: 0.8087 || timer: 0.0900 sec.
iter 268120 || Loss: 0.8495 || timer: 0.0846 sec.
iter 268130 || Loss: 0.8758 || timer: 0.0931 sec.
iter 268140 || Loss: 0.8977 || timer: 0.0859 sec.
iter 268150 || Loss: 1.0774 || timer: 0.0835 sec.
iter 268160 || Loss: 0.6602 || timer: 0.0823 sec.
iter 268170 || Loss: 1.1782 || timer: 0.0824 sec.
iter 268180 || Loss: 0.6783 || timer: 0.0269 sec.
iter 268190 || Loss: 0.3880 || timer: 0.0828 sec.
iter 268200 || Loss: 0.7985 || timer: 0.0892 sec.
iter 268210 || Loss: 0.7103 || timer: 0.0884 sec.
iter 268220 || Loss: 0.7715 || timer: 0.0874 sec.
iter 268230 || Loss: 1.0488 || timer: 0.0843 sec.
iter 268240 || Loss: 0.7415 || timer: 0.0876 sec.
iter 268250 || Loss: 0.7770 || timer: 0.1045 sec.
iter 268260 || Loss: 1.1830 || timer: 0.0933 sec.
iter 268270 || Loss: 0.9070 || timer: 0.0823 sec.
iter 268280 || Loss: 0.8497 || timer: 0.0993 sec.
iter 268290 || Loss: 0.8436 || timer: 0.1105 sec.
iter 268300 || Loss: 1.3350 || timer: 0.0823 sec.
iter 268310 || Loss: 0.7293 || timer: 0.0815 sec.
iter 268320 || Loss: 0.9790 || timer: 0.0929 sec.
iter 268330 || Loss: 1.2354 || timer: 0.0959 sec.
iter 268340 || Loss: 0.6028 || timer: 0.0829 sec.
iter 268350 || Loss: 0.8057 || timer: 0.1113 sec.
iter 268360 || Loss: 0.7210 || timer: 0.0897 sec.
iter 268370 || Loss: 1.2511 || timer: 0.1080 sec.
iter 268380 || Loss: 1.0636 || timer: 0.1127 sec.
iter 268390 || Loss: 0.7422 || timer: 0.0803 sec.
iter 268400 || Loss: 0.9627 || timer: 0.0902 sec.
iter 268410 || Loss: 0.8080 || timer: 0.0920 sec.
iter 268420 || Loss: 0.7795 || timer: 0.0744 sec.
iter 268430 || Loss: 0.9274 || timer: 0.0903 sec.
iter 268440 || Loss: 0.8365 || timer: 0.0823 sec.
iter 268450 || Loss: 0.5211 || timer: 0.0888 sec.
iter 268460 || Loss: 1.1028 || timer: 0.0853 sec.
iter 268470 || Loss: 0.8770 || timer: 0.1041 sec.
iter 268480 || Loss: 1.2419 || timer: 0.0825 sec.
iter 268490 || Loss: 0.8585 || timer: 0.0947 sec.
iter 268500 || Loss: 1.3150 || timer: 0.1056 sec.
iter 268510 || Loss: 1.1439 || timer: 0.0247 sec.
iter 268520 || Loss: 2.3962 || timer: 0.0911 sec.
iter 268530 || Loss: 1.0520 || timer: 0.0907 sec.
iter 268540 || Loss: 1.0169 || timer: 0.0956 sec.
iter 268550 || Loss: 1.0308 || timer: 0.0834 sec.
iter 268560 || Loss: 0.9397 || timer: 0.1032 sec.
iter 268570 || Loss: 1.5195 || timer: 0.1007 sec.
iter 268580 || Loss: 1.0192 || timer: 0.0850 sec.
iter 268590 || Loss: 0.9313 || timer: 0.0909 sec.
iter 268600 || Loss: 0.9296 || timer: 0.0924 sec.
iter 268610 || Loss: 0.8592 || timer: 0.1181 sec.
iter 268620 || Loss: 0.9441 || timer: 0.0921 sec.
iter 268630 || Loss: 0.8493 || timer: 0.1012 sec.
iter 268640 || Loss: 1.1397 || timer: 0.0884 sec.
iter 268650 || Loss: 1.1842 || timer: 0.0853 sec.
iter 268660 || Loss: 1.0421 || timer: 0.1063 sec.
iter 268670 || Loss: 0.8897 || timer: 0.0840 sec.
iter 268680 || Loss: 0.6871 || timer: 0.0921 sec.
iter 268690 || Loss: 0.8918 || timer: 0.0902 sec.
iter 268700 || Loss: 0.7444 || timer: 0.0903 sec.
iter 268710 || Loss: 0.8447 || timer: 0.0924 sec.
iter 268720 || Loss: 0.9139 || timer: 0.0921 sec.
iter 268730 || Loss: 0.9123 || timer: 0.0841 sec.
iter 268740 || Loss: 0.7985 || timer: 0.0913 sec.
iter 268750 || Loss: 1.0827 || timer: 0.0951 sec.
iter 268760 || Loss: 0.8139 || timer: 0.0829 sec.
iter 268770 || Loss: 1.1028 || timer: 0.0975 sec.
iter 268780 || Loss: 0.9695 || timer: 0.0917 sec.
iter 268790 || Loss: 0.9128 || timer: 0.0942 sec.
iter 268800 || Loss: 0.7050 || timer: 0.0891 sec.
iter 268810 || Loss: 0.8240 || timer: 0.0909 sec.
iter 268820 || Loss: 0.9879 || timer: 0.0895 sec.
iter 268830 || Loss: 1.4702 || timer: 0.0896 sec.
iter 268840 || Loss: 0.9699 || timer: 0.0267 sec.
iter 268850 || Loss: 1.2984 || timer: 0.1162 sec.
iter 268860 || Loss: 1.0092 || timer: 0.0969 sec.
iter 268870 || Loss: 0.9905 || timer: 0.0922 sec.
iter 268880 || Loss: 0.7948 || timer: 0.1118 sec.
iter 268890 || Loss: 0.5968 || timer: 0.0973 sec.
iter 268900 || Loss: 1.6000 || timer: 0.0934 sec.
iter 268910 || Loss: 0.8055 || timer: 0.0831 sec.
iter 268920 || Loss: 0.5925 || timer: 0.0891 sec.
iter 268930 || Loss: 1.0169 || timer: 0.0900 sec.
iter 268940 || Loss: 0.7298 || timer: 0.1089 sec.
iter 268950 || Loss: 0.5356 || timer: 0.1164 sec.
iter 268960 || Loss: 0.9633 || timer: 0.1019 sec.
iter 268970 || Loss: 0.5628 || timer: 0.0865 sec.
iter 268980 || Loss: 0.8076 || timer: 0.0906 sec.
iter 268990 || Loss: 0.9315 || timer: 0.0882 sec.
iter 269000 || Loss: 0.6067 || timer: 0.0833 sec.
iter 269010 || Loss: 0.5724 || timer: 0.0910 sec.
iter 269020 || Loss: 1.2356 || timer: 0.1232 sec.
iter 269030 || Loss: 0.7605 || timer: 0.0995 sec.
iter 269040 || Loss: 0.9211 || timer: 0.0833 sec.
iter 269050 || Loss: 0.6544 || timer: 0.1046 sec.
iter 269060 || Loss: 0.8497 || timer: 0.0902 sec.
iter 269070 || Loss: 0.8639 || timer: 0.0874 sec.
iter 269080 || Loss: 0.7125 || timer: 0.0922 sec.
iter 269090 || Loss: 0.9503 || timer: 0.0902 sec.
iter 269100 || Loss: 0.9173 || timer: 0.0893 sec.
iter 269110 || Loss: 0.7453 || timer: 0.1076 sec.
iter 269120 || Loss: 1.4494 || timer: 0.1012 sec.
iter 269130 || Loss: 0.9060 || timer: 0.0850 sec.
iter 269140 || Loss: 1.0428 || timer: 0.0929 sec.
iter 269150 || Loss: 0.7108 || timer: 0.0873 sec.
iter 269160 || Loss: 0.8893 || timer: 0.0872 sec.
iter 269170 || Loss: 1.0090 || timer: 0.0212 sec.
iter 269180 || Loss: 0.6659 || timer: 0.0889 sec.
iter 269190 || Loss: 1.0284 || timer: 0.0898 sec.
iter 269200 || Loss: 1.0368 || timer: 0.0844 sec.
iter 269210 || Loss: 0.9760 || timer: 0.1002 sec.
iter 269220 || Loss: 0.7659 || timer: 0.0909 sec.
iter 269230 || Loss: 0.8014 || timer: 0.0827 sec.
iter 269240 || Loss: 0.9517 || timer: 0.0917 sec.
iter 269250 || Loss: 0.9065 || timer: 0.0878 sec.
iter 269260 || Loss: 1.1778 || timer: 0.1125 sec.
iter 269270 || Loss: 1.1234 || timer: 0.0966 sec.
iter 269280 || Loss: 0.9099 || timer: 0.1047 sec.
iter 269290 || Loss: 1.0708 || timer: 0.1063 sec.
iter 269300 || Loss: 0.9500 || timer: 0.0915 sec.
iter 269310 || Loss: 0.7849 || timer: 0.0898 sec.
iter 269320 || Loss: 1.0413 || timer: 0.0964 sec.
iter 269330 || Loss: 0.7433 || timer: 0.0929 sec.
iter 269340 || Loss: 0.9184 || timer: 0.0918 sec.
iter 269350 || Loss: 1.1710 || timer: 0.1031 sec.
iter 269360 || Loss: 0.9756 || timer: 0.0853 sec.
iter 269370 || Loss: 0.9613 || timer: 0.0826 sec.
iter 269380 || Loss: 0.9723 || timer: 0.1021 sec.
iter 269390 || Loss: 0.8065 || timer: 0.0912 sec.
iter 269400 || Loss: 0.8591 || timer: 0.0893 sec.
iter 269410 || Loss: 0.7795 || timer: 0.0917 sec.
iter 269420 || Loss: 0.7539 || timer: 0.1087 sec.
iter 269430 || Loss: 1.0462 || timer: 0.0920 sec.
iter 269440 || Loss: 0.9528 || timer: 0.0908 sec.
iter 269450 || Loss: 0.7908 || timer: 0.0900 sec.
iter 269460 || Loss: 0.7703 || timer: 0.0899 sec.
iter 269470 || Loss: 0.9652 || timer: 0.0826 sec.
iter 269480 || Loss: 0.8698 || timer: 0.0813 sec.
iter 269490 || Loss: 1.0690 || timer: 0.0881 sec.
iter 269500 || Loss: 0.8691 || timer: 0.0245 sec.
iter 269510 || Loss: 0.5850 || timer: 0.1072 sec.
iter 269520 || Loss: 0.9376 || timer: 0.0826 sec.
iter 269530 || Loss: 1.2233 || timer: 0.0827 sec.
iter 269540 || Loss: 0.9079 || timer: 0.0882 sec.
iter 269550 || Loss: 0.8459 || timer: 0.0907 sec.
iter 269560 || Loss: 0.9214 || timer: 0.0894 sec.
iter 269570 || Loss: 0.8341 || timer: 0.0919 sec.
iter 269580 || Loss: 0.6528 || timer: 0.0844 sec.
iter 269590 || Loss: 1.0217 || timer: 0.0905 sec.
iter 269600 || Loss: 1.3747 || timer: 0.1367 sec.
iter 269610 || Loss: 0.8934 || timer: 0.0922 sec.
iter 269620 || Loss: 0.7856 || timer: 0.0912 sec.
iter 269630 || Loss: 0.8359 || timer: 0.0917 sec.
iter 269640 || Loss: 0.9106 || timer: 0.1118 sec.
iter 269650 || Loss: 0.7017 || timer: 0.0889 sec.
iter 269660 || Loss: 0.8483 || timer: 0.0839 sec.
iter 269670 || Loss: 0.8796 || timer: 0.1080 sec.
iter 269680 || Loss: 0.8450 || timer: 0.1160 sec.
iter 269690 || Loss: 0.8622 || timer: 0.0899 sec.
iter 269700 || Loss: 0.8759 || timer: 0.0922 sec.
iter 269710 || Loss: 0.8284 || timer: 0.0836 sec.
iter 269720 || Loss: 0.7761 || timer: 0.1096 sec.
iter 269730 || Loss: 0.8266 || timer: 0.0826 sec.
iter 269740 || Loss: 0.7248 || timer: 0.1045 sec.
iter 269750 || Loss: 0.7173 || timer: 0.0830 sec.
iter 269760 || Loss: 0.9649 || timer: 0.0895 sec.
iter 269770 || Loss: 0.8466 || timer: 0.0839 sec.
iter 269780 || Loss: 0.8409 || timer: 0.0919 sec.
iter 269790 || Loss: 0.8163 || timer: 0.0826 sec.
iter 269800 || Loss: 0.8851 || timer: 0.0931 sec.
iter 269810 || Loss: 1.4200 || timer: 0.0821 sec.
iter 269820 || Loss: 0.6754 || timer: 0.0825 sec.
iter 269830 || Loss: 0.8036 || timer: 0.0165 sec.
iter 269840 || Loss: 0.2374 || timer: 0.0954 sec.
iter 269850 || Loss: 0.7420 || timer: 0.0913 sec.
iter 269860 || Loss: 0.6202 || timer: 0.0900 sec.
iter 269870 || Loss: 1.0532 || timer: 0.0893 sec.
iter 269880 || Loss: 0.9623 || timer: 0.0839 sec.
iter 269890 || Loss: 0.9016 || timer: 0.0877 sec.
iter 269900 || Loss: 0.8901 || timer: 0.0903 sec.
iter 269910 || Loss: 0.8206 || timer: 0.0917 sec.
iter 269920 || Loss: 0.7816 || timer: 0.0826 sec.
iter 269930 || Loss: 0.8114 || timer: 0.0993 sec.
iter 269940 || Loss: 0.8586 || timer: 0.1012 sec.
iter 269950 || Loss: 1.0125 || timer: 0.1012 sec.
iter 269960 || Loss: 0.8977 || timer: 0.1038 sec.
iter 269970 || Loss: 1.0659 || timer: 0.1022 sec.
iter 269980 || Loss: 0.9083 || timer: 0.1351 sec.
iter 269990 || Loss: 0.7287 || timer: 0.1005 sec.
iter 270000 || Loss: 0.7558 || Saving state, iter: 270000
timer: 0.0895 sec.
iter 270010 || Loss: 0.7173 || timer: 0.0921 sec.
iter 270020 || Loss: 1.1255 || timer: 0.0826 sec.
iter 270030 || Loss: 0.9799 || timer: 0.0827 sec.
iter 270040 || Loss: 0.7120 || timer: 0.0903 sec.
iter 270050 || Loss: 0.7907 || timer: 0.0830 sec.
iter 270060 || Loss: 0.6491 || timer: 0.0910 sec.
iter 270070 || Loss: 1.0293 || timer: 0.0845 sec.
iter 270080 || Loss: 1.0864 || timer: 0.1321 sec.
iter 270090 || Loss: 1.0482 || timer: 0.0927 sec.
iter 270100 || Loss: 0.8124 || timer: 0.0811 sec.
iter 270110 || Loss: 0.8884 || timer: 0.0886 sec.
iter 270120 || Loss: 0.6792 || timer: 0.0830 sec.
iter 270130 || Loss: 0.6419 || timer: 0.0922 sec.
iter 270140 || Loss: 0.8678 || timer: 0.0755 sec.
iter 270150 || Loss: 0.8537 || timer: 0.0758 sec.
iter 270160 || Loss: 0.7680 || timer: 0.0145 sec.
iter 270170 || Loss: 1.0255 || timer: 0.0830 sec.
iter 270180 || Loss: 0.9688 || timer: 0.1200 sec.
iter 270190 || Loss: 0.8901 || timer: 0.1196 sec.
iter 270200 || Loss: 1.4950 || timer: 0.0868 sec.
iter 270210 || Loss: 0.8171 || timer: 0.0892 sec.
iter 270220 || Loss: 1.0917 || timer: 0.0915 sec.
iter 270230 || Loss: 1.0995 || timer: 0.0926 sec.
iter 270240 || Loss: 0.8386 || timer: 0.0984 sec.
iter 270250 || Loss: 0.9947 || timer: 0.0867 sec.
iter 270260 || Loss: 0.9075 || timer: 0.0990 sec.
iter 270270 || Loss: 0.7719 || timer: 0.0846 sec.
iter 270280 || Loss: 0.7906 || timer: 0.0824 sec.
iter 270290 || Loss: 0.8818 || timer: 0.0899 sec.
iter 270300 || Loss: 0.6617 || timer: 0.0828 sec.
iter 270310 || Loss: 0.8679 || timer: 0.0902 sec.
iter 270320 || Loss: 1.0730 || timer: 0.0931 sec.
iter 270330 || Loss: 0.5591 || timer: 0.0875 sec.
iter 270340 || Loss: 1.0234 || timer: 0.0869 sec.
iter 270350 || Loss: 0.9525 || timer: 0.0882 sec.
iter 270360 || Loss: 0.6660 || timer: 0.0922 sec.
iter 270370 || Loss: 0.6927 || timer: 0.1016 sec.
iter 270380 || Loss: 1.1455 || timer: 0.0884 sec.
iter 270390 || Loss: 0.9176 || timer: 0.0809 sec.
iter 270400 || Loss: 1.1592 || timer: 0.0854 sec.
iter 270410 || Loss: 1.1199 || timer: 0.0833 sec.
iter 270420 || Loss: 1.0527 || timer: 0.0854 sec.
iter 270430 || Loss: 0.8016 || timer: 0.0958 sec.
iter 270440 || Loss: 1.0378 || timer: 0.0848 sec.
iter 270450 || Loss: 0.6843 || timer: 0.0812 sec.
iter 270460 || Loss: 0.7503 || timer: 0.0846 sec.
iter 270470 || Loss: 0.6240 || timer: 0.0865 sec.
iter 270480 || Loss: 0.7177 || timer: 0.0851 sec.
iter 270490 || Loss: 0.6894 || timer: 0.0214 sec.
iter 270500 || Loss: 0.4787 || timer: 0.0741 sec.
iter 270510 || Loss: 1.0247 || timer: 0.0912 sec.
iter 270520 || Loss: 1.0016 || timer: 0.0899 sec.
iter 270530 || Loss: 0.8110 || timer: 0.0829 sec.
iter 270540 || Loss: 0.9141 || timer: 0.0904 sec.
iter 270550 || Loss: 1.0394 || timer: 0.0829 sec.
iter 270560 || Loss: 1.2933 || timer: 0.0935 sec.
iter 270570 || Loss: 1.1751 || timer: 0.0958 sec.
iter 270580 || Loss: 1.0674 || timer: 0.0816 sec.
iter 270590 || Loss: 0.9738 || timer: 0.0995 sec.
iter 270600 || Loss: 0.6654 || timer: 0.0821 sec.
iter 270610 || Loss: 1.2803 || timer: 0.0833 sec.
iter 270620 || Loss: 0.9786 || timer: 0.0820 sec.
iter 270630 || Loss: 1.0407 || timer: 0.0895 sec.
iter 270640 || Loss: 0.7508 || timer: 0.0784 sec.
iter 270650 || Loss: 0.7635 || timer: 0.0920 sec.
iter 270660 || Loss: 0.7605 || timer: 0.0896 sec.
iter 270670 || Loss: 0.8152 || timer: 0.0895 sec.
iter 270680 || Loss: 0.8907 || timer: 0.0873 sec.
iter 270690 || Loss: 0.8947 || timer: 0.0969 sec.
iter 270700 || Loss: 0.9780 || timer: 0.0826 sec.
iter 270710 || Loss: 0.8707 || timer: 0.0854 sec.
iter 270720 || Loss: 0.8086 || timer: 0.0885 sec.
iter 270730 || Loss: 0.6701 || timer: 0.0885 sec.
iter 270740 || Loss: 0.9611 || timer: 0.0992 sec.
iter 270750 || Loss: 1.1175 || timer: 0.0912 sec.
iter 270760 || Loss: 0.8528 || timer: 0.0884 sec.
iter 270770 || Loss: 0.7752 || timer: 0.0898 sec.
iter 270780 || Loss: 0.8660 || timer: 0.0858 sec.
iter 270790 || Loss: 0.7058 || timer: 0.0883 sec.
iter 270800 || Loss: 0.9855 || timer: 0.1010 sec.
iter 270810 || Loss: 1.1638 || timer: 0.0822 sec.
iter 270820 || Loss: 0.9853 || timer: 0.0225 sec.
iter 270830 || Loss: 0.3995 || timer: 0.0885 sec.
iter 270840 || Loss: 0.9710 || timer: 0.1139 sec.
iter 270850 || Loss: 1.0247 || timer: 0.0903 sec.
iter 270860 || Loss: 0.9970 || timer: 0.0904 sec.
iter 270870 || Loss: 0.7204 || timer: 0.0877 sec.
iter 270880 || Loss: 0.6633 || timer: 0.0923 sec.
iter 270890 || Loss: 0.7111 || timer: 0.0924 sec.
iter 270900 || Loss: 0.7992 || timer: 0.0914 sec.
iter 270910 || Loss: 1.0047 || timer: 0.0827 sec.
iter 270920 || Loss: 0.8820 || timer: 0.0981 sec.
iter 270930 || Loss: 1.0354 || timer: 0.0825 sec.
iter 270940 || Loss: 0.5759 || timer: 0.0824 sec.
iter 270950 || Loss: 0.6085 || timer: 0.0828 sec.
iter 270960 || Loss: 1.0758 || timer: 0.0833 sec.
iter 270970 || Loss: 0.6932 || timer: 0.0872 sec.
iter 270980 || Loss: 1.1382 || timer: 0.0846 sec.
iter 270990 || Loss: 0.8465 || timer: 0.0826 sec.
iter 271000 || Loss: 0.9712 || timer: 0.0857 sec.
iter 271010 || Loss: 0.7830 || timer: 0.0909 sec.
iter 271020 || Loss: 0.5713 || timer: 0.0935 sec.
iter 271030 || Loss: 0.9334 || timer: 0.1071 sec.
iter 271040 || Loss: 1.0140 || timer: 0.1167 sec.
iter 271050 || Loss: 0.6802 || timer: 0.0811 sec.
iter 271060 || Loss: 0.7242 || timer: 0.0906 sec.
iter 271070 || Loss: 0.8127 || timer: 0.0893 sec.
iter 271080 || Loss: 0.8718 || timer: 0.0965 sec.
iter 271090 || Loss: 0.6896 || timer: 0.0825 sec.
iter 271100 || Loss: 1.2415 || timer: 0.1041 sec.
iter 271110 || Loss: 1.2465 || timer: 0.0891 sec.
iter 271120 || Loss: 1.6791 || timer: 0.0824 sec.
iter 271130 || Loss: 1.1378 || timer: 0.0815 sec.
iter 271140 || Loss: 0.9213 || timer: 0.0757 sec.
iter 271150 || Loss: 0.9151 || timer: 0.0161 sec.
iter 271160 || Loss: 0.4028 || timer: 0.0916 sec.
iter 271170 || Loss: 1.0778 || timer: 0.0927 sec.
iter 271180 || Loss: 0.8006 || timer: 0.0894 sec.
iter 271190 || Loss: 0.8782 || timer: 0.1171 sec.
iter 271200 || Loss: 0.8160 || timer: 0.0859 sec.
iter 271210 || Loss: 0.8279 || timer: 0.0923 sec.
iter 271220 || Loss: 0.8161 || timer: 0.0834 sec.
iter 271230 || Loss: 0.9329 || timer: 0.0916 sec.
iter 271240 || Loss: 1.0961 || timer: 0.0822 sec.
iter 271250 || Loss: 0.8386 || timer: 0.1078 sec.
iter 271260 || Loss: 1.0992 || timer: 0.0895 sec.
iter 271270 || Loss: 1.0241 || timer: 0.0826 sec.
iter 271280 || Loss: 0.8142 || timer: 0.0890 sec.
iter 271290 || Loss: 0.8855 || timer: 0.0834 sec.
iter 271300 || Loss: 1.1614 || timer: 0.0911 sec.
iter 271310 || Loss: 0.9008 || timer: 0.0901 sec.
iter 271320 || Loss: 0.8834 || timer: 0.0888 sec.
iter 271330 || Loss: 1.3392 || timer: 0.0888 sec.
iter 271340 || Loss: 0.8744 || timer: 0.0905 sec.
iter 271350 || Loss: 0.5651 || timer: 0.0846 sec.
iter 271360 || Loss: 0.7491 || timer: 0.0922 sec.
iter 271370 || Loss: 1.0152 || timer: 0.0901 sec.
iter 271380 || Loss: 0.7276 || timer: 0.0816 sec.
iter 271390 || Loss: 0.7732 || timer: 0.0897 sec.
iter 271400 || Loss: 1.4598 || timer: 0.0845 sec.
iter 271410 || Loss: 0.8351 || timer: 0.0926 sec.
iter 271420 || Loss: 0.6798 || timer: 0.0922 sec.
iter 271430 || Loss: 0.8959 || timer: 0.0838 sec.
iter 271440 || Loss: 1.1633 || timer: 0.0880 sec.
iter 271450 || Loss: 0.5971 || timer: 0.0909 sec.
iter 271460 || Loss: 1.2370 || timer: 0.0812 sec.
iter 271470 || Loss: 0.7876 || timer: 0.0827 sec.
iter 271480 || Loss: 0.7320 || timer: 0.0196 sec.
iter 271490 || Loss: 1.0359 || timer: 0.0830 sec.
iter 271500 || Loss: 0.7301 || timer: 0.1137 sec.
iter 271510 || Loss: 1.0179 || timer: 0.0877 sec.
iter 271520 || Loss: 0.6459 || timer: 0.0886 sec.
iter 271530 || Loss: 0.9566 || timer: 0.1089 sec.
iter 271540 || Loss: 0.7792 || timer: 0.0994 sec.
iter 271550 || Loss: 0.8223 || timer: 0.0824 sec.
iter 271560 || Loss: 0.9715 || timer: 0.0830 sec.
iter 271570 || Loss: 0.7044 || timer: 0.0958 sec.
iter 271580 || Loss: 1.0911 || timer: 0.1084 sec.
iter 271590 || Loss: 0.7841 || timer: 0.1070 sec.
iter 271600 || Loss: 0.8505 || timer: 0.0826 sec.
iter 271610 || Loss: 0.7820 || timer: 0.0803 sec.
iter 271620 || Loss: 0.8363 || timer: 0.1008 sec.
iter 271630 || Loss: 0.7859 || timer: 0.0906 sec.
iter 271640 || Loss: 1.0714 || timer: 0.0789 sec.
iter 271650 || Loss: 0.8234 || timer: 0.0747 sec.
iter 271660 || Loss: 0.7299 || timer: 0.1158 sec.
iter 271670 || Loss: 0.6264 || timer: 0.0823 sec.
iter 271680 || Loss: 0.8010 || timer: 0.0962 sec.
iter 271690 || Loss: 0.8520 || timer: 0.0860 sec.
iter 271700 || Loss: 0.6367 || timer: 0.0833 sec.
iter 271710 || Loss: 0.8317 || timer: 0.0781 sec.
iter 271720 || Loss: 1.1612 || timer: 0.0898 sec.
iter 271730 || Loss: 0.8173 || timer: 0.0828 sec.
iter 271740 || Loss: 0.7765 || timer: 0.0857 sec.
iter 271750 || Loss: 0.6887 || timer: 0.1082 sec.
iter 271760 || Loss: 0.8664 || timer: 0.0888 sec.
iter 271770 || Loss: 0.7231 || timer: 0.0897 sec.
iter 271780 || Loss: 0.8016 || timer: 0.0839 sec.
iter 271790 || Loss: 1.1296 || timer: 0.1098 sec.
iter 271800 || Loss: 0.5659 || timer: 0.0885 sec.
iter 271810 || Loss: 0.7810 || timer: 0.0238 sec.
iter 271820 || Loss: 4.1802 || timer: 0.0914 sec.
iter 271830 || Loss: 1.1349 || timer: 0.1107 sec.
iter 271840 || Loss: 0.7082 || timer: 0.0853 sec.
iter 271850 || Loss: 0.7774 || timer: 0.0819 sec.
iter 271860 || Loss: 1.1570 || timer: 0.0925 sec.
iter 271870 || Loss: 0.8717 || timer: 0.0826 sec.
iter 271880 || Loss: 0.8355 || timer: 0.0851 sec.
iter 271890 || Loss: 0.8352 || timer: 0.0897 sec.
iter 271900 || Loss: 0.6259 || timer: 0.0886 sec.
iter 271910 || Loss: 0.9204 || timer: 0.1112 sec.
iter 271920 || Loss: 0.8890 || timer: 0.0913 sec.
iter 271930 || Loss: 0.9205 || timer: 0.1130 sec.
iter 271940 || Loss: 1.1794 || timer: 0.0891 sec.
iter 271950 || Loss: 1.0890 || timer: 0.0831 sec.
iter 271960 || Loss: 1.2648 || timer: 0.0913 sec.
iter 271970 || Loss: 0.7935 || timer: 0.0861 sec.
iter 271980 || Loss: 1.0385 || timer: 0.0832 sec.
iter 271990 || Loss: 0.7225 || timer: 0.0846 sec.
iter 272000 || Loss: 1.0169 || timer: 0.1010 sec.
iter 272010 || Loss: 1.0488 || timer: 0.0829 sec.
iter 272020 || Loss: 0.8032 || timer: 0.1055 sec.
iter 272030 || Loss: 1.1467 || timer: 0.0902 sec.
iter 272040 || Loss: 0.7173 || timer: 0.0822 sec.
iter 272050 || Loss: 0.8733 || timer: 0.0922 sec.
iter 272060 || Loss: 1.1192 || timer: 0.0868 sec.
iter 272070 || Loss: 0.8931 || timer: 0.0940 sec.
iter 272080 || Loss: 0.8647 || timer: 0.0884 sec.
iter 272090 || Loss: 1.0368 || timer: 0.0900 sec.
iter 272100 || Loss: 0.9240 || timer: 0.0867 sec.
iter 272110 || Loss: 0.8303 || timer: 0.0782 sec.
iter 272120 || Loss: 0.9123 || timer: 0.0913 sec.
iter 272130 || Loss: 0.7200 || timer: 0.0901 sec.
iter 272140 || Loss: 0.9785 || timer: 0.0233 sec.
iter 272150 || Loss: 0.7764 || timer: 0.0894 sec.
iter 272160 || Loss: 0.7152 || timer: 0.0912 sec.
iter 272170 || Loss: 0.7721 || timer: 0.0828 sec.
iter 272180 || Loss: 0.5549 || timer: 0.0989 sec.
iter 272190 || Loss: 0.7963 || timer: 0.0818 sec.
iter 272200 || Loss: 0.8124 || timer: 0.0846 sec.
iter 272210 || Loss: 0.9045 || timer: 0.0904 sec.
iter 272220 || Loss: 0.8398 || timer: 0.0910 sec.
iter 272230 || Loss: 0.7858 || timer: 0.0830 sec.
iter 272240 || Loss: 1.1528 || timer: 0.1121 sec.
iter 272250 || Loss: 0.7353 || timer: 0.1132 sec.
iter 272260 || Loss: 0.8711 || timer: 0.0873 sec.
iter 272270 || Loss: 0.9630 || timer: 0.0826 sec.
iter 272280 || Loss: 0.9885 || timer: 0.0754 sec.
iter 272290 || Loss: 0.7963 || timer: 0.0833 sec.
iter 272300 || Loss: 0.8256 || timer: 0.0922 sec.
iter 272310 || Loss: 1.0607 || timer: 0.0848 sec.
iter 272320 || Loss: 0.7619 || timer: 0.0829 sec.
iter 272330 || Loss: 1.1227 || timer: 0.0914 sec.
iter 272340 || Loss: 0.7636 || timer: 0.0835 sec.
iter 272350 || Loss: 0.7669 || timer: 0.1087 sec.
iter 272360 || Loss: 1.1033 || timer: 0.0912 sec.
iter 272370 || Loss: 1.0677 || timer: 0.0821 sec.
iter 272380 || Loss: 1.0643 || timer: 0.1422 sec.
iter 272390 || Loss: 0.9635 || timer: 0.0896 sec.
iter 272400 || Loss: 0.9528 || timer: 0.0954 sec.
iter 272410 || Loss: 1.0863 || timer: 0.0845 sec.
iter 272420 || Loss: 0.9311 || timer: 0.0822 sec.
iter 272430 || Loss: 0.7296 || timer: 0.0833 sec.
iter 272440 || Loss: 0.7884 || timer: 0.0881 sec.
iter 272450 || Loss: 0.7925 || timer: 0.0907 sec.
iter 272460 || Loss: 0.8172 || timer: 0.0891 sec.
iter 272470 || Loss: 0.9337 || timer: 0.0284 sec.
iter 272480 || Loss: 0.8779 || timer: 0.0834 sec.
iter 272490 || Loss: 1.0302 || timer: 0.0929 sec.
iter 272500 || Loss: 1.0775 || timer: 0.0920 sec.
iter 272510 || Loss: 0.6862 || timer: 0.0926 sec.
iter 272520 || Loss: 1.2933 || timer: 0.1050 sec.
iter 272530 || Loss: 0.6947 || timer: 0.0920 sec.
iter 272540 || Loss: 0.7995 || timer: 0.0826 sec.
iter 272550 || Loss: 0.8626 || timer: 0.0828 sec.
iter 272560 || Loss: 0.9805 || timer: 0.1091 sec.
iter 272570 || Loss: 1.0662 || timer: 0.1079 sec.
iter 272580 || Loss: 0.7036 || timer: 0.0931 sec.
iter 272590 || Loss: 1.1946 || timer: 0.0830 sec.
iter 272600 || Loss: 1.2000 || timer: 0.0832 sec.
iter 272610 || Loss: 0.9421 || timer: 0.0917 sec.
iter 272620 || Loss: 0.9616 || timer: 0.0897 sec.
iter 272630 || Loss: 0.8573 || timer: 0.1033 sec.
iter 272640 || Loss: 1.1014 || timer: 0.0832 sec.
iter 272650 || Loss: 1.0666 || timer: 0.0840 sec.
iter 272660 || Loss: 0.8510 || timer: 0.1033 sec.
iter 272670 || Loss: 1.1145 || timer: 0.0903 sec.
iter 272680 || Loss: 0.7827 || timer: 0.0818 sec.
iter 272690 || Loss: 0.8046 || timer: 0.0896 sec.
iter 272700 || Loss: 0.8931 || timer: 0.0806 sec.
iter 272710 || Loss: 1.4008 || timer: 0.1026 sec.
iter 272720 || Loss: 0.9434 || timer: 0.0916 sec.
iter 272730 || Loss: 1.0173 || timer: 0.0813 sec.
iter 272740 || Loss: 0.8725 || timer: 0.0939 sec.
iter 272750 || Loss: 0.5276 || timer: 0.1111 sec.
iter 272760 || Loss: 0.9950 || timer: 0.0826 sec.
iter 272770 || Loss: 0.7307 || timer: 0.0903 sec.
iter 272780 || Loss: 0.9590 || timer: 0.0916 sec.
iter 272790 || Loss: 0.8806 || timer: 0.1015 sec.
iter 272800 || Loss: 0.7560 || timer: 0.0146 sec.
iter 272810 || Loss: 2.0962 || timer: 0.0755 sec.
iter 272820 || Loss: 1.0178 || timer: 0.0912 sec.
iter 272830 || Loss: 1.1012 || timer: 0.0852 sec.
iter 272840 || Loss: 0.7294 || timer: 0.0889 sec.
iter 272850 || Loss: 1.0826 || timer: 0.0907 sec.
iter 272860 || Loss: 0.6869 || timer: 0.0825 sec.
iter 272870 || Loss: 0.7125 || timer: 0.0828 sec.
iter 272880 || Loss: 0.7089 || timer: 0.0909 sec.
iter 272890 || Loss: 0.5608 || timer: 0.0890 sec.
iter 272900 || Loss: 0.8252 || timer: 0.1089 sec.
iter 272910 || Loss: 0.6111 || timer: 0.1006 sec.
iter 272920 || Loss: 0.6854 || timer: 0.0823 sec.
iter 272930 || Loss: 0.5815 || timer: 0.0887 sec.
iter 272940 || Loss: 0.9165 || timer: 0.0878 sec.
iter 272950 || Loss: 0.8201 || timer: 0.0875 sec.
iter 272960 || Loss: 0.7608 || timer: 0.0866 sec.
iter 272970 || Loss: 0.9674 || timer: 0.0933 sec.
iter 272980 || Loss: 0.7103 || timer: 0.0755 sec.
iter 272990 || Loss: 1.2499 || timer: 0.0906 sec.
iter 273000 || Loss: 0.8479 || timer: 0.0834 sec.
iter 273010 || Loss: 0.8678 || timer: 0.0989 sec.
iter 273020 || Loss: 1.1597 || timer: 0.0827 sec.
iter 273030 || Loss: 0.9715 || timer: 0.0897 sec.
iter 273040 || Loss: 0.7566 || timer: 0.0828 sec.
iter 273050 || Loss: 0.8297 || timer: 0.0898 sec.
iter 273060 || Loss: 1.0239 || timer: 0.0919 sec.
iter 273070 || Loss: 1.3684 || timer: 0.0906 sec.
iter 273080 || Loss: 0.8376 || timer: 0.1008 sec.
iter 273090 || Loss: 0.8787 || timer: 0.0903 sec.
iter 273100 || Loss: 0.7163 || timer: 0.0935 sec.
iter 273110 || Loss: 1.0594 || timer: 0.1025 sec.
iter 273120 || Loss: 1.0008 || timer: 0.1179 sec.
iter 273130 || Loss: 0.9721 || timer: 0.0194 sec.
iter 273140 || Loss: 0.3428 || timer: 0.0834 sec.
iter 273150 || Loss: 0.9424 || timer: 0.0921 sec.
iter 273160 || Loss: 0.8716 || timer: 0.0808 sec.
iter 273170 || Loss: 0.8769 || timer: 0.0825 sec.
iter 273180 || Loss: 0.6848 || timer: 0.0893 sec.
iter 273190 || Loss: 0.9613 || timer: 0.0903 sec.
iter 273200 || Loss: 1.1239 || timer: 0.0907 sec.
iter 273210 || Loss: 0.8527 || timer: 0.1039 sec.
iter 273220 || Loss: 1.2971 || timer: 0.0765 sec.
iter 273230 || Loss: 1.0650 || timer: 0.0920 sec.
iter 273240 || Loss: 1.1428 || timer: 0.0892 sec.
iter 273250 || Loss: 0.9790 || timer: 0.1143 sec.
iter 273260 || Loss: 0.9108 || timer: 0.0869 sec.
iter 273270 || Loss: 0.6829 || timer: 0.0837 sec.
iter 273280 || Loss: 1.1545 || timer: 0.0922 sec.
iter 273290 || Loss: 0.6535 || timer: 0.0919 sec.
iter 273300 || Loss: 0.9386 || timer: 0.0899 sec.
iter 273310 || Loss: 0.8217 || timer: 0.1072 sec.
iter 273320 || Loss: 0.5393 || timer: 0.0858 sec.
iter 273330 || Loss: 0.9731 || timer: 0.0922 sec.
iter 273340 || Loss: 0.7141 || timer: 0.0878 sec.
iter 273350 || Loss: 0.7058 || timer: 0.0835 sec.
iter 273360 || Loss: 0.5399 || timer: 0.0733 sec.
iter 273370 || Loss: 0.8032 || timer: 0.1071 sec.
iter 273380 || Loss: 0.8042 || timer: 0.0899 sec.
iter 273390 || Loss: 0.8747 || timer: 0.0880 sec.
iter 273400 || Loss: 0.9904 || timer: 0.0817 sec.
iter 273410 || Loss: 0.7645 || timer: 0.1114 sec.
iter 273420 || Loss: 0.8458 || timer: 0.0917 sec.
iter 273430 || Loss: 0.9086 || timer: 0.0934 sec.
iter 273440 || Loss: 0.9733 || timer: 0.0838 sec.
iter 273450 || Loss: 0.6892 || timer: 0.0911 sec.
iter 273460 || Loss: 0.8933 || timer: 0.0235 sec.
iter 273470 || Loss: 0.6775 || timer: 0.1271 sec.
iter 273480 || Loss: 0.8754 || timer: 0.1012 sec.
iter 273490 || Loss: 0.8032 || timer: 0.0867 sec.
iter 273500 || Loss: 0.9721 || timer: 0.0822 sec.
iter 273510 || Loss: 0.9721 || timer: 0.0968 sec.
iter 273520 || Loss: 0.8969 || timer: 0.0877 sec.
iter 273530 || Loss: 0.9045 || timer: 0.0813 sec.
iter 273540 || Loss: 1.0565 || timer: 0.1071 sec.
iter 273550 || Loss: 1.0873 || timer: 0.0829 sec.
iter 273560 || Loss: 0.7548 || timer: 0.0998 sec.
iter 273570 || Loss: 1.0880 || timer: 0.1012 sec.
iter 273580 || Loss: 0.8633 || timer: 0.1002 sec.
iter 273590 || Loss: 0.9231 || timer: 0.0813 sec.
iter 273600 || Loss: 0.8580 || timer: 0.0832 sec.
iter 273610 || Loss: 1.0817 || timer: 0.0781 sec.
iter 273620 || Loss: 0.7826 || timer: 0.0827 sec.
iter 273630 || Loss: 0.9443 || timer: 0.1008 sec.
iter 273640 || Loss: 0.7634 || timer: 0.0919 sec.
iter 273650 || Loss: 0.9121 || timer: 0.1111 sec.
iter 273660 || Loss: 0.6044 || timer: 0.0830 sec.
iter 273670 || Loss: 0.9295 || timer: 0.0954 sec.
iter 273680 || Loss: 0.9029 || timer: 0.0903 sec.
iter 273690 || Loss: 0.5592 || timer: 0.0832 sec.
iter 273700 || Loss: 0.8055 || timer: 0.0905 sec.
iter 273710 || Loss: 1.2548 || timer: 0.0832 sec.
iter 273720 || Loss: 0.8867 || timer: 0.0816 sec.
iter 273730 || Loss: 0.8362 || timer: 0.0818 sec.
iter 273740 || Loss: 0.9230 || timer: 0.0921 sec.
iter 273750 || Loss: 0.7519 || timer: 0.0876 sec.
iter 273760 || Loss: 0.9384 || timer: 0.1306 sec.
iter 273770 || Loss: 0.9693 || timer: 0.0904 sec.
iter 273780 || Loss: 0.9934 || timer: 0.0914 sec.
iter 273790 || Loss: 0.5235 || timer: 0.0177 sec.
iter 273800 || Loss: 0.4434 || timer: 0.1090 sec.
iter 273810 || Loss: 0.6592 || timer: 0.0852 sec.
iter 273820 || Loss: 0.5962 || timer: 0.0824 sec.
iter 273830 || Loss: 1.2488 || timer: 0.0824 sec.
iter 273840 || Loss: 0.7584 || timer: 0.0911 sec.
iter 273850 || Loss: 0.7939 || timer: 0.0852 sec.
iter 273860 || Loss: 1.2466 || timer: 0.0903 sec.
iter 273870 || Loss: 0.8441 || timer: 0.1101 sec.
iter 273880 || Loss: 1.0571 || timer: 0.0922 sec.
iter 273890 || Loss: 0.9404 || timer: 0.0994 sec.
iter 273900 || Loss: 0.7741 || timer: 0.0828 sec.
iter 273910 || Loss: 0.9794 || timer: 0.0816 sec.
iter 273920 || Loss: 1.0481 || timer: 0.0803 sec.
iter 273930 || Loss: 1.0310 || timer: 0.0767 sec.
iter 273940 || Loss: 1.0692 || timer: 0.0870 sec.
iter 273950 || Loss: 0.9647 || timer: 0.0896 sec.
iter 273960 || Loss: 0.9296 || timer: 0.0875 sec.
iter 273970 || Loss: 0.9297 || timer: 0.1049 sec.
iter 273980 || Loss: 0.9151 || timer: 0.0898 sec.
iter 273990 || Loss: 0.9551 || timer: 0.0858 sec.
iter 274000 || Loss: 0.7617 || timer: 0.0828 sec.
iter 274010 || Loss: 0.8350 || timer: 0.0832 sec.
iter 274020 || Loss: 0.9175 || timer: 0.0907 sec.
iter 274030 || Loss: 0.8523 || timer: 0.0840 sec.
iter 274040 || Loss: 0.7390 || timer: 0.0905 sec.
iter 274050 || Loss: 0.8421 || timer: 0.0823 sec.
iter 274060 || Loss: 1.0409 || timer: 0.0920 sec.
iter 274070 || Loss: 0.7706 || timer: 0.0937 sec.
iter 274080 || Loss: 0.9115 || timer: 0.0915 sec.
iter 274090 || Loss: 0.5316 || timer: 0.0904 sec.
iter 274100 || Loss: 1.1642 || timer: 0.1427 sec.
iter 274110 || Loss: 0.9337 || timer: 0.1213 sec.
iter 274120 || Loss: 0.8854 || timer: 0.0252 sec.
iter 274130 || Loss: 0.4608 || timer: 0.0909 sec.
iter 274140 || Loss: 0.6842 || timer: 0.0916 sec.
iter 274150 || Loss: 0.7801 || timer: 0.0917 sec.
iter 274160 || Loss: 0.7971 || timer: 0.0868 sec.
iter 274170 || Loss: 0.8327 || timer: 0.0812 sec.
iter 274180 || Loss: 0.9163 || timer: 0.1097 sec.
iter 274190 || Loss: 0.7385 || timer: 0.0833 sec.
iter 274200 || Loss: 0.7262 || timer: 0.0839 sec.
iter 274210 || Loss: 0.7531 || timer: 0.0838 sec.
iter 274220 || Loss: 0.8879 || timer: 0.0903 sec.
iter 274230 || Loss: 0.7566 || timer: 0.0746 sec.
iter 274240 || Loss: 0.9875 || timer: 0.0832 sec.
iter 274250 || Loss: 0.8352 || timer: 0.0823 sec.
iter 274260 || Loss: 0.8538 || timer: 0.0909 sec.
iter 274270 || Loss: 0.5762 || timer: 0.0931 sec.
iter 274280 || Loss: 0.9593 || timer: 0.0906 sec.
iter 274290 || Loss: 0.9538 || timer: 0.0881 sec.
iter 274300 || Loss: 0.8287 || timer: 0.0903 sec.
iter 274310 || Loss: 0.8511 || timer: 0.0914 sec.
iter 274320 || Loss: 0.7640 || timer: 0.0925 sec.
iter 274330 || Loss: 0.7777 || timer: 0.0889 sec.
iter 274340 || Loss: 0.9181 || timer: 0.0826 sec.
iter 274350 || Loss: 1.0047 || timer: 0.0945 sec.
iter 274360 || Loss: 0.7622 || timer: 0.0894 sec.
iter 274370 || Loss: 0.7539 || timer: 0.0820 sec.
iter 274380 || Loss: 0.8047 || timer: 0.0826 sec.
iter 274390 || Loss: 0.8791 || timer: 0.0948 sec.
iter 274400 || Loss: 0.8337 || timer: 0.0829 sec.
iter 274410 || Loss: 1.0223 || timer: 0.1178 sec.
iter 274420 || Loss: 1.0486 || timer: 0.0911 sec.
iter 274430 || Loss: 0.8683 || timer: 0.0904 sec.
iter 274440 || Loss: 1.0288 || timer: 0.0909 sec.
iter 274450 || Loss: 0.6996 || timer: 0.0316 sec.
iter 274460 || Loss: 1.0686 || timer: 0.0756 sec.
iter 274470 || Loss: 0.8770 || timer: 0.0757 sec.
iter 274480 || Loss: 0.7882 || timer: 0.0811 sec.
iter 274490 || Loss: 0.9771 || timer: 0.0876 sec.
iter 274500 || Loss: 1.1412 || timer: 0.0807 sec.
iter 274510 || Loss: 1.1833 || timer: 0.0829 sec.
iter 274520 || Loss: 0.6851 || timer: 0.1034 sec.
iter 274530 || Loss: 2.7070 || timer: 0.0798 sec.
iter 274540 || Loss: 1.8112 || timer: 0.0762 sec.
iter 274550 || Loss: 1.5614 || timer: 0.1045 sec.
iter 274560 || Loss: 1.2063 || timer: 0.0909 sec.
iter 274570 || Loss: 1.1856 || timer: 0.0892 sec.
iter 274580 || Loss: 1.1296 || timer: 0.0967 sec.
iter 274590 || Loss: 0.8544 || timer: 0.0996 sec.
iter 274600 || Loss: 0.8946 || timer: 0.1078 sec.
iter 274610 || Loss: 0.9261 || timer: 0.0896 sec.
iter 274620 || Loss: 1.0275 || timer: 0.0823 sec.
iter 274630 || Loss: 0.9095 || timer: 0.0832 sec.
iter 274640 || Loss: 1.1354 || timer: 0.0910 sec.
iter 274650 || Loss: 1.0207 || timer: 0.0898 sec.
iter 274660 || Loss: 0.9478 || timer: 0.0820 sec.
iter 274670 || Loss: 0.8014 || timer: 0.1105 sec.
iter 274680 || Loss: 1.0442 || timer: 0.0889 sec.
iter 274690 || Loss: 0.8246 || timer: 0.1094 sec.
iter 274700 || Loss: 1.1397 || timer: 0.1000 sec.
iter 274710 || Loss: 0.7638 || timer: 0.1025 sec.
iter 274720 || Loss: 1.1852 || timer: 0.0893 sec.
iter 274730 || Loss: 0.7950 || timer: 0.1061 sec.
iter 274740 || Loss: 0.7423 || timer: 0.0807 sec.
iter 274750 || Loss: 0.8916 || timer: 0.0829 sec.
iter 274760 || Loss: 0.9286 || timer: 0.0966 sec.
iter 274770 || Loss: 0.9166 || timer: 0.1043 sec.
iter 274780 || Loss: 0.9340 || timer: 0.0252 sec.
iter 274790 || Loss: 6.7862 || timer: 0.0919 sec.
iter 274800 || Loss: 0.9052 || timer: 0.0841 sec.
iter 274810 || Loss: 1.5630 || timer: 0.0840 sec.
iter 274820 || Loss: 1.1260 || timer: 0.0909 sec.
iter 274830 || Loss: 1.2348 || timer: 0.0894 sec.
iter 274840 || Loss: 1.3513 || timer: 0.0908 sec.
iter 274850 || Loss: 0.9631 || timer: 0.0918 sec.
iter 274860 || Loss: 0.6437 || timer: 0.0888 sec.
iter 274870 || Loss: 0.9351 || timer: 0.0921 sec.
iter 274880 || Loss: 1.1223 || timer: 0.0955 sec.
iter 274890 || Loss: 1.0426 || timer: 0.0908 sec.
iter 274900 || Loss: 1.2314 || timer: 0.0826 sec.
iter 274910 || Loss: 0.9737 || timer: 0.0889 sec.
iter 274920 || Loss: 1.2438 || timer: 0.0915 sec.
iter 274930 || Loss: 1.2262 || timer: 0.0927 sec.
iter 274940 || Loss: 0.9688 || timer: 0.0843 sec.
iter 274950 || Loss: 0.8856 || timer: 0.0824 sec.
iter 274960 || Loss: 1.0165 || timer: 0.1119 sec.
iter 274970 || Loss: 0.8223 || timer: 0.0807 sec.
iter 274980 || Loss: 0.7947 || timer: 0.0833 sec.
iter 274990 || Loss: 1.0600 || timer: 0.0965 sec.
iter 275000 || Loss: 0.7138 || Saving state, iter: 275000
timer: 0.0935 sec.
iter 275010 || Loss: 0.7306 || timer: 0.0877 sec.
iter 275020 || Loss: 0.7263 || timer: 0.1110 sec.
iter 275030 || Loss: 0.8892 || timer: 0.0823 sec.
iter 275040 || Loss: 1.2358 || timer: 0.0925 sec.
iter 275050 || Loss: 0.6830 || timer: 0.0828 sec.
iter 275060 || Loss: 1.2009 || timer: 0.0821 sec.
iter 275070 || Loss: 0.5793 || timer: 0.0893 sec.
iter 275080 || Loss: 1.0838 || timer: 0.0862 sec.
iter 275090 || Loss: 0.8583 || timer: 0.0915 sec.
iter 275100 || Loss: 0.6582 || timer: 0.1155 sec.
iter 275110 || Loss: 0.9338 || timer: 0.0256 sec.
iter 275120 || Loss: 0.4975 || timer: 0.0898 sec.
iter 275130 || Loss: 0.7900 || timer: 0.0921 sec.
iter 275140 || Loss: 0.6905 || timer: 0.0858 sec.
iter 275150 || Loss: 0.6828 || timer: 0.0924 sec.
iter 275160 || Loss: 0.9352 || timer: 0.1094 sec.
iter 275170 || Loss: 0.7160 || timer: 0.0757 sec.
iter 275180 || Loss: 1.0807 || timer: 0.0754 sec.
iter 275190 || Loss: 0.8579 || timer: 0.0894 sec.
iter 275200 || Loss: 0.9708 || timer: 0.0828 sec.
iter 275210 || Loss: 0.9837 || timer: 0.1092 sec.
iter 275220 || Loss: 0.6572 || timer: 0.0872 sec.
iter 275230 || Loss: 0.8139 || timer: 0.1138 sec.
iter 275240 || Loss: 0.8670 || timer: 0.0833 sec.
iter 275250 || Loss: 0.7739 || timer: 0.0905 sec.
iter 275260 || Loss: 0.5407 || timer: 0.0952 sec.
iter 275270 || Loss: 1.2966 || timer: 0.0826 sec.
iter 275280 || Loss: 0.7637 || timer: 0.0928 sec.
iter 275290 || Loss: 1.2343 || timer: 0.0893 sec.
iter 275300 || Loss: 0.6694 || timer: 0.0902 sec.
iter 275310 || Loss: 1.1576 || timer: 0.0955 sec.
iter 275320 || Loss: 0.7632 || timer: 0.0835 sec.
iter 275330 || Loss: 1.1665 || timer: 0.0912 sec.
iter 275340 || Loss: 1.0332 || timer: 0.0935 sec.
iter 275350 || Loss: 0.8157 || timer: 0.1080 sec.
iter 275360 || Loss: 0.8090 || timer: 0.1088 sec.
iter 275370 || Loss: 1.3610 || timer: 0.0894 sec.
iter 275380 || Loss: 0.6971 || timer: 0.0927 sec.
iter 275390 || Loss: 1.1640 || timer: 0.0928 sec.
iter 275400 || Loss: 0.7628 || timer: 0.1040 sec.
iter 275410 || Loss: 0.5437 || timer: 0.0894 sec.
iter 275420 || Loss: 0.8503 || timer: 0.1199 sec.
iter 275430 || Loss: 1.2893 || timer: 0.1140 sec.
iter 275440 || Loss: 0.7228 || timer: 0.0213 sec.
iter 275450 || Loss: 0.5447 || timer: 0.0867 sec.
iter 275460 || Loss: 0.5459 || timer: 0.0902 sec.
iter 275470 || Loss: 0.6531 || timer: 0.0925 sec.
iter 275480 || Loss: 0.7850 || timer: 0.1045 sec.
iter 275490 || Loss: 0.7020 || timer: 0.1068 sec.
iter 275500 || Loss: 0.8708 || timer: 0.0910 sec.
iter 275510 || Loss: 0.7567 || timer: 0.0903 sec.
iter 275520 || Loss: 0.8335 || timer: 0.0954 sec.
iter 275530 || Loss: 0.7926 || timer: 0.0917 sec.
iter 275540 || Loss: 0.9526 || timer: 0.1261 sec.
iter 275550 || Loss: 0.9966 || timer: 0.0901 sec.
iter 275560 || Loss: 0.7652 || timer: 0.0901 sec.
iter 275570 || Loss: 0.9975 || timer: 0.0907 sec.
iter 275580 || Loss: 0.8479 || timer: 0.0934 sec.
iter 275590 || Loss: 0.6527 || timer: 0.0941 sec.
iter 275600 || Loss: 0.9282 || timer: 0.0900 sec.
iter 275610 || Loss: 0.6808 || timer: 0.0840 sec.
iter 275620 || Loss: 0.8085 || timer: 0.0894 sec.
iter 275630 || Loss: 0.7090 || timer: 0.0912 sec.
iter 275640 || Loss: 0.9141 || timer: 0.0872 sec.
iter 275650 || Loss: 0.8002 || timer: 0.0888 sec.
iter 275660 || Loss: 0.8776 || timer: 0.0851 sec.
iter 275670 || Loss: 0.5644 || timer: 0.0842 sec.
iter 275680 || Loss: 1.0344 || timer: 0.0824 sec.
iter 275690 || Loss: 0.8198 || timer: 0.0893 sec.
iter 275700 || Loss: 0.6974 || timer: 0.0906 sec.
iter 275710 || Loss: 0.5890 || timer: 0.0906 sec.
iter 275720 || Loss: 0.7320 || timer: 0.0831 sec.
iter 275730 || Loss: 0.6949 || timer: 0.0866 sec.
iter 275740 || Loss: 0.8608 || timer: 0.0902 sec.
iter 275750 || Loss: 0.7752 || timer: 0.0939 sec.
iter 275760 || Loss: 1.3689 || timer: 0.1055 sec.
iter 275770 || Loss: 1.2996 || timer: 0.0213 sec.
iter 275780 || Loss: 0.4328 || timer: 0.0898 sec.
iter 275790 || Loss: 0.7771 || timer: 0.0972 sec.
iter 275800 || Loss: 0.9466 || timer: 0.0997 sec.
iter 275810 || Loss: 0.8491 || timer: 0.0902 sec.
iter 275820 || Loss: 0.9296 || timer: 0.0895 sec.
iter 275830 || Loss: 0.9164 || timer: 0.0913 sec.
iter 275840 || Loss: 0.6197 || timer: 0.0843 sec.
iter 275850 || Loss: 0.9375 || timer: 0.0750 sec.
iter 275860 || Loss: 1.1367 || timer: 0.0901 sec.
iter 275870 || Loss: 0.9174 || timer: 0.1097 sec.
iter 275880 || Loss: 0.8797 || timer: 0.0891 sec.
iter 275890 || Loss: 1.3328 || timer: 0.0898 sec.
iter 275900 || Loss: 0.8744 || timer: 0.0875 sec.
iter 275910 || Loss: 1.4220 || timer: 0.0823 sec.
iter 275920 || Loss: 1.0522 || timer: 0.0917 sec.
iter 275930 || Loss: 1.1947 || timer: 0.0912 sec.
iter 275940 || Loss: 0.6800 || timer: 0.0886 sec.
iter 275950 || Loss: 0.9053 || timer: 0.0900 sec.
iter 275960 || Loss: 0.8008 || timer: 0.0924 sec.
iter 275970 || Loss: 0.8489 || timer: 0.0880 sec.
iter 275980 || Loss: 0.7041 || timer: 0.1110 sec.
iter 275990 || Loss: 1.1060 || timer: 0.1107 sec.
iter 276000 || Loss: 0.9008 || timer: 0.0906 sec.
iter 276010 || Loss: 0.7410 || timer: 0.1099 sec.
iter 276020 || Loss: 0.9012 || timer: 0.0906 sec.
iter 276030 || Loss: 0.8060 || timer: 0.0906 sec.
iter 276040 || Loss: 0.7685 || timer: 0.1111 sec.
iter 276050 || Loss: 1.1062 || timer: 0.0918 sec.
iter 276060 || Loss: 0.7521 || timer: 0.0996 sec.
iter 276070 || Loss: 1.1007 || timer: 0.0920 sec.
iter 276080 || Loss: 0.9386 || timer: 0.0898 sec.
iter 276090 || Loss: 0.8030 || timer: 0.0817 sec.
iter 276100 || Loss: 0.8110 || timer: 0.0192 sec.
iter 276110 || Loss: 1.0637 || timer: 0.0891 sec.
iter 276120 || Loss: 0.8575 || timer: 0.0900 sec.
iter 276130 || Loss: 0.8764 || timer: 0.0931 sec.
iter 276140 || Loss: 0.9270 || timer: 0.0901 sec.
iter 276150 || Loss: 1.0622 || timer: 0.0834 sec.
iter 276160 || Loss: 0.6962 || timer: 0.0895 sec.
iter 276170 || Loss: 0.8426 || timer: 0.0916 sec.
iter 276180 || Loss: 1.0775 || timer: 0.0826 sec.
iter 276190 || Loss: 0.8581 || timer: 0.0907 sec.
iter 276200 || Loss: 0.9291 || timer: 0.1333 sec.
iter 276210 || Loss: 0.9595 || timer: 0.0827 sec.
iter 276220 || Loss: 0.8124 || timer: 0.0907 sec.
iter 276230 || Loss: 1.1282 || timer: 0.0911 sec.
iter 276240 || Loss: 0.7666 || timer: 0.0909 sec.
iter 276250 || Loss: 0.7542 || timer: 0.1087 sec.
iter 276260 || Loss: 1.0396 || timer: 0.1028 sec.
iter 276270 || Loss: 0.6307 || timer: 0.0972 sec.
iter 276280 || Loss: 0.8399 || timer: 0.0883 sec.
iter 276290 || Loss: 0.8766 || timer: 0.0903 sec.
iter 276300 || Loss: 1.1419 || timer: 0.1092 sec.
iter 276310 || Loss: 1.0149 || timer: 0.0895 sec.
iter 276320 || Loss: 0.6541 || timer: 0.0886 sec.
iter 276330 || Loss: 0.9652 || timer: 0.0915 sec.
iter 276340 || Loss: 0.8065 || timer: 0.0977 sec.
iter 276350 || Loss: 1.0533 || timer: 0.0890 sec.
iter 276360 || Loss: 0.7284 || timer: 0.0903 sec.
iter 276370 || Loss: 0.7971 || timer: 0.0941 sec.
iter 276380 || Loss: 0.8123 || timer: 0.1041 sec.
iter 276390 || Loss: 0.6902 || timer: 0.0995 sec.
iter 276400 || Loss: 0.9985 || timer: 0.0886 sec.
iter 276410 || Loss: 0.9433 || timer: 0.0917 sec.
iter 276420 || Loss: 0.8205 || timer: 0.0883 sec.
iter 276430 || Loss: 0.6203 || timer: 0.0277 sec.
iter 276440 || Loss: 1.3037 || timer: 0.0898 sec.
iter 276450 || Loss: 0.7178 || timer: 0.0897 sec.
iter 276460 || Loss: 0.9565 || timer: 0.0832 sec.
iter 276470 || Loss: 0.8382 || timer: 0.0914 sec.
iter 276480 || Loss: 0.7066 || timer: 0.0906 sec.
iter 276490 || Loss: 0.8511 || timer: 0.1001 sec.
iter 276500 || Loss: 0.8072 || timer: 0.0918 sec.
iter 276510 || Loss: 0.6415 || timer: 0.0917 sec.
iter 276520 || Loss: 0.7822 || timer: 0.0908 sec.
iter 276530 || Loss: 1.1718 || timer: 0.1060 sec.
iter 276540 || Loss: 0.7859 || timer: 0.0901 sec.
iter 276550 || Loss: 0.9129 || timer: 0.0904 sec.
iter 276560 || Loss: 0.6381 || timer: 0.0897 sec.
iter 276570 || Loss: 0.9632 || timer: 0.0900 sec.
iter 276580 || Loss: 0.8381 || timer: 0.0907 sec.
iter 276590 || Loss: 0.7080 || timer: 0.1083 sec.
iter 276600 || Loss: 0.7123 || timer: 0.0886 sec.
iter 276610 || Loss: 1.1404 || timer: 0.0894 sec.
iter 276620 || Loss: 0.8727 || timer: 0.0900 sec.
iter 276630 || Loss: 0.8668 || timer: 0.0953 sec.
iter 276640 || Loss: 0.7898 || timer: 0.0905 sec.
iter 276650 || Loss: 0.7821 || timer: 0.0900 sec.
iter 276660 || Loss: 0.9661 || timer: 0.0932 sec.
iter 276670 || Loss: 0.7273 || timer: 0.1267 sec.
iter 276680 || Loss: 1.1205 || timer: 0.0908 sec.
iter 276690 || Loss: 0.9669 || timer: 0.0928 sec.
iter 276700 || Loss: 1.0489 || timer: 0.0879 sec.
iter 276710 || Loss: 0.7208 || timer: 0.0920 sec.
iter 276720 || Loss: 1.0580 || timer: 0.1005 sec.
iter 276730 || Loss: 0.8244 || timer: 0.0829 sec.
iter 276740 || Loss: 1.0224 || timer: 0.0902 sec.
iter 276750 || Loss: 0.8158 || timer: 0.0907 sec.
iter 276760 || Loss: 0.7963 || timer: 0.0208 sec.
iter 276770 || Loss: 0.5296 || timer: 0.0919 sec.
iter 276780 || Loss: 0.7623 || timer: 0.1082 sec.
iter 276790 || Loss: 0.6922 || timer: 0.0870 sec.
iter 276800 || Loss: 0.8679 || timer: 0.1048 sec.
iter 276810 || Loss: 0.7137 || timer: 0.0981 sec.
iter 276820 || Loss: 1.0126 || timer: 0.0877 sec.
iter 276830 || Loss: 0.8099 || timer: 0.1097 sec.
iter 276840 || Loss: 0.7964 || timer: 0.0852 sec.
iter 276850 || Loss: 1.0113 || timer: 0.0852 sec.
iter 276860 || Loss: 1.0732 || timer: 0.1120 sec.
iter 276870 || Loss: 1.1530 || timer: 0.0908 sec.
iter 276880 || Loss: 0.7528 || timer: 0.0852 sec.
iter 276890 || Loss: 0.6986 || timer: 0.0879 sec.
iter 276900 || Loss: 0.9038 || timer: 0.0907 sec.
iter 276910 || Loss: 0.8021 || timer: 0.0898 sec.
iter 276920 || Loss: 0.9038 || timer: 0.0821 sec.
iter 276930 || Loss: 0.6813 || timer: 0.0905 sec.
iter 276940 || Loss: 0.8955 || timer: 0.0919 sec.
iter 276950 || Loss: 0.7889 || timer: 0.0919 sec.
iter 276960 || Loss: 0.7834 || timer: 0.0857 sec.
iter 276970 || Loss: 0.7658 || timer: 0.0913 sec.
iter 276980 || Loss: 1.1509 || timer: 0.0922 sec.
iter 276990 || Loss: 0.7331 || timer: 0.0919 sec.
iter 277000 || Loss: 0.8211 || timer: 0.0915 sec.
iter 277010 || Loss: 0.7319 || timer: 0.0919 sec.
iter 277020 || Loss: 0.5670 || timer: 0.0824 sec.
iter 277030 || Loss: 0.6772 || timer: 0.0940 sec.
iter 277040 || Loss: 0.8632 || timer: 0.0936 sec.
iter 277050 || Loss: 0.8729 || timer: 0.0904 sec.
iter 277060 || Loss: 0.7819 || timer: 0.0921 sec.
iter 277070 || Loss: 0.9077 || timer: 0.1094 sec.
iter 277080 || Loss: 0.7863 || timer: 0.1060 sec.
iter 277090 || Loss: 0.7928 || timer: 0.0143 sec.
iter 277100 || Loss: 2.5056 || timer: 0.0860 sec.
iter 277110 || Loss: 0.8266 || timer: 0.0913 sec.
iter 277120 || Loss: 0.7796 || timer: 0.0927 sec.
iter 277130 || Loss: 1.0268 || timer: 0.0832 sec.
iter 277140 || Loss: 0.7326 || timer: 0.0891 sec.
iter 277150 || Loss: 0.6838 || timer: 0.0895 sec.
iter 277160 || Loss: 1.0121 || timer: 0.0909 sec.
iter 277170 || Loss: 0.8422 || timer: 0.0894 sec.
iter 277180 || Loss: 1.0061 || timer: 0.0875 sec.
iter 277190 || Loss: 0.7365 || timer: 0.0988 sec.
iter 277200 || Loss: 0.9338 || timer: 0.0844 sec.
iter 277210 || Loss: 1.1182 || timer: 0.0973 sec.
iter 277220 || Loss: 0.7174 || timer: 0.0914 sec.
iter 277230 || Loss: 0.7465 || timer: 0.0830 sec.
iter 277240 || Loss: 1.0019 || timer: 0.0842 sec.
iter 277250 || Loss: 0.9919 || timer: 0.0900 sec.
iter 277260 || Loss: 0.6646 || timer: 0.0924 sec.
iter 277270 || Loss: 0.7936 || timer: 0.0931 sec.
iter 277280 || Loss: 0.9396 || timer: 0.0900 sec.
iter 277290 || Loss: 0.6813 || timer: 0.0900 sec.
iter 277300 || Loss: 0.9998 || timer: 0.0899 sec.
iter 277310 || Loss: 0.8481 || timer: 0.0916 sec.
iter 277320 || Loss: 1.0093 || timer: 0.0835 sec.
iter 277330 || Loss: 0.7584 || timer: 0.0931 sec.
iter 277340 || Loss: 1.0515 || timer: 0.0841 sec.
iter 277350 || Loss: 0.8510 || timer: 0.0978 sec.
iter 277360 || Loss: 1.0451 || timer: 0.0923 sec.
iter 277370 || Loss: 0.9943 || timer: 0.0920 sec.
iter 277380 || Loss: 0.6340 || timer: 0.0903 sec.
iter 277390 || Loss: 0.7877 || timer: 0.0890 sec.
iter 277400 || Loss: 0.6331 || timer: 0.0875 sec.
iter 277410 || Loss: 0.7809 || timer: 0.0900 sec.
iter 277420 || Loss: 0.8535 || timer: 0.0293 sec.
iter 277430 || Loss: 0.1525 || timer: 0.0902 sec.
iter 277440 || Loss: 0.7708 || timer: 0.0896 sec.
iter 277450 || Loss: 0.5623 || timer: 0.0920 sec.
iter 277460 || Loss: 0.8795 || timer: 0.0866 sec.
iter 277470 || Loss: 0.7471 || timer: 0.0896 sec.
iter 277480 || Loss: 0.7335 || timer: 0.0924 sec.
iter 277490 || Loss: 0.4836 || timer: 0.0848 sec.
iter 277500 || Loss: 0.7080 || timer: 0.0896 sec.
iter 277510 || Loss: 0.9100 || timer: 0.1013 sec.
iter 277520 || Loss: 0.9607 || timer: 0.1217 sec.
iter 277530 || Loss: 0.9140 || timer: 0.0827 sec.
iter 277540 || Loss: 0.8858 || timer: 0.0909 sec.
iter 277550 || Loss: 0.7672 || timer: 0.0831 sec.
iter 277560 || Loss: 0.7653 || timer: 0.0916 sec.
iter 277570 || Loss: 0.8165 || timer: 0.0927 sec.
iter 277580 || Loss: 1.0217 || timer: 0.0889 sec.
iter 277590 || Loss: 0.8566 || timer: 0.0826 sec.
iter 277600 || Loss: 1.0442 || timer: 0.0914 sec.
iter 277610 || Loss: 0.5931 || timer: 0.0843 sec.
iter 277620 || Loss: 0.8615 || timer: 0.0823 sec.
iter 277630 || Loss: 0.8858 || timer: 0.0921 sec.
iter 277640 || Loss: 0.7974 || timer: 0.0823 sec.
iter 277650 || Loss: 0.6982 || timer: 0.0814 sec.
iter 277660 || Loss: 0.8114 || timer: 0.1073 sec.
iter 277670 || Loss: 0.8753 || timer: 0.0818 sec.
iter 277680 || Loss: 0.9504 || timer: 0.0825 sec.
iter 277690 || Loss: 0.8919 || timer: 0.0820 sec.
iter 277700 || Loss: 0.5568 || timer: 0.0868 sec.
iter 277710 || Loss: 1.0230 || timer: 0.0929 sec.
iter 277720 || Loss: 0.6093 || timer: 0.0896 sec.
iter 277730 || Loss: 0.6133 || timer: 0.0826 sec.
iter 277740 || Loss: 0.7686 || timer: 0.0820 sec.
iter 277750 || Loss: 0.6388 || timer: 0.0218 sec.
iter 277760 || Loss: 0.1559 || timer: 0.0884 sec.
iter 277770 || Loss: 0.7761 || timer: 0.0826 sec.
iter 277780 || Loss: 0.7618 || timer: 0.0903 sec.
iter 277790 || Loss: 0.7607 || timer: 0.0897 sec.
iter 277800 || Loss: 0.8460 || timer: 0.0920 sec.
iter 277810 || Loss: 0.9476 || timer: 0.0939 sec.
iter 277820 || Loss: 0.7963 || timer: 0.0823 sec.
iter 277830 || Loss: 0.9839 || timer: 0.0849 sec.
iter 277840 || Loss: 0.9623 || timer: 0.0757 sec.
iter 277850 || Loss: 0.8299 || timer: 0.0933 sec.
iter 277860 || Loss: 0.9789 || timer: 0.0925 sec.
iter 277870 || Loss: 1.1351 || timer: 0.0950 sec.
iter 277880 || Loss: 0.8721 || timer: 0.0738 sec.
iter 277890 || Loss: 0.9623 || timer: 0.0794 sec.
iter 277900 || Loss: 0.8612 || timer: 0.0955 sec.
iter 277910 || Loss: 0.6749 || timer: 0.0760 sec.
iter 277920 || Loss: 0.9649 || timer: 0.0818 sec.
iter 277930 || Loss: 0.8961 || timer: 0.0898 sec.
iter 277940 || Loss: 1.0883 || timer: 0.0824 sec.
iter 277950 || Loss: 0.7445 || timer: 0.0915 sec.
iter 277960 || Loss: 0.6832 || timer: 0.0830 sec.
iter 277970 || Loss: 0.9246 || timer: 0.0908 sec.
iter 277980 || Loss: 1.3934 || timer: 0.0835 sec.
iter 277990 || Loss: 1.0005 || timer: 0.0787 sec.
iter 278000 || Loss: 0.9868 || timer: 0.0895 sec.
iter 278010 || Loss: 0.9819 || timer: 0.0854 sec.
iter 278020 || Loss: 0.6968 || timer: 0.0917 sec.
iter 278030 || Loss: 0.6843 || timer: 0.0876 sec.
iter 278040 || Loss: 0.9178 || timer: 0.0925 sec.
iter 278050 || Loss: 1.1026 || timer: 0.0869 sec.
iter 278060 || Loss: 0.7842 || timer: 0.1019 sec.
iter 278070 || Loss: 0.8757 || timer: 0.1004 sec.
iter 278080 || Loss: 0.8669 || timer: 0.0228 sec.
iter 278090 || Loss: 1.9727 || timer: 0.0905 sec.
iter 278100 || Loss: 1.0000 || timer: 0.0902 sec.
iter 278110 || Loss: 0.7954 || timer: 0.0820 sec.
iter 278120 || Loss: 0.7274 || timer: 0.0825 sec.
iter 278130 || Loss: 0.7639 || timer: 0.0882 sec.
iter 278140 || Loss: 0.8067 || timer: 0.0918 sec.
iter 278150 || Loss: 0.8055 || timer: 0.0865 sec.
iter 278160 || Loss: 0.6888 || timer: 0.0825 sec.
iter 278170 || Loss: 0.8832 || timer: 0.0894 sec.
iter 278180 || Loss: 1.0955 || timer: 0.1076 sec.
iter 278190 || Loss: 0.6657 || timer: 0.0828 sec.
iter 278200 || Loss: 0.8449 || timer: 0.0834 sec.
iter 278210 || Loss: 0.7692 || timer: 0.0735 sec.
iter 278220 || Loss: 0.8498 || timer: 0.0904 sec.
iter 278230 || Loss: 0.7712 || timer: 0.0882 sec.
iter 278240 || Loss: 1.0002 || timer: 0.0904 sec.
iter 278250 || Loss: 0.9585 || timer: 0.0886 sec.
iter 278260 || Loss: 0.6895 || timer: 0.1138 sec.
iter 278270 || Loss: 1.0946 || timer: 0.0918 sec.
iter 278280 || Loss: 0.8120 || timer: 0.0826 sec.
iter 278290 || Loss: 0.7627 || timer: 0.0905 sec.
iter 278300 || Loss: 1.0242 || timer: 0.0906 sec.
iter 278310 || Loss: 0.6740 || timer: 0.0896 sec.
iter 278320 || Loss: 0.9156 || timer: 0.1052 sec.
iter 278330 || Loss: 1.1103 || timer: 0.0903 sec.
iter 278340 || Loss: 0.7133 || timer: 0.0909 sec.
iter 278350 || Loss: 1.0628 || timer: 0.0894 sec.
iter 278360 || Loss: 0.9048 || timer: 0.0756 sec.
iter 278370 || Loss: 1.1312 || timer: 0.0888 sec.
iter 278380 || Loss: 0.9682 || timer: 0.1025 sec.
iter 278390 || Loss: 0.8269 || timer: 0.0925 sec.
iter 278400 || Loss: 0.9997 || timer: 0.0910 sec.
iter 278410 || Loss: 0.9445 || timer: 0.0213 sec.
iter 278420 || Loss: 1.6811 || timer: 0.0932 sec.
iter 278430 || Loss: 0.9685 || timer: 0.1019 sec.
iter 278440 || Loss: 0.7941 || timer: 0.0896 sec.
iter 278450 || Loss: 0.7261 || timer: 0.0895 sec.
iter 278460 || Loss: 1.0183 || timer: 0.0889 sec.
iter 278470 || Loss: 0.9996 || timer: 0.1131 sec.
iter 278480 || Loss: 0.6544 || timer: 0.0924 sec.
iter 278490 || Loss: 1.0702 || timer: 0.0827 sec.
iter 278500 || Loss: 1.1516 || timer: 0.0915 sec.
iter 278510 || Loss: 0.7631 || timer: 0.1018 sec.
iter 278520 || Loss: 0.7728 || timer: 0.0883 sec.
iter 278530 || Loss: 0.7691 || timer: 0.0920 sec.
iter 278540 || Loss: 0.7548 || timer: 0.0912 sec.
iter 278550 || Loss: 0.8034 || timer: 0.0832 sec.
iter 278560 || Loss: 1.0947 || timer: 0.1042 sec.
iter 278570 || Loss: 1.0809 || timer: 0.0821 sec.
iter 278580 || Loss: 0.9741 || timer: 0.0911 sec.
iter 278590 || Loss: 0.7372 || timer: 0.0827 sec.
iter 278600 || Loss: 0.7235 || timer: 0.0902 sec.
iter 278610 || Loss: 0.8437 || timer: 0.1249 sec.
iter 278620 || Loss: 0.7248 || timer: 0.0911 sec.
iter 278630 || Loss: 0.7720 || timer: 0.0813 sec.
iter 278640 || Loss: 0.9119 || timer: 0.0822 sec.
iter 278650 || Loss: 0.6836 || timer: 0.0892 sec.
iter 278660 || Loss: 0.9020 || timer: 0.0901 sec.
iter 278670 || Loss: 0.7520 || timer: 0.0820 sec.
iter 278680 || Loss: 0.7290 || timer: 0.1016 sec.
iter 278690 || Loss: 0.7387 || timer: 0.0880 sec.
iter 278700 || Loss: 1.0382 || timer: 0.0813 sec.
iter 278710 || Loss: 0.8295 || timer: 0.0825 sec.
iter 278720 || Loss: 0.8439 || timer: 0.1115 sec.
iter 278730 || Loss: 0.6313 || timer: 0.0833 sec.
iter 278740 || Loss: 0.9050 || timer: 0.0196 sec.
iter 278750 || Loss: 3.4528 || timer: 0.0823 sec.
iter 278760 || Loss: 1.0019 || timer: 0.0876 sec.
iter 278770 || Loss: 0.8937 || timer: 0.0868 sec.
iter 278780 || Loss: 1.0539 || timer: 0.0863 sec.
iter 278790 || Loss: 0.8183 || timer: 0.1098 sec.
iter 278800 || Loss: 1.0940 || timer: 0.0913 sec.
iter 278810 || Loss: 0.8147 || timer: 0.0897 sec.
iter 278820 || Loss: 0.9584 || timer: 0.0842 sec.
iter 278830 || Loss: 0.7297 || timer: 0.0901 sec.
iter 278840 || Loss: 1.1041 || timer: 0.1017 sec.
iter 278850 || Loss: 0.8203 || timer: 0.0829 sec.
iter 278860 || Loss: 0.8867 || timer: 0.0828 sec.
iter 278870 || Loss: 0.8020 || timer: 0.0808 sec.
iter 278880 || Loss: 0.8953 || timer: 0.0928 sec.
iter 278890 || Loss: 0.7811 || timer: 0.1007 sec.
iter 278900 || Loss: 0.8627 || timer: 0.0755 sec.
iter 278910 || Loss: 0.6267 || timer: 0.1097 sec.
iter 278920 || Loss: 0.8280 || timer: 0.0793 sec.
iter 278930 || Loss: 1.0542 || timer: 0.0807 sec.
iter 278940 || Loss: 0.7577 || timer: 0.0819 sec.
iter 278950 || Loss: 0.8180 || timer: 0.0829 sec.
iter 278960 || Loss: 0.7164 || timer: 0.0912 sec.
iter 278970 || Loss: 0.7573 || timer: 0.0885 sec.
iter 278980 || Loss: 0.7309 || timer: 0.0888 sec.
iter 278990 || Loss: 0.7223 || timer: 0.0913 sec.
iter 279000 || Loss: 0.7691 || timer: 0.0889 sec.
iter 279010 || Loss: 0.7456 || timer: 0.0880 sec.
iter 279020 || Loss: 0.9498 || timer: 0.0988 sec.
iter 279030 || Loss: 0.8400 || timer: 0.0870 sec.
iter 279040 || Loss: 0.7309 || timer: 0.0903 sec.
iter 279050 || Loss: 0.8551 || timer: 0.0893 sec.
iter 279060 || Loss: 0.6859 || timer: 0.0841 sec.
iter 279070 || Loss: 0.9284 || timer: 0.0229 sec.
iter 279080 || Loss: 1.8524 || timer: 0.1086 sec.
iter 279090 || Loss: 0.8941 || timer: 0.0914 sec.
iter 279100 || Loss: 0.7461 || timer: 0.0896 sec.
iter 279110 || Loss: 0.6675 || timer: 0.0828 sec.
iter 279120 || Loss: 1.1786 || timer: 0.0830 sec.
iter 279130 || Loss: 0.8326 || timer: 0.0820 sec.
iter 279140 || Loss: 0.9399 || timer: 0.0902 sec.
iter 279150 || Loss: 0.7078 || timer: 0.0825 sec.
iter 279160 || Loss: 0.7591 || timer: 0.0903 sec.
iter 279170 || Loss: 0.7577 || timer: 0.0955 sec.
iter 279180 || Loss: 0.7098 || timer: 0.1282 sec.
iter 279190 || Loss: 0.7965 || timer: 0.0889 sec.
iter 279200 || Loss: 1.1099 || timer: 0.0791 sec.
iter 279210 || Loss: 0.6360 || timer: 0.1138 sec.
iter 279220 || Loss: 1.0303 || timer: 0.0852 sec.
iter 279230 || Loss: 0.9191 || timer: 0.1042 sec.
iter 279240 || Loss: 1.1008 || timer: 0.0827 sec.
iter 279250 || Loss: 0.9224 || timer: 0.0830 sec.
iter 279260 || Loss: 0.8321 || timer: 0.0832 sec.
iter 279270 || Loss: 0.7688 || timer: 0.0837 sec.
iter 279280 || Loss: 1.0618 || timer: 0.0912 sec.
iter 279290 || Loss: 0.6322 || timer: 0.0890 sec.
iter 279300 || Loss: 0.8710 || timer: 0.0882 sec.
iter 279310 || Loss: 1.0246 || timer: 0.1080 sec.
iter 279320 || Loss: 0.6251 || timer: 0.0754 sec.
iter 279330 || Loss: 0.7559 || timer: 0.0899 sec.
iter 279340 || Loss: 0.5721 || timer: 0.0931 sec.
iter 279350 || Loss: 1.1099 || timer: 0.1035 sec.
iter 279360 || Loss: 1.3946 || timer: 0.0888 sec.
iter 279370 || Loss: 0.7182 || timer: 0.0830 sec.
iter 279380 || Loss: 0.9515 || timer: 0.0821 sec.
iter 279390 || Loss: 0.7713 || timer: 0.1074 sec.
iter 279400 || Loss: 1.0382 || timer: 0.0258 sec.
iter 279410 || Loss: 0.3161 || timer: 0.0828 sec.
iter 279420 || Loss: 0.9006 || timer: 0.0907 sec.
iter 279430 || Loss: 0.7166 || timer: 0.0816 sec.
iter 279440 || Loss: 0.8029 || timer: 0.0833 sec.
iter 279450 || Loss: 0.9969 || timer: 0.0840 sec.
iter 279460 || Loss: 1.0788 || timer: 0.0825 sec.
iter 279470 || Loss: 0.9700 || timer: 0.0946 sec.
iter 279480 || Loss: 0.6433 || timer: 0.0903 sec.
iter 279490 || Loss: 0.9423 || timer: 0.0890 sec.
iter 279500 || Loss: 1.0083 || timer: 0.0932 sec.
iter 279510 || Loss: 0.9875 || timer: 0.0821 sec.
iter 279520 || Loss: 0.8286 || timer: 0.0891 sec.
iter 279530 || Loss: 0.6900 || timer: 0.0879 sec.
iter 279540 || Loss: 0.8197 || timer: 0.0897 sec.
iter 279550 || Loss: 0.7508 || timer: 0.0868 sec.
iter 279560 || Loss: 0.9759 || timer: 0.1026 sec.
iter 279570 || Loss: 0.8932 || timer: 0.0898 sec.
iter 279580 || Loss: 0.8160 || timer: 0.0826 sec.
iter 279590 || Loss: 0.7935 || timer: 0.1158 sec.
iter 279600 || Loss: 0.8710 || timer: 0.0828 sec.
iter 279610 || Loss: 0.8019 || timer: 0.0839 sec.
iter 279620 || Loss: 0.8459 || timer: 0.0837 sec.
iter 279630 || Loss: 1.2597 || timer: 0.0815 sec.
iter 279640 || Loss: 0.7451 || timer: 0.0897 sec.
iter 279650 || Loss: 0.8137 || timer: 0.0860 sec.
iter 279660 || Loss: 0.7306 || timer: 0.0901 sec.
iter 279670 || Loss: 1.1678 || timer: 0.0758 sec.
iter 279680 || Loss: 0.8462 || timer: 0.0871 sec.
iter 279690 || Loss: 1.1161 || timer: 0.0925 sec.
iter 279700 || Loss: 0.8556 || timer: 0.0823 sec.
iter 279710 || Loss: 0.6916 || timer: 0.1076 sec.
iter 279720 || Loss: 0.9034 || timer: 0.0931 sec.
iter 279730 || Loss: 0.6920 || timer: 0.0159 sec.
iter 279740 || Loss: 0.8740 || timer: 0.0827 sec.
iter 279750 || Loss: 0.7534 || timer: 0.0821 sec.
iter 279760 || Loss: 0.7906 || timer: 0.0887 sec.
iter 279770 || Loss: 0.7861 || timer: 0.0871 sec.
iter 279780 || Loss: 1.0247 || timer: 0.0916 sec.
iter 279790 || Loss: 1.3310 || timer: 0.0828 sec.
iter 279800 || Loss: 0.8506 || timer: 0.0894 sec.
iter 279810 || Loss: 1.1341 || timer: 0.1000 sec.
iter 279820 || Loss: 0.8226 || timer: 0.0988 sec.
iter 279830 || Loss: 0.7149 || timer: 0.1112 sec.
iter 279840 || Loss: 0.8947 || timer: 0.0904 sec.
iter 279850 || Loss: 0.8905 || timer: 0.0833 sec.
iter 279860 || Loss: 1.0381 || timer: 0.0893 sec.
iter 279870 || Loss: 1.2821 || timer: 0.0872 sec.
iter 279880 || Loss: 1.1863 || timer: 0.0834 sec.
iter 279890 || Loss: 1.0547 || timer: 0.0896 sec.
iter 279900 || Loss: 0.7093 || timer: 0.0896 sec.
iter 279910 || Loss: 0.8246 || timer: 0.0831 sec.
iter 279920 || Loss: 1.0227 || timer: 0.0888 sec.
iter 279930 || Loss: 0.7822 || timer: 0.0832 sec.
iter 279940 || Loss: 0.6473 || timer: 0.0864 sec.
iter 279950 || Loss: 0.6952 || timer: 0.0920 sec.
iter 279960 || Loss: 1.1595 || timer: 0.0826 sec.
iter 279970 || Loss: 0.9297 || timer: 0.1139 sec.
iter 279980 || Loss: 0.9987 || timer: 0.1026 sec.
iter 279990 || Loss: 0.7911 || timer: 0.0810 sec.
iter 280000 || Loss: 0.7979 || Saving state, iter: 280000
timer: 0.0916 sec.
iter 280010 || Loss: 0.7237 || timer: 0.1245 sec.
iter 280020 || Loss: 0.8963 || timer: 0.0948 sec.
iter 280030 || Loss: 0.7279 || timer: 0.0832 sec.
iter 280040 || Loss: 0.9250 || timer: 0.0982 sec.
iter 280050 || Loss: 1.0782 || timer: 0.0830 sec.
iter 280060 || Loss: 0.8512 || timer: 0.0172 sec.
iter 280070 || Loss: 0.4677 || timer: 0.0806 sec.
iter 280080 || Loss: 0.8052 || timer: 0.0847 sec.
iter 280090 || Loss: 0.6770 || timer: 0.0917 sec.
iter 280100 || Loss: 0.5454 || timer: 0.0904 sec.
iter 280110 || Loss: 0.7233 || timer: 0.0923 sec.
iter 280120 || Loss: 0.7660 || timer: 0.0914 sec.
iter 280130 || Loss: 0.7349 || timer: 0.0844 sec.
iter 280140 || Loss: 0.9500 || timer: 0.0846 sec.
iter 280150 || Loss: 0.7504 || timer: 0.0924 sec.
iter 280160 || Loss: 0.7188 || timer: 0.1038 sec.
iter 280170 || Loss: 0.8123 || timer: 0.0916 sec.
iter 280180 || Loss: 0.8072 || timer: 0.0899 sec.
iter 280190 || Loss: 0.7024 || timer: 0.0899 sec.
iter 280200 || Loss: 0.8056 || timer: 0.0911 sec.
iter 280210 || Loss: 0.7428 || timer: 0.0913 sec.
iter 280220 || Loss: 0.8344 || timer: 0.0927 sec.
iter 280230 || Loss: 0.7333 || timer: 0.0902 sec.
iter 280240 || Loss: 0.7039 || timer: 0.1006 sec.
iter 280250 || Loss: 0.8122 || timer: 0.0899 sec.
iter 280260 || Loss: 0.6880 || timer: 0.0927 sec.
iter 280270 || Loss: 0.6917 || timer: 0.0939 sec.
iter 280280 || Loss: 0.5884 || timer: 0.0875 sec.
iter 280290 || Loss: 0.6612 || timer: 0.0873 sec.
iter 280300 || Loss: 0.8374 || timer: 0.1080 sec.
iter 280310 || Loss: 0.9347 || timer: 0.1105 sec.
iter 280320 || Loss: 0.6276 || timer: 0.0944 sec.
iter 280330 || Loss: 0.8267 || timer: 0.0842 sec.
iter 280340 || Loss: 0.7726 || timer: 0.0895 sec.
iter 280350 || Loss: 0.6899 || timer: 0.1201 sec.
iter 280360 || Loss: 0.5094 || timer: 0.0920 sec.
iter 280370 || Loss: 0.6679 || timer: 0.0847 sec.
iter 280380 || Loss: 0.6865 || timer: 0.1018 sec.
iter 280390 || Loss: 0.9094 || timer: 0.0180 sec.
iter 280400 || Loss: 0.5563 || timer: 0.0871 sec.
iter 280410 || Loss: 0.8228 || timer: 0.0907 sec.
iter 280420 || Loss: 0.8699 || timer: 0.0934 sec.
iter 280430 || Loss: 1.1464 || timer: 0.0912 sec.
iter 280440 || Loss: 0.8180 || timer: 0.0962 sec.
iter 280450 || Loss: 0.6756 || timer: 0.0837 sec.
iter 280460 || Loss: 0.7462 || timer: 0.0851 sec.
iter 280470 || Loss: 0.8249 || timer: 0.0810 sec.
iter 280480 || Loss: 0.5542 || timer: 0.0858 sec.
iter 280490 || Loss: 0.9505 || timer: 0.0922 sec.
iter 280500 || Loss: 0.8031 || timer: 0.0872 sec.
iter 280510 || Loss: 0.8978 || timer: 0.0873 sec.
iter 280520 || Loss: 0.5293 || timer: 0.0911 sec.
iter 280530 || Loss: 0.9686 || timer: 0.1005 sec.
iter 280540 || Loss: 0.8175 || timer: 0.0823 sec.
iter 280550 || Loss: 0.5933 || timer: 0.0826 sec.
iter 280560 || Loss: 0.9758 || timer: 0.1047 sec.
iter 280570 || Loss: 0.7155 || timer: 0.0900 sec.
iter 280580 || Loss: 0.7737 || timer: 0.0826 sec.
iter 280590 || Loss: 0.7440 || timer: 0.0914 sec.
iter 280600 || Loss: 0.6389 || timer: 0.0829 sec.
iter 280610 || Loss: 1.0273 || timer: 0.0916 sec.
iter 280620 || Loss: 0.8344 || timer: 0.0896 sec.
iter 280630 || Loss: 0.6917 || timer: 0.1008 sec.
iter 280640 || Loss: 0.8019 || timer: 0.0826 sec.
iter 280650 || Loss: 0.8268 || timer: 0.0834 sec.
iter 280660 || Loss: 0.6431 || timer: 0.0834 sec.
iter 280670 || Loss: 1.0203 || timer: 0.0887 sec.
iter 280680 || Loss: 0.7170 || timer: 0.0887 sec.
iter 280690 || Loss: 0.7286 || timer: 0.1165 sec.
iter 280700 || Loss: 0.8997 || timer: 0.0894 sec.
iter 280710 || Loss: 0.7249 || timer: 0.1017 sec.
iter 280720 || Loss: 0.7150 || timer: 0.0206 sec.
iter 280730 || Loss: 2.3077 || timer: 0.0936 sec.
iter 280740 || Loss: 0.9178 || timer: 0.0856 sec.
iter 280750 || Loss: 0.9820 || timer: 0.0804 sec.
iter 280760 || Loss: 0.8342 || timer: 0.0825 sec.
iter 280770 || Loss: 0.7736 || timer: 0.0835 sec.
iter 280780 || Loss: 0.8094 || timer: 0.0876 sec.
iter 280790 || Loss: 0.7192 || timer: 0.0996 sec.
iter 280800 || Loss: 0.6903 || timer: 0.0892 sec.
iter 280810 || Loss: 0.7197 || timer: 0.1121 sec.
iter 280820 || Loss: 0.6927 || timer: 0.1389 sec.
iter 280830 || Loss: 0.7937 || timer: 0.1055 sec.
iter 280840 || Loss: 0.9009 || timer: 0.0891 sec.
iter 280850 || Loss: 0.4515 || timer: 0.0896 sec.
iter 280860 || Loss: 0.5955 || timer: 0.1025 sec.
iter 280870 || Loss: 0.7385 || timer: 0.0757 sec.
iter 280880 || Loss: 0.8185 || timer: 0.1119 sec.
iter 280890 || Loss: 0.7636 || timer: 0.0967 sec.
iter 280900 || Loss: 0.7962 || timer: 0.0914 sec.
iter 280910 || Loss: 0.9106 || timer: 0.0895 sec.
iter 280920 || Loss: 0.8557 || timer: 0.0893 sec.
iter 280930 || Loss: 0.6087 || timer: 0.0823 sec.
iter 280940 || Loss: 0.6760 || timer: 0.0898 sec.
iter 280950 || Loss: 0.8148 || timer: 0.0894 sec.
iter 280960 || Loss: 0.6901 || timer: 0.0760 sec.
iter 280970 || Loss: 0.8216 || timer: 0.0847 sec.
iter 280980 || Loss: 0.7189 || timer: 0.0895 sec.
iter 280990 || Loss: 0.5944 || timer: 0.0916 sec.
iter 281000 || Loss: 0.6337 || timer: 0.0848 sec.
iter 281010 || Loss: 1.0568 || timer: 0.0989 sec.
iter 281020 || Loss: 0.7755 || timer: 0.0897 sec.
iter 281030 || Loss: 0.7171 || timer: 0.0916 sec.
iter 281040 || Loss: 0.7163 || timer: 0.0981 sec.
iter 281050 || Loss: 0.6334 || timer: 0.0198 sec.
iter 281060 || Loss: 0.5692 || timer: 0.0924 sec.
iter 281070 || Loss: 0.5551 || timer: 0.1249 sec.
iter 281080 || Loss: 0.8375 || timer: 0.0835 sec.
iter 281090 || Loss: 0.9062 || timer: 0.0885 sec.
iter 281100 || Loss: 0.6098 || timer: 0.0903 sec.
iter 281110 || Loss: 0.8591 || timer: 0.0866 sec.
iter 281120 || Loss: 0.8281 || timer: 0.0898 sec.
iter 281130 || Loss: 0.7998 || timer: 0.0904 sec.
iter 281140 || Loss: 0.7033 || timer: 0.0894 sec.
iter 281150 || Loss: 1.0346 || timer: 0.0951 sec.
iter 281160 || Loss: 0.4234 || timer: 0.0911 sec.
iter 281170 || Loss: 0.8003 || timer: 0.0956 sec.
iter 281180 || Loss: 0.6627 || timer: 0.0838 sec.
iter 281190 || Loss: 0.9167 || timer: 0.0823 sec.
iter 281200 || Loss: 0.8183 || timer: 0.0813 sec.
iter 281210 || Loss: 0.7484 || timer: 0.0876 sec.
iter 281220 || Loss: 0.4397 || timer: 0.0925 sec.
iter 281230 || Loss: 0.6003 || timer: 0.0883 sec.
iter 281240 || Loss: 0.7052 || timer: 0.1070 sec.
iter 281250 || Loss: 0.5162 || timer: 0.0838 sec.
iter 281260 || Loss: 0.7417 || timer: 0.0813 sec.
iter 281270 || Loss: 0.5348 || timer: 0.0938 sec.
iter 281280 || Loss: 0.7560 || timer: 0.0930 sec.
iter 281290 || Loss: 0.6928 || timer: 0.0906 sec.
iter 281300 || Loss: 0.7051 || timer: 0.1053 sec.
iter 281310 || Loss: 0.6932 || timer: 0.1051 sec.
iter 281320 || Loss: 0.7291 || timer: 0.0818 sec.
iter 281330 || Loss: 1.0119 || timer: 0.0894 sec.
iter 281340 || Loss: 0.7628 || timer: 0.0873 sec.
iter 281350 || Loss: 0.5852 || timer: 0.0910 sec.
iter 281360 || Loss: 0.7680 || timer: 0.0878 sec.
iter 281370 || Loss: 0.8681 || timer: 0.0784 sec.
iter 281380 || Loss: 0.8298 || timer: 0.0194 sec.
iter 281390 || Loss: 1.6102 || timer: 0.0826 sec.
iter 281400 || Loss: 0.5635 || timer: 0.0898 sec.
iter 281410 || Loss: 0.7423 || timer: 0.0822 sec.
iter 281420 || Loss: 1.0214 || timer: 0.0820 sec.
iter 281430 || Loss: 0.7984 || timer: 0.0870 sec.
iter 281440 || Loss: 0.6792 || timer: 0.1060 sec.
iter 281450 || Loss: 0.8487 || timer: 0.0840 sec.
iter 281460 || Loss: 0.7781 || timer: 0.1091 sec.
iter 281470 || Loss: 1.0789 || timer: 0.1025 sec.
iter 281480 || Loss: 0.5275 || timer: 0.1086 sec.
iter 281490 || Loss: 0.8422 || timer: 0.0894 sec.
iter 281500 || Loss: 0.7302 || timer: 0.0852 sec.
iter 281510 || Loss: 0.6159 || timer: 0.0852 sec.
iter 281520 || Loss: 0.7474 || timer: 0.0832 sec.
iter 281530 || Loss: 0.8429 || timer: 0.0961 sec.
iter 281540 || Loss: 0.7909 || timer: 0.0903 sec.
iter 281550 || Loss: 0.8422 || timer: 0.0926 sec.
iter 281560 || Loss: 0.9047 || timer: 0.1124 sec.
iter 281570 || Loss: 0.7354 || timer: 0.0918 sec.
iter 281580 || Loss: 0.9593 || timer: 0.0873 sec.
iter 281590 || Loss: 0.7089 || timer: 0.0897 sec.
iter 281600 || Loss: 1.0775 || timer: 0.0914 sec.
iter 281610 || Loss: 0.7037 || timer: 0.0878 sec.
iter 281620 || Loss: 0.6039 || timer: 0.0910 sec.
iter 281630 || Loss: 0.7777 || timer: 0.1035 sec.
iter 281640 || Loss: 0.6992 || timer: 0.0817 sec.
iter 281650 || Loss: 0.9161 || timer: 0.1118 sec.
iter 281660 || Loss: 0.6186 || timer: 0.1238 sec.
iter 281670 || Loss: 0.6341 || timer: 0.0897 sec.
iter 281680 || Loss: 0.8372 || timer: 0.0830 sec.
iter 281690 || Loss: 0.6652 || timer: 0.0902 sec.
iter 281700 || Loss: 0.8831 || timer: 0.0933 sec.
iter 281710 || Loss: 0.6484 || timer: 0.0164 sec.
iter 281720 || Loss: 1.1725 || timer: 0.0918 sec.
iter 281730 || Loss: 0.5984 || timer: 0.0915 sec.
iter 281740 || Loss: 0.6567 || timer: 0.1014 sec.
iter 281750 || Loss: 0.6392 || timer: 0.0903 sec.
iter 281760 || Loss: 0.8541 || timer: 0.0832 sec.
iter 281770 || Loss: 0.6820 || timer: 0.0899 sec.
iter 281780 || Loss: 0.7877 || timer: 0.0840 sec.
iter 281790 || Loss: 0.8003 || timer: 0.0882 sec.
iter 281800 || Loss: 0.6365 || timer: 0.1265 sec.
iter 281810 || Loss: 0.6757 || timer: 0.1280 sec.
iter 281820 || Loss: 0.6200 || timer: 0.0903 sec.
iter 281830 || Loss: 0.7372 || timer: 0.0896 sec.
iter 281840 || Loss: 0.6328 || timer: 0.0837 sec.
iter 281850 || Loss: 1.1142 || timer: 0.0879 sec.
iter 281860 || Loss: 0.8185 || timer: 0.0821 sec.
iter 281870 || Loss: 0.7482 || timer: 0.1000 sec.
iter 281880 || Loss: 0.7053 || timer: 0.1074 sec.
iter 281890 || Loss: 0.5088 || timer: 0.0983 sec.
iter 281900 || Loss: 0.6549 || timer: 0.0902 sec.
iter 281910 || Loss: 0.8766 || timer: 0.0861 sec.
iter 281920 || Loss: 0.7940 || timer: 0.1221 sec.
iter 281930 || Loss: 0.6182 || timer: 0.0918 sec.
iter 281940 || Loss: 0.9391 || timer: 0.0835 sec.
iter 281950 || Loss: 0.6062 || timer: 0.1035 sec.
iter 281960 || Loss: 0.7593 || timer: 0.0838 sec.
iter 281970 || Loss: 0.5166 || timer: 0.0879 sec.
iter 281980 || Loss: 0.7304 || timer: 0.0990 sec.
iter 281990 || Loss: 1.3654 || timer: 0.0921 sec.
iter 282000 || Loss: 0.7240 || timer: 0.0916 sec.
iter 282010 || Loss: 0.8310 || timer: 0.0866 sec.
iter 282020 || Loss: 0.8366 || timer: 0.0915 sec.
iter 282030 || Loss: 0.7220 || timer: 0.0918 sec.
iter 282040 || Loss: 0.7810 || timer: 0.0217 sec.
iter 282050 || Loss: 0.9883 || timer: 0.0897 sec.
iter 282060 || Loss: 0.7637 || timer: 0.0896 sec.
iter 282070 || Loss: 0.9061 || timer: 0.0910 sec.
iter 282080 || Loss: 0.6618 || timer: 0.0939 sec.
iter 282090 || Loss: 0.8247 || timer: 0.0914 sec.
iter 282100 || Loss: 0.7470 || timer: 0.0914 sec.
iter 282110 || Loss: 0.9064 || timer: 0.0945 sec.
iter 282120 || Loss: 0.8066 || timer: 0.0885 sec.
iter 282130 || Loss: 0.6884 || timer: 0.0919 sec.
iter 282140 || Loss: 0.6240 || timer: 0.0997 sec.
iter 282150 || Loss: 0.7548 || timer: 0.0818 sec.
iter 282160 || Loss: 0.8353 || timer: 0.0864 sec.
iter 282170 || Loss: 0.6476 || timer: 0.0879 sec.
iter 282180 || Loss: 0.5870 || timer: 0.0933 sec.
iter 282190 || Loss: 0.7023 || timer: 0.1079 sec.
iter 282200 || Loss: 0.7048 || timer: 0.0885 sec.
iter 282210 || Loss: 0.8753 || timer: 0.0929 sec.
iter 282220 || Loss: 0.6182 || timer: 0.0901 sec.
iter 282230 || Loss: 0.5461 || timer: 0.0910 sec.
iter 282240 || Loss: 0.9372 || timer: 0.0958 sec.
iter 282250 || Loss: 0.8692 || timer: 0.1087 sec.
iter 282260 || Loss: 0.6410 || timer: 0.0909 sec.
iter 282270 || Loss: 0.8441 || timer: 0.0911 sec.
iter 282280 || Loss: 0.8962 || timer: 0.0951 sec.
iter 282290 || Loss: 0.7866 || timer: 0.0964 sec.
iter 282300 || Loss: 0.7725 || timer: 0.1154 sec.
iter 282310 || Loss: 0.8730 || timer: 0.0927 sec.
iter 282320 || Loss: 0.7151 || timer: 0.0830 sec.
iter 282330 || Loss: 0.7434 || timer: 0.0913 sec.
iter 282340 || Loss: 0.7753 || timer: 0.1057 sec.
iter 282350 || Loss: 1.1853 || timer: 0.0926 sec.
iter 282360 || Loss: 0.5991 || timer: 0.0912 sec.
iter 282370 || Loss: 0.6683 || timer: 0.0179 sec.
iter 282380 || Loss: 0.7326 || timer: 0.0831 sec.
iter 282390 || Loss: 0.5621 || timer: 0.0908 sec.
iter 282400 || Loss: 0.7887 || timer: 0.0989 sec.
iter 282410 || Loss: 0.9043 || timer: 0.1055 sec.
iter 282420 || Loss: 0.5927 || timer: 0.0913 sec.
iter 282430 || Loss: 0.5267 || timer: 0.0923 sec.
iter 282440 || Loss: 0.7691 || timer: 0.0840 sec.
iter 282450 || Loss: 0.5272 || timer: 0.0847 sec.
iter 282460 || Loss: 0.8608 || timer: 0.0985 sec.
iter 282470 || Loss: 0.9424 || timer: 0.0943 sec.
iter 282480 || Loss: 0.9406 || timer: 0.0974 sec.
iter 282490 || Loss: 0.7545 || timer: 0.0871 sec.
iter 282500 || Loss: 0.9830 || timer: 0.0924 sec.
iter 282510 || Loss: 0.5468 || timer: 0.0925 sec.
iter 282520 || Loss: 0.5632 || timer: 0.0880 sec.
iter 282530 || Loss: 0.7925 || timer: 0.0832 sec.
iter 282540 || Loss: 0.6980 || timer: 0.0834 sec.
iter 282550 || Loss: 0.7986 || timer: 0.0841 sec.
iter 282560 || Loss: 0.5674 || timer: 0.1068 sec.
iter 282570 || Loss: 1.0793 || timer: 0.0928 sec.
iter 282580 || Loss: 0.7150 || timer: 0.0945 sec.
iter 282590 || Loss: 0.5708 || timer: 0.1155 sec.
iter 282600 || Loss: 0.6712 || timer: 0.0833 sec.
iter 282610 || Loss: 0.4848 || timer: 0.0911 sec.
iter 282620 || Loss: 0.6682 || timer: 0.0909 sec.
iter 282630 || Loss: 0.6995 || timer: 0.0832 sec.
iter 282640 || Loss: 0.7598 || timer: 0.1089 sec.
iter 282650 || Loss: 0.9252 || timer: 0.0898 sec.
iter 282660 || Loss: 0.6170 || timer: 0.1076 sec.
iter 282670 || Loss: 0.7207 || timer: 0.1103 sec.
iter 282680 || Loss: 0.6978 || timer: 0.0904 sec.
iter 282690 || Loss: 0.6711 || timer: 0.0910 sec.
iter 282700 || Loss: 0.8383 || timer: 0.0239 sec.
iter 282710 || Loss: 2.3373 || timer: 0.0914 sec.
iter 282720 || Loss: 0.9457 || timer: 0.0842 sec.
iter 282730 || Loss: 0.7637 || timer: 0.0952 sec.
iter 282740 || Loss: 0.6263 || timer: 0.0837 sec.
iter 282750 || Loss: 0.9697 || timer: 0.0920 sec.
iter 282760 || Loss: 0.6701 || timer: 0.0910 sec.
iter 282770 || Loss: 0.8938 || timer: 0.0914 sec.
iter 282780 || Loss: 0.8957 || timer: 0.0950 sec.
iter 282790 || Loss: 0.8488 || timer: 0.0962 sec.
iter 282800 || Loss: 0.7806 || timer: 0.0946 sec.
iter 282810 || Loss: 0.8005 || timer: 0.0835 sec.
iter 282820 || Loss: 0.7948 || timer: 0.0836 sec.
iter 282830 || Loss: 0.6427 || timer: 0.1069 sec.
iter 282840 || Loss: 0.7638 || timer: 0.0922 sec.
iter 282850 || Loss: 0.7720 || timer: 0.0924 sec.
iter 282860 || Loss: 0.6858 || timer: 0.0876 sec.
iter 282870 || Loss: 0.9297 || timer: 0.1051 sec.
iter 282880 || Loss: 0.6695 || timer: 0.0898 sec.
iter 282890 || Loss: 0.6901 || timer: 0.1338 sec.
iter 282900 || Loss: 1.0782 || timer: 0.0904 sec.
iter 282910 || Loss: 0.8059 || timer: 0.0913 sec.
iter 282920 || Loss: 0.8894 || timer: 0.0872 sec.
iter 282930 || Loss: 0.5953 || timer: 0.0902 sec.
iter 282940 || Loss: 1.0038 || timer: 0.0852 sec.
iter 282950 || Loss: 0.8723 || timer: 0.0889 sec.
iter 282960 || Loss: 0.6650 || timer: 0.0831 sec.
iter 282970 || Loss: 0.6418 || timer: 0.1090 sec.
iter 282980 || Loss: 0.7376 || timer: 0.0910 sec.
iter 282990 || Loss: 0.5236 || timer: 0.0946 sec.
iter 283000 || Loss: 0.8139 || timer: 0.0927 sec.
iter 283010 || Loss: 0.6400 || timer: 0.1068 sec.
iter 283020 || Loss: 0.6625 || timer: 0.0898 sec.
iter 283030 || Loss: 0.7784 || timer: 0.0176 sec.
iter 283040 || Loss: 0.2734 || timer: 0.0917 sec.
iter 283050 || Loss: 0.8799 || timer: 0.0888 sec.
iter 283060 || Loss: 1.0133 || timer: 0.0883 sec.
iter 283070 || Loss: 1.0636 || timer: 0.0876 sec.
iter 283080 || Loss: 0.9082 || timer: 0.0903 sec.
iter 283090 || Loss: 0.7270 || timer: 0.0831 sec.
iter 283100 || Loss: 0.8003 || timer: 0.0887 sec.
iter 283110 || Loss: 0.6170 || timer: 0.0902 sec.
iter 283120 || Loss: 0.5549 || timer: 0.0923 sec.
iter 283130 || Loss: 0.5985 || timer: 0.1001 sec.
iter 283140 || Loss: 0.5695 || timer: 0.0835 sec.
iter 283150 || Loss: 0.8995 || timer: 0.0943 sec.
iter 283160 || Loss: 0.6446 || timer: 0.0858 sec.
iter 283170 || Loss: 0.8472 || timer: 0.0880 sec.
iter 283180 || Loss: 0.8092 || timer: 0.0844 sec.
iter 283190 || Loss: 1.1297 || timer: 0.0992 sec.
iter 283200 || Loss: 0.7253 || timer: 0.0895 sec.
iter 283210 || Loss: 0.7921 || timer: 0.0855 sec.
iter 283220 || Loss: 1.2421 || timer: 0.1064 sec.
iter 283230 || Loss: 0.4872 || timer: 0.0919 sec.
iter 283240 || Loss: 0.6975 || timer: 0.0895 sec.
iter 283250 || Loss: 0.7525 || timer: 0.0825 sec.
iter 283260 || Loss: 0.6150 || timer: 0.0883 sec.
iter 283270 || Loss: 0.6440 || timer: 0.0883 sec.
iter 283280 || Loss: 0.6171 || timer: 0.0927 sec.
iter 283290 || Loss: 0.9709 || timer: 0.0895 sec.
iter 283300 || Loss: 0.7176 || timer: 0.0862 sec.
iter 283310 || Loss: 0.6377 || timer: 0.0956 sec.
iter 283320 || Loss: 0.9820 || timer: 0.1068 sec.
iter 283330 || Loss: 0.7087 || timer: 0.0896 sec.
iter 283340 || Loss: 0.6883 || timer: 0.0908 sec.
iter 283350 || Loss: 0.4875 || timer: 0.0882 sec.
iter 283360 || Loss: 0.7138 || timer: 0.0182 sec.
iter 283370 || Loss: 0.5828 || timer: 0.0832 sec.
iter 283380 || Loss: 1.0521 || timer: 0.1098 sec.
iter 283390 || Loss: 0.7529 || timer: 0.0870 sec.
iter 283400 || Loss: 0.8998 || timer: 0.0839 sec.
iter 283410 || Loss: 0.7897 || timer: 0.0828 sec.
iter 283420 || Loss: 0.8497 || timer: 0.0833 sec.
iter 283430 || Loss: 0.7107 || timer: 0.1115 sec.
iter 283440 || Loss: 0.5271 || timer: 0.0919 sec.
iter 283450 || Loss: 0.5849 || timer: 0.0823 sec.
iter 283460 || Loss: 0.9018 || timer: 0.0938 sec.
iter 283470 || Loss: 0.7313 || timer: 0.1033 sec.
iter 283480 || Loss: 0.7902 || timer: 0.1003 sec.
iter 283490 || Loss: 0.5522 || timer: 0.0817 sec.
iter 283500 || Loss: 0.6848 || timer: 0.0899 sec.
iter 283510 || Loss: 0.6880 || timer: 0.0757 sec.
iter 283520 || Loss: 0.6544 || timer: 0.0837 sec.
iter 283530 || Loss: 0.5907 || timer: 0.0912 sec.
iter 283540 || Loss: 0.6457 || timer: 0.0895 sec.
iter 283550 || Loss: 0.5813 || timer: 0.0924 sec.
iter 283560 || Loss: 0.7265 || timer: 0.0874 sec.
iter 283570 || Loss: 0.6919 || timer: 0.0844 sec.
iter 283580 || Loss: 0.7564 || timer: 0.0901 sec.
iter 283590 || Loss: 0.7917 || timer: 0.0808 sec.
iter 283600 || Loss: 0.8611 || timer: 0.0872 sec.
iter 283610 || Loss: 0.4549 || timer: 0.0914 sec.
iter 283620 || Loss: 0.5745 || timer: 0.0821 sec.
iter 283630 || Loss: 0.6575 || timer: 0.0839 sec.
iter 283640 || Loss: 0.6438 || timer: 0.0826 sec.
iter 283650 || Loss: 0.6206 || timer: 0.0898 sec.
iter 283660 || Loss: 0.6280 || timer: 0.0897 sec.
iter 283670 || Loss: 0.9575 || timer: 0.0861 sec.
iter 283680 || Loss: 1.0455 || timer: 0.0762 sec.
iter 283690 || Loss: 0.6207 || timer: 0.0160 sec.
iter 283700 || Loss: 0.4605 || timer: 0.0928 sec.
iter 283710 || Loss: 0.7143 || timer: 0.1152 sec.
iter 283720 || Loss: 0.7673 || timer: 0.0824 sec.
iter 283730 || Loss: 0.6708 || timer: 0.0884 sec.
iter 283740 || Loss: 0.7222 || timer: 0.0875 sec.
iter 283750 || Loss: 0.8269 || timer: 0.0897 sec.
iter 283760 || Loss: 0.9854 || timer: 0.0908 sec.
iter 283770 || Loss: 0.8604 || timer: 0.0856 sec.
iter 283780 || Loss: 0.6607 || timer: 0.0901 sec.
iter 283790 || Loss: 1.0308 || timer: 0.1027 sec.
iter 283800 || Loss: 0.7244 || timer: 0.0872 sec.
iter 283810 || Loss: 0.5563 || timer: 0.0921 sec.
iter 283820 || Loss: 0.6445 || timer: 0.0887 sec.
iter 283830 || Loss: 0.6741 || timer: 0.0923 sec.
iter 283840 || Loss: 0.6071 || timer: 0.1043 sec.
iter 283850 || Loss: 0.5470 || timer: 0.0820 sec.
iter 283860 || Loss: 0.9075 || timer: 0.0840 sec.
iter 283870 || Loss: 0.8903 || timer: 0.0856 sec.
iter 283880 || Loss: 0.6101 || timer: 0.0925 sec.
iter 283890 || Loss: 0.5750 || timer: 0.1120 sec.
iter 283900 || Loss: 0.8754 || timer: 0.0896 sec.
iter 283910 || Loss: 0.8236 || timer: 0.0995 sec.
iter 283920 || Loss: 0.7178 || timer: 0.0872 sec.
iter 283930 || Loss: 0.7991 || timer: 0.0810 sec.
iter 283940 || Loss: 0.7122 || timer: 0.0904 sec.
iter 283950 || Loss: 0.5869 || timer: 0.0899 sec.
iter 283960 || Loss: 0.9377 || timer: 0.0824 sec.
iter 283970 || Loss: 0.8720 || timer: 0.0833 sec.
iter 283980 || Loss: 0.7155 || timer: 0.0865 sec.
iter 283990 || Loss: 0.8554 || timer: 0.0827 sec.
iter 284000 || Loss: 0.9004 || timer: 0.0905 sec.
iter 284010 || Loss: 0.7272 || timer: 0.0829 sec.
iter 284020 || Loss: 0.6540 || timer: 0.0162 sec.
iter 284030 || Loss: 1.4151 || timer: 0.0909 sec.
iter 284040 || Loss: 0.6920 || timer: 0.0983 sec.
iter 284050 || Loss: 0.7885 || timer: 0.0921 sec.
iter 284060 || Loss: 0.7529 || timer: 0.0907 sec.
iter 284070 || Loss: 0.7022 || timer: 0.1120 sec.
iter 284080 || Loss: 0.8375 || timer: 0.0941 sec.
iter 284090 || Loss: 0.5178 || timer: 0.0855 sec.
iter 284100 || Loss: 0.7807 || timer: 0.0955 sec.
iter 284110 || Loss: 1.0471 || timer: 0.0900 sec.
iter 284120 || Loss: 0.6203 || timer: 0.1094 sec.
iter 284130 || Loss: 0.7567 || timer: 0.1085 sec.
iter 284140 || Loss: 0.8420 || timer: 0.1070 sec.
iter 284150 || Loss: 0.9887 || timer: 0.0846 sec.
iter 284160 || Loss: 1.0162 || timer: 0.0829 sec.
iter 284170 || Loss: 0.7533 || timer: 0.0948 sec.
iter 284180 || Loss: 0.7266 || timer: 0.1030 sec.
iter 284190 || Loss: 0.7242 || timer: 0.0851 sec.
iter 284200 || Loss: 0.3841 || timer: 0.0844 sec.
iter 284210 || Loss: 0.7278 || timer: 0.0970 sec.
iter 284220 || Loss: 0.8793 || timer: 0.0833 sec.
iter 284230 || Loss: 0.8961 || timer: 0.0826 sec.
iter 284240 || Loss: 0.5560 || timer: 0.0820 sec.
iter 284250 || Loss: 0.7171 || timer: 0.0855 sec.
iter 284260 || Loss: 0.8079 || timer: 0.0891 sec.
iter 284270 || Loss: 0.6664 || timer: 0.0900 sec.
iter 284280 || Loss: 0.7330 || timer: 0.0903 sec.
iter 284290 || Loss: 1.3695 || timer: 0.0905 sec.
iter 284300 || Loss: 0.8285 || timer: 0.0826 sec.
iter 284310 || Loss: 0.7441 || timer: 0.0918 sec.
iter 284320 || Loss: 0.4858 || timer: 0.0829 sec.
iter 284330 || Loss: 0.8214 || timer: 0.0853 sec.
iter 284340 || Loss: 0.7871 || timer: 0.0816 sec.
iter 284350 || Loss: 0.9344 || timer: 0.0260 sec.
iter 284360 || Loss: 0.6291 || timer: 0.0828 sec.
iter 284370 || Loss: 0.7130 || timer: 0.0847 sec.
iter 284380 || Loss: 0.6647 || timer: 0.0747 sec.
iter 284390 || Loss: 0.7311 || timer: 0.0866 sec.
iter 284400 || Loss: 1.1019 || timer: 0.1045 sec.
iter 284410 || Loss: 0.6336 || timer: 0.0905 sec.
iter 284420 || Loss: 0.7696 || timer: 0.0891 sec.
iter 284430 || Loss: 0.6810 || timer: 0.1103 sec.
iter 284440 || Loss: 0.9031 || timer: 0.0892 sec.
iter 284450 || Loss: 0.7337 || timer: 0.0953 sec.
iter 284460 || Loss: 0.6230 || timer: 0.0921 sec.
iter 284470 || Loss: 0.8384 || timer: 0.0893 sec.
iter 284480 || Loss: 0.8824 || timer: 0.0890 sec.
iter 284490 || Loss: 0.5235 || timer: 0.0892 sec.
iter 284500 || Loss: 1.0042 || timer: 0.0894 sec.
iter 284510 || Loss: 0.9406 || timer: 0.0907 sec.
iter 284520 || Loss: 0.7301 || timer: 0.0929 sec.
iter 284530 || Loss: 0.8808 || timer: 0.1018 sec.
iter 284540 || Loss: 0.8909 || timer: 0.0888 sec.
iter 284550 || Loss: 0.7077 || timer: 0.0994 sec.
iter 284560 || Loss: 0.4722 || timer: 0.1023 sec.
iter 284570 || Loss: 0.9322 || timer: 0.0893 sec.
iter 284580 || Loss: 0.7724 || timer: 0.0868 sec.
iter 284590 || Loss: 0.7125 || timer: 0.0919 sec.
iter 284600 || Loss: 0.7010 || timer: 0.0894 sec.
iter 284610 || Loss: 0.6748 || timer: 0.0894 sec.
iter 284620 || Loss: 0.4674 || timer: 0.0822 sec.
iter 284630 || Loss: 0.8506 || timer: 0.1072 sec.
iter 284640 || Loss: 0.7512 || timer: 0.0877 sec.
iter 284650 || Loss: 0.7012 || timer: 0.0900 sec.
iter 284660 || Loss: 0.6266 || timer: 0.1054 sec.
iter 284670 || Loss: 1.0569 || timer: 0.0825 sec.
iter 284680 || Loss: 0.5213 || timer: 0.0154 sec.
iter 284690 || Loss: 0.6044 || timer: 0.0830 sec.
iter 284700 || Loss: 0.6579 || timer: 0.1070 sec.
iter 284710 || Loss: 0.4650 || timer: 0.0888 sec.
iter 284720 || Loss: 0.5675 || timer: 0.0872 sec.
iter 284730 || Loss: 0.6465 || timer: 0.0905 sec.
iter 284740 || Loss: 0.8902 || timer: 0.0831 sec.
iter 284750 || Loss: 0.8106 || timer: 0.0895 sec.
iter 284760 || Loss: 0.7364 || timer: 0.0899 sec.
iter 284770 || Loss: 0.7510 || timer: 0.1026 sec.
iter 284780 || Loss: 0.6270 || timer: 0.0999 sec.
iter 284790 || Loss: 0.6957 || timer: 0.0905 sec.
iter 284800 || Loss: 0.6630 || timer: 0.1265 sec.
iter 284810 || Loss: 0.9108 || timer: 0.0899 sec.
iter 284820 || Loss: 0.6799 || timer: 0.0828 sec.
iter 284830 || Loss: 0.7797 || timer: 0.0825 sec.
iter 284840 || Loss: 1.1153 || timer: 0.0868 sec.
iter 284850 || Loss: 0.8446 || timer: 0.1069 sec.
iter 284860 || Loss: 0.6867 || timer: 0.0914 sec.
iter 284870 || Loss: 0.8577 || timer: 0.1089 sec.
iter 284880 || Loss: 0.6683 || timer: 0.0838 sec.
iter 284890 || Loss: 0.7158 || timer: 0.0908 sec.
iter 284900 || Loss: 0.9210 || timer: 0.0813 sec.
iter 284910 || Loss: 0.6612 || timer: 0.0826 sec.
iter 284920 || Loss: 0.8239 || timer: 0.0878 sec.
iter 284930 || Loss: 0.5082 || timer: 0.0827 sec.
iter 284940 || Loss: 0.5474 || timer: 0.1054 sec.
iter 284950 || Loss: 0.6790 || timer: 0.0901 sec.
iter 284960 || Loss: 0.7578 || timer: 0.0893 sec.
iter 284970 || Loss: 1.0125 || timer: 0.0904 sec.
iter 284980 || Loss: 0.4001 || timer: 0.0828 sec.
iter 284990 || Loss: 0.7452 || timer: 0.0920 sec.
iter 285000 || Loss: 0.6785 || Saving state, iter: 285000
timer: 0.0832 sec.
iter 285010 || Loss: 1.0112 || timer: 0.0248 sec.
iter 285020 || Loss: 0.5134 || timer: 0.0924 sec.
iter 285030 || Loss: 0.7766 || timer: 0.0831 sec.
iter 285040 || Loss: 0.4700 || timer: 0.0831 sec.
iter 285050 || Loss: 0.9542 || timer: 0.0906 sec.
iter 285060 || Loss: 0.6712 || timer: 0.0906 sec.
iter 285070 || Loss: 0.6924 || timer: 0.1050 sec.
iter 285080 || Loss: 0.6500 || timer: 0.0870 sec.
iter 285090 || Loss: 0.7821 || timer: 0.0977 sec.
iter 285100 || Loss: 1.0455 || timer: 0.0850 sec.
iter 285110 || Loss: 0.5121 || timer: 0.1019 sec.
iter 285120 || Loss: 0.8496 || timer: 0.0901 sec.
iter 285130 || Loss: 0.9155 || timer: 0.0829 sec.
iter 285140 || Loss: 0.6052 || timer: 0.0820 sec.
iter 285150 || Loss: 0.5590 || timer: 0.0926 sec.
iter 285160 || Loss: 0.9014 || timer: 0.0825 sec.
iter 285170 || Loss: 1.0164 || timer: 0.0927 sec.
iter 285180 || Loss: 0.6904 || timer: 0.0918 sec.
iter 285190 || Loss: 0.8127 || timer: 0.0877 sec.
iter 285200 || Loss: 0.6574 || timer: 0.0830 sec.
iter 285210 || Loss: 0.8349 || timer: 0.0910 sec.
iter 285220 || Loss: 0.8336 || timer: 0.1006 sec.
iter 285230 || Loss: 0.9250 || timer: 0.0986 sec.
iter 285240 || Loss: 0.6859 || timer: 0.0901 sec.
iter 285250 || Loss: 0.8376 || timer: 0.0919 sec.
iter 285260 || Loss: 0.9069 || timer: 0.0918 sec.
iter 285270 || Loss: 0.8301 || timer: 0.0945 sec.
iter 285280 || Loss: 0.9294 || timer: 0.0913 sec.
iter 285290 || Loss: 0.8550 || timer: 0.0830 sec.
iter 285300 || Loss: 0.7137 || timer: 0.1096 sec.
iter 285310 || Loss: 0.6274 || timer: 0.0834 sec.
iter 285320 || Loss: 0.6573 || timer: 0.0878 sec.
iter 285330 || Loss: 0.8625 || timer: 0.0939 sec.
iter 285340 || Loss: 0.6051 || timer: 0.0222 sec.
iter 285350 || Loss: 1.2237 || timer: 0.0832 sec.
iter 285360 || Loss: 1.0618 || timer: 0.0919 sec.
iter 285370 || Loss: 0.6907 || timer: 0.1009 sec.
iter 285380 || Loss: 0.7006 || timer: 0.0915 sec.
iter 285390 || Loss: 0.9206 || timer: 0.0896 sec.
iter 285400 || Loss: 0.8114 || timer: 0.0915 sec.
iter 285410 || Loss: 0.7646 || timer: 0.0841 sec.
iter 285420 || Loss: 1.0789 || timer: 0.0906 sec.
iter 285430 || Loss: 0.7832 || timer: 0.0949 sec.
iter 285440 || Loss: 0.5406 || timer: 0.0979 sec.
iter 285450 || Loss: 0.8555 || timer: 0.0825 sec.
iter 285460 || Loss: 0.6597 || timer: 0.0776 sec.
iter 285470 || Loss: 0.7402 || timer: 0.0892 sec.
iter 285480 || Loss: 0.5343 || timer: 0.0918 sec.
iter 285490 || Loss: 0.7147 || timer: 0.0914 sec.
iter 285500 || Loss: 0.8024 || timer: 0.1042 sec.
iter 285510 || Loss: 0.7604 || timer: 0.1045 sec.
iter 285520 || Loss: 0.7316 || timer: 0.1070 sec.
iter 285530 || Loss: 0.6179 || timer: 0.0838 sec.
iter 285540 || Loss: 0.6984 || timer: 0.0924 sec.
iter 285550 || Loss: 0.8119 || timer: 0.0917 sec.
iter 285560 || Loss: 0.8616 || timer: 0.0837 sec.
iter 285570 || Loss: 0.9434 || timer: 0.0892 sec.
iter 285580 || Loss: 0.7732 || timer: 0.0898 sec.
iter 285590 || Loss: 0.7396 || timer: 0.0892 sec.
iter 285600 || Loss: 0.7002 || timer: 0.1051 sec.
iter 285610 || Loss: 0.5467 || timer: 0.1090 sec.
iter 285620 || Loss: 0.8686 || timer: 0.0917 sec.
iter 285630 || Loss: 0.9144 || timer: 0.0894 sec.
iter 285640 || Loss: 0.8793 || timer: 0.0863 sec.
iter 285650 || Loss: 0.6267 || timer: 0.0778 sec.
iter 285660 || Loss: 0.8582 || timer: 0.0837 sec.
iter 285670 || Loss: 0.9418 || timer: 0.0155 sec.
iter 285680 || Loss: 0.3641 || timer: 0.0878 sec.
iter 285690 || Loss: 0.9207 || timer: 0.0884 sec.
iter 285700 || Loss: 0.6696 || timer: 0.0903 sec.
iter 285710 || Loss: 0.8797 || timer: 0.0881 sec.
iter 285720 || Loss: 0.7027 || timer: 0.0914 sec.
iter 285730 || Loss: 0.5908 || timer: 0.0832 sec.
iter 285740 || Loss: 0.6634 || timer: 0.1126 sec.
iter 285750 || Loss: 0.6803 || timer: 0.0891 sec.
iter 285760 || Loss: 0.8347 || timer: 0.1053 sec.
iter 285770 || Loss: 0.5722 || timer: 0.1113 sec.
iter 285780 || Loss: 0.9286 || timer: 0.0889 sec.
iter 285790 || Loss: 0.8162 || timer: 0.0807 sec.
iter 285800 || Loss: 0.9301 || timer: 0.0840 sec.
iter 285810 || Loss: 0.5862 || timer: 0.0890 sec.
iter 285820 || Loss: 0.6742 || timer: 0.0888 sec.
iter 285830 || Loss: 0.6949 || timer: 0.0887 sec.
iter 285840 || Loss: 1.2490 || timer: 0.1058 sec.
iter 285850 || Loss: 0.6581 || timer: 0.0907 sec.
iter 285860 || Loss: 0.7217 || timer: 0.0907 sec.
iter 285870 || Loss: 0.6675 || timer: 0.0871 sec.
iter 285880 || Loss: 0.8419 || timer: 0.0859 sec.
iter 285890 || Loss: 0.8558 || timer: 0.0993 sec.
iter 285900 || Loss: 0.8633 || timer: 0.0926 sec.
iter 285910 || Loss: 0.6795 || timer: 0.0880 sec.
iter 285920 || Loss: 0.6501 || timer: 0.0855 sec.
iter 285930 || Loss: 0.8591 || timer: 0.0930 sec.
iter 285940 || Loss: 0.8896 || timer: 0.0846 sec.
iter 285950 || Loss: 0.8899 || timer: 0.0824 sec.
iter 285960 || Loss: 0.7730 || timer: 0.0897 sec.
iter 285970 || Loss: 0.7924 || timer: 0.0802 sec.
iter 285980 || Loss: 0.7185 || timer: 0.0822 sec.
iter 285990 || Loss: 0.4944 || timer: 0.0899 sec.
iter 286000 || Loss: 0.6571 || timer: 0.0251 sec.
iter 286010 || Loss: 0.3712 || timer: 0.0892 sec.
iter 286020 || Loss: 0.7960 || timer: 0.1098 sec.
iter 286030 || Loss: 0.9898 || timer: 0.0903 sec.
iter 286040 || Loss: 0.5911 || timer: 0.0898 sec.
iter 286050 || Loss: 0.5579 || timer: 0.0857 sec.
iter 286060 || Loss: 0.7510 || timer: 0.0896 sec.
iter 286070 || Loss: 0.8682 || timer: 0.0914 sec.
iter 286080 || Loss: 0.8284 || timer: 0.0887 sec.
iter 286090 || Loss: 0.5868 || timer: 0.0812 sec.
iter 286100 || Loss: 0.9475 || timer: 0.1278 sec.
iter 286110 || Loss: 0.7499 || timer: 0.0912 sec.
iter 286120 || Loss: 0.6801 || timer: 0.0888 sec.
iter 286130 || Loss: 0.8157 || timer: 0.0938 sec.
iter 286140 || Loss: 0.7315 || timer: 0.0942 sec.
iter 286150 || Loss: 0.7524 || timer: 0.0904 sec.
iter 286160 || Loss: 0.9644 || timer: 0.1115 sec.
iter 286170 || Loss: 1.0210 || timer: 0.1095 sec.
iter 286180 || Loss: 0.5344 || timer: 0.0827 sec.
iter 286190 || Loss: 0.7636 || timer: 0.0892 sec.
iter 286200 || Loss: 0.8459 || timer: 0.0821 sec.
iter 286210 || Loss: 0.8216 || timer: 0.1020 sec.
iter 286220 || Loss: 1.0814 || timer: 0.0809 sec.
iter 286230 || Loss: 0.4581 || timer: 0.0876 sec.
iter 286240 || Loss: 0.6625 || timer: 0.0809 sec.
iter 286250 || Loss: 0.6686 || timer: 0.0885 sec.
iter 286260 || Loss: 0.5211 || timer: 0.0901 sec.
iter 286270 || Loss: 0.6343 || timer: 0.1155 sec.
iter 286280 || Loss: 0.6499 || timer: 0.1044 sec.
iter 286290 || Loss: 0.6260 || timer: 0.0917 sec.
iter 286300 || Loss: 1.0510 || timer: 0.0907 sec.
iter 286310 || Loss: 0.6874 || timer: 0.0944 sec.
iter 286320 || Loss: 0.9788 || timer: 0.0898 sec.
iter 286330 || Loss: 0.6587 || timer: 0.0310 sec.
iter 286340 || Loss: 0.5739 || timer: 0.1033 sec.
iter 286350 || Loss: 0.7494 || timer: 0.0809 sec.
iter 286360 || Loss: 0.7981 || timer: 0.0908 sec.
iter 286370 || Loss: 0.7363 || timer: 0.0901 sec.
iter 286380 || Loss: 0.5784 || timer: 0.1195 sec.
iter 286390 || Loss: 0.8151 || timer: 0.0889 sec.
iter 286400 || Loss: 0.5317 || timer: 0.0887 sec.
iter 286410 || Loss: 0.8290 || timer: 0.0828 sec.
iter 286420 || Loss: 0.7895 || timer: 0.0891 sec.
iter 286430 || Loss: 0.6278 || timer: 0.0950 sec.
iter 286440 || Loss: 1.1675 || timer: 0.1024 sec.
iter 286450 || Loss: 0.6074 || timer: 0.0892 sec.
iter 286460 || Loss: 0.6698 || timer: 0.0898 sec.
iter 286470 || Loss: 0.3750 || timer: 0.0900 sec.
iter 286480 || Loss: 0.8533 || timer: 0.0819 sec.
iter 286490 || Loss: 0.5093 || timer: 0.0895 sec.
iter 286500 || Loss: 0.9428 || timer: 0.1037 sec.
iter 286510 || Loss: 0.7122 || timer: 0.0805 sec.
iter 286520 || Loss: 0.9468 || timer: 0.1022 sec.
iter 286530 || Loss: 0.9276 || timer: 0.0859 sec.
iter 286540 || Loss: 0.7803 || timer: 0.0862 sec.
iter 286550 || Loss: 0.8771 || timer: 0.1234 sec.
iter 286560 || Loss: 0.8767 || timer: 0.0886 sec.
iter 286570 || Loss: 0.7601 || timer: 0.1047 sec.
iter 286580 || Loss: 0.6775 || timer: 0.0901 sec.
iter 286590 || Loss: 0.6773 || timer: 0.0885 sec.
iter 286600 || Loss: 0.7654 || timer: 0.0908 sec.
iter 286610 || Loss: 0.5886 || timer: 0.0970 sec.
iter 286620 || Loss: 0.9413 || timer: 0.0897 sec.
iter 286630 || Loss: 0.7361 || timer: 0.0921 sec.
iter 286640 || Loss: 0.7056 || timer: 0.0910 sec.
iter 286650 || Loss: 0.7097 || timer: 0.0801 sec.
iter 286660 || Loss: 0.9409 || timer: 0.0298 sec.
iter 286670 || Loss: 0.4620 || timer: 0.0830 sec.
iter 286680 || Loss: 0.7122 || timer: 0.0773 sec.
iter 286690 || Loss: 0.7751 || timer: 0.0911 sec.
iter 286700 || Loss: 0.7043 || timer: 0.0841 sec.
iter 286710 || Loss: 0.9077 || timer: 0.0928 sec.
iter 286720 || Loss: 0.7242 || timer: 0.0843 sec.
iter 286730 || Loss: 0.5636 || timer: 0.1140 sec.
iter 286740 || Loss: 0.8139 || timer: 0.0892 sec.
iter 286750 || Loss: 0.5864 || timer: 0.0898 sec.
iter 286760 || Loss: 0.6053 || timer: 0.0949 sec.
iter 286770 || Loss: 0.5940 || timer: 0.0872 sec.
iter 286780 || Loss: 0.7514 || timer: 0.0919 sec.
iter 286790 || Loss: 0.6767 || timer: 0.0966 sec.
iter 286800 || Loss: 0.9423 || timer: 0.0894 sec.
iter 286810 || Loss: 0.5359 || timer: 0.0833 sec.
iter 286820 || Loss: 0.7981 || timer: 0.0839 sec.
iter 286830 || Loss: 0.7764 || timer: 0.0984 sec.
iter 286840 || Loss: 0.5651 || timer: 0.0839 sec.
iter 286850 || Loss: 0.8786 || timer: 0.0973 sec.
iter 286860 || Loss: 0.7178 || timer: 0.0913 sec.
iter 286870 || Loss: 0.7984 || timer: 0.1261 sec.
iter 286880 || Loss: 0.8989 || timer: 0.0909 sec.
iter 286890 || Loss: 0.8193 || timer: 0.0818 sec.
iter 286900 || Loss: 0.7658 || timer: 0.0910 sec.
iter 286910 || Loss: 0.5956 || timer: 0.1004 sec.
iter 286920 || Loss: 0.8031 || timer: 0.1001 sec.
iter 286930 || Loss: 0.7777 || timer: 0.0846 sec.
iter 286940 || Loss: 0.7122 || timer: 0.0909 sec.
iter 286950 || Loss: 0.9306 || timer: 0.1104 sec.
iter 286960 || Loss: 0.7420 || timer: 0.0860 sec.
iter 286970 || Loss: 0.4877 || timer: 0.0903 sec.
iter 286980 || Loss: 0.5852 || timer: 0.0848 sec.
iter 286990 || Loss: 0.6658 || timer: 0.0215 sec.
iter 287000 || Loss: 0.4206 || timer: 0.0902 sec.
iter 287010 || Loss: 0.6615 || timer: 0.0906 sec.
iter 287020 || Loss: 0.7701 || timer: 0.0900 sec.
iter 287030 || Loss: 0.6755 || timer: 0.0980 sec.
iter 287040 || Loss: 0.6950 || timer: 0.0840 sec.
iter 287050 || Loss: 0.8115 || timer: 0.1003 sec.
iter 287060 || Loss: 0.4693 || timer: 0.1007 sec.
iter 287070 || Loss: 0.8689 || timer: 0.0886 sec.
iter 287080 || Loss: 0.6576 || timer: 0.0960 sec.
iter 287090 || Loss: 0.7159 || timer: 0.1085 sec.
iter 287100 || Loss: 0.7601 || timer: 0.0824 sec.
iter 287110 || Loss: 0.7051 || timer: 0.0825 sec.
iter 287120 || Loss: 0.6874 || timer: 0.0893 sec.
iter 287130 || Loss: 0.5756 || timer: 0.0885 sec.
iter 287140 || Loss: 0.7092 || timer: 0.0859 sec.
iter 287150 || Loss: 0.7341 || timer: 0.0863 sec.
iter 287160 || Loss: 0.6015 || timer: 0.0946 sec.
iter 287170 || Loss: 0.7803 || timer: 0.0899 sec.
iter 287180 || Loss: 0.7592 || timer: 0.0928 sec.
iter 287190 || Loss: 1.2999 || timer: 0.0863 sec.
iter 287200 || Loss: 0.7411 || timer: 0.1339 sec.
iter 287210 || Loss: 0.7595 || timer: 0.0821 sec.
iter 287220 || Loss: 0.8356 || timer: 0.0862 sec.
iter 287230 || Loss: 0.6638 || timer: 0.0814 sec.
iter 287240 || Loss: 0.8804 || timer: 0.0921 sec.
iter 287250 || Loss: 0.8358 || timer: 0.1040 sec.
iter 287260 || Loss: 0.7433 || timer: 0.0857 sec.
iter 287270 || Loss: 0.8318 || timer: 0.0813 sec.
iter 287280 || Loss: 0.7885 || timer: 0.0875 sec.
iter 287290 || Loss: 0.6255 || timer: 0.0880 sec.
iter 287300 || Loss: 0.5217 || timer: 0.0877 sec.
iter 287310 || Loss: 0.4724 || timer: 0.0926 sec.
iter 287320 || Loss: 1.0540 || timer: 0.0209 sec.
iter 287330 || Loss: 0.6477 || timer: 0.0801 sec.
iter 287340 || Loss: 0.6972 || timer: 0.0985 sec.
iter 287350 || Loss: 0.6365 || timer: 0.0897 sec.
iter 287360 || Loss: 0.8249 || timer: 0.0885 sec.
iter 287370 || Loss: 0.8220 || timer: 0.0894 sec.
iter 287380 || Loss: 0.7497 || timer: 0.0959 sec.
iter 287390 || Loss: 0.5762 || timer: 0.0875 sec.
iter 287400 || Loss: 0.5690 || timer: 0.0797 sec.
iter 287410 || Loss: 0.6270 || timer: 0.0865 sec.
iter 287420 || Loss: 0.6991 || timer: 0.1078 sec.
iter 287430 || Loss: 0.6987 || timer: 0.0899 sec.
iter 287440 || Loss: 1.0188 || timer: 0.0891 sec.
iter 287450 || Loss: 0.6610 || timer: 0.0880 sec.
iter 287460 || Loss: 0.6762 || timer: 0.1062 sec.
iter 287470 || Loss: 0.5312 || timer: 0.0815 sec.
iter 287480 || Loss: 0.7104 || timer: 0.0815 sec.
iter 287490 || Loss: 0.5034 || timer: 0.0891 sec.
iter 287500 || Loss: 0.7768 || timer: 0.1128 sec.
iter 287510 || Loss: 0.7572 || timer: 0.0852 sec.
iter 287520 || Loss: 0.5498 || timer: 0.0815 sec.
iter 287530 || Loss: 0.6108 || timer: 0.0885 sec.
iter 287540 || Loss: 0.7265 || timer: 0.0990 sec.
iter 287550 || Loss: 0.7318 || timer: 0.0951 sec.
iter 287560 || Loss: 0.7574 || timer: 0.1073 sec.
iter 287570 || Loss: 0.7720 || timer: 0.0883 sec.
iter 287580 || Loss: 0.6801 || timer: 0.0916 sec.
iter 287590 || Loss: 1.0870 || timer: 0.0881 sec.
iter 287600 || Loss: 0.8192 || timer: 0.0810 sec.
iter 287610 || Loss: 0.5853 || timer: 0.1041 sec.
iter 287620 || Loss: 0.8295 || timer: 0.0968 sec.
iter 287630 || Loss: 0.9147 || timer: 0.0885 sec.
iter 287640 || Loss: 0.8199 || timer: 0.0825 sec.
iter 287650 || Loss: 0.6043 || timer: 0.0276 sec.
iter 287660 || Loss: 0.5646 || timer: 0.1171 sec.
iter 287670 || Loss: 0.3830 || timer: 0.0929 sec.
iter 287680 || Loss: 0.8121 || timer: 0.0832 sec.
iter 287690 || Loss: 0.6949 || timer: 0.0934 sec.
iter 287700 || Loss: 0.5547 || timer: 0.1069 sec.
iter 287710 || Loss: 0.6366 || timer: 0.0823 sec.
iter 287720 || Loss: 0.5509 || timer: 0.0897 sec.
iter 287730 || Loss: 0.6731 || timer: 0.0961 sec.
iter 287740 || Loss: 1.2525 || timer: 0.0990 sec.
iter 287750 || Loss: 0.8463 || timer: 0.0974 sec.
iter 287760 || Loss: 0.5256 || timer: 0.0907 sec.
iter 287770 || Loss: 0.5692 || timer: 0.0892 sec.
iter 287780 || Loss: 0.5409 || timer: 0.0899 sec.
iter 287790 || Loss: 0.8412 || timer: 0.0899 sec.
iter 287800 || Loss: 0.5757 || timer: 0.0819 sec.
iter 287810 || Loss: 0.6107 || timer: 0.0881 sec.
iter 287820 || Loss: 0.5630 || timer: 0.1114 sec.
iter 287830 || Loss: 0.7781 || timer: 0.0817 sec.
iter 287840 || Loss: 0.6167 || timer: 0.0804 sec.
iter 287850 || Loss: 0.4392 || timer: 0.0939 sec.
iter 287860 || Loss: 0.7284 || timer: 0.0918 sec.
iter 287870 || Loss: 0.7442 || timer: 0.1198 sec.
iter 287880 || Loss: 0.5304 || timer: 0.0928 sec.
iter 287890 || Loss: 0.6447 || timer: 0.0878 sec.
iter 287900 || Loss: 0.6769 || timer: 0.0893 sec.
iter 287910 || Loss: 0.6651 || timer: 0.0885 sec.
iter 287920 || Loss: 0.7755 || timer: 0.0817 sec.
iter 287930 || Loss: 1.0082 || timer: 0.0961 sec.
iter 287940 || Loss: 0.5877 || timer: 0.0744 sec.
iter 287950 || Loss: 0.7390 || timer: 0.0892 sec.
iter 287960 || Loss: 0.9418 || timer: 0.0844 sec.
iter 287970 || Loss: 0.6101 || timer: 0.0884 sec.
iter 287980 || Loss: 0.6937 || timer: 0.0181 sec.
iter 287990 || Loss: 0.5204 || timer: 0.0903 sec.
iter 288000 || Loss: 0.7737 || timer: 0.0834 sec.
iter 288010 || Loss: 0.8624 || timer: 0.0820 sec.
iter 288020 || Loss: 0.6624 || timer: 0.0879 sec.
iter 288030 || Loss: 0.7428 || timer: 0.0820 sec.
iter 288040 || Loss: 0.7792 || timer: 0.0913 sec.
iter 288050 || Loss: 0.8107 || timer: 0.0832 sec.
iter 288060 || Loss: 0.6506 || timer: 0.0905 sec.
iter 288070 || Loss: 0.5548 || timer: 0.0890 sec.
iter 288080 || Loss: 0.7615 || timer: 0.1106 sec.
iter 288090 || Loss: 0.7045 || timer: 0.0819 sec.
iter 288100 || Loss: 0.7006 || timer: 0.0887 sec.
iter 288110 || Loss: 0.7622 || timer: 0.0895 sec.
iter 288120 || Loss: 0.5770 || timer: 0.0884 sec.
iter 288130 || Loss: 0.7561 || timer: 0.0822 sec.
iter 288140 || Loss: 0.6357 || timer: 0.0922 sec.
iter 288150 || Loss: 0.6465 || timer: 0.0820 sec.
iter 288160 || Loss: 0.6282 || timer: 0.0887 sec.
iter 288170 || Loss: 0.5441 || timer: 0.0794 sec.
iter 288180 || Loss: 0.6253 || timer: 0.0810 sec.
iter 288190 || Loss: 0.6784 || timer: 0.0810 sec.
iter 288200 || Loss: 0.5143 || timer: 0.0999 sec.
iter 288210 || Loss: 0.5929 || timer: 0.1066 sec.
iter 288220 || Loss: 1.1200 || timer: 0.0976 sec.
iter 288230 || Loss: 0.8517 || timer: 0.0842 sec.
iter 288240 || Loss: 0.6761 || timer: 0.0885 sec.
iter 288250 || Loss: 0.7623 || timer: 0.0879 sec.
iter 288260 || Loss: 0.6823 || timer: 0.0907 sec.
iter 288270 || Loss: 0.9102 || timer: 0.0862 sec.
iter 288280 || Loss: 0.6427 || timer: 0.0833 sec.
iter 288290 || Loss: 1.0852 || timer: 0.0840 sec.
iter 288300 || Loss: 0.7615 || timer: 0.0888 sec.
iter 288310 || Loss: 0.6278 || timer: 0.0237 sec.
iter 288320 || Loss: 0.8912 || timer: 0.0924 sec.
iter 288330 || Loss: 0.7586 || timer: 0.0986 sec.
iter 288340 || Loss: 0.8568 || timer: 0.0825 sec.
iter 288350 || Loss: 0.6426 || timer: 0.1146 sec.
iter 288360 || Loss: 0.6950 || timer: 0.1053 sec.
iter 288370 || Loss: 0.6685 || timer: 0.0849 sec.
iter 288380 || Loss: 0.6193 || timer: 0.0818 sec.
iter 288390 || Loss: 0.7668 || timer: 0.1118 sec.
iter 288400 || Loss: 0.7794 || timer: 0.0871 sec.
iter 288410 || Loss: 0.8183 || timer: 0.1256 sec.
iter 288420 || Loss: 0.7890 || timer: 0.0960 sec.
iter 288430 || Loss: 0.6740 || timer: 0.0885 sec.
iter 288440 || Loss: 0.9196 || timer: 0.0896 sec.
iter 288450 || Loss: 0.7791 || timer: 0.0923 sec.
iter 288460 || Loss: 0.5064 || timer: 0.0911 sec.
iter 288470 || Loss: 0.9258 || timer: 0.0806 sec.
iter 288480 || Loss: 0.7043 || timer: 0.0903 sec.
iter 288490 || Loss: 0.8839 || timer: 0.0947 sec.
iter 288500 || Loss: 0.8380 || timer: 0.0902 sec.
iter 288510 || Loss: 0.6265 || timer: 0.1120 sec.
iter 288520 || Loss: 0.6653 || timer: 0.0804 sec.
iter 288530 || Loss: 0.9933 || timer: 0.0917 sec.
iter 288540 || Loss: 0.7690 || timer: 0.0811 sec.
iter 288550 || Loss: 0.7060 || timer: 0.1042 sec.
iter 288560 || Loss: 0.8624 || timer: 0.1202 sec.
iter 288570 || Loss: 1.0499 || timer: 0.1205 sec.
iter 288580 || Loss: 0.6267 || timer: 0.0950 sec.
iter 288590 || Loss: 0.6846 || timer: 0.0816 sec.
iter 288600 || Loss: 0.9698 || timer: 0.0806 sec.
iter 288610 || Loss: 0.7929 || timer: 0.0863 sec.
iter 288620 || Loss: 0.4606 || timer: 0.0908 sec.
iter 288630 || Loss: 0.6918 || timer: 0.0866 sec.
iter 288640 || Loss: 0.8126 || timer: 0.0239 sec.
iter 288650 || Loss: 0.2011 || timer: 0.0873 sec.
iter 288660 || Loss: 0.6362 || timer: 0.1055 sec.
iter 288670 || Loss: 0.7372 || timer: 0.0856 sec.
iter 288680 || Loss: 0.6178 || timer: 0.0922 sec.
iter 288690 || Loss: 0.7511 || timer: 0.1237 sec.
iter 288700 || Loss: 0.7773 || timer: 0.0899 sec.
iter 288710 || Loss: 0.8459 || timer: 0.0866 sec.
iter 288720 || Loss: 0.6916 || timer: 0.0897 sec.
iter 288730 || Loss: 0.6703 || timer: 0.0827 sec.
iter 288740 || Loss: 0.9756 || timer: 0.1116 sec.
iter 288750 || Loss: 1.0394 || timer: 0.0901 sec.
iter 288760 || Loss: 0.7439 || timer: 0.0897 sec.
iter 288770 || Loss: 0.9542 || timer: 0.0811 sec.
iter 288780 || Loss: 0.8591 || timer: 0.1068 sec.
iter 288790 || Loss: 0.6525 || timer: 0.0890 sec.
iter 288800 || Loss: 0.7723 || timer: 0.0885 sec.
iter 288810 || Loss: 0.8252 || timer: 0.1370 sec.
iter 288820 || Loss: 0.7474 || timer: 0.0822 sec.
iter 288830 || Loss: 0.5780 || timer: 0.0895 sec.
iter 288840 || Loss: 0.8301 || timer: 0.0821 sec.
iter 288850 || Loss: 0.6432 || timer: 0.0954 sec.
iter 288860 || Loss: 0.9123 || timer: 0.0923 sec.
iter 288870 || Loss: 0.6769 || timer: 0.0912 sec.
iter 288880 || Loss: 0.5622 || timer: 0.0803 sec.
iter 288890 || Loss: 0.7516 || timer: 0.0831 sec.
iter 288900 || Loss: 0.4085 || timer: 0.0896 sec.
iter 288910 || Loss: 0.8865 || timer: 0.0885 sec.
iter 288920 || Loss: 1.1766 || timer: 0.0909 sec.
iter 288930 || Loss: 0.4813 || timer: 0.1042 sec.
iter 288940 || Loss: 0.9346 || timer: 0.0878 sec.
iter 288950 || Loss: 0.7464 || timer: 0.1006 sec.
iter 288960 || Loss: 1.0323 || timer: 0.0940 sec.
iter 288970 || Loss: 0.7200 || timer: 0.0163 sec.
iter 288980 || Loss: 1.1858 || timer: 0.0911 sec.
iter 288990 || Loss: 0.6634 || timer: 0.0805 sec.
iter 289000 || Loss: 0.7271 || timer: 0.1003 sec.
iter 289010 || Loss: 0.5654 || timer: 0.0930 sec.
iter 289020 || Loss: 0.7346 || timer: 0.0934 sec.
iter 289030 || Loss: 0.5965 || timer: 0.0824 sec.
iter 289040 || Loss: 0.6744 || timer: 0.0814 sec.
iter 289050 || Loss: 0.8503 || timer: 0.0802 sec.
iter 289060 || Loss: 0.6865 || timer: 0.0890 sec.
iter 289070 || Loss: 0.7016 || timer: 0.0992 sec.
iter 289080 || Loss: 0.8729 || timer: 0.0904 sec.
iter 289090 || Loss: 0.9442 || timer: 0.0864 sec.
iter 289100 || Loss: 0.6099 || timer: 0.0912 sec.
iter 289110 || Loss: 0.5870 || timer: 0.0944 sec.
iter 289120 || Loss: 0.8496 || timer: 0.0886 sec.
iter 289130 || Loss: 0.8546 || timer: 0.0893 sec.
iter 289140 || Loss: 0.7630 || timer: 0.0938 sec.
iter 289150 || Loss: 0.7585 || timer: 0.0918 sec.
iter 289160 || Loss: 0.4778 || timer: 0.0866 sec.
iter 289170 || Loss: 0.5599 || timer: 0.1446 sec.
iter 289180 || Loss: 0.6450 || timer: 0.0926 sec.
iter 289190 || Loss: 0.9505 || timer: 0.1007 sec.
iter 289200 || Loss: 0.7631 || timer: 0.0898 sec.
iter 289210 || Loss: 0.6690 || timer: 0.0815 sec.
iter 289220 || Loss: 0.6156 || timer: 0.1031 sec.
iter 289230 || Loss: 0.6969 || timer: 0.0893 sec.
iter 289240 || Loss: 0.9702 || timer: 0.0987 sec.
iter 289250 || Loss: 0.6569 || timer: 0.0900 sec.
iter 289260 || Loss: 0.9171 || timer: 0.0884 sec.
iter 289270 || Loss: 0.6364 || timer: 0.0890 sec.
iter 289280 || Loss: 0.7762 || timer: 0.0900 sec.
iter 289290 || Loss: 0.6466 || timer: 0.0819 sec.
iter 289300 || Loss: 0.7579 || timer: 0.0237 sec.
iter 289310 || Loss: 0.9206 || timer: 0.0892 sec.
iter 289320 || Loss: 0.7673 || timer: 0.0903 sec.
iter 289330 || Loss: 0.8151 || timer: 0.0835 sec.
iter 289340 || Loss: 0.7231 || timer: 0.0879 sec.
iter 289350 || Loss: 0.7575 || timer: 0.0916 sec.
iter 289360 || Loss: 0.7725 || timer: 0.0863 sec.
iter 289370 || Loss: 0.6987 || timer: 0.0889 sec.
iter 289380 || Loss: 0.6499 || timer: 0.0904 sec.
iter 289390 || Loss: 0.5726 || timer: 0.0825 sec.
iter 289400 || Loss: 0.7624 || timer: 0.0989 sec.
iter 289410 || Loss: 0.7874 || timer: 0.0807 sec.
iter 289420 || Loss: 0.5577 || timer: 0.0809 sec.
iter 289430 || Loss: 0.6249 || timer: 0.0816 sec.
iter 289440 || Loss: 0.5689 || timer: 0.0862 sec.
iter 289450 || Loss: 0.7189 || timer: 0.0904 sec.
iter 289460 || Loss: 0.6788 || timer: 0.0890 sec.
iter 289470 || Loss: 0.6329 || timer: 0.1024 sec.
iter 289480 || Loss: 0.6544 || timer: 0.0897 sec.
iter 289490 || Loss: 0.5785 || timer: 0.0831 sec.
iter 289500 || Loss: 0.6756 || timer: 0.0897 sec.
iter 289510 || Loss: 0.7794 || timer: 0.0822 sec.
iter 289520 || Loss: 0.4818 || timer: 0.0943 sec.
iter 289530 || Loss: 0.6506 || timer: 0.0814 sec.
iter 289540 || Loss: 0.6076 || timer: 0.0903 sec.
iter 289550 || Loss: 0.6628 || timer: 0.0849 sec.
iter 289560 || Loss: 0.4873 || timer: 0.0858 sec.
iter 289570 || Loss: 0.7831 || timer: 0.0887 sec.
iter 289580 || Loss: 0.6003 || timer: 0.1302 sec.
iter 289590 || Loss: 0.6250 || timer: 0.1027 sec.
iter 289600 || Loss: 0.7515 || timer: 0.0895 sec.
iter 289610 || Loss: 1.1149 || timer: 0.0888 sec.
iter 289620 || Loss: 0.7871 || timer: 0.0870 sec.
iter 289630 || Loss: 0.5403 || timer: 0.0197 sec.
iter 289640 || Loss: 0.1771 || timer: 0.0851 sec.
iter 289650 || Loss: 0.5579 || timer: 0.0863 sec.
iter 289660 || Loss: 0.6776 || timer: 0.0873 sec.
iter 289670 || Loss: 0.7808 || timer: 0.0834 sec.
iter 289680 || Loss: 0.7603 || timer: 0.0823 sec.
iter 289690 || Loss: 0.7805 || timer: 0.0807 sec.
iter 289700 || Loss: 0.5631 || timer: 0.0892 sec.
iter 289710 || Loss: 0.8670 || timer: 0.0895 sec.
iter 289720 || Loss: 0.8312 || timer: 0.0958 sec.
iter 289730 || Loss: 0.8508 || timer: 0.1186 sec.
iter 289740 || Loss: 0.5152 || timer: 0.0910 sec.
iter 289750 || Loss: 0.6456 || timer: 0.0902 sec.
iter 289760 || Loss: 0.5618 || timer: 0.0887 sec.
iter 289770 || Loss: 0.6290 || timer: 0.0810 sec.
iter 289780 || Loss: 0.5892 || timer: 0.0903 sec.
iter 289790 || Loss: 0.6825 || timer: 0.1048 sec.
iter 289800 || Loss: 0.5434 || timer: 0.1051 sec.
iter 289810 || Loss: 0.9934 || timer: 0.1007 sec.
iter 289820 || Loss: 0.9801 || timer: 0.0821 sec.
iter 289830 || Loss: 0.4910 || timer: 0.0812 sec.
iter 289840 || Loss: 0.7118 || timer: 0.1087 sec.
iter 289850 || Loss: 0.8468 || timer: 0.0907 sec.
iter 289860 || Loss: 0.6707 || timer: 0.0891 sec.
iter 289870 || Loss: 0.5834 || timer: 0.0821 sec.
iter 289880 || Loss: 0.6140 || timer: 0.0826 sec.
iter 289890 || Loss: 0.8934 || timer: 0.0819 sec.
iter 289900 || Loss: 0.6467 || timer: 0.0817 sec.
iter 289910 || Loss: 0.6639 || timer: 0.0912 sec.
iter 289920 || Loss: 0.5334 || timer: 0.0911 sec.
iter 289930 || Loss: 0.9812 || timer: 0.0818 sec.
iter 289940 || Loss: 0.6196 || timer: 0.0864 sec.
iter 289950 || Loss: 0.5527 || timer: 0.0889 sec.
iter 289960 || Loss: 0.8882 || timer: 0.0228 sec.
iter 289970 || Loss: 1.5545 || timer: 0.0888 sec.
iter 289980 || Loss: 0.5119 || timer: 0.1184 sec.
iter 289990 || Loss: 0.9412 || timer: 0.0818 sec.
iter 290000 || Loss: 0.5769 || Saving state, iter: 290000
timer: 0.1027 sec.
iter 290010 || Loss: 0.6986 || timer: 0.0888 sec.
iter 290020 || Loss: 1.0703 || timer: 0.0812 sec.
iter 290030 || Loss: 0.6387 || timer: 0.0972 sec.
iter 290040 || Loss: 0.6798 || timer: 0.0822 sec.
iter 290050 || Loss: 0.6422 || timer: 0.1130 sec.
iter 290060 || Loss: 0.7250 || timer: 0.1413 sec.
iter 290070 || Loss: 0.6562 || timer: 0.0943 sec.
iter 290080 || Loss: 0.6653 || timer: 0.0845 sec.
iter 290090 || Loss: 0.6402 || timer: 0.0903 sec.
iter 290100 || Loss: 0.6413 || timer: 0.0906 sec.
iter 290110 || Loss: 0.5939 || timer: 0.1029 sec.
iter 290120 || Loss: 0.8993 || timer: 0.0909 sec.
iter 290130 || Loss: 0.6834 || timer: 0.0894 sec.
iter 290140 || Loss: 0.6890 || timer: 0.0812 sec.
iter 290150 || Loss: 0.8073 || timer: 0.0993 sec.
iter 290160 || Loss: 0.4827 || timer: 0.1015 sec.
iter 290170 || Loss: 0.7172 || timer: 0.0808 sec.
iter 290180 || Loss: 0.5426 || timer: 0.0863 sec.
iter 290190 || Loss: 0.8430 || timer: 0.0897 sec.
iter 290200 || Loss: 0.6955 || timer: 0.0812 sec.
iter 290210 || Loss: 0.9053 || timer: 0.0854 sec.
iter 290220 || Loss: 0.5755 || timer: 0.0909 sec.
iter 290230 || Loss: 0.7375 || timer: 0.0945 sec.
iter 290240 || Loss: 0.5232 || timer: 0.0905 sec.
iter 290250 || Loss: 0.6352 || timer: 0.0900 sec.
iter 290260 || Loss: 0.7782 || timer: 0.0880 sec.
iter 290270 || Loss: 0.8128 || timer: 0.1001 sec.
iter 290280 || Loss: 0.8712 || timer: 0.0825 sec.
iter 290290 || Loss: 0.8078 || timer: 0.0146 sec.
iter 290300 || Loss: 0.4737 || timer: 0.0860 sec.
iter 290310 || Loss: 0.5827 || timer: 0.1073 sec.
iter 290320 || Loss: 0.8634 || timer: 0.0831 sec.
iter 290330 || Loss: 0.7277 || timer: 0.0805 sec.
iter 290340 || Loss: 0.5417 || timer: 0.0907 sec.
iter 290350 || Loss: 0.6548 || timer: 0.0920 sec.
iter 290360 || Loss: 1.1684 || timer: 0.0940 sec.
iter 290370 || Loss: 0.8108 || timer: 0.0879 sec.
iter 290380 || Loss: 0.8153 || timer: 0.0915 sec.
iter 290390 || Loss: 0.6329 || timer: 0.1491 sec.
iter 290400 || Loss: 0.6248 || timer: 0.0822 sec.
iter 290410 || Loss: 0.7851 || timer: 0.0873 sec.
iter 290420 || Loss: 0.7851 || timer: 0.0810 sec.
iter 290430 || Loss: 0.9008 || timer: 0.0739 sec.
iter 290440 || Loss: 0.7981 || timer: 0.0811 sec.
iter 290450 || Loss: 0.8846 || timer: 0.0818 sec.
iter 290460 || Loss: 0.5405 || timer: 0.0865 sec.
iter 290470 || Loss: 0.9336 || timer: 0.1055 sec.
iter 290480 || Loss: 0.7166 || timer: 0.0956 sec.
iter 290490 || Loss: 0.9208 || timer: 0.0910 sec.
iter 290500 || Loss: 0.6561 || timer: 0.1172 sec.
iter 290510 || Loss: 0.5326 || timer: 0.0868 sec.
iter 290520 || Loss: 0.7742 || timer: 0.0818 sec.
iter 290530 || Loss: 0.6632 || timer: 0.0914 sec.
iter 290540 || Loss: 0.6623 || timer: 0.0907 sec.
iter 290550 || Loss: 0.6859 || timer: 0.1033 sec.
iter 290560 || Loss: 0.7205 || timer: 0.0804 sec.
iter 290570 || Loss: 0.6527 || timer: 0.0849 sec.
iter 290580 || Loss: 0.9269 || timer: 0.1044 sec.
iter 290590 || Loss: 0.7397 || timer: 0.0882 sec.
iter 290600 || Loss: 0.7021 || timer: 0.1056 sec.
iter 290610 || Loss: 0.3695 || timer: 0.0884 sec.
iter 290620 || Loss: 0.7891 || timer: 0.0281 sec.
iter 290630 || Loss: 0.3694 || timer: 0.0869 sec.
iter 290640 || Loss: 0.7712 || timer: 0.0902 sec.
iter 290650 || Loss: 0.8617 || timer: 0.0890 sec.
iter 290660 || Loss: 0.5387 || timer: 0.0997 sec.
iter 290670 || Loss: 0.6275 || timer: 0.0919 sec.
iter 290680 || Loss: 0.7803 || timer: 0.0926 sec.
iter 290690 || Loss: 0.9481 || timer: 0.0808 sec.
iter 290700 || Loss: 0.7865 || timer: 0.0873 sec.
iter 290710 || Loss: 0.8210 || timer: 0.0890 sec.
iter 290720 || Loss: 0.6065 || timer: 0.0990 sec.
iter 290730 || Loss: 0.8766 || timer: 0.0802 sec.
iter 290740 || Loss: 0.7578 || timer: 0.0877 sec.
iter 290750 || Loss: 0.6005 || timer: 0.0885 sec.
iter 290760 || Loss: 0.8338 || timer: 0.1094 sec.
iter 290770 || Loss: 0.7264 || timer: 0.0882 sec.
iter 290780 || Loss: 0.5761 || timer: 0.0901 sec.
iter 290790 || Loss: 0.6909 || timer: 0.0991 sec.
iter 290800 || Loss: 0.6537 || timer: 0.0793 sec.
iter 290810 || Loss: 0.6945 || timer: 0.0907 sec.
iter 290820 || Loss: 0.8243 || timer: 0.0900 sec.
iter 290830 || Loss: 0.4909 || timer: 0.0826 sec.
iter 290840 || Loss: 0.5550 || timer: 0.0877 sec.
iter 290850 || Loss: 0.9200 || timer: 0.0811 sec.
iter 290860 || Loss: 0.5959 || timer: 0.0901 sec.
iter 290870 || Loss: 0.9000 || timer: 0.0830 sec.
iter 290880 || Loss: 0.7801 || timer: 0.1132 sec.
iter 290890 || Loss: 0.5010 || timer: 0.0811 sec.
iter 290900 || Loss: 0.6533 || timer: 0.1004 sec.
iter 290910 || Loss: 0.6326 || timer: 0.1073 sec.
iter 290920 || Loss: 0.6675 || timer: 0.0915 sec.
iter 290930 || Loss: 0.5419 || timer: 0.0811 sec.
iter 290940 || Loss: 0.5622 || timer: 0.0825 sec.
iter 290950 || Loss: 0.6589 || timer: 0.0189 sec.
iter 290960 || Loss: 0.3404 || timer: 0.0890 sec.
iter 290970 || Loss: 0.7458 || timer: 0.0946 sec.
iter 290980 || Loss: 1.0298 || timer: 0.0814 sec.
iter 290990 || Loss: 0.7661 || timer: 0.0920 sec.
iter 291000 || Loss: 0.6999 || timer: 0.0991 sec.
iter 291010 || Loss: 0.9662 || timer: 0.1117 sec.
iter 291020 || Loss: 0.7353 || timer: 0.0896 sec.
iter 291030 || Loss: 0.7101 || timer: 0.0816 sec.
iter 291040 || Loss: 0.6288 || timer: 0.0884 sec.
iter 291050 || Loss: 0.6334 || timer: 0.1161 sec.
iter 291060 || Loss: 0.8455 || timer: 0.0916 sec.
iter 291070 || Loss: 0.7773 || timer: 0.0800 sec.
iter 291080 || Loss: 0.8557 || timer: 0.0892 sec.
iter 291090 || Loss: 0.6939 || timer: 0.0807 sec.
iter 291100 || Loss: 0.5680 || timer: 0.0809 sec.
iter 291110 || Loss: 0.8527 || timer: 0.0903 sec.
iter 291120 || Loss: 0.6103 || timer: 0.0909 sec.
iter 291130 || Loss: 0.7887 || timer: 0.0900 sec.
iter 291140 || Loss: 0.7686 || timer: 0.0896 sec.
iter 291150 || Loss: 0.5566 || timer: 0.0831 sec.
iter 291160 || Loss: 0.5506 || timer: 0.0821 sec.
iter 291170 || Loss: 0.4474 || timer: 0.1083 sec.
iter 291180 || Loss: 0.8055 || timer: 0.0891 sec.
iter 291190 || Loss: 0.7251 || timer: 0.1018 sec.
iter 291200 || Loss: 0.9948 || timer: 0.1030 sec.
iter 291210 || Loss: 0.5441 || timer: 0.0906 sec.
iter 291220 || Loss: 0.8570 || timer: 0.1043 sec.
iter 291230 || Loss: 0.7144 || timer: 0.0882 sec.
iter 291240 || Loss: 0.6921 || timer: 0.1021 sec.
iter 291250 || Loss: 0.7683 || timer: 0.0819 sec.
iter 291260 || Loss: 0.4866 || timer: 0.0800 sec.
iter 291270 || Loss: 1.0221 || timer: 0.0902 sec.
iter 291280 || Loss: 0.6165 || timer: 0.0164 sec.
iter 291290 || Loss: 0.3564 || timer: 0.1144 sec.
iter 291300 || Loss: 0.5343 || timer: 0.0809 sec.
iter 291310 || Loss: 0.9487 || timer: 0.0827 sec.
iter 291320 || Loss: 0.8567 || timer: 0.1151 sec.
iter 291330 || Loss: 0.7703 || timer: 0.0891 sec.
iter 291340 || Loss: 0.8078 || timer: 0.0900 sec.
iter 291350 || Loss: 0.7449 || timer: 0.0970 sec.
iter 291360 || Loss: 0.8139 || timer: 0.0902 sec.
iter 291370 || Loss: 0.8424 || timer: 0.0999 sec.
iter 291380 || Loss: 0.8774 || timer: 0.1056 sec.
iter 291390 || Loss: 0.9916 || timer: 0.0825 sec.
iter 291400 || Loss: 0.6271 || timer: 0.0910 sec.
iter 291410 || Loss: 0.8246 || timer: 0.0860 sec.
iter 291420 || Loss: 0.4477 || timer: 0.0889 sec.
iter 291430 || Loss: 0.6535 || timer: 0.0915 sec.
iter 291440 || Loss: 0.7056 || timer: 0.0872 sec.
iter 291450 || Loss: 0.8496 || timer: 0.0890 sec.
iter 291460 || Loss: 0.6772 || timer: 0.0829 sec.
iter 291470 || Loss: 0.5757 || timer: 0.1117 sec.
iter 291480 || Loss: 0.9556 || timer: 0.0859 sec.
iter 291490 || Loss: 0.7352 || timer: 0.0812 sec.
iter 291500 || Loss: 0.7760 || timer: 0.1083 sec.
iter 291510 || Loss: 0.7444 || timer: 0.1070 sec.
iter 291520 || Loss: 0.7634 || timer: 0.0859 sec.
iter 291530 || Loss: 0.5653 || timer: 0.0914 sec.
iter 291540 || Loss: 0.6848 || timer: 0.0909 sec.
iter 291550 || Loss: 0.7376 || timer: 0.0816 sec.
iter 291560 || Loss: 0.8185 || timer: 0.1090 sec.
iter 291570 || Loss: 0.9814 || timer: 0.1059 sec.
iter 291580 || Loss: 0.9002 || timer: 0.0826 sec.
iter 291590 || Loss: 0.5751 || timer: 0.0995 sec.
iter 291600 || Loss: 0.7678 || timer: 0.0889 sec.
iter 291610 || Loss: 0.4963 || timer: 0.0227 sec.
iter 291620 || Loss: 1.2188 || timer: 0.0732 sec.
iter 291630 || Loss: 0.6706 || timer: 0.0815 sec.
iter 291640 || Loss: 0.8968 || timer: 0.1042 sec.
iter 291650 || Loss: 0.9242 || timer: 0.0967 sec.
iter 291660 || Loss: 0.8088 || timer: 0.0823 sec.
iter 291670 || Loss: 0.7053 || timer: 0.0900 sec.
iter 291680 || Loss: 1.0222 || timer: 0.0927 sec.
iter 291690 || Loss: 0.5405 || timer: 0.0928 sec.
iter 291700 || Loss: 1.0399 || timer: 0.0931 sec.
iter 291710 || Loss: 0.7135 || timer: 0.1093 sec.
iter 291720 || Loss: 0.7899 || timer: 0.0906 sec.
iter 291730 || Loss: 0.9147 || timer: 0.0920 sec.
iter 291740 || Loss: 0.8502 || timer: 0.0902 sec.
iter 291750 || Loss: 0.7518 || timer: 0.0895 sec.
iter 291760 || Loss: 0.9459 || timer: 0.0827 sec.
iter 291770 || Loss: 0.6763 || timer: 0.0897 sec.
iter 291780 || Loss: 0.7424 || timer: 0.1030 sec.
iter 291790 || Loss: 0.5732 || timer: 0.0836 sec.
iter 291800 || Loss: 0.6277 || timer: 0.1259 sec.
iter 291810 || Loss: 0.7744 || timer: 0.0810 sec.
iter 291820 || Loss: 0.4952 || timer: 0.0877 sec.
iter 291830 || Loss: 0.5200 || timer: 0.0874 sec.
iter 291840 || Loss: 0.5802 || timer: 0.0899 sec.
iter 291850 || Loss: 0.8691 || timer: 0.0813 sec.
iter 291860 || Loss: 0.6466 || timer: 0.0830 sec.
iter 291870 || Loss: 0.9119 || timer: 0.0871 sec.
iter 291880 || Loss: 0.6146 || timer: 0.0967 sec.
iter 291890 || Loss: 0.8847 || timer: 0.0871 sec.
iter 291900 || Loss: 0.6670 || timer: 0.0886 sec.
iter 291910 || Loss: 0.9688 || timer: 0.1066 sec.
iter 291920 || Loss: 0.5621 || timer: 0.0816 sec.
iter 291930 || Loss: 0.8923 || timer: 0.0885 sec.
iter 291940 || Loss: 0.7278 || timer: 0.0198 sec.
iter 291950 || Loss: 0.1186 || timer: 0.0861 sec.
iter 291960 || Loss: 0.6596 || timer: 0.0936 sec.
iter 291970 || Loss: 0.6687 || timer: 0.0889 sec.
iter 291980 || Loss: 0.6829 || timer: 0.0832 sec.
iter 291990 || Loss: 0.8226 || timer: 0.0833 sec.
iter 292000 || Loss: 0.8864 || timer: 0.0828 sec.
iter 292010 || Loss: 0.7763 || timer: 0.0994 sec.
iter 292020 || Loss: 0.5758 || timer: 0.0925 sec.
iter 292030 || Loss: 0.7903 || timer: 0.1058 sec.
iter 292040 || Loss: 0.6657 || timer: 0.0942 sec.
iter 292050 || Loss: 0.6455 || timer: 0.0896 sec.
iter 292060 || Loss: 0.7994 || timer: 0.0955 sec.
iter 292070 || Loss: 0.8625 || timer: 0.0919 sec.
iter 292080 || Loss: 0.6547 || timer: 0.0912 sec.
iter 292090 || Loss: 0.7371 || timer: 0.0845 sec.
iter 292100 || Loss: 0.7273 || timer: 0.0822 sec.
iter 292110 || Loss: 0.7314 || timer: 0.0986 sec.
iter 292120 || Loss: 0.6328 || timer: 0.1052 sec.
iter 292130 || Loss: 0.6021 || timer: 0.0855 sec.
iter 292140 || Loss: 0.6043 || timer: 0.0922 sec.
iter 292150 || Loss: 0.8575 || timer: 0.0831 sec.
iter 292160 || Loss: 0.6488 || timer: 0.0944 sec.
iter 292170 || Loss: 0.6127 || timer: 0.0905 sec.
iter 292180 || Loss: 0.7788 || timer: 0.0930 sec.
iter 292190 || Loss: 0.9000 || timer: 0.0920 sec.
iter 292200 || Loss: 0.7548 || timer: 0.0893 sec.
iter 292210 || Loss: 1.0076 || timer: 0.0845 sec.
iter 292220 || Loss: 0.6349 || timer: 0.1041 sec.
iter 292230 || Loss: 0.6698 || timer: 0.0890 sec.
iter 292240 || Loss: 0.8178 || timer: 0.1044 sec.
iter 292250 || Loss: 0.7813 || timer: 0.0829 sec.
iter 292260 || Loss: 0.6788 || timer: 0.0909 sec.
iter 292270 || Loss: 0.7961 || timer: 0.0185 sec.
iter 292280 || Loss: 0.6653 || timer: 0.0987 sec.
iter 292290 || Loss: 0.7214 || timer: 0.1052 sec.
iter 292300 || Loss: 0.7751 || timer: 0.0822 sec.
iter 292310 || Loss: 0.5874 || timer: 0.0891 sec.
iter 292320 || Loss: 0.6795 || timer: 0.0915 sec.
iter 292330 || Loss: 0.9336 || timer: 0.1061 sec.
iter 292340 || Loss: 0.5976 || timer: 0.0906 sec.
iter 292350 || Loss: 1.0120 || timer: 0.0912 sec.
iter 292360 || Loss: 0.6653 || timer: 0.0897 sec.
iter 292370 || Loss: 0.8139 || timer: 0.1023 sec.
iter 292380 || Loss: 0.9277 || timer: 0.0822 sec.
iter 292390 || Loss: 0.7624 || timer: 0.0961 sec.
iter 292400 || Loss: 0.8601 || timer: 0.0832 sec.
iter 292410 || Loss: 0.9251 || timer: 0.0826 sec.
iter 292420 || Loss: 0.6236 || timer: 0.0828 sec.
iter 292430 || Loss: 0.6814 || timer: 0.0894 sec.
iter 292440 || Loss: 0.5822 || timer: 0.1004 sec.
iter 292450 || Loss: 0.7099 || timer: 0.0893 sec.
iter 292460 || Loss: 0.7981 || timer: 0.0898 sec.
iter 292470 || Loss: 0.8380 || timer: 0.1123 sec.
iter 292480 || Loss: 0.7383 || timer: 0.0920 sec.
iter 292490 || Loss: 0.6829 || timer: 0.0878 sec.
iter 292500 || Loss: 0.8939 || timer: 0.0907 sec.
iter 292510 || Loss: 0.7045 || timer: 0.0822 sec.
iter 292520 || Loss: 0.5849 || timer: 0.0904 sec.
iter 292530 || Loss: 0.8128 || timer: 0.0842 sec.
iter 292540 || Loss: 0.7455 || timer: 0.0916 sec.
iter 292550 || Loss: 0.5264 || timer: 0.1051 sec.
iter 292560 || Loss: 0.5945 || timer: 0.0889 sec.
iter 292570 || Loss: 0.6558 || timer: 0.1122 sec.
iter 292580 || Loss: 1.1228 || timer: 0.0877 sec.
iter 292590 || Loss: 0.8042 || timer: 0.0929 sec.
iter 292600 || Loss: 0.6764 || timer: 0.0242 sec.
iter 292610 || Loss: 0.4165 || timer: 0.0824 sec.
iter 292620 || Loss: 0.6972 || timer: 0.0897 sec.
iter 292630 || Loss: 0.6260 || timer: 0.0851 sec.
iter 292640 || Loss: 0.7850 || timer: 0.0885 sec.
iter 292650 || Loss: 0.7378 || timer: 0.0835 sec.
iter 292660 || Loss: 1.0300 || timer: 0.0880 sec.
iter 292670 || Loss: 0.7954 || timer: 0.0918 sec.
iter 292680 || Loss: 0.6800 || timer: 0.0897 sec.
iter 292690 || Loss: 0.5215 || timer: 0.0966 sec.
iter 292700 || Loss: 0.6759 || timer: 0.1193 sec.
iter 292710 || Loss: 0.6402 || timer: 0.1073 sec.
iter 292720 || Loss: 0.9131 || timer: 0.0902 sec.
iter 292730 || Loss: 0.6317 || timer: 0.1048 sec.
iter 292740 || Loss: 0.9149 || timer: 0.0818 sec.
iter 292750 || Loss: 0.8771 || timer: 0.0894 sec.
iter 292760 || Loss: 0.6242 || timer: 0.0892 sec.
iter 292770 || Loss: 0.8691 || timer: 0.1070 sec.
iter 292780 || Loss: 0.5064 || timer: 0.0903 sec.
iter 292790 || Loss: 0.8305 || timer: 0.0907 sec.
iter 292800 || Loss: 0.7100 || timer: 0.0926 sec.
iter 292810 || Loss: 0.6290 || timer: 0.0897 sec.
iter 292820 || Loss: 1.0856 || timer: 0.0892 sec.
iter 292830 || Loss: 0.5832 || timer: 0.0848 sec.
iter 292840 || Loss: 0.5711 || timer: 0.0910 sec.
iter 292850 || Loss: 0.8563 || timer: 0.0957 sec.
iter 292860 || Loss: 0.8066 || timer: 0.0905 sec.
iter 292870 || Loss: 0.7619 || timer: 0.0904 sec.
iter 292880 || Loss: 0.7265 || timer: 0.0894 sec.
iter 292890 || Loss: 0.6173 || timer: 0.0934 sec.
iter 292900 || Loss: 0.6258 || timer: 0.0899 sec.
iter 292910 || Loss: 0.7549 || timer: 0.0833 sec.
iter 292920 || Loss: 0.7942 || timer: 0.1021 sec.
iter 292930 || Loss: 0.7151 || timer: 0.0227 sec.
iter 292940 || Loss: 0.2765 || timer: 0.1112 sec.
iter 292950 || Loss: 0.6856 || timer: 0.0918 sec.
iter 292960 || Loss: 0.5575 || timer: 0.0967 sec.
iter 292970 || Loss: 0.8651 || timer: 0.0874 sec.
iter 292980 || Loss: 0.8995 || timer: 0.0872 sec.
iter 292990 || Loss: 0.6084 || timer: 0.0868 sec.
iter 293000 || Loss: 0.7321 || timer: 0.1020 sec.
iter 293010 || Loss: 0.8725 || timer: 0.0984 sec.
iter 293020 || Loss: 0.8538 || timer: 0.0892 sec.
iter 293030 || Loss: 0.9021 || timer: 0.0949 sec.
iter 293040 || Loss: 0.8063 || timer: 0.1043 sec.
iter 293050 || Loss: 0.7791 || timer: 0.0944 sec.
iter 293060 || Loss: 0.7094 || timer: 0.0921 sec.
iter 293070 || Loss: 1.0477 || timer: 0.0923 sec.
iter 293080 || Loss: 0.9721 || timer: 0.1074 sec.
iter 293090 || Loss: 0.5456 || timer: 0.1058 sec.
iter 293100 || Loss: 0.7739 || timer: 0.0841 sec.
iter 293110 || Loss: 0.6235 || timer: 0.1133 sec.
iter 293120 || Loss: 0.5462 || timer: 0.0877 sec.
iter 293130 || Loss: 0.5416 || timer: 0.0917 sec.
iter 293140 || Loss: 0.8816 || timer: 0.0903 sec.
iter 293150 || Loss: 0.5427 || timer: 0.0926 sec.
iter 293160 || Loss: 0.7765 || timer: 0.0840 sec.
iter 293170 || Loss: 0.7381 || timer: 0.0901 sec.
iter 293180 || Loss: 0.6719 || timer: 0.0846 sec.
iter 293190 || Loss: 0.9518 || timer: 0.0906 sec.
iter 293200 || Loss: 0.8250 || timer: 0.0914 sec.
iter 293210 || Loss: 0.6637 || timer: 0.0959 sec.
iter 293220 || Loss: 0.8478 || timer: 0.1091 sec.
iter 293230 || Loss: 0.9807 || timer: 0.0821 sec.
iter 293240 || Loss: 0.5982 || timer: 0.1054 sec.
iter 293250 || Loss: 0.5740 || timer: 0.0946 sec.
iter 293260 || Loss: 0.7840 || timer: 0.0267 sec.
iter 293270 || Loss: 0.6033 || timer: 0.0873 sec.
iter 293280 || Loss: 0.8206 || timer: 0.0869 sec.
iter 293290 || Loss: 0.4614 || timer: 0.1026 sec.
iter 293300 || Loss: 0.8510 || timer: 0.0830 sec.
iter 293310 || Loss: 1.2401 || timer: 0.0908 sec.
iter 293320 || Loss: 1.0363 || timer: 0.0895 sec.
iter 293330 || Loss: 0.6369 || timer: 0.1035 sec.
iter 293340 || Loss: 0.7352 || timer: 0.1046 sec.
iter 293350 || Loss: 0.8698 || timer: 0.0839 sec.
iter 293360 || Loss: 0.8289 || timer: 0.1016 sec.
iter 293370 || Loss: 0.9899 || timer: 0.0980 sec.
iter 293380 || Loss: 0.4709 || timer: 0.0876 sec.
iter 293390 || Loss: 0.6906 || timer: 0.0891 sec.
iter 293400 || Loss: 0.7335 || timer: 0.0890 sec.
iter 293410 || Loss: 0.5312 || timer: 0.0910 sec.
iter 293420 || Loss: 0.6633 || timer: 0.0829 sec.
iter 293430 || Loss: 0.7400 || timer: 0.0942 sec.
iter 293440 || Loss: 1.0432 || timer: 0.0904 sec.
iter 293450 || Loss: 0.7941 || timer: 0.0869 sec.
iter 293460 || Loss: 0.8837 || timer: 0.0827 sec.
iter 293470 || Loss: 1.0485 || timer: 0.0856 sec.
iter 293480 || Loss: 0.7638 || timer: 0.0890 sec.
iter 293490 || Loss: 0.8724 || timer: 0.0952 sec.
iter 293500 || Loss: 0.6571 || timer: 0.0828 sec.
iter 293510 || Loss: 1.2204 || timer: 0.0920 sec.
iter 293520 || Loss: 0.6554 || timer: 0.0918 sec.
iter 293530 || Loss: 0.6912 || timer: 0.0830 sec.
iter 293540 || Loss: 0.6103 || timer: 0.1019 sec.
iter 293550 || Loss: 0.8465 || timer: 0.0843 sec.
iter 293560 || Loss: 0.7885 || timer: 0.0896 sec.
iter 293570 || Loss: 0.7260 || timer: 0.0913 sec.
iter 293580 || Loss: 0.8856 || timer: 0.0956 sec.
iter 293590 || Loss: 0.6149 || timer: 0.0205 sec.
iter 293600 || Loss: 0.3457 || timer: 0.0911 sec.
iter 293610 || Loss: 0.6825 || timer: 0.0902 sec.
iter 293620 || Loss: 0.6778 || timer: 0.0858 sec.
iter 293630 || Loss: 0.7212 || timer: 0.0898 sec.
iter 293640 || Loss: 0.6947 || timer: 0.0909 sec.
iter 293650 || Loss: 0.8379 || timer: 0.1083 sec.
iter 293660 || Loss: 1.0200 || timer: 0.0915 sec.
iter 293670 || Loss: 0.4741 || timer: 0.0993 sec.
iter 293680 || Loss: 0.7203 || timer: 0.0858 sec.
iter 293690 || Loss: 0.9384 || timer: 0.1064 sec.
iter 293700 || Loss: 0.6520 || timer: 0.1039 sec.
iter 293710 || Loss: 0.9230 || timer: 0.0887 sec.
iter 293720 || Loss: 0.7673 || timer: 0.0913 sec.
iter 293730 || Loss: 0.6279 || timer: 0.1085 sec.
iter 293740 || Loss: 0.5764 || timer: 0.0899 sec.
iter 293750 || Loss: 0.5960 || timer: 0.1082 sec.
iter 293760 || Loss: 0.6527 || timer: 0.0913 sec.
iter 293770 || Loss: 0.6078 || timer: 0.1025 sec.
iter 293780 || Loss: 0.8024 || timer: 0.0902 sec.
iter 293790 || Loss: 0.7622 || timer: 0.0911 sec.
iter 293800 || Loss: 0.6102 || timer: 0.0887 sec.
iter 293810 || Loss: 0.7591 || timer: 0.1110 sec.
iter 293820 || Loss: 0.7636 || timer: 0.0866 sec.
iter 293830 || Loss: 0.8235 || timer: 0.0916 sec.
iter 293840 || Loss: 0.8516 || timer: 0.0939 sec.
iter 293850 || Loss: 0.6905 || timer: 0.0876 sec.
iter 293860 || Loss: 0.7178 || timer: 0.0837 sec.
iter 293870 || Loss: 0.8355 || timer: 0.1178 sec.
iter 293880 || Loss: 0.6671 || timer: 0.0952 sec.
iter 293890 || Loss: 0.8100 || timer: 0.0908 sec.
iter 293900 || Loss: 0.9504 || timer: 0.0953 sec.
iter 293910 || Loss: 0.8021 || timer: 0.0826 sec.
iter 293920 || Loss: 0.8971 || timer: 0.0285 sec.
iter 293930 || Loss: 0.2797 || timer: 0.0914 sec.
iter 293940 || Loss: 0.5191 || timer: 0.0859 sec.
iter 293950 || Loss: 0.7176 || timer: 0.0844 sec.
iter 293960 || Loss: 0.6668 || timer: 0.0904 sec.
iter 293970 || Loss: 0.6931 || timer: 0.0925 sec.
iter 293980 || Loss: 0.5836 || timer: 0.0856 sec.
iter 293990 || Loss: 0.7290 || timer: 0.0902 sec.
iter 294000 || Loss: 0.6975 || timer: 0.0838 sec.
iter 294010 || Loss: 0.8162 || timer: 0.0912 sec.
iter 294020 || Loss: 0.4187 || timer: 0.1180 sec.
iter 294030 || Loss: 0.8088 || timer: 0.0767 sec.
iter 294040 || Loss: 0.7250 || timer: 0.0966 sec.
iter 294050 || Loss: 0.8865 || timer: 0.0915 sec.
iter 294060 || Loss: 0.4158 || timer: 0.0865 sec.
iter 294070 || Loss: 0.6745 || timer: 0.1117 sec.
iter 294080 || Loss: 0.6992 || timer: 0.0907 sec.
iter 294090 || Loss: 0.7256 || timer: 0.1107 sec.
iter 294100 || Loss: 0.9786 || timer: 0.0841 sec.
iter 294110 || Loss: 0.8192 || timer: 0.1077 sec.
iter 294120 || Loss: 0.8457 || timer: 0.0888 sec.
iter 294130 || Loss: 0.8181 || timer: 0.0898 sec.
iter 294140 || Loss: 0.5861 || timer: 0.0900 sec.
iter 294150 || Loss: 0.6417 || timer: 0.0838 sec.
iter 294160 || Loss: 1.1138 || timer: 0.0919 sec.
iter 294170 || Loss: 0.5126 || timer: 0.1044 sec.
iter 294180 || Loss: 0.4320 || timer: 0.1035 sec.
iter 294190 || Loss: 0.6313 || timer: 0.0908 sec.
iter 294200 || Loss: 0.9523 || timer: 0.0927 sec.
iter 294210 || Loss: 0.8008 || timer: 0.0914 sec.
iter 294220 || Loss: 0.8643 || timer: 0.0901 sec.
iter 294230 || Loss: 0.5931 || timer: 0.0832 sec.
iter 294240 || Loss: 0.7408 || timer: 0.0883 sec.
iter 294250 || Loss: 0.8706 || timer: 0.0198 sec.
iter 294260 || Loss: 0.1717 || timer: 0.0985 sec.
iter 294270 || Loss: 0.7949 || timer: 0.0911 sec.
iter 294280 || Loss: 0.5887 || timer: 0.1027 sec.
iter 294290 || Loss: 0.7000 || timer: 0.0916 sec.
iter 294300 || Loss: 1.0659 || timer: 0.0909 sec.
iter 294310 || Loss: 0.6707 || timer: 0.0825 sec.
iter 294320 || Loss: 0.6252 || timer: 0.0830 sec.
iter 294330 || Loss: 0.6420 || timer: 0.0910 sec.
iter 294340 || Loss: 0.5028 || timer: 0.0910 sec.
iter 294350 || Loss: 0.8277 || timer: 0.0967 sec.
iter 294360 || Loss: 0.5942 || timer: 0.0859 sec.
iter 294370 || Loss: 0.4476 || timer: 0.0861 sec.
iter 294380 || Loss: 0.8825 || timer: 0.0943 sec.
iter 294390 || Loss: 0.7390 || timer: 0.0959 sec.
iter 294400 || Loss: 0.6866 || timer: 0.0945 sec.
iter 294410 || Loss: 0.6734 || timer: 0.0830 sec.
iter 294420 || Loss: 0.5991 || timer: 0.0928 sec.
iter 294430 || Loss: 1.6594 || timer: 0.0969 sec.
iter 294440 || Loss: 0.7358 || timer: 0.0923 sec.
iter 294450 || Loss: 0.5819 || timer: 0.1049 sec.
iter 294460 || Loss: 0.8639 || timer: 0.0998 sec.
iter 294470 || Loss: 0.9333 || timer: 0.0903 sec.
iter 294480 || Loss: 0.7325 || timer: 0.0903 sec.
iter 294490 || Loss: 0.6296 || timer: 0.0894 sec.
iter 294500 || Loss: 0.5489 || timer: 0.0918 sec.
iter 294510 || Loss: 0.7439 || timer: 0.1256 sec.
iter 294520 || Loss: 0.5954 || timer: 0.1110 sec.
iter 294530 || Loss: 0.8548 || timer: 0.0894 sec.
iter 294540 || Loss: 0.5290 || timer: 0.0897 sec.
iter 294550 || Loss: 0.5734 || timer: 0.0863 sec.
iter 294560 || Loss: 0.5540 || timer: 0.0896 sec.
iter 294570 || Loss: 0.8209 || timer: 0.0954 sec.
iter 294580 || Loss: 0.6790 || timer: 0.0245 sec.
iter 294590 || Loss: 0.3352 || timer: 0.0961 sec.
iter 294600 || Loss: 0.5932 || timer: 0.0966 sec.
iter 294610 || Loss: 0.5473 || timer: 0.1004 sec.
iter 294620 || Loss: 0.5053 || timer: 0.0897 sec.
iter 294630 || Loss: 0.5442 || timer: 0.0898 sec.
iter 294640 || Loss: 0.6526 || timer: 0.0835 sec.
iter 294650 || Loss: 0.9398 || timer: 0.1036 sec.
iter 294660 || Loss: 0.5980 || timer: 0.0957 sec.
iter 294670 || Loss: 1.2434 || timer: 0.0925 sec.
iter 294680 || Loss: 0.5995 || timer: 0.1428 sec.
iter 294690 || Loss: 0.5849 || timer: 0.0842 sec.
iter 294700 || Loss: 0.5525 || timer: 0.0910 sec.
iter 294710 || Loss: 0.6685 || timer: 0.0883 sec.
iter 294720 || Loss: 0.8812 || timer: 0.0907 sec.
iter 294730 || Loss: 0.7260 || timer: 0.0836 sec.
iter 294740 || Loss: 0.6003 || timer: 0.0846 sec.
iter 294750 || Loss: 1.2501 || timer: 0.0860 sec.
iter 294760 || Loss: 0.6256 || timer: 0.0838 sec.
iter 294770 || Loss: 0.3678 || timer: 0.0914 sec.
iter 294780 || Loss: 0.9057 || timer: 0.0904 sec.
iter 294790 || Loss: 0.7010 || timer: 0.0981 sec.
iter 294800 || Loss: 0.7912 || timer: 0.0824 sec.
iter 294810 || Loss: 0.8124 || timer: 0.0892 sec.
iter 294820 || Loss: 1.0029 || timer: 0.0899 sec.
iter 294830 || Loss: 0.6551 || timer: 0.0902 sec.
iter 294840 || Loss: 0.6803 || timer: 0.1057 sec.
iter 294850 || Loss: 0.7199 || timer: 0.0905 sec.
iter 294860 || Loss: 0.6200 || timer: 0.0915 sec.
iter 294870 || Loss: 1.0563 || timer: 0.0949 sec.
iter 294880 || Loss: 0.3525 || timer: 0.0824 sec.
iter 294890 || Loss: 0.9552 || timer: 0.0917 sec.
iter 294900 || Loss: 0.5789 || timer: 0.0918 sec.
iter 294910 || Loss: 0.7433 || timer: 0.0175 sec.
iter 294920 || Loss: 0.2402 || timer: 0.0908 sec.
iter 294930 || Loss: 0.6956 || timer: 0.0983 sec.
iter 294940 || Loss: 0.8684 || timer: 0.1053 sec.
iter 294950 || Loss: 0.6337 || timer: 0.0831 sec.
iter 294960 || Loss: 0.7240 || timer: 0.1023 sec.
iter 294970 || Loss: 0.6609 || timer: 0.1056 sec.
iter 294980 || Loss: 0.5403 || timer: 0.1050 sec.
iter 294990 || Loss: 0.9277 || timer: 0.0909 sec.
iter 295000 || Loss: 0.7996 || Saving state, iter: 295000
timer: 0.1110 sec.
iter 295010 || Loss: 0.7481 || timer: 0.1013 sec.
iter 295020 || Loss: 0.6342 || timer: 0.0915 sec.
iter 295030 || Loss: 0.8465 || timer: 0.0905 sec.
iter 295040 || Loss: 0.6710 || timer: 0.0907 sec.
iter 295050 || Loss: 0.5840 || timer: 0.0928 sec.
iter 295060 || Loss: 0.8514 || timer: 0.0893 sec.
iter 295070 || Loss: 0.4704 || timer: 0.0898 sec.
iter 295080 || Loss: 0.6601 || timer: 0.0906 sec.
iter 295090 || Loss: 0.7072 || timer: 0.0878 sec.
iter 295100 || Loss: 0.5415 || timer: 0.0904 sec.
iter 295110 || Loss: 0.7119 || timer: 0.1013 sec.
iter 295120 || Loss: 0.8102 || timer: 0.0859 sec.
iter 295130 || Loss: 0.5866 || timer: 0.0836 sec.
iter 295140 || Loss: 0.7697 || timer: 0.0874 sec.
iter 295150 || Loss: 0.7231 || timer: 0.0909 sec.
iter 295160 || Loss: 0.8642 || timer: 0.0884 sec.
iter 295170 || Loss: 0.9282 || timer: 0.0833 sec.
iter 295180 || Loss: 0.7941 || timer: 0.1068 sec.
iter 295190 || Loss: 0.4990 || timer: 0.1172 sec.
iter 295200 || Loss: 0.5598 || timer: 0.0915 sec.
iter 295210 || Loss: 0.6034 || timer: 0.0913 sec.
iter 295220 || Loss: 1.1483 || timer: 0.0920 sec.
iter 295230 || Loss: 0.6121 || timer: 0.0916 sec.
iter 295240 || Loss: 0.8698 || timer: 0.0267 sec.
iter 295250 || Loss: 0.7529 || timer: 0.0823 sec.
iter 295260 || Loss: 1.1197 || timer: 0.0916 sec.
iter 295270 || Loss: 0.7330 || timer: 0.0920 sec.
iter 295280 || Loss: 0.9190 || timer: 0.0827 sec.
iter 295290 || Loss: 0.6263 || timer: 0.0851 sec.
iter 295300 || Loss: 0.6472 || timer: 0.0919 sec.
iter 295310 || Loss: 0.6623 || timer: 0.0905 sec.
iter 295320 || Loss: 0.5627 || timer: 0.0915 sec.
iter 295330 || Loss: 0.7865 || timer: 0.0840 sec.
iter 295340 || Loss: 0.5136 || timer: 0.1091 sec.
iter 295350 || Loss: 0.5535 || timer: 0.0841 sec.
iter 295360 || Loss: 0.7270 || timer: 0.0923 sec.
iter 295370 || Loss: 0.7037 || timer: 0.0898 sec.
iter 295380 || Loss: 0.6742 || timer: 0.0918 sec.
iter 295390 || Loss: 0.7660 || timer: 0.0960 sec.
iter 295400 || Loss: 0.7820 || timer: 0.1051 sec.
iter 295410 || Loss: 0.6879 || timer: 0.1029 sec.
iter 295420 || Loss: 0.5319 || timer: 0.0940 sec.
iter 295430 || Loss: 0.9924 || timer: 0.1013 sec.
iter 295440 || Loss: 0.6103 || timer: 0.0867 sec.
iter 295450 || Loss: 1.2719 || timer: 0.0908 sec.
iter 295460 || Loss: 0.6691 || timer: 0.0895 sec.
iter 295470 || Loss: 0.8239 || timer: 0.0830 sec.
iter 295480 || Loss: 0.6816 || timer: 0.1070 sec.
iter 295490 || Loss: 0.7119 || timer: 0.0912 sec.
iter 295500 || Loss: 0.8606 || timer: 0.0844 sec.
iter 295510 || Loss: 0.7505 || timer: 0.0998 sec.
iter 295520 || Loss: 0.8630 || timer: 0.0897 sec.
iter 295530 || Loss: 0.7212 || timer: 0.1029 sec.
iter 295540 || Loss: 0.6897 || timer: 0.0892 sec.
iter 295550 || Loss: 0.5672 || timer: 0.0925 sec.
iter 295560 || Loss: 0.5336 || timer: 0.0969 sec.
iter 295570 || Loss: 0.8724 || timer: 0.0181 sec.
iter 295580 || Loss: 2.3049 || timer: 0.0918 sec.
iter 295590 || Loss: 0.6281 || timer: 0.1079 sec.
iter 295600 || Loss: 0.5101 || timer: 0.0907 sec.
iter 295610 || Loss: 0.9530 || timer: 0.0826 sec.
iter 295620 || Loss: 0.7561 || timer: 0.0913 sec.
iter 295630 || Loss: 0.8265 || timer: 0.0959 sec.
iter 295640 || Loss: 0.7490 || timer: 0.1051 sec.
iter 295650 || Loss: 0.7089 || timer: 0.0827 sec.
iter 295660 || Loss: 0.5953 || timer: 0.0870 sec.
iter 295670 || Loss: 0.8994 || timer: 0.1184 sec.
iter 295680 || Loss: 0.8522 || timer: 0.0905 sec.
iter 295690 || Loss: 0.8425 || timer: 0.0893 sec.
iter 295700 || Loss: 0.4638 || timer: 0.0907 sec.
iter 295710 || Loss: 0.8306 || timer: 0.0894 sec.
iter 295720 || Loss: 0.7717 || timer: 0.0907 sec.
iter 295730 || Loss: 0.5864 || timer: 0.0893 sec.
iter 295740 || Loss: 0.7676 || timer: 0.1081 sec.
iter 295750 || Loss: 0.6764 || timer: 0.0828 sec.
iter 295760 || Loss: 1.1125 || timer: 0.0929 sec.
iter 295770 || Loss: 0.5615 || timer: 0.1035 sec.
iter 295780 || Loss: 0.6990 || timer: 0.0895 sec.
iter 295790 || Loss: 0.8128 || timer: 0.1136 sec.
iter 295800 || Loss: 0.9434 || timer: 0.0925 sec.
iter 295810 || Loss: 1.2193 || timer: 0.0910 sec.
iter 295820 || Loss: 0.6555 || timer: 0.0830 sec.
iter 295830 || Loss: 0.7150 || timer: 0.0913 sec.
iter 295840 || Loss: 0.6586 || timer: 0.0999 sec.
iter 295850 || Loss: 0.7534 || timer: 0.0889 sec.
iter 295860 || Loss: 0.7115 || timer: 0.1009 sec.
iter 295870 || Loss: 0.7141 || timer: 0.0833 sec.
iter 295880 || Loss: 0.5421 || timer: 0.1023 sec.
iter 295890 || Loss: 0.4625 || timer: 0.0857 sec.
iter 295900 || Loss: 1.0293 || timer: 0.0221 sec.
iter 295910 || Loss: 0.2926 || timer: 0.0827 sec.
iter 295920 || Loss: 0.5923 || timer: 0.1174 sec.
iter 295930 || Loss: 0.5953 || timer: 0.0841 sec.
iter 295940 || Loss: 0.8514 || timer: 0.0918 sec.
iter 295950 || Loss: 0.5911 || timer: 0.0975 sec.
iter 295960 || Loss: 0.5377 || timer: 0.0835 sec.
iter 295970 || Loss: 0.4791 || timer: 0.0851 sec.
iter 295980 || Loss: 0.7242 || timer: 0.0893 sec.
iter 295990 || Loss: 0.7784 || timer: 0.0841 sec.
iter 296000 || Loss: 0.6425 || timer: 0.0988 sec.
iter 296010 || Loss: 0.5884 || timer: 0.1050 sec.
iter 296020 || Loss: 0.6091 || timer: 0.0932 sec.
iter 296030 || Loss: 0.7787 || timer: 0.0900 sec.
iter 296040 || Loss: 0.6031 || timer: 0.0940 sec.
iter 296050 || Loss: 0.6963 || timer: 0.0921 sec.
iter 296060 || Loss: 0.7785 || timer: 0.0841 sec.
iter 296070 || Loss: 0.8741 || timer: 0.0897 sec.
iter 296080 || Loss: 0.6962 || timer: 0.0895 sec.
iter 296090 || Loss: 0.5658 || timer: 0.0907 sec.
iter 296100 || Loss: 0.6995 || timer: 0.1068 sec.
iter 296110 || Loss: 0.9099 || timer: 0.1221 sec.
iter 296120 || Loss: 0.5175 || timer: 0.0908 sec.
iter 296130 || Loss: 0.7966 || timer: 0.0908 sec.
iter 296140 || Loss: 0.6322 || timer: 0.0905 sec.
iter 296150 || Loss: 0.8080 || timer: 0.0858 sec.
iter 296160 || Loss: 0.5693 || timer: 0.0957 sec.
iter 296170 || Loss: 0.9457 || timer: 0.1031 sec.
iter 296180 || Loss: 0.7113 || timer: 0.0842 sec.
iter 296190 || Loss: 0.8506 || timer: 0.0917 sec.
iter 296200 || Loss: 0.7722 || timer: 0.0832 sec.
iter 296210 || Loss: 0.7292 || timer: 0.1027 sec.
iter 296220 || Loss: 0.7702 || timer: 0.0890 sec.
iter 296230 || Loss: 0.6212 || timer: 0.0281 sec.
iter 296240 || Loss: 0.2091 || timer: 0.0958 sec.
iter 296250 || Loss: 0.8730 || timer: 0.1107 sec.
iter 296260 || Loss: 0.6583 || timer: 0.1081 sec.
iter 296270 || Loss: 0.6084 || timer: 0.0888 sec.
iter 296280 || Loss: 0.5795 || timer: 0.0869 sec.
iter 296290 || Loss: 0.6960 || timer: 0.1251 sec.
iter 296300 || Loss: 0.4776 || timer: 0.0903 sec.
iter 296310 || Loss: 0.6896 || timer: 0.0930 sec.
iter 296320 || Loss: 0.7454 || timer: 0.1001 sec.
iter 296330 || Loss: 1.1402 || timer: 0.1219 sec.
iter 296340 || Loss: 0.5391 || timer: 0.0830 sec.
iter 296350 || Loss: 0.9790 || timer: 0.0896 sec.
iter 296360 || Loss: 0.7102 || timer: 0.0901 sec.
iter 296370 || Loss: 0.6468 || timer: 0.1012 sec.
iter 296380 || Loss: 0.8501 || timer: 0.0917 sec.
iter 296390 || Loss: 0.7070 || timer: 0.0847 sec.
iter 296400 || Loss: 0.8308 || timer: 0.0896 sec.
iter 296410 || Loss: 0.7395 || timer: 0.0885 sec.
iter 296420 || Loss: 0.6800 || timer: 0.1012 sec.
iter 296430 || Loss: 0.5703 || timer: 0.0873 sec.
iter 296440 || Loss: 0.5972 || timer: 0.0892 sec.
iter 296450 || Loss: 0.5904 || timer: 0.0880 sec.
iter 296460 || Loss: 0.6154 || timer: 0.0905 sec.
iter 296470 || Loss: 0.5662 || timer: 0.0933 sec.
iter 296480 || Loss: 0.8302 || timer: 0.0913 sec.
iter 296490 || Loss: 0.7141 || timer: 0.1043 sec.
iter 296500 || Loss: 0.7163 || timer: 0.0831 sec.
iter 296510 || Loss: 0.5844 || timer: 0.1226 sec.
iter 296520 || Loss: 0.6675 || timer: 0.0906 sec.
iter 296530 || Loss: 0.5344 || timer: 0.0887 sec.
iter 296540 || Loss: 0.6420 || timer: 0.0825 sec.
iter 296550 || Loss: 0.7310 || timer: 0.0900 sec.
iter 296560 || Loss: 0.6874 || timer: 0.0196 sec.
iter 296570 || Loss: 0.7793 || timer: 0.0892 sec.
iter 296580 || Loss: 0.7492 || timer: 0.1037 sec.
iter 296590 || Loss: 0.7987 || timer: 0.0747 sec.
iter 296600 || Loss: 1.0342 || timer: 0.1409 sec.
iter 296610 || Loss: 0.4981 || timer: 0.0922 sec.
iter 296620 || Loss: 0.6630 || timer: 0.0832 sec.
iter 296630 || Loss: 0.5349 || timer: 0.0950 sec.
iter 296640 || Loss: 0.6575 || timer: 0.0836 sec.
iter 296650 || Loss: 0.5907 || timer: 0.0838 sec.
iter 296660 || Loss: 0.6697 || timer: 0.1134 sec.
iter 296670 || Loss: 0.5695 || timer: 0.0877 sec.
iter 296680 || Loss: 0.6394 || timer: 0.0835 sec.
iter 296690 || Loss: 0.8397 || timer: 0.0824 sec.
iter 296700 || Loss: 0.9194 || timer: 0.0753 sec.
iter 296710 || Loss: 0.6532 || timer: 0.0918 sec.
iter 296720 || Loss: 0.7831 || timer: 0.0941 sec.
iter 296730 || Loss: 0.7534 || timer: 0.0845 sec.
iter 296740 || Loss: 0.7893 || timer: 0.1082 sec.
iter 296750 || Loss: 0.5961 || timer: 0.0926 sec.
iter 296760 || Loss: 0.7076 || timer: 0.0859 sec.
iter 296770 || Loss: 0.7036 || timer: 0.0841 sec.
iter 296780 || Loss: 0.7998 || timer: 0.0982 sec.
iter 296790 || Loss: 0.5697 || timer: 0.0866 sec.
iter 296800 || Loss: 0.7030 || timer: 0.0952 sec.
iter 296810 || Loss: 0.6570 || timer: 0.1076 sec.
iter 296820 || Loss: 0.7752 || timer: 0.0825 sec.
iter 296830 || Loss: 0.3538 || timer: 0.0912 sec.
iter 296840 || Loss: 0.5204 || timer: 0.0905 sec.
iter 296850 || Loss: 0.6136 || timer: 0.0845 sec.
iter 296860 || Loss: 0.7347 || timer: 0.0839 sec.
iter 296870 || Loss: 0.6934 || timer: 0.0837 sec.
iter 296880 || Loss: 0.5898 || timer: 0.1155 sec.
iter 296890 || Loss: 0.6852 || timer: 0.0171 sec.
iter 296900 || Loss: 0.3209 || timer: 0.0907 sec.
iter 296910 || Loss: 0.6386 || timer: 0.0997 sec.
iter 296920 || Loss: 0.9652 || timer: 0.0825 sec.
iter 296930 || Loss: 0.7093 || timer: 0.0946 sec.
iter 296940 || Loss: 0.9833 || timer: 0.0890 sec.
iter 296950 || Loss: 1.0269 || timer: 0.0898 sec.
iter 296960 || Loss: 0.7402 || timer: 0.0915 sec.
iter 296970 || Loss: 0.5637 || timer: 0.1116 sec.
iter 296980 || Loss: 0.5238 || timer: 0.0895 sec.
iter 296990 || Loss: 0.8161 || timer: 0.1283 sec.
iter 297000 || Loss: 1.0189 || timer: 0.0906 sec.
iter 297010 || Loss: 0.4257 || timer: 0.0906 sec.
iter 297020 || Loss: 0.6354 || timer: 0.1080 sec.
iter 297030 || Loss: 0.8081 || timer: 0.0949 sec.
iter 297040 || Loss: 0.7566 || timer: 0.0873 sec.
iter 297050 || Loss: 0.4779 || timer: 0.0939 sec.
iter 297060 || Loss: 0.4497 || timer: 0.0931 sec.
iter 297070 || Loss: 0.8434 || timer: 0.1101 sec.
iter 297080 || Loss: 0.6257 || timer: 0.0890 sec.
iter 297090 || Loss: 0.6782 || timer: 0.0910 sec.
iter 297100 || Loss: 0.9136 || timer: 0.0755 sec.
iter 297110 || Loss: 0.6520 || timer: 0.0916 sec.
iter 297120 || Loss: 0.5801 || timer: 0.0835 sec.
iter 297130 || Loss: 0.6321 || timer: 0.0858 sec.
iter 297140 || Loss: 0.7796 || timer: 0.0841 sec.
iter 297150 || Loss: 0.7072 || timer: 0.0830 sec.
iter 297160 || Loss: 0.7791 || timer: 0.0845 sec.
iter 297170 || Loss: 0.6823 || timer: 0.0918 sec.
iter 297180 || Loss: 0.6511 || timer: 0.0849 sec.
iter 297190 || Loss: 0.6761 || timer: 0.0886 sec.
iter 297200 || Loss: 0.6348 || timer: 0.0885 sec.
iter 297210 || Loss: 0.8547 || timer: 0.0810 sec.
iter 297220 || Loss: 0.8655 || timer: 0.0262 sec.
iter 297230 || Loss: 0.1928 || timer: 0.0846 sec.
iter 297240 || Loss: 0.7940 || timer: 0.0939 sec.
iter 297250 || Loss: 0.7722 || timer: 0.0897 sec.
iter 297260 || Loss: 0.8073 || timer: 0.0904 sec.
iter 297270 || Loss: 0.6685 || timer: 0.0899 sec.
iter 297280 || Loss: 0.5562 || timer: 0.0890 sec.
iter 297290 || Loss: 0.5458 || timer: 0.0832 sec.
iter 297300 || Loss: 0.8792 || timer: 0.1056 sec.
iter 297310 || Loss: 0.8758 || timer: 0.0914 sec.
iter 297320 || Loss: 0.8150 || timer: 0.0941 sec.
iter 297330 || Loss: 0.5698 || timer: 0.0964 sec.
iter 297340 || Loss: 0.7838 || timer: 0.0931 sec.
iter 297350 || Loss: 0.9867 || timer: 0.0838 sec.
iter 297360 || Loss: 0.6516 || timer: 0.0838 sec.
iter 297370 || Loss: 0.8286 || timer: 0.0904 sec.
iter 297380 || Loss: 0.7160 || timer: 0.0833 sec.
iter 297390 || Loss: 0.5702 || timer: 0.0835 sec.
iter 297400 || Loss: 0.6936 || timer: 0.1118 sec.
iter 297410 || Loss: 0.6770 || timer: 0.1071 sec.
iter 297420 || Loss: 0.9465 || timer: 0.0835 sec.
iter 297430 || Loss: 0.6481 || timer: 0.0813 sec.
iter 297440 || Loss: 0.5302 || timer: 0.0914 sec.
iter 297450 || Loss: 0.9220 || timer: 0.0829 sec.
iter 297460 || Loss: 0.9928 || timer: 0.0896 sec.
iter 297470 || Loss: 0.6812 || timer: 0.0906 sec.
iter 297480 || Loss: 0.6576 || timer: 0.0907 sec.
iter 297490 || Loss: 0.9230 || timer: 0.0923 sec.
iter 297500 || Loss: 0.7993 || timer: 0.0916 sec.
iter 297510 || Loss: 0.8935 || timer: 0.0881 sec.
iter 297520 || Loss: 0.5987 || timer: 0.1047 sec.
iter 297530 || Loss: 0.9190 || timer: 0.1061 sec.
iter 297540 || Loss: 0.7925 || timer: 0.1252 sec.
iter 297550 || Loss: 0.6893 || timer: 0.0233 sec.
iter 297560 || Loss: 1.1070 || timer: 0.0864 sec.
iter 297570 || Loss: 0.7976 || timer: 0.0958 sec.
iter 297580 || Loss: 0.6168 || timer: 0.0902 sec.
iter 297590 || Loss: 0.8618 || timer: 0.0903 sec.
iter 297600 || Loss: 0.9219 || timer: 0.0870 sec.
iter 297610 || Loss: 0.7933 || timer: 0.0894 sec.
iter 297620 || Loss: 0.8747 || timer: 0.0912 sec.
iter 297630 || Loss: 0.6425 || timer: 0.0841 sec.
iter 297640 || Loss: 0.5282 || timer: 0.0969 sec.
iter 297650 || Loss: 0.8325 || timer: 0.1157 sec.
iter 297660 || Loss: 0.9797 || timer: 0.0918 sec.
iter 297670 || Loss: 0.8871 || timer: 0.0914 sec.
iter 297680 || Loss: 0.7094 || timer: 0.0908 sec.
iter 297690 || Loss: 1.1405 || timer: 0.0995 sec.
iter 297700 || Loss: 0.9348 || timer: 0.0882 sec.
iter 297710 || Loss: 0.6247 || timer: 0.0835 sec.
iter 297720 || Loss: 0.5966 || timer: 0.0908 sec.
iter 297730 || Loss: 0.6852 || timer: 0.0950 sec.
iter 297740 || Loss: 0.5598 || timer: 0.1041 sec.
iter 297750 || Loss: 0.5827 || timer: 0.0846 sec.
iter 297760 || Loss: 1.0111 || timer: 0.0894 sec.
iter 297770 || Loss: 0.8661 || timer: 0.0860 sec.
iter 297780 || Loss: 0.6509 || timer: 0.0881 sec.
iter 297790 || Loss: 0.6703 || timer: 0.1057 sec.
iter 297800 || Loss: 0.7047 || timer: 0.0890 sec.
iter 297810 || Loss: 1.1136 || timer: 0.0895 sec.
iter 297820 || Loss: 0.6254 || timer: 0.0860 sec.
iter 297830 || Loss: 0.9138 || timer: 0.0840 sec.
iter 297840 || Loss: 0.5559 || timer: 0.0873 sec.
iter 297850 || Loss: 0.6421 || timer: 0.0833 sec.
iter 297860 || Loss: 0.8307 || timer: 0.0963 sec.
iter 297870 || Loss: 0.6353 || timer: 0.0834 sec.
iter 297880 || Loss: 0.7744 || timer: 0.0256 sec.
iter 297890 || Loss: 0.1311 || timer: 0.0824 sec.
iter 297900 || Loss: 0.7759 || timer: 0.0899 sec.
iter 297910 || Loss: 0.6740 || timer: 0.0916 sec.
iter 297920 || Loss: 0.6325 || timer: 0.0881 sec.
iter 297930 || Loss: 0.7248 || timer: 0.0901 sec.
iter 297940 || Loss: 0.8631 || timer: 0.0965 sec.
iter 297950 || Loss: 0.8403 || timer: 0.0824 sec.
iter 297960 || Loss: 0.6195 || timer: 0.0920 sec.
iter 297970 || Loss: 0.6614 || timer: 0.0900 sec.
iter 297980 || Loss: 0.5992 || timer: 0.0958 sec.
iter 297990 || Loss: 0.5881 || timer: 0.0999 sec.
iter 298000 || Loss: 0.7467 || timer: 0.0913 sec.
iter 298010 || Loss: 0.7322 || timer: 0.0899 sec.
iter 298020 || Loss: 0.7248 || timer: 0.0983 sec.
iter 298030 || Loss: 0.8386 || timer: 0.0933 sec.
iter 298040 || Loss: 0.9154 || timer: 0.0893 sec.
iter 298050 || Loss: 0.6946 || timer: 0.0924 sec.
iter 298060 || Loss: 0.5448 || timer: 0.0830 sec.
iter 298070 || Loss: 0.8747 || timer: 0.0902 sec.
iter 298080 || Loss: 0.5770 || timer: 0.0926 sec.
iter 298090 || Loss: 0.5351 || timer: 0.0958 sec.
iter 298100 || Loss: 0.8285 || timer: 0.0989 sec.
iter 298110 || Loss: 0.9928 || timer: 0.0827 sec.
iter 298120 || Loss: 0.5580 || timer: 0.0840 sec.
iter 298130 || Loss: 0.8488 || timer: 0.0845 sec.
iter 298140 || Loss: 0.7063 || timer: 0.0918 sec.
iter 298150 || Loss: 0.9555 || timer: 0.0900 sec.
iter 298160 || Loss: 0.7602 || timer: 0.0901 sec.
iter 298170 || Loss: 0.7147 || timer: 0.1214 sec.
iter 298180 || Loss: 0.8092 || timer: 0.0918 sec.
iter 298190 || Loss: 0.6443 || timer: 0.0899 sec.
iter 298200 || Loss: 0.6917 || timer: 0.0832 sec.
iter 298210 || Loss: 0.7651 || timer: 0.0204 sec.
iter 298220 || Loss: 0.5338 || timer: 0.1114 sec.
iter 298230 || Loss: 0.5551 || timer: 0.0886 sec.
iter 298240 || Loss: 0.6577 || timer: 0.0899 sec.
iter 298250 || Loss: 0.6661 || timer: 0.0897 sec.
iter 298260 || Loss: 0.6618 || timer: 0.0907 sec.
iter 298270 || Loss: 0.6329 || timer: 0.0911 sec.
iter 298280 || Loss: 0.8741 || timer: 0.1006 sec.
iter 298290 || Loss: 0.7543 || timer: 0.1096 sec.
iter 298300 || Loss: 0.5299 || timer: 0.0932 sec.
iter 298310 || Loss: 0.6751 || timer: 0.1061 sec.
iter 298320 || Loss: 0.7851 || timer: 0.0883 sec.
iter 298330 || Loss: 0.5609 || timer: 0.0907 sec.
iter 298340 || Loss: 0.7319 || timer: 0.0902 sec.
iter 298350 || Loss: 0.8016 || timer: 0.0826 sec.
iter 298360 || Loss: 0.7383 || timer: 0.0835 sec.
iter 298370 || Loss: 0.4116 || timer: 0.0877 sec.
iter 298380 || Loss: 0.9101 || timer: 0.0878 sec.
iter 298390 || Loss: 0.6699 || timer: 0.0832 sec.
iter 298400 || Loss: 0.7122 || timer: 0.0996 sec.
iter 298410 || Loss: 0.3921 || timer: 0.0873 sec.
iter 298420 || Loss: 0.7837 || timer: 0.0825 sec.
iter 298430 || Loss: 0.7768 || timer: 0.0917 sec.
iter 298440 || Loss: 0.9462 || timer: 0.0837 sec.
iter 298450 || Loss: 0.5726 || timer: 0.0887 sec.
iter 298460 || Loss: 1.1110 || timer: 0.0833 sec.
iter 298470 || Loss: 0.5089 || timer: 0.0894 sec.
iter 298480 || Loss: 0.5300 || timer: 0.0919 sec.
iter 298490 || Loss: 0.7234 || timer: 0.0824 sec.
iter 298500 || Loss: 0.4970 || timer: 0.1134 sec.
iter 298510 || Loss: 0.7574 || timer: 0.1071 sec.
iter 298520 || Loss: 0.7171 || timer: 0.1159 sec.
iter 298530 || Loss: 0.8336 || timer: 0.1051 sec.
iter 298540 || Loss: 0.7359 || timer: 0.0245 sec.
iter 298550 || Loss: 1.0662 || timer: 0.0905 sec.
iter 298560 || Loss: 0.6392 || timer: 0.1152 sec.
iter 298570 || Loss: 0.6717 || timer: 0.0840 sec.
iter 298580 || Loss: 0.5668 || timer: 0.0926 sec.
iter 298590 || Loss: 0.6107 || timer: 0.0977 sec.
iter 298600 || Loss: 0.7994 || timer: 0.0942 sec.
iter 298610 || Loss: 0.6025 || timer: 0.0918 sec.
iter 298620 || Loss: 0.6454 || timer: 0.0842 sec.
iter 298630 || Loss: 0.7270 || timer: 0.1152 sec.
iter 298640 || Loss: 0.6810 || timer: 0.0993 sec.
iter 298650 || Loss: 0.6529 || timer: 0.0859 sec.
iter 298660 || Loss: 0.7699 || timer: 0.0968 sec.
iter 298670 || Loss: 0.6151 || timer: 0.0850 sec.
iter 298680 || Loss: 0.6672 || timer: 0.0841 sec.
iter 298690 || Loss: 0.5483 || timer: 0.0975 sec.
iter 298700 || Loss: 0.7447 || timer: 0.0832 sec.
iter 298710 || Loss: 0.9205 || timer: 0.0828 sec.
iter 298720 || Loss: 0.7845 || timer: 0.0775 sec.
iter 298730 || Loss: 0.5550 || timer: 0.0911 sec.
iter 298740 || Loss: 0.6697 || timer: 0.0882 sec.
iter 298750 || Loss: 0.5717 || timer: 0.0939 sec.
iter 298760 || Loss: 0.7203 || timer: 0.0937 sec.
iter 298770 || Loss: 0.8277 || timer: 0.0950 sec.
iter 298780 || Loss: 0.9951 || timer: 0.0918 sec.
iter 298790 || Loss: 0.7684 || timer: 0.0913 sec.
iter 298800 || Loss: 0.6441 || timer: 0.0891 sec.
iter 298810 || Loss: 1.0285 || timer: 0.0824 sec.
iter 298820 || Loss: 0.7313 || timer: 0.0825 sec.
iter 298830 || Loss: 0.8461 || timer: 0.0876 sec.
iter 298840 || Loss: 0.4815 || timer: 0.0928 sec.
iter 298850 || Loss: 0.5065 || timer: 0.0769 sec.
iter 298860 || Loss: 0.8132 || timer: 0.0964 sec.
iter 298870 || Loss: 0.5654 || timer: 0.0271 sec.
iter 298880 || Loss: 1.1593 || timer: 0.0900 sec.
iter 298890 || Loss: 0.5875 || timer: 0.0928 sec.
iter 298900 || Loss: 0.7675 || timer: 0.0791 sec.
iter 298910 || Loss: 0.4073 || timer: 0.0869 sec.
iter 298920 || Loss: 0.6070 || timer: 0.0893 sec.
iter 298930 || Loss: 0.8387 || timer: 0.0904 sec.
iter 298940 || Loss: 0.5119 || timer: 0.0941 sec.
iter 298950 || Loss: 0.5975 || timer: 0.1111 sec.
iter 298960 || Loss: 0.7545 || timer: 0.0919 sec.
iter 298970 || Loss: 0.7615 || timer: 0.1022 sec.
iter 298980 || Loss: 0.5621 || timer: 0.0915 sec.
iter 298990 || Loss: 0.7226 || timer: 0.0912 sec.
iter 299000 || Loss: 0.7989 || timer: 0.0916 sec.
iter 299010 || Loss: 0.6616 || timer: 0.0899 sec.
iter 299020 || Loss: 0.6106 || timer: 0.0903 sec.
iter 299030 || Loss: 0.6035 || timer: 0.0867 sec.
iter 299040 || Loss: 1.0111 || timer: 0.0965 sec.
iter 299050 || Loss: 0.7643 || timer: 0.0897 sec.
iter 299060 || Loss: 0.7513 || timer: 0.1088 sec.
iter 299070 || Loss: 0.9983 || timer: 0.1032 sec.
iter 299080 || Loss: 0.7940 || timer: 0.0841 sec.
iter 299090 || Loss: 0.8173 || timer: 0.0889 sec.
iter 299100 || Loss: 0.5968 || timer: 0.0909 sec.
iter 299110 || Loss: 0.7436 || timer: 0.0819 sec.
iter 299120 || Loss: 0.4754 || timer: 0.1006 sec.
iter 299130 || Loss: 0.6963 || timer: 0.0975 sec.
iter 299140 || Loss: 1.0011 || timer: 0.1031 sec.
iter 299150 || Loss: 0.9130 || timer: 0.1010 sec.
iter 299160 || Loss: 0.6626 || timer: 0.1063 sec.
iter 299170 || Loss: 0.9480 || timer: 0.0842 sec.
iter 299180 || Loss: 0.6132 || timer: 0.0923 sec.
iter 299190 || Loss: 0.9344 || timer: 0.0897 sec.
iter 299200 || Loss: 0.5665 || timer: 0.0244 sec.
iter 299210 || Loss: 0.3512 || timer: 0.0913 sec.
iter 299220 || Loss: 0.7180 || timer: 0.0825 sec.
iter 299230 || Loss: 0.6562 || timer: 0.1006 sec.
iter 299240 || Loss: 0.7660 || timer: 0.0893 sec.
iter 299250 || Loss: 0.7621 || timer: 0.0987 sec.
iter 299260 || Loss: 0.5616 || timer: 0.0824 sec.
iter 299270 || Loss: 0.6405 || timer: 0.0934 sec.
iter 299280 || Loss: 0.6889 || timer: 0.0892 sec.
iter 299290 || Loss: 0.6661 || timer: 0.0894 sec.
iter 299300 || Loss: 0.7678 || timer: 0.1371 sec.
iter 299310 || Loss: 0.7956 || timer: 0.0817 sec.
iter 299320 || Loss: 0.7311 || timer: 0.0902 sec.
iter 299330 || Loss: 0.7696 || timer: 0.0882 sec.
iter 299340 || Loss: 0.4525 || timer: 0.0807 sec.
iter 299350 || Loss: 0.5345 || timer: 0.0814 sec.
iter 299360 || Loss: 0.6602 || timer: 0.0918 sec.
iter 299370 || Loss: 0.6536 || timer: 0.0920 sec.
iter 299380 || Loss: 0.8547 || timer: 0.0873 sec.
iter 299390 || Loss: 0.9895 || timer: 0.0912 sec.
iter 299400 || Loss: 0.5627 || timer: 0.1030 sec.
iter 299410 || Loss: 0.6499 || timer: 0.0813 sec.
iter 299420 || Loss: 0.7627 || timer: 0.1115 sec.
iter 299430 || Loss: 0.7194 || timer: 0.1025 sec.
iter 299440 || Loss: 0.8200 || timer: 0.0898 sec.
iter 299450 || Loss: 0.9764 || timer: 0.0874 sec.
iter 299460 || Loss: 0.7973 || timer: 0.0826 sec.
iter 299470 || Loss: 0.7523 || timer: 0.1203 sec.
iter 299480 || Loss: 0.6714 || timer: 0.0987 sec.
iter 299490 || Loss: 0.8859 || timer: 0.0811 sec.
iter 299500 || Loss: 0.8143 || timer: 0.1013 sec.
iter 299510 || Loss: 1.1586 || timer: 0.0844 sec.
iter 299520 || Loss: 1.1916 || timer: 0.0824 sec.
iter 299530 || Loss: 0.8285 || timer: 0.0172 sec.
iter 299540 || Loss: 1.6933 || timer: 0.0820 sec.
iter 299550 || Loss: 0.7652 || timer: 0.0875 sec.
iter 299560 || Loss: 0.6964 || timer: 0.0926 sec.
iter 299570 || Loss: 0.5887 || timer: 0.0823 sec.
iter 299580 || Loss: 0.9584 || timer: 0.0892 sec.
iter 299590 || Loss: 0.5296 || timer: 0.0919 sec.
iter 299600 || Loss: 0.7219 || timer: 0.0880 sec.
iter 299610 || Loss: 0.7203 || timer: 0.0918 sec.
iter 299620 || Loss: 0.5883 || timer: 0.1022 sec.
iter 299630 || Loss: 0.5729 || timer: 0.1101 sec.
iter 299640 || Loss: 0.4747 || timer: 0.0897 sec.
iter 299650 || Loss: 0.7727 || timer: 0.1012 sec.
iter 299660 || Loss: 0.6501 || timer: 0.0873 sec.
iter 299670 || Loss: 0.6441 || timer: 0.0920 sec.
iter 299680 || Loss: 0.7087 || timer: 0.0951 sec.
iter 299690 || Loss: 0.8537 || timer: 0.0903 sec.
iter 299700 || Loss: 0.5450 || timer: 0.0898 sec.
iter 299710 || Loss: 0.7224 || timer: 0.0824 sec.
iter 299720 || Loss: 0.7310 || timer: 0.0852 sec.
iter 299730 || Loss: 1.1685 || timer: 0.0902 sec.
iter 299740 || Loss: 0.7509 || timer: 0.0805 sec.
iter 299750 || Loss: 0.5667 || timer: 0.0884 sec.
iter 299760 || Loss: 0.6634 || timer: 0.0826 sec.
iter 299770 || Loss: 0.6134 || timer: 0.0928 sec.
iter 299780 || Loss: 0.7171 || timer: 0.0895 sec.
iter 299790 || Loss: 0.6528 || timer: 0.0937 sec.
iter 299800 || Loss: 0.6923 || timer: 0.0880 sec.
iter 299810 || Loss: 0.7862 || timer: 0.0857 sec.
iter 299820 || Loss: 0.6608 || timer: 0.0895 sec.
iter 299830 || Loss: 0.5636 || timer: 0.0933 sec.
iter 299840 || Loss: 0.5372 || timer: 0.0905 sec.
iter 299850 || Loss: 1.0742 || timer: 0.0884 sec.
iter 299860 || Loss: 0.4951 || timer: 0.0182 sec.
iter 299870 || Loss: 0.2166 || timer: 0.0827 sec.
iter 299880 || Loss: 0.9062 || timer: 0.0901 sec.
iter 299890 || Loss: 0.8805 || timer: 0.0813 sec.
iter 299900 || Loss: 0.7877 || timer: 0.0851 sec.
iter 299910 || Loss: 0.7696 || timer: 0.0951 sec.
iter 299920 || Loss: 0.8073 || timer: 0.0891 sec.
iter 299930 || Loss: 0.6932 || timer: 0.0857 sec.
iter 299940 || Loss: 0.6549 || timer: 0.0951 sec.
iter 299950 || Loss: 0.9833 || timer: 0.0814 sec.
iter 299960 || Loss: 0.6009 || timer: 0.1191 sec.
iter 299970 || Loss: 0.5744 || timer: 0.1076 sec.
iter 299980 || Loss: 0.8585 || timer: 0.0846 sec.
iter 299990 || Loss: 0.6130 || timer: 0.0893 sec.
iter 300000 || Loss: 0.9193 || Saving state, iter: 300000
timer: 0.0904 sec.
iter 300010 || Loss: 0.7471 || timer: 0.0872 sec.
iter 300020 || Loss: 0.8014 || timer: 0.0843 sec.
iter 300030 || Loss: 0.5867 || timer: 0.0892 sec.
iter 300040 || Loss: 0.5084 || timer: 0.0927 sec.
iter 300050 || Loss: 0.5609 || timer: 0.0835 sec.
iter 300060 || Loss: 0.5622 || timer: 0.0878 sec.
iter 300070 || Loss: 0.7306 || timer: 0.0947 sec.
iter 300080 || Loss: 0.7546 || timer: 0.0899 sec.
iter 300090 || Loss: 0.8881 || timer: 0.0846 sec.
iter 300100 || Loss: 0.8245 || timer: 0.0963 sec.
iter 300110 || Loss: 0.6118 || timer: 0.0887 sec.
iter 300120 || Loss: 0.8395 || timer: 0.0822 sec.
iter 300130 || Loss: 0.9051 || timer: 0.0941 sec.
iter 300140 || Loss: 0.3972 || timer: 0.0819 sec.
iter 300150 || Loss: 0.7955 || timer: 0.0907 sec.
iter 300160 || Loss: 0.6125 || timer: 0.0939 sec.
iter 300170 || Loss: 0.8097 || timer: 0.0879 sec.
iter 300180 || Loss: 0.6353 || timer: 0.0808 sec.
iter 300190 || Loss: 0.7204 || timer: 0.0202 sec.
iter 300200 || Loss: 0.1285 || timer: 0.1046 sec.
iter 300210 || Loss: 0.8930 || timer: 0.1102 sec.
iter 300220 || Loss: 0.7845 || timer: 0.0903 sec.
iter 300230 || Loss: 0.5627 || timer: 0.0847 sec.
iter 300240 || Loss: 0.7703 || timer: 0.0967 sec.
iter 300250 || Loss: 0.6425 || timer: 0.0902 sec.
iter 300260 || Loss: 0.6797 || timer: 0.0854 sec.
iter 300270 || Loss: 0.9775 || timer: 0.1110 sec.
iter 300280 || Loss: 0.6501 || timer: 0.0891 sec.
iter 300290 || Loss: 0.7658 || timer: 0.0993 sec.
iter 300300 || Loss: 0.5223 || timer: 0.0860 sec.
iter 300310 || Loss: 0.7377 || timer: 0.1326 sec.
iter 300320 || Loss: 0.7737 || timer: 0.0877 sec.
iter 300330 || Loss: 0.6042 || timer: 0.1070 sec.
iter 300340 || Loss: 0.8264 || timer: 0.0905 sec.
iter 300350 || Loss: 0.5998 || timer: 0.0852 sec.
iter 300360 || Loss: 0.6017 || timer: 0.0895 sec.
iter 300370 || Loss: 0.8088 || timer: 0.0919 sec.
iter 300380 || Loss: 0.5202 || timer: 0.1040 sec.
iter 300390 || Loss: 0.6226 || timer: 0.0821 sec.
iter 300400 || Loss: 0.9881 || timer: 0.0805 sec.
iter 300410 || Loss: 0.6177 || timer: 0.1137 sec.
iter 300420 || Loss: 0.5474 || timer: 0.0815 sec.
iter 300430 || Loss: 0.8501 || timer: 0.1148 sec.
iter 300440 || Loss: 0.7055 || timer: 0.1132 sec.
iter 300450 || Loss: 0.6506 || timer: 0.0844 sec.
iter 300460 || Loss: 0.6117 || timer: 0.0823 sec.
iter 300470 || Loss: 0.5730 || timer: 0.0893 sec.
iter 300480 || Loss: 0.8535 || timer: 0.0896 sec.
iter 300490 || Loss: 0.5482 || timer: 0.0892 sec.
iter 300500 || Loss: 0.4019 || timer: 0.1041 sec.
iter 300510 || Loss: 0.8486 || timer: 0.0853 sec.
iter 300520 || Loss: 0.7051 || timer: 0.0173 sec.
iter 300530 || Loss: 0.6523 || timer: 0.0910 sec.
iter 300540 || Loss: 0.5583 || timer: 0.1033 sec.
iter 300550 || Loss: 0.7781 || timer: 0.0901 sec.
iter 300560 || Loss: 0.5832 || timer: 0.0858 sec.
iter 300570 || Loss: 0.7473 || timer: 0.1220 sec.
iter 300580 || Loss: 1.1221 || timer: 0.0876 sec.
iter 300590 || Loss: 0.7432 || timer: 0.0809 sec.
iter 300600 || Loss: 0.9075 || timer: 0.1020 sec.
iter 300610 || Loss: 1.2266 || timer: 0.0822 sec.
iter 300620 || Loss: 0.8051 || timer: 0.0996 sec.
iter 300630 || Loss: 0.5491 || timer: 0.0913 sec.
iter 300640 || Loss: 0.5565 || timer: 0.0880 sec.
iter 300650 || Loss: 0.8557 || timer: 0.0955 sec.
iter 300660 || Loss: 0.6657 || timer: 0.0826 sec.
iter 300670 || Loss: 0.5260 || timer: 0.0838 sec.
iter 300680 || Loss: 0.6829 || timer: 0.0811 sec.
iter 300690 || Loss: 0.5576 || timer: 0.0836 sec.
iter 300700 || Loss: 1.0738 || timer: 0.0818 sec.
iter 300710 || Loss: 0.8632 || timer: 0.0945 sec.
iter 300720 || Loss: 0.6937 || timer: 0.0928 sec.
iter 300730 || Loss: 0.7011 || timer: 0.0911 sec.
iter 300740 || Loss: 0.9506 || timer: 0.0880 sec.
iter 300750 || Loss: 0.8836 || timer: 0.0898 sec.
iter 300760 || Loss: 0.8558 || timer: 0.0990 sec.
iter 300770 || Loss: 0.7092 || timer: 0.0928 sec.
iter 300780 || Loss: 0.6443 || timer: 0.0882 sec.
iter 300790 || Loss: 0.4607 || timer: 0.0898 sec.
iter 300800 || Loss: 0.7085 || timer: 0.1058 sec.
iter 300810 || Loss: 0.9084 || timer: 0.0809 sec.
iter 300820 || Loss: 0.7465 || timer: 0.0892 sec.
iter 300830 || Loss: 0.5825 || timer: 0.0890 sec.
iter 300840 || Loss: 0.5793 || timer: 0.0909 sec.
iter 300850 || Loss: 0.6452 || timer: 0.0236 sec.
iter 300860 || Loss: 1.2883 || timer: 0.0894 sec.
iter 300870 || Loss: 0.9671 || timer: 0.0991 sec.
iter 300880 || Loss: 0.6221 || timer: 0.0902 sec.
iter 300890 || Loss: 1.1450 || timer: 0.0894 sec.
iter 300900 || Loss: 0.7646 || timer: 0.0824 sec.
iter 300910 || Loss: 0.7237 || timer: 0.0856 sec.
iter 300920 || Loss: 0.5682 || timer: 0.0806 sec.
iter 300930 || Loss: 0.6100 || timer: 0.1059 sec.
iter 300940 || Loss: 0.5325 || timer: 0.1346 sec.
iter 300950 || Loss: 0.7352 || timer: 0.0942 sec.
iter 300960 || Loss: 0.6692 || timer: 0.0907 sec.
iter 300970 || Loss: 0.6265 || timer: 0.1012 sec.
iter 300980 || Loss: 0.5421 || timer: 0.0891 sec.
iter 300990 || Loss: 0.7588 || timer: 0.1123 sec.
iter 301000 || Loss: 0.8381 || timer: 0.0929 sec.
iter 301010 || Loss: 0.4753 || timer: 0.1040 sec.
iter 301020 || Loss: 0.6192 || timer: 0.1067 sec.
iter 301030 || Loss: 0.8920 || timer: 0.0992 sec.
iter 301040 || Loss: 1.0706 || timer: 0.0881 sec.
iter 301050 || Loss: 0.8081 || timer: 0.0917 sec.
iter 301060 || Loss: 0.6931 || timer: 0.0931 sec.
iter 301070 || Loss: 0.6795 || timer: 0.0902 sec.
iter 301080 || Loss: 0.5156 || timer: 0.0834 sec.
iter 301090 || Loss: 0.8476 || timer: 0.1059 sec.
iter 301100 || Loss: 0.6929 || timer: 0.0940 sec.
iter 301110 || Loss: 0.7267 || timer: 0.0816 sec.
iter 301120 || Loss: 0.8390 || timer: 0.0825 sec.
iter 301130 || Loss: 0.7307 || timer: 0.1084 sec.
iter 301140 || Loss: 0.8711 || timer: 0.0893 sec.
iter 301150 || Loss: 0.7451 || timer: 0.0876 sec.
iter 301160 || Loss: 0.6548 || timer: 0.0877 sec.
iter 301170 || Loss: 0.9507 || timer: 0.0885 sec.
iter 301180 || Loss: 0.5955 || timer: 0.0160 sec.
iter 301190 || Loss: 0.2228 || timer: 0.0895 sec.
iter 301200 || Loss: 0.8562 || timer: 0.0866 sec.
iter 301210 || Loss: 0.7951 || timer: 0.0811 sec.
iter 301220 || Loss: 0.7154 || timer: 0.0901 sec.
iter 301230 || Loss: 0.5469 || timer: 0.0868 sec.
iter 301240 || Loss: 0.7146 || timer: 0.0900 sec.
iter 301250 || Loss: 0.6893 || timer: 0.0804 sec.
iter 301260 || Loss: 0.6742 || timer: 0.0868 sec.
iter 301270 || Loss: 0.6277 || timer: 0.1112 sec.
iter 301280 || Loss: 0.5939 || timer: 0.0949 sec.
iter 301290 || Loss: 0.9913 || timer: 0.1145 sec.
iter 301300 || Loss: 0.7858 || timer: 0.0890 sec.
iter 301310 || Loss: 0.8908 || timer: 0.0910 sec.
iter 301320 || Loss: 0.8942 || timer: 0.1020 sec.
iter 301330 || Loss: 0.6903 || timer: 0.0903 sec.
iter 301340 || Loss: 0.9772 || timer: 0.0943 sec.
iter 301350 || Loss: 0.5558 || timer: 0.0865 sec.
iter 301360 || Loss: 0.7814 || timer: 0.0935 sec.
iter 301370 || Loss: 1.0184 || timer: 0.1086 sec.
iter 301380 || Loss: 0.4986 || timer: 0.0901 sec.
iter 301390 || Loss: 0.9453 || timer: 0.1004 sec.
iter 301400 || Loss: 0.7586 || timer: 0.0877 sec.
iter 301410 || Loss: 0.6465 || timer: 0.0813 sec.
iter 301420 || Loss: 0.5248 || timer: 0.1033 sec.
iter 301430 || Loss: 0.5977 || timer: 0.1088 sec.
iter 301440 || Loss: 0.5882 || timer: 0.0878 sec.
iter 301450 || Loss: 0.5154 || timer: 0.0942 sec.
iter 301460 || Loss: 1.1115 || timer: 0.0832 sec.
iter 301470 || Loss: 0.7319 || timer: 0.0868 sec.
iter 301480 || Loss: 0.6170 || timer: 0.0883 sec.
iter 301490 || Loss: 0.6550 || timer: 0.0884 sec.
iter 301500 || Loss: 1.0294 || timer: 0.0882 sec.
iter 301510 || Loss: 0.7466 || timer: 0.0157 sec.
iter 301520 || Loss: 0.7826 || timer: 0.1182 sec.
iter 301530 || Loss: 0.8984 || timer: 0.0859 sec.
iter 301540 || Loss: 0.7362 || timer: 0.1032 sec.
iter 301550 || Loss: 0.5871 || timer: 0.0900 sec.
iter 301560 || Loss: 0.6756 || timer: 0.0893 sec.
iter 301570 || Loss: 0.7845 || timer: 0.0829 sec.
iter 301580 || Loss: 0.6527 || timer: 0.1070 sec.
iter 301590 || Loss: 0.6226 || timer: 0.0822 sec.
iter 301600 || Loss: 0.7740 || timer: 0.1055 sec.
iter 301610 || Loss: 0.6281 || timer: 0.0955 sec.
iter 301620 || Loss: 0.7619 || timer: 0.0837 sec.
iter 301630 || Loss: 0.7883 || timer: 0.0892 sec.
iter 301640 || Loss: 0.8741 || timer: 0.0811 sec.
iter 301650 || Loss: 0.6588 || timer: 0.0927 sec.
iter 301660 || Loss: 0.5667 || timer: 0.0886 sec.
iter 301670 || Loss: 0.6080 || timer: 0.0924 sec.
iter 301680 || Loss: 0.8036 || timer: 0.0907 sec.
iter 301690 || Loss: 0.7771 || timer: 0.0817 sec.
iter 301700 || Loss: 0.8274 || timer: 0.0921 sec.
iter 301710 || Loss: 0.6100 || timer: 0.0902 sec.
iter 301720 || Loss: 0.6071 || timer: 0.0985 sec.
iter 301730 || Loss: 0.9879 || timer: 0.0926 sec.
iter 301740 || Loss: 1.0549 || timer: 0.0814 sec.
iter 301750 || Loss: 0.4786 || timer: 0.0903 sec.
iter 301760 || Loss: 0.6390 || timer: 0.0892 sec.
iter 301770 || Loss: 0.6179 || timer: 0.0912 sec.
iter 301780 || Loss: 0.5349 || timer: 0.0884 sec.
iter 301790 || Loss: 0.6404 || timer: 0.0875 sec.
iter 301800 || Loss: 0.5071 || timer: 0.0915 sec.
iter 301810 || Loss: 0.5582 || timer: 0.0915 sec.
iter 301820 || Loss: 0.7545 || timer: 0.0909 sec.
iter 301830 || Loss: 0.7790 || timer: 0.0868 sec.
iter 301840 || Loss: 0.7679 || timer: 0.0166 sec.
iter 301850 || Loss: 2.1183 || timer: 0.0862 sec.
iter 301860 || Loss: 0.5550 || timer: 0.1136 sec.
iter 301870 || Loss: 0.7426 || timer: 0.1117 sec.
iter 301880 || Loss: 0.9132 || timer: 0.0926 sec.
iter 301890 || Loss: 0.7310 || timer: 0.0910 sec.
iter 301900 || Loss: 0.6304 || timer: 0.1028 sec.
iter 301910 || Loss: 0.7817 || timer: 0.1005 sec.
iter 301920 || Loss: 0.5461 || timer: 0.0871 sec.
iter 301930 || Loss: 0.8574 || timer: 0.0881 sec.
iter 301940 || Loss: 0.7253 || timer: 0.1030 sec.
iter 301950 || Loss: 0.6702 || timer: 0.0806 sec.
iter 301960 || Loss: 0.7055 || timer: 0.0895 sec.
iter 301970 || Loss: 0.5460 || timer: 0.0816 sec.
iter 301980 || Loss: 0.7635 || timer: 0.0910 sec.
iter 301990 || Loss: 0.5774 || timer: 0.0877 sec.
iter 302000 || Loss: 0.5923 || timer: 0.0913 sec.
iter 302010 || Loss: 0.7421 || timer: 0.0892 sec.
iter 302020 || Loss: 0.7512 || timer: 0.0943 sec.
iter 302030 || Loss: 0.5973 || timer: 0.0925 sec.
iter 302040 || Loss: 0.5692 || timer: 0.0902 sec.
iter 302050 || Loss: 0.5605 || timer: 0.0891 sec.
iter 302060 || Loss: 0.6118 || timer: 0.0837 sec.
iter 302070 || Loss: 0.7070 || timer: 0.0850 sec.
iter 302080 || Loss: 0.7042 || timer: 0.0894 sec.
iter 302090 || Loss: 0.9947 || timer: 0.0805 sec.
iter 302100 || Loss: 1.2012 || timer: 0.0966 sec.
iter 302110 || Loss: 0.6887 || timer: 0.0889 sec.
iter 302120 || Loss: 0.8317 || timer: 0.0997 sec.
iter 302130 || Loss: 0.6323 || timer: 0.1106 sec.
iter 302140 || Loss: 0.6981 || timer: 0.0906 sec.
iter 302150 || Loss: 0.7692 || timer: 0.0813 sec.
iter 302160 || Loss: 0.6520 || timer: 0.0848 sec.
iter 302170 || Loss: 0.6264 || timer: 0.0236 sec.
iter 302180 || Loss: 0.2365 || timer: 0.0918 sec.
iter 302190 || Loss: 0.7221 || timer: 0.0947 sec.
iter 302200 || Loss: 0.8575 || timer: 0.0902 sec.
iter 302210 || Loss: 0.7143 || timer: 0.0843 sec.
iter 302220 || Loss: 0.6394 || timer: 0.0887 sec.
iter 302230 || Loss: 0.7745 || timer: 0.0851 sec.
iter 302240 || Loss: 0.7085 || timer: 0.0899 sec.
iter 302250 || Loss: 0.8636 || timer: 0.0810 sec.
iter 302260 || Loss: 0.7773 || timer: 0.0859 sec.
iter 302270 || Loss: 0.9704 || timer: 0.1083 sec.
iter 302280 || Loss: 0.6854 || timer: 0.1348 sec.
iter 302290 || Loss: 0.7663 || timer: 0.1064 sec.
iter 302300 || Loss: 0.7686 || timer: 0.0819 sec.
iter 302310 || Loss: 0.5304 || timer: 0.0881 sec.
iter 302320 || Loss: 0.5546 || timer: 0.0853 sec.
iter 302330 || Loss: 0.6775 || timer: 0.0807 sec.
iter 302340 || Loss: 0.7074 || timer: 0.1130 sec.
iter 302350 || Loss: 1.1370 || timer: 0.1186 sec.
iter 302360 || Loss: 0.9339 || timer: 0.0959 sec.
iter 302370 || Loss: 0.6499 || timer: 0.0846 sec.
iter 302380 || Loss: 0.6902 || timer: 0.0844 sec.
iter 302390 || Loss: 0.7009 || timer: 0.0883 sec.
iter 302400 || Loss: 0.6152 || timer: 0.0901 sec.
iter 302410 || Loss: 0.5472 || timer: 0.1038 sec.
iter 302420 || Loss: 0.9703 || timer: 0.0996 sec.
iter 302430 || Loss: 0.5840 || timer: 0.0912 sec.
iter 302440 || Loss: 0.6341 || timer: 0.0904 sec.
iter 302450 || Loss: 0.8448 || timer: 0.0811 sec.
iter 302460 || Loss: 0.6506 || timer: 0.0923 sec.
iter 302470 || Loss: 0.7753 || timer: 0.0820 sec.
iter 302480 || Loss: 0.8793 || timer: 0.1051 sec.
iter 302490 || Loss: 0.6613 || timer: 0.1203 sec.
iter 302500 || Loss: 0.6322 || timer: 0.0172 sec.
iter 302510 || Loss: 1.3904 || timer: 0.1071 sec.
iter 302520 || Loss: 0.7439 || timer: 0.1159 sec.
iter 302530 || Loss: 0.7929 || timer: 0.0810 sec.
iter 302540 || Loss: 0.6113 || timer: 0.0800 sec.
iter 302550 || Loss: 0.9147 || timer: 0.0825 sec.
iter 302560 || Loss: 0.9040 || timer: 0.0885 sec.
iter 302570 || Loss: 0.8076 || timer: 0.0809 sec.
iter 302580 || Loss: 0.6149 || timer: 0.0899 sec.
iter 302590 || Loss: 0.6359 || timer: 0.0823 sec.
iter 302600 || Loss: 0.5494 || timer: 0.1087 sec.
iter 302610 || Loss: 0.6415 || timer: 0.0826 sec.
iter 302620 || Loss: 0.7194 || timer: 0.1071 sec.
iter 302630 || Loss: 0.6448 || timer: 0.0827 sec.
iter 302640 || Loss: 0.7154 || timer: 0.0898 sec.
iter 302650 || Loss: 0.6424 || timer: 0.0891 sec.
iter 302660 || Loss: 0.8510 || timer: 0.1214 sec.
iter 302670 || Loss: 0.6850 || timer: 0.1035 sec.
iter 302680 || Loss: 0.5020 || timer: 0.0913 sec.
iter 302690 || Loss: 0.8792 || timer: 0.0856 sec.
iter 302700 || Loss: 0.6179 || timer: 0.0927 sec.
iter 302710 || Loss: 0.5927 || timer: 0.0915 sec.
iter 302720 || Loss: 0.9693 || timer: 0.0894 sec.
iter 302730 || Loss: 0.5580 || timer: 0.1076 sec.
iter 302740 || Loss: 0.8037 || timer: 0.1056 sec.
iter 302750 || Loss: 0.6991 || timer: 0.0888 sec.
iter 302760 || Loss: 0.7586 || timer: 0.0874 sec.
iter 302770 || Loss: 0.6728 || timer: 0.0804 sec.
iter 302780 || Loss: 0.9277 || timer: 0.0864 sec.
iter 302790 || Loss: 0.6563 || timer: 0.0909 sec.
iter 302800 || Loss: 1.0378 || timer: 0.0810 sec.
iter 302810 || Loss: 0.7149 || timer: 0.1063 sec.
iter 302820 || Loss: 0.8263 || timer: 0.0806 sec.
iter 302830 || Loss: 0.4564 || timer: 0.0255 sec.
iter 302840 || Loss: 0.9444 || timer: 0.1025 sec.
iter 302850 || Loss: 0.7123 || timer: 0.0878 sec.
iter 302860 || Loss: 0.6160 || timer: 0.0925 sec.
iter 302870 || Loss: 0.4970 || timer: 0.0906 sec.
iter 302880 || Loss: 0.5751 || timer: 0.0897 sec.
iter 302890 || Loss: 0.6586 || timer: 0.1110 sec.
iter 302900 || Loss: 0.6473 || timer: 0.0810 sec.
iter 302910 || Loss: 0.6906 || timer: 0.0823 sec.
iter 302920 || Loss: 0.5990 || timer: 0.0821 sec.
iter 302930 || Loss: 0.6861 || timer: 0.1505 sec.
iter 302940 || Loss: 0.5164 || timer: 0.0804 sec.
iter 302950 || Loss: 0.7742 || timer: 0.0904 sec.
iter 302960 || Loss: 0.5655 || timer: 0.0902 sec.
iter 302970 || Loss: 0.8604 || timer: 0.0878 sec.
iter 302980 || Loss: 0.6734 || timer: 0.0903 sec.
iter 302990 || Loss: 0.7385 || timer: 0.0861 sec.
iter 303000 || Loss: 1.0828 || timer: 0.0984 sec.
iter 303010 || Loss: 0.7169 || timer: 0.0826 sec.
iter 303020 || Loss: 0.6910 || timer: 0.0870 sec.
iter 303030 || Loss: 0.6264 || timer: 0.0983 sec.
iter 303040 || Loss: 0.8412 || timer: 0.0809 sec.
iter 303050 || Loss: 0.7605 || timer: 0.0979 sec.
iter 303060 || Loss: 0.7612 || timer: 0.0911 sec.
iter 303070 || Loss: 0.9475 || timer: 0.0823 sec.
iter 303080 || Loss: 0.7920 || timer: 0.0862 sec.
iter 303090 || Loss: 0.8326 || timer: 0.1208 sec.
iter 303100 || Loss: 0.6124 || timer: 0.1321 sec.
iter 303110 || Loss: 1.0484 || timer: 0.0828 sec.
iter 303120 || Loss: 0.6767 || timer: 0.0902 sec.
iter 303130 || Loss: 0.6289 || timer: 0.0894 sec.
iter 303140 || Loss: 0.7652 || timer: 0.1002 sec.
iter 303150 || Loss: 0.6278 || timer: 0.0850 sec.
iter 303160 || Loss: 0.8660 || timer: 0.0263 sec.
iter 303170 || Loss: 0.5070 || timer: 0.0838 sec.
iter 303180 || Loss: 0.6487 || timer: 0.0815 sec.
iter 303190 || Loss: 0.5047 || timer: 0.1126 sec.
iter 303200 || Loss: 0.5260 || timer: 0.0948 sec.
iter 303210 || Loss: 0.8099 || timer: 0.0834 sec.
iter 303220 || Loss: 0.6161 || timer: 0.1263 sec.
iter 303230 || Loss: 0.7403 || timer: 0.0818 sec.
iter 303240 || Loss: 0.4680 || timer: 0.1024 sec.
iter 303250 || Loss: 0.5196 || timer: 0.0884 sec.
iter 303260 || Loss: 0.8095 || timer: 0.0928 sec.
iter 303270 || Loss: 0.7651 || timer: 0.0875 sec.
iter 303280 || Loss: 0.6960 || timer: 0.0895 sec.
iter 303290 || Loss: 0.6983 || timer: 0.0928 sec.
iter 303300 || Loss: 0.5259 || timer: 0.0965 sec.
iter 303310 || Loss: 0.7780 || timer: 0.0834 sec.
iter 303320 || Loss: 0.5888 || timer: 0.1079 sec.
iter 303330 || Loss: 0.6941 || timer: 0.0912 sec.
iter 303340 || Loss: 0.5482 || timer: 0.0880 sec.
iter 303350 || Loss: 0.6948 || timer: 0.0827 sec.
iter 303360 || Loss: 0.4550 || timer: 0.0897 sec.
iter 303370 || Loss: 0.6830 || timer: 0.0891 sec.
iter 303380 || Loss: 0.7707 || timer: 0.0825 sec.
iter 303390 || Loss: 0.5891 || timer: 0.0821 sec.
iter 303400 || Loss: 0.6751 || timer: 0.0909 sec.
iter 303410 || Loss: 0.5931 || timer: 0.0883 sec.
iter 303420 || Loss: 0.5392 || timer: 0.0904 sec.
iter 303430 || Loss: 0.6990 || timer: 0.0903 sec.
iter 303440 || Loss: 0.7133 || timer: 0.0893 sec.
iter 303450 || Loss: 0.7433 || timer: 0.0956 sec.
iter 303460 || Loss: 0.7635 || timer: 0.0912 sec.
iter 303470 || Loss: 0.6584 || timer: 0.1017 sec.
iter 303480 || Loss: 0.6629 || timer: 0.0823 sec.
iter 303490 || Loss: 0.8259 || timer: 0.0266 sec.
iter 303500 || Loss: 1.6233 || timer: 0.0889 sec.
iter 303510 || Loss: 0.8415 || timer: 0.0884 sec.
iter 303520 || Loss: 0.7373 || timer: 0.0818 sec.
iter 303530 || Loss: 0.9060 || timer: 0.0893 sec.
iter 303540 || Loss: 0.4648 || timer: 0.0811 sec.
iter 303550 || Loss: 0.7328 || timer: 0.0910 sec.
iter 303560 || Loss: 0.7643 || timer: 0.0908 sec.
iter 303570 || Loss: 0.8580 || timer: 0.0897 sec.
iter 303580 || Loss: 0.7253 || timer: 0.0932 sec.
iter 303590 || Loss: 0.8185 || timer: 0.1005 sec.
iter 303600 || Loss: 0.4463 || timer: 0.0890 sec.
iter 303610 || Loss: 0.6619 || timer: 0.0979 sec.
iter 303620 || Loss: 0.5904 || timer: 0.0820 sec.
iter 303630 || Loss: 0.7420 || timer: 0.0843 sec.
iter 303640 || Loss: 0.6970 || timer: 0.0909 sec.
iter 303650 || Loss: 0.6731 || timer: 0.0884 sec.
iter 303660 || Loss: 0.7305 || timer: 0.0934 sec.
iter 303670 || Loss: 0.7190 || timer: 0.0824 sec.
iter 303680 || Loss: 1.2940 || timer: 0.0871 sec.
iter 303690 || Loss: 0.7700 || timer: 0.0974 sec.
iter 303700 || Loss: 0.7645 || timer: 0.1012 sec.
iter 303710 || Loss: 0.7294 || timer: 0.0885 sec.
iter 303720 || Loss: 0.8461 || timer: 0.0911 sec.
iter 303730 || Loss: 1.0411 || timer: 0.0919 sec.
iter 303740 || Loss: 0.6265 || timer: 0.0935 sec.
iter 303750 || Loss: 0.7004 || timer: 0.0906 sec.
iter 303760 || Loss: 0.7616 || timer: 0.0954 sec.
iter 303770 || Loss: 0.4806 || timer: 0.0827 sec.
iter 303780 || Loss: 0.9545 || timer: 0.0849 sec.
iter 303790 || Loss: 0.7831 || timer: 0.0848 sec.
iter 303800 || Loss: 0.8049 || timer: 0.1000 sec.
iter 303810 || Loss: 0.8792 || timer: 0.0816 sec.
iter 303820 || Loss: 0.6864 || timer: 0.0152 sec.
iter 303830 || Loss: 0.2206 || timer: 0.0782 sec.
iter 303840 || Loss: 0.5162 || timer: 0.1095 sec.
iter 303850 || Loss: 0.7220 || timer: 0.0826 sec.
iter 303860 || Loss: 0.5673 || timer: 0.0826 sec.
iter 303870 || Loss: 0.9318 || timer: 0.0921 sec.
iter 303880 || Loss: 0.6938 || timer: 0.0834 sec.
iter 303890 || Loss: 0.7405 || timer: 0.0944 sec.
iter 303900 || Loss: 0.8008 || timer: 0.0843 sec.
iter 303910 || Loss: 0.5578 || timer: 0.1076 sec.
iter 303920 || Loss: 0.6958 || timer: 0.0991 sec.
iter 303930 || Loss: 0.6767 || timer: 0.1208 sec.
iter 303940 || Loss: 0.5871 || timer: 0.1103 sec.
iter 303950 || Loss: 1.3005 || timer: 0.0905 sec.
iter 303960 || Loss: 0.7297 || timer: 0.1002 sec.
iter 303970 || Loss: 0.8716 || timer: 0.0861 sec.
iter 303980 || Loss: 0.9690 || timer: 0.0821 sec.
iter 303990 || Loss: 0.9222 || timer: 0.0826 sec.
iter 304000 || Loss: 0.6521 || timer: 0.0876 sec.
iter 304010 || Loss: 0.9324 || timer: 0.0950 sec.
iter 304020 || Loss: 0.5310 || timer: 0.1069 sec.
iter 304030 || Loss: 1.0379 || timer: 0.0962 sec.
iter 304040 || Loss: 0.8183 || timer: 0.0879 sec.
iter 304050 || Loss: 0.7790 || timer: 0.0848 sec.
iter 304060 || Loss: 0.4275 || timer: 0.0919 sec.
iter 304070 || Loss: 0.8007 || timer: 0.0881 sec.
iter 304080 || Loss: 0.6890 || timer: 0.1011 sec.
iter 304090 || Loss: 0.5773 || timer: 0.0998 sec.
iter 304100 || Loss: 0.8954 || timer: 0.0890 sec.
iter 304110 || Loss: 0.7416 || timer: 0.1120 sec.
iter 304120 || Loss: 0.6981 || timer: 0.0918 sec.
iter 304130 || Loss: 0.6771 || timer: 0.0921 sec.
iter 304140 || Loss: 0.5992 || timer: 0.0865 sec.
iter 304150 || Loss: 0.5692 || timer: 0.0253 sec.
iter 304160 || Loss: 0.4856 || timer: 0.0885 sec.
iter 304170 || Loss: 0.5317 || timer: 0.0888 sec.
iter 304180 || Loss: 1.0768 || timer: 0.0927 sec.
iter 304190 || Loss: 0.8311 || timer: 0.1235 sec.
iter 304200 || Loss: 0.6048 || timer: 0.0945 sec.
iter 304210 || Loss: 0.9581 || timer: 0.1261 sec.
iter 304220 || Loss: 0.7959 || timer: 0.0817 sec.
iter 304230 || Loss: 0.4290 || timer: 0.0931 sec.
iter 304240 || Loss: 0.7779 || timer: 0.0925 sec.
iter 304250 || Loss: 0.5402 || timer: 0.0987 sec.
iter 304260 || Loss: 0.5269 || timer: 0.0905 sec.
iter 304270 || Loss: 0.7399 || timer: 0.0916 sec.
iter 304280 || Loss: 0.7971 || timer: 0.0893 sec.
iter 304290 || Loss: 0.5910 || timer: 0.0932 sec.
iter 304300 || Loss: 0.8484 || timer: 0.0812 sec.
iter 304310 || Loss: 0.5902 || timer: 0.0991 sec.
iter 304320 || Loss: 0.6280 || timer: 0.1109 sec.
iter 304330 || Loss: 0.6352 || timer: 0.0906 sec.
iter 304340 || Loss: 0.9701 || timer: 0.0871 sec.
iter 304350 || Loss: 0.7358 || timer: 0.0997 sec.
iter 304360 || Loss: 0.5610 || timer: 0.0912 sec.
iter 304370 || Loss: 0.6888 || timer: 0.0807 sec.
iter 304380 || Loss: 0.9089 || timer: 0.0880 sec.
iter 304390 || Loss: 0.7875 || timer: 0.0796 sec.
iter 304400 || Loss: 0.7255 || timer: 0.0807 sec.
iter 304410 || Loss: 0.6337 || timer: 0.0885 sec.
iter 304420 || Loss: 0.8121 || timer: 0.0852 sec.
iter 304430 || Loss: 0.8764 || timer: 0.0886 sec.
iter 304440 || Loss: 0.5499 || timer: 0.0809 sec.
iter 304450 || Loss: 0.6238 || timer: 0.0805 sec.
iter 304460 || Loss: 0.5190 || timer: 0.0804 sec.
iter 304470 || Loss: 0.7368 || timer: 0.0804 sec.
iter 304480 || Loss: 0.7145 || timer: 0.0187 sec.
iter 304490 || Loss: 0.6987 || timer: 0.0889 sec.
iter 304500 || Loss: 0.6250 || timer: 0.0872 sec.
iter 304510 || Loss: 0.7698 || timer: 0.0889 sec.
iter 304520 || Loss: 0.5708 || timer: 0.0929 sec.
iter 304530 || Loss: 0.7668 || timer: 0.0830 sec.
iter 304540 || Loss: 1.0435 || timer: 0.0871 sec.
iter 304550 || Loss: 0.6258 || timer: 0.0939 sec.
iter 304560 || Loss: 0.7777 || timer: 0.0901 sec.
iter 304570 || Loss: 0.7572 || timer: 0.0895 sec.
iter 304580 || Loss: 0.6718 || timer: 0.1093 sec.
iter 304590 || Loss: 0.8049 || timer: 0.1014 sec.
iter 304600 || Loss: 0.6354 || timer: 0.1105 sec.
iter 304610 || Loss: 0.6963 || timer: 0.0817 sec.
iter 304620 || Loss: 0.5778 || timer: 0.1011 sec.
iter 304630 || Loss: 0.6344 || timer: 0.0812 sec.
iter 304640 || Loss: 0.7085 || timer: 0.1064 sec.
iter 304650 || Loss: 0.8348 || timer: 0.1223 sec.
iter 304660 || Loss: 0.8907 || timer: 0.0889 sec.
iter 304670 || Loss: 0.6266 || timer: 0.0865 sec.
iter 304680 || Loss: 0.6198 || timer: 0.0992 sec.
iter 304690 || Loss: 0.8417 || timer: 0.0863 sec.
iter 304700 || Loss: 0.5657 || timer: 0.0894 sec.
iter 304710 || Loss: 0.5681 || timer: 0.0906 sec.
iter 304720 || Loss: 0.8831 || timer: 0.0945 sec.
iter 304730 || Loss: 0.7837 || timer: 0.0902 sec.
iter 304740 || Loss: 0.9249 || timer: 0.1068 sec.
iter 304750 || Loss: 0.6639 || timer: 0.0887 sec.
iter 304760 || Loss: 0.8491 || timer: 0.1118 sec.
iter 304770 || Loss: 0.5369 || timer: 0.0818 sec.
iter 304780 || Loss: 0.7320 || timer: 0.0916 sec.
iter 304790 || Loss: 0.6340 || timer: 0.0911 sec.
iter 304800 || Loss: 0.7145 || timer: 0.1028 sec.
iter 304810 || Loss: 0.7686 || timer: 0.0212 sec.
iter 304820 || Loss: 2.1734 || timer: 0.0978 sec.
iter 304830 || Loss: 0.9065 || timer: 0.1075 sec.
iter 304840 || Loss: 0.6825 || timer: 0.0873 sec.
iter 304850 || Loss: 0.7955 || timer: 0.0912 sec.
iter 304860 || Loss: 0.6525 || timer: 0.0806 sec.
iter 304870 || Loss: 0.5740 || timer: 0.0821 sec.
iter 304880 || Loss: 0.6372 || timer: 0.0826 sec.
iter 304890 || Loss: 0.6769 || timer: 0.1063 sec.
iter 304900 || Loss: 0.6658 || timer: 0.0820 sec.
iter 304910 || Loss: 0.7012 || timer: 0.0964 sec.
iter 304920 || Loss: 0.6708 || timer: 0.0818 sec.
iter 304930 || Loss: 0.6022 || timer: 0.0996 sec.
iter 304940 || Loss: 0.7089 || timer: 0.0915 sec.
iter 304950 || Loss: 0.5775 || timer: 0.1139 sec.
iter 304960 || Loss: 0.6469 || timer: 0.0853 sec.
iter 304970 || Loss: 0.7613 || timer: 0.0841 sec.
iter 304980 || Loss: 0.7989 || timer: 0.0894 sec.
iter 304990 || Loss: 0.8310 || timer: 0.1129 sec.
iter 305000 || Loss: 0.6101 || Saving state, iter: 305000
timer: 0.0847 sec.
iter 305010 || Loss: 0.6816 || timer: 0.0860 sec.
iter 305020 || Loss: 0.6805 || timer: 0.0889 sec.
iter 305030 || Loss: 0.7317 || timer: 0.1121 sec.
iter 305040 || Loss: 1.0501 || timer: 0.0886 sec.
iter 305050 || Loss: 0.6496 || timer: 0.0880 sec.
iter 305060 || Loss: 0.6151 || timer: 0.1294 sec.
iter 305070 || Loss: 0.7725 || timer: 0.0918 sec.
iter 305080 || Loss: 0.6878 || timer: 0.0910 sec.
iter 305090 || Loss: 0.6902 || timer: 0.1000 sec.
iter 305100 || Loss: 0.6846 || timer: 0.0800 sec.
iter 305110 || Loss: 0.6808 || timer: 0.0803 sec.
iter 305120 || Loss: 0.6831 || timer: 0.1039 sec.
iter 305130 || Loss: 1.0265 || timer: 0.0826 sec.
iter 305140 || Loss: 0.9193 || timer: 0.0165 sec.
iter 305150 || Loss: 0.5495 || timer: 0.0914 sec.
iter 305160 || Loss: 0.7622 || timer: 0.0811 sec.
iter 305170 || Loss: 0.7668 || timer: 0.0886 sec.
iter 305180 || Loss: 0.6827 || timer: 0.0811 sec.
iter 305190 || Loss: 0.6908 || timer: 0.0910 sec.
iter 305200 || Loss: 0.8341 || timer: 0.0890 sec.
iter 305210 || Loss: 0.7743 || timer: 0.0876 sec.
iter 305220 || Loss: 0.7947 || timer: 0.0860 sec.
iter 305230 || Loss: 0.5727 || timer: 0.0865 sec.
iter 305240 || Loss: 1.0325 || timer: 0.1031 sec.
iter 305250 || Loss: 0.6230 || timer: 0.1040 sec.
iter 305260 || Loss: 0.7781 || timer: 0.0909 sec.
iter 305270 || Loss: 0.8924 || timer: 0.0909 sec.
iter 305280 || Loss: 0.8243 || timer: 0.0806 sec.
iter 305290 || Loss: 0.6273 || timer: 0.1026 sec.
iter 305300 || Loss: 0.9702 || timer: 0.0835 sec.
iter 305310 || Loss: 0.6618 || timer: 0.0905 sec.
iter 305320 || Loss: 0.9189 || timer: 0.0899 sec.
iter 305330 || Loss: 0.9418 || timer: 0.0921 sec.
iter 305340 || Loss: 0.8412 || timer: 0.0902 sec.
iter 305350 || Loss: 0.7257 || timer: 0.0808 sec.
iter 305360 || Loss: 0.7892 || timer: 0.0826 sec.
iter 305370 || Loss: 0.6819 || timer: 0.0906 sec.
iter 305380 || Loss: 0.5599 || timer: 0.1029 sec.
iter 305390 || Loss: 0.8349 || timer: 0.0814 sec.
iter 305400 || Loss: 0.7531 || timer: 0.0864 sec.
iter 305410 || Loss: 0.6941 || timer: 0.0808 sec.
iter 305420 || Loss: 0.8833 || timer: 0.1190 sec.
iter 305430 || Loss: 0.6890 || timer: 0.0897 sec.
iter 305440 || Loss: 1.0446 || timer: 0.0913 sec.
iter 305450 || Loss: 0.4364 || timer: 0.0906 sec.
iter 305460 || Loss: 0.7291 || timer: 0.0879 sec.
iter 305470 || Loss: 0.4918 || timer: 0.0226 sec.
iter 305480 || Loss: 0.4259 || timer: 0.0824 sec.
iter 305490 || Loss: 0.5485 || timer: 0.0886 sec.
iter 305500 || Loss: 0.9955 || timer: 0.0820 sec.
iter 305510 || Loss: 0.7410 || timer: 0.1055 sec.
iter 305520 || Loss: 0.7578 || timer: 0.0884 sec.
iter 305530 || Loss: 1.1564 || timer: 0.0924 sec.
iter 305540 || Loss: 0.6020 || timer: 0.1123 sec.
iter 305550 || Loss: 0.7274 || timer: 0.0813 sec.
iter 305560 || Loss: 0.6126 || timer: 0.0826 sec.
iter 305570 || Loss: 0.4414 || timer: 0.0990 sec.
iter 305580 || Loss: 0.8186 || timer: 0.0822 sec.
iter 305590 || Loss: 0.6752 || timer: 0.0843 sec.
iter 305600 || Loss: 0.7182 || timer: 0.0843 sec.
iter 305610 || Loss: 0.6071 || timer: 0.0908 sec.
iter 305620 || Loss: 0.8243 || timer: 0.0959 sec.
iter 305630 || Loss: 0.7162 || timer: 0.0916 sec.
iter 305640 || Loss: 0.4123 || timer: 0.1079 sec.
iter 305650 || Loss: 0.6505 || timer: 0.0916 sec.
iter 305660 || Loss: 0.8110 || timer: 0.0829 sec.
iter 305670 || Loss: 0.5618 || timer: 0.1046 sec.
iter 305680 || Loss: 0.9131 || timer: 0.0824 sec.
iter 305690 || Loss: 0.6983 || timer: 0.0820 sec.
iter 305700 || Loss: 0.4653 || timer: 0.0915 sec.
iter 305710 || Loss: 0.7074 || timer: 0.0864 sec.
iter 305720 || Loss: 0.5935 || timer: 0.0907 sec.
iter 305730 || Loss: 0.5896 || timer: 0.0939 sec.
iter 305740 || Loss: 0.7153 || timer: 0.0822 sec.
iter 305750 || Loss: 0.6608 || timer: 0.0911 sec.
iter 305760 || Loss: 0.5891 || timer: 0.0809 sec.
iter 305770 || Loss: 0.7117 || timer: 0.0920 sec.
iter 305780 || Loss: 0.6989 || timer: 0.0886 sec.
iter 305790 || Loss: 0.5293 || timer: 0.0828 sec.
iter 305800 || Loss: 0.5996 || timer: 0.0158 sec.
iter 305810 || Loss: 0.2819 || timer: 0.0892 sec.
iter 305820 || Loss: 0.8523 || timer: 0.0986 sec.
iter 305830 || Loss: 0.7524 || timer: 0.0895 sec.
iter 305840 || Loss: 0.5520 || timer: 0.0898 sec.
iter 305850 || Loss: 0.6347 || timer: 0.1309 sec.
iter 305860 || Loss: 0.7095 || timer: 0.0901 sec.
iter 305870 || Loss: 0.4842 || timer: 0.0849 sec.
iter 305880 || Loss: 0.6050 || timer: 0.0911 sec.
iter 305890 || Loss: 0.6795 || timer: 0.0825 sec.
iter 305900 || Loss: 0.7291 || timer: 0.1005 sec.
iter 305910 || Loss: 0.8677 || timer: 0.0897 sec.
iter 305920 || Loss: 0.6870 || timer: 0.0850 sec.
iter 305930 || Loss: 0.5362 || timer: 0.0886 sec.
iter 305940 || Loss: 0.7924 || timer: 0.1029 sec.
iter 305950 || Loss: 0.7704 || timer: 0.0817 sec.
iter 305960 || Loss: 0.6976 || timer: 0.1212 sec.
iter 305970 || Loss: 0.7777 || timer: 0.1175 sec.
iter 305980 || Loss: 0.5032 || timer: 0.0882 sec.
iter 305990 || Loss: 0.8133 || timer: 0.0887 sec.
iter 306000 || Loss: 1.0673 || timer: 0.0929 sec.
iter 306010 || Loss: 0.6136 || timer: 0.0897 sec.
iter 306020 || Loss: 0.6530 || timer: 0.0899 sec.
iter 306030 || Loss: 0.5862 || timer: 0.0808 sec.
iter 306040 || Loss: 1.1452 || timer: 0.0886 sec.
iter 306050 || Loss: 0.8410 || timer: 0.0857 sec.
iter 306060 || Loss: 0.5142 || timer: 0.0896 sec.
iter 306070 || Loss: 0.7003 || timer: 0.0805 sec.
iter 306080 || Loss: 0.6165 || timer: 0.0875 sec.
iter 306090 || Loss: 0.6595 || timer: 0.1143 sec.
iter 306100 || Loss: 0.9291 || timer: 0.0886 sec.
iter 306110 || Loss: 0.7929 || timer: 0.0953 sec.
iter 306120 || Loss: 0.6837 || timer: 0.0886 sec.
iter 306130 || Loss: 1.0814 || timer: 0.0155 sec.
iter 306140 || Loss: 0.1167 || timer: 0.0932 sec.
iter 306150 || Loss: 0.4534 || timer: 0.0998 sec.
iter 306160 || Loss: 1.0174 || timer: 0.0904 sec.
iter 306170 || Loss: 0.9855 || timer: 0.0907 sec.
iter 306180 || Loss: 0.6523 || timer: 0.0894 sec.
iter 306190 || Loss: 0.9322 || timer: 0.0887 sec.
iter 306200 || Loss: 0.5845 || timer: 0.0944 sec.
iter 306210 || Loss: 0.6373 || timer: 0.1046 sec.
iter 306220 || Loss: 0.6713 || timer: 0.1034 sec.
iter 306230 || Loss: 0.9791 || timer: 0.0892 sec.
iter 306240 || Loss: 0.5710 || timer: 0.0929 sec.
iter 306250 || Loss: 0.5930 || timer: 0.0885 sec.
iter 306260 || Loss: 0.6109 || timer: 0.0945 sec.
iter 306270 || Loss: 0.7286 || timer: 0.0901 sec.
iter 306280 || Loss: 0.3993 || timer: 0.0852 sec.
iter 306290 || Loss: 0.8797 || timer: 0.0881 sec.
iter 306300 || Loss: 0.5693 || timer: 0.0902 sec.
iter 306310 || Loss: 0.6726 || timer: 0.0813 sec.
iter 306320 || Loss: 0.5239 || timer: 0.0890 sec.
iter 306330 || Loss: 0.9880 || timer: 0.0832 sec.
iter 306340 || Loss: 0.8973 || timer: 0.0804 sec.
iter 306350 || Loss: 0.6347 || timer: 0.0813 sec.
iter 306360 || Loss: 0.6852 || timer: 0.0818 sec.
iter 306370 || Loss: 0.5170 || timer: 0.0907 sec.
iter 306380 || Loss: 0.9673 || timer: 0.0817 sec.
iter 306390 || Loss: 0.7575 || timer: 0.0977 sec.
iter 306400 || Loss: 0.6513 || timer: 0.0885 sec.
iter 306410 || Loss: 0.6400 || timer: 0.0879 sec.
iter 306420 || Loss: 0.5042 || timer: 0.0870 sec.
iter 306430 || Loss: 0.7775 || timer: 0.1099 sec.
iter 306440 || Loss: 0.9492 || timer: 0.1223 sec.
iter 306450 || Loss: 0.8629 || timer: 0.0827 sec.
iter 306460 || Loss: 0.8445 || timer: 0.0236 sec.
iter 306470 || Loss: 0.2599 || timer: 0.0887 sec.
iter 306480 || Loss: 0.6813 || timer: 0.1167 sec.
iter 306490 || Loss: 0.6123 || timer: 0.0890 sec.
iter 306500 || Loss: 0.6364 || timer: 0.0834 sec.
iter 306510 || Loss: 1.2024 || timer: 0.0908 sec.
iter 306520 || Loss: 0.6535 || timer: 0.0901 sec.
iter 306530 || Loss: 0.5650 || timer: 0.0909 sec.
iter 306540 || Loss: 0.5243 || timer: 0.0822 sec.
iter 306550 || Loss: 0.7610 || timer: 0.0881 sec.
iter 306560 || Loss: 0.7753 || timer: 0.0919 sec.
iter 306570 || Loss: 0.5696 || timer: 0.0909 sec.
iter 306580 || Loss: 0.6833 || timer: 0.0813 sec.
iter 306590 || Loss: 0.7531 || timer: 0.0812 sec.
iter 306600 || Loss: 0.6891 || timer: 0.0812 sec.
iter 306610 || Loss: 0.7075 || timer: 0.0808 sec.
iter 306620 || Loss: 0.7107 || timer: 0.0888 sec.
iter 306630 || Loss: 0.6042 || timer: 0.1044 sec.
iter 306640 || Loss: 0.7195 || timer: 0.0871 sec.
iter 306650 || Loss: 0.6296 || timer: 0.0871 sec.
iter 306660 || Loss: 0.5058 || timer: 0.0919 sec.
iter 306670 || Loss: 0.7641 || timer: 0.0926 sec.
iter 306680 || Loss: 0.6535 || timer: 0.0806 sec.
iter 306690 || Loss: 0.8800 || timer: 0.0814 sec.
iter 306700 || Loss: 0.6683 || timer: 0.0893 sec.
iter 306710 || Loss: 0.7247 || timer: 0.0877 sec.
iter 306720 || Loss: 0.5291 || timer: 0.0907 sec.
iter 306730 || Loss: 0.7969 || timer: 0.0828 sec.
iter 306740 || Loss: 0.7600 || timer: 0.0820 sec.
iter 306750 || Loss: 0.6698 || timer: 0.1019 sec.
iter 306760 || Loss: 0.5300 || timer: 0.0891 sec.
iter 306770 || Loss: 0.6320 || timer: 0.0888 sec.
iter 306780 || Loss: 0.6275 || timer: 0.0954 sec.
iter 306790 || Loss: 0.6568 || timer: 0.0166 sec.
iter 306800 || Loss: 0.1108 || timer: 0.0852 sec.
iter 306810 || Loss: 0.5315 || timer: 0.0896 sec.
iter 306820 || Loss: 0.4567 || timer: 0.0810 sec.
iter 306830 || Loss: 0.7191 || timer: 0.0815 sec.
iter 306840 || Loss: 0.8878 || timer: 0.0821 sec.
iter 306850 || Loss: 0.9565 || timer: 0.0806 sec.
iter 306860 || Loss: 0.5996 || timer: 0.0827 sec.
iter 306870 || Loss: 0.7728 || timer: 0.0883 sec.
iter 306880 || Loss: 1.0727 || timer: 0.0832 sec.
iter 306890 || Loss: 0.7004 || timer: 0.1345 sec.
iter 306900 || Loss: 0.8377 || timer: 0.0873 sec.
iter 306910 || Loss: 0.8641 || timer: 0.0897 sec.
iter 306920 || Loss: 0.6578 || timer: 0.0825 sec.
iter 306930 || Loss: 0.5456 || timer: 0.0913 sec.
iter 306940 || Loss: 0.9607 || timer: 0.1008 sec.
iter 306950 || Loss: 0.6389 || timer: 0.1024 sec.
iter 306960 || Loss: 0.7265 || timer: 0.0895 sec.
iter 306970 || Loss: 0.8806 || timer: 0.0895 sec.
iter 306980 || Loss: 0.9483 || timer: 0.0896 sec.
iter 306990 || Loss: 0.6890 || timer: 0.0893 sec.
iter 307000 || Loss: 0.7562 || timer: 0.0822 sec.
iter 307010 || Loss: 0.7704 || timer: 0.0829 sec.
iter 307020 || Loss: 0.6927 || timer: 0.0815 sec.
iter 307030 || Loss: 0.5822 || timer: 0.0918 sec.
iter 307040 || Loss: 0.5239 || timer: 0.0936 sec.
iter 307050 || Loss: 0.9366 || timer: 0.0907 sec.
iter 307060 || Loss: 0.7107 || timer: 0.0903 sec.
iter 307070 || Loss: 0.7645 || timer: 0.0821 sec.
iter 307080 || Loss: 0.5038 || timer: 0.0880 sec.
iter 307090 || Loss: 0.9146 || timer: 0.0891 sec.
iter 307100 || Loss: 0.6793 || timer: 0.0899 sec.
iter 307110 || Loss: 0.6860 || timer: 0.0823 sec.
iter 307120 || Loss: 0.5783 || timer: 0.0236 sec.
iter 307130 || Loss: 0.6720 || timer: 0.0841 sec.
iter 307140 || Loss: 0.7969 || timer: 0.0911 sec.
iter 307150 || Loss: 0.7160 || timer: 0.0826 sec.
iter 307160 || Loss: 0.8463 || timer: 0.0832 sec.
iter 307170 || Loss: 0.6797 || timer: 0.0911 sec.
iter 307180 || Loss: 0.6089 || timer: 0.0753 sec.
iter 307190 || Loss: 0.7189 || timer: 0.1076 sec.
iter 307200 || Loss: 1.0052 || timer: 0.0846 sec.
iter 307210 || Loss: 0.8569 || timer: 0.1025 sec.
iter 307220 || Loss: 0.5285 || timer: 0.0964 sec.
iter 307230 || Loss: 0.6674 || timer: 0.1047 sec.
iter 307240 || Loss: 0.6041 || timer: 0.1039 sec.
iter 307250 || Loss: 0.6550 || timer: 0.0881 sec.
iter 307260 || Loss: 0.5458 || timer: 0.0863 sec.
iter 307270 || Loss: 0.6518 || timer: 0.1048 sec.
iter 307280 || Loss: 0.9324 || timer: 0.0996 sec.
iter 307290 || Loss: 0.5864 || timer: 0.1068 sec.
iter 307300 || Loss: 0.6533 || timer: 0.1146 sec.
iter 307310 || Loss: 0.5245 || timer: 0.0893 sec.
iter 307320 || Loss: 0.6450 || timer: 0.0934 sec.
iter 307330 || Loss: 0.9361 || timer: 0.0924 sec.
iter 307340 || Loss: 0.7224 || timer: 0.0922 sec.
iter 307350 || Loss: 0.6182 || timer: 0.0844 sec.
iter 307360 || Loss: 0.6851 || timer: 0.0878 sec.
iter 307370 || Loss: 0.7658 || timer: 0.0906 sec.
iter 307380 || Loss: 0.6766 || timer: 0.0934 sec.
iter 307390 || Loss: 0.5287 || timer: 0.0929 sec.
iter 307400 || Loss: 0.6060 || timer: 0.0839 sec.
iter 307410 || Loss: 0.8440 || timer: 0.0924 sec.
iter 307420 || Loss: 0.7991 || timer: 0.0997 sec.
iter 307430 || Loss: 0.5923 || timer: 0.0936 sec.
iter 307440 || Loss: 1.0026 || timer: 0.0910 sec.
iter 307450 || Loss: 0.6315 || timer: 0.0174 sec.
iter 307460 || Loss: 0.3757 || timer: 0.0828 sec.
iter 307470 || Loss: 1.0028 || timer: 0.0912 sec.
iter 307480 || Loss: 0.7324 || timer: 0.0919 sec.
iter 307490 || Loss: 0.7993 || timer: 0.0972 sec.
iter 307500 || Loss: 0.8334 || timer: 0.0921 sec.
iter 307510 || Loss: 0.5239 || timer: 0.0891 sec.
iter 307520 || Loss: 0.6943 || timer: 0.0868 sec.
iter 307530 || Loss: 0.7360 || timer: 0.1207 sec.
iter 307540 || Loss: 0.8008 || timer: 0.1433 sec.
iter 307550 || Loss: 0.7914 || timer: 0.0972 sec.
iter 307560 || Loss: 0.5662 || timer: 0.0830 sec.
iter 307570 || Loss: 0.7701 || timer: 0.0916 sec.
iter 307580 || Loss: 0.6447 || timer: 0.0912 sec.
iter 307590 || Loss: 0.8480 || timer: 0.0847 sec.
iter 307600 || Loss: 0.5517 || timer: 0.0840 sec.
iter 307610 || Loss: 0.6004 || timer: 0.0857 sec.
iter 307620 || Loss: 0.6126 || timer: 0.0857 sec.
iter 307630 || Loss: 0.4865 || timer: 0.0912 sec.
iter 307640 || Loss: 0.6142 || timer: 0.0829 sec.
iter 307650 || Loss: 0.7763 || timer: 0.0875 sec.
iter 307660 || Loss: 0.6930 || timer: 0.0911 sec.
iter 307670 || Loss: 0.7059 || timer: 0.1055 sec.
iter 307680 || Loss: 1.1120 || timer: 0.0903 sec.
iter 307690 || Loss: 0.9276 || timer: 0.1172 sec.
iter 307700 || Loss: 0.6811 || timer: 0.0840 sec.
iter 307710 || Loss: 0.7389 || timer: 0.0820 sec.
iter 307720 || Loss: 0.8238 || timer: 0.0886 sec.
iter 307730 || Loss: 0.5603 || timer: 0.0758 sec.
iter 307740 || Loss: 0.6614 || timer: 0.0907 sec.
iter 307750 || Loss: 0.6275 || timer: 0.1089 sec.
iter 307760 || Loss: 0.6488 || timer: 0.0974 sec.
iter 307770 || Loss: 0.5906 || timer: 0.0905 sec.
iter 307780 || Loss: 0.8967 || timer: 0.0173 sec.
iter 307790 || Loss: 0.5582 || timer: 0.0902 sec.
iter 307800 || Loss: 0.8714 || timer: 0.0828 sec.
iter 307810 || Loss: 0.4935 || timer: 0.1152 sec.
iter 307820 || Loss: 0.5760 || timer: 0.0878 sec.
iter 307830 || Loss: 0.6899 || timer: 0.1045 sec.
iter 307840 || Loss: 0.4904 || timer: 0.0918 sec.
iter 307850 || Loss: 0.7965 || timer: 0.0920 sec.
iter 307860 || Loss: 0.6854 || timer: 0.0921 sec.
iter 307870 || Loss: 0.9409 || timer: 0.1088 sec.
iter 307880 || Loss: 0.5609 || timer: 0.1065 sec.
iter 307890 || Loss: 0.4134 || timer: 0.0827 sec.
iter 307900 || Loss: 0.5829 || timer: 0.0850 sec.
iter 307910 || Loss: 0.6128 || timer: 0.0901 sec.
iter 307920 || Loss: 0.6762 || timer: 0.0923 sec.
iter 307930 || Loss: 0.9007 || timer: 0.0834 sec.
iter 307940 || Loss: 0.6818 || timer: 0.0863 sec.
iter 307950 || Loss: 0.7006 || timer: 0.0893 sec.
iter 307960 || Loss: 0.6535 || timer: 0.0827 sec.
iter 307970 || Loss: 0.6607 || timer: 0.1069 sec.
iter 307980 || Loss: 0.5819 || timer: 0.1002 sec.
iter 307990 || Loss: 0.9829 || timer: 0.1185 sec.
iter 308000 || Loss: 0.5444 || timer: 0.0945 sec.
iter 308010 || Loss: 0.5791 || timer: 0.0897 sec.
iter 308020 || Loss: 0.7713 || timer: 0.0946 sec.
iter 308030 || Loss: 0.6377 || timer: 0.0834 sec.
iter 308040 || Loss: 0.6048 || timer: 0.1066 sec.
iter 308050 || Loss: 0.4995 || timer: 0.0903 sec.
iter 308060 || Loss: 0.8433 || timer: 0.0847 sec.
iter 308070 || Loss: 0.5445 || timer: 0.0891 sec.
iter 308080 || Loss: 0.8248 || timer: 0.0904 sec.
iter 308090 || Loss: 0.7190 || timer: 0.0897 sec.
iter 308100 || Loss: 0.4498 || timer: 0.1020 sec.
iter 308110 || Loss: 0.5792 || timer: 0.0231 sec.
iter 308120 || Loss: 0.6851 || timer: 0.1147 sec.
iter 308130 || Loss: 0.6399 || timer: 0.0903 sec.
iter 308140 || Loss: 0.7760 || timer: 0.0894 sec.
iter 308150 || Loss: 0.8435 || timer: 0.0896 sec.
iter 308160 || Loss: 0.6253 || timer: 0.0900 sec.
iter 308170 || Loss: 0.6893 || timer: 0.0897 sec.
iter 308180 || Loss: 0.6044 || timer: 0.0925 sec.
iter 308190 || Loss: 1.0473 || timer: 0.0912 sec.
iter 308200 || Loss: 0.9152 || timer: 0.0922 sec.
iter 308210 || Loss: 0.7317 || timer: 0.1106 sec.
iter 308220 || Loss: 0.5614 || timer: 0.0924 sec.
iter 308230 || Loss: 0.7994 || timer: 0.1092 sec.
iter 308240 || Loss: 0.4539 || timer: 0.1096 sec.
iter 308250 || Loss: 0.9429 || timer: 0.0895 sec.
iter 308260 || Loss: 0.5953 || timer: 0.0919 sec.
iter 308270 || Loss: 0.6911 || timer: 0.1114 sec.
iter 308280 || Loss: 0.8369 || timer: 0.0853 sec.
iter 308290 || Loss: 0.6477 || timer: 0.0889 sec.
iter 308300 || Loss: 0.6471 || timer: 0.0896 sec.
iter 308310 || Loss: 0.5373 || timer: 0.0828 sec.
iter 308320 || Loss: 0.6290 || timer: 0.0890 sec.
iter 308330 || Loss: 0.7215 || timer: 0.0920 sec.
iter 308340 || Loss: 0.5792 || timer: 0.0907 sec.
iter 308350 || Loss: 0.6817 || timer: 0.0925 sec.
iter 308360 || Loss: 0.6971 || timer: 0.0931 sec.
iter 308370 || Loss: 0.6698 || timer: 0.0901 sec.
iter 308380 || Loss: 0.8594 || timer: 0.0936 sec.
iter 308390 || Loss: 0.9375 || timer: 0.0918 sec.
iter 308400 || Loss: 0.5976 || timer: 0.0832 sec.
iter 308410 || Loss: 0.6968 || timer: 0.0909 sec.
iter 308420 || Loss: 0.8163 || timer: 0.0921 sec.
iter 308430 || Loss: 0.6028 || timer: 0.0918 sec.
iter 308440 || Loss: 0.6319 || timer: 0.0209 sec.
iter 308450 || Loss: 2.2137 || timer: 0.0890 sec.
iter 308460 || Loss: 0.7335 || timer: 0.0924 sec.
iter 308470 || Loss: 0.9147 || timer: 0.0839 sec.
iter 308480 || Loss: 0.9601 || timer: 0.0821 sec.
iter 308490 || Loss: 0.5814 || timer: 0.0897 sec.
iter 308500 || Loss: 0.6681 || timer: 0.0936 sec.
iter 308510 || Loss: 0.7601 || timer: 0.0915 sec.
iter 308520 || Loss: 0.9709 || timer: 0.0854 sec.
iter 308530 || Loss: 0.9091 || timer: 0.0879 sec.
iter 308540 || Loss: 0.5876 || timer: 0.0992 sec.
iter 308550 || Loss: 0.6710 || timer: 0.0903 sec.
iter 308560 || Loss: 0.6216 || timer: 0.1074 sec.
iter 308570 || Loss: 0.7423 || timer: 0.0931 sec.
iter 308580 || Loss: 0.9747 || timer: 0.0837 sec.
iter 308590 || Loss: 0.9007 || timer: 0.1020 sec.
iter 308600 || Loss: 0.6109 || timer: 0.1367 sec.
iter 308610 || Loss: 0.5890 || timer: 0.0877 sec.
iter 308620 || Loss: 0.8369 || timer: 0.0834 sec.
iter 308630 || Loss: 0.7901 || timer: 0.0956 sec.
iter 308640 || Loss: 0.6971 || timer: 0.0861 sec.
iter 308650 || Loss: 0.6664 || timer: 0.0907 sec.
iter 308660 || Loss: 0.6187 || timer: 0.0884 sec.
iter 308670 || Loss: 0.8122 || timer: 0.0895 sec.
iter 308680 || Loss: 0.7752 || timer: 0.1127 sec.
iter 308690 || Loss: 0.7303 || timer: 0.0908 sec.
iter 308700 || Loss: 0.6488 || timer: 0.0902 sec.
iter 308710 || Loss: 0.7137 || timer: 0.0845 sec.
iter 308720 || Loss: 0.5775 || timer: 0.0911 sec.
iter 308730 || Loss: 0.8269 || timer: 0.0880 sec.
iter 308740 || Loss: 0.8689 || timer: 0.0886 sec.
iter 308750 || Loss: 0.7241 || timer: 0.0903 sec.
iter 308760 || Loss: 0.7843 || timer: 0.0929 sec.
iter 308770 || Loss: 0.6278 || timer: 0.0278 sec.
iter 308780 || Loss: 0.3165 || timer: 0.0833 sec.
iter 308790 || Loss: 0.5492 || timer: 0.1039 sec.
iter 308800 || Loss: 0.4303 || timer: 0.0937 sec.
iter 308810 || Loss: 0.5718 || timer: 0.1050 sec.
iter 308820 || Loss: 0.8816 || timer: 0.0825 sec.
iter 308830 || Loss: 0.5232 || timer: 0.1004 sec.
iter 308840 || Loss: 0.5609 || timer: 0.0912 sec.
iter 308850 || Loss: 0.6904 || timer: 0.0923 sec.
iter 308860 || Loss: 0.5766 || timer: 0.0892 sec.
iter 308870 || Loss: 0.8335 || timer: 0.1168 sec.
iter 308880 || Loss: 0.6069 || timer: 0.0836 sec.
iter 308890 || Loss: 0.6762 || timer: 0.0901 sec.
iter 308900 || Loss: 0.7077 || timer: 0.0917 sec.
iter 308910 || Loss: 0.5885 || timer: 0.0833 sec.
iter 308920 || Loss: 0.9755 || timer: 0.0868 sec.
iter 308930 || Loss: 0.7024 || timer: 0.0882 sec.
iter 308940 || Loss: 0.7371 || timer: 0.0824 sec.
iter 308950 || Loss: 0.6166 || timer: 0.1312 sec.
iter 308960 || Loss: 0.6254 || timer: 0.0896 sec.
iter 308970 || Loss: 0.8451 || timer: 0.0915 sec.
iter 308980 || Loss: 0.8023 || timer: 0.0898 sec.
iter 308990 || Loss: 0.8486 || timer: 0.0920 sec.
iter 309000 || Loss: 0.5023 || timer: 0.0909 sec.
iter 309010 || Loss: 0.7783 || timer: 0.0910 sec.
iter 309020 || Loss: 0.4775 || timer: 0.0903 sec.
iter 309030 || Loss: 0.6876 || timer: 0.0999 sec.
iter 309040 || Loss: 0.8063 || timer: 0.0935 sec.
iter 309050 || Loss: 0.8013 || timer: 0.1054 sec.
iter 309060 || Loss: 0.7643 || timer: 0.0843 sec.
iter 309070 || Loss: 1.3499 || timer: 0.1165 sec.
iter 309080 || Loss: 0.7572 || timer: 0.0855 sec.
iter 309090 || Loss: 0.7796 || timer: 0.0837 sec.
iter 309100 || Loss: 0.4552 || timer: 0.0268 sec.
iter 309110 || Loss: 0.3898 || timer: 0.0831 sec.
iter 309120 || Loss: 0.7371 || timer: 0.0914 sec.
iter 309130 || Loss: 0.7630 || timer: 0.0926 sec.
iter 309140 || Loss: 0.5040 || timer: 0.1030 sec.
iter 309150 || Loss: 0.7470 || timer: 0.0826 sec.
iter 309160 || Loss: 0.6685 || timer: 0.0835 sec.
iter 309170 || Loss: 0.5051 || timer: 0.0894 sec.
iter 309180 || Loss: 0.7689 || timer: 0.0871 sec.
iter 309190 || Loss: 0.7877 || timer: 0.0995 sec.
iter 309200 || Loss: 0.8633 || timer: 0.1223 sec.
iter 309210 || Loss: 0.9452 || timer: 0.0878 sec.
iter 309220 || Loss: 1.0085 || timer: 0.0887 sec.
iter 309230 || Loss: 0.8058 || timer: 0.0837 sec.
iter 309240 || Loss: 1.0457 || timer: 0.0893 sec.
iter 309250 || Loss: 0.7960 || timer: 0.1096 sec.
iter 309260 || Loss: 0.6020 || timer: 0.0916 sec.
iter 309270 || Loss: 0.6724 || timer: 0.0922 sec.
iter 309280 || Loss: 0.9405 || timer: 0.0912 sec.
iter 309290 || Loss: 0.6651 || timer: 0.1037 sec.
iter 309300 || Loss: 0.7768 || timer: 0.0888 sec.
iter 309310 || Loss: 0.6395 || timer: 0.0922 sec.
iter 309320 || Loss: 0.6569 || timer: 0.0903 sec.
iter 309330 || Loss: 0.5825 || timer: 0.0833 sec.
iter 309340 || Loss: 0.6589 || timer: 0.0887 sec.
iter 309350 || Loss: 0.7754 || timer: 0.0895 sec.
iter 309360 || Loss: 0.8800 || timer: 0.1311 sec.
iter 309370 || Loss: 0.7627 || timer: 0.0825 sec.
iter 309380 || Loss: 0.7857 || timer: 0.1093 sec.
iter 309390 || Loss: 0.5151 || timer: 0.0820 sec.
iter 309400 || Loss: 0.6928 || timer: 0.0932 sec.
iter 309410 || Loss: 0.7810 || timer: 0.0895 sec.
iter 309420 || Loss: 1.1812 || timer: 0.0870 sec.
iter 309430 || Loss: 0.4865 || timer: 0.0222 sec.
iter 309440 || Loss: 0.6402 || timer: 0.1002 sec.
iter 309450 || Loss: 0.5698 || timer: 0.0981 sec.
iter 309460 || Loss: 0.6555 || timer: 0.0848 sec.
iter 309470 || Loss: 0.6212 || timer: 0.0915 sec.
iter 309480 || Loss: 0.6060 || timer: 0.1094 sec.
iter 309490 || Loss: 0.6436 || timer: 0.0898 sec.
iter 309500 || Loss: 0.5141 || timer: 0.0777 sec.
iter 309510 || Loss: 0.5838 || timer: 0.0761 sec.
iter 309520 || Loss: 0.5568 || timer: 0.0824 sec.
iter 309530 || Loss: 0.7073 || timer: 0.1119 sec.
iter 309540 || Loss: 0.6085 || timer: 0.0905 sec.
iter 309550 || Loss: 0.6639 || timer: 0.0863 sec.
iter 309560 || Loss: 0.7237 || timer: 0.0915 sec.
iter 309570 || Loss: 0.8054 || timer: 0.0826 sec.
iter 309580 || Loss: 0.6592 || timer: 0.0854 sec.
iter 309590 || Loss: 0.8325 || timer: 0.0975 sec.
iter 309600 || Loss: 0.8672 || timer: 0.1076 sec.
iter 309610 || Loss: 0.8092 || timer: 0.0849 sec.
iter 309620 || Loss: 0.6852 || timer: 0.1053 sec.
iter 309630 || Loss: 0.7337 || timer: 0.1027 sec.
iter 309640 || Loss: 1.0871 || timer: 0.0828 sec.
iter 309650 || Loss: 0.7205 || timer: 0.0973 sec.
iter 309660 || Loss: 0.7398 || timer: 0.0917 sec.
iter 309670 || Loss: 1.1316 || timer: 0.0937 sec.
iter 309680 || Loss: 0.7592 || timer: 0.0876 sec.
iter 309690 || Loss: 0.4839 || timer: 0.1021 sec.
iter 309700 || Loss: 1.1376 || timer: 0.0863 sec.
iter 309710 || Loss: 0.4897 || timer: 0.1336 sec.
iter 309720 || Loss: 0.8735 || timer: 0.0915 sec.
iter 309730 || Loss: 0.6292 || timer: 0.0894 sec.
iter 309740 || Loss: 0.7260 || timer: 0.1140 sec.
iter 309750 || Loss: 0.7760 || timer: 0.0897 sec.
iter 309760 || Loss: 0.5741 || timer: 0.0192 sec.
iter 309770 || Loss: 1.1631 || timer: 0.1065 sec.
iter 309780 || Loss: 0.6219 || timer: 0.0912 sec.
iter 309790 || Loss: 0.6558 || timer: 0.0872 sec.
iter 309800 || Loss: 0.7064 || timer: 0.0930 sec.
iter 309810 || Loss: 0.8547 || timer: 0.0998 sec.
iter 309820 || Loss: 0.6178 || timer: 0.0905 sec.
iter 309830 || Loss: 0.7857 || timer: 0.0902 sec.
iter 309840 || Loss: 1.2326 || timer: 0.0890 sec.
iter 309850 || Loss: 0.7527 || timer: 0.0910 sec.
iter 309860 || Loss: 0.8866 || timer: 0.1456 sec.
iter 309870 || Loss: 0.7538 || timer: 0.0942 sec.
iter 309880 || Loss: 0.6598 || timer: 0.0845 sec.
iter 309890 || Loss: 0.6213 || timer: 0.0892 sec.
iter 309900 || Loss: 0.8386 || timer: 0.0925 sec.
iter 309910 || Loss: 0.5838 || timer: 0.1029 sec.
iter 309920 || Loss: 0.9831 || timer: 0.0913 sec.
iter 309930 || Loss: 0.8235 || timer: 0.0904 sec.
iter 309940 || Loss: 0.4087 || timer: 0.0919 sec.
iter 309950 || Loss: 1.0332 || timer: 0.0918 sec.
iter 309960 || Loss: 0.5547 || timer: 0.0876 sec.
iter 309970 || Loss: 0.7108 || timer: 0.0908 sec.
iter 309980 || Loss: 0.6353 || timer: 0.1115 sec.
iter 309990 || Loss: 0.8238 || timer: 0.0870 sec.
iter 310000 || Loss: 0.7788 || Saving state, iter: 310000
timer: 0.0850 sec.
iter 310010 || Loss: 0.7521 || timer: 0.0918 sec.
iter 310020 || Loss: 0.9097 || timer: 0.0844 sec.
iter 310030 || Loss: 0.6069 || timer: 0.0860 sec.
iter 310040 || Loss: 0.6279 || timer: 0.0926 sec.
iter 310050 || Loss: 0.6371 || timer: 0.0828 sec.
iter 310060 || Loss: 0.5603 || timer: 0.0961 sec.
iter 310070 || Loss: 0.7323 || timer: 0.0937 sec.
iter 310080 || Loss: 0.7296 || timer: 0.0860 sec.
iter 310090 || Loss: 0.8524 || timer: 0.0238 sec.
iter 310100 || Loss: 0.4467 || timer: 0.0945 sec.
iter 310110 || Loss: 0.4666 || timer: 0.1189 sec.
iter 310120 || Loss: 0.5185 || timer: 0.1027 sec.
iter 310130 || Loss: 1.1763 || timer: 0.0890 sec.
iter 310140 || Loss: 0.7543 || timer: 0.0895 sec.
iter 310150 || Loss: 0.6675 || timer: 0.0911 sec.
iter 310160 || Loss: 0.5077 || timer: 0.1137 sec.
iter 310170 || Loss: 0.8085 || timer: 0.0832 sec.
iter 310180 || Loss: 0.7980 || timer: 0.0836 sec.
iter 310190 || Loss: 0.6635 || timer: 0.1101 sec.
iter 310200 || Loss: 0.4221 || timer: 0.0890 sec.
iter 310210 || Loss: 0.6763 || timer: 0.1046 sec.
iter 310220 || Loss: 0.6065 || timer: 0.0837 sec.
iter 310230 || Loss: 0.6192 || timer: 0.0903 sec.
iter 310240 || Loss: 0.7948 || timer: 0.0837 sec.
iter 310250 || Loss: 0.6548 || timer: 0.0905 sec.
iter 310260 || Loss: 0.7451 || timer: 0.1092 sec.
iter 310270 || Loss: 0.5933 || timer: 0.0834 sec.
iter 310280 || Loss: 0.7078 || timer: 0.0914 sec.
iter 310290 || Loss: 0.6886 || timer: 0.0841 sec.
iter 310300 || Loss: 0.7157 || timer: 0.0919 sec.
iter 310310 || Loss: 0.7928 || timer: 0.0897 sec.
iter 310320 || Loss: 0.7507 || timer: 0.0905 sec.
iter 310330 || Loss: 0.8905 || timer: 0.1024 sec.
iter 310340 || Loss: 0.5754 || timer: 0.0911 sec.
iter 310350 || Loss: 0.5872 || timer: 0.0926 sec.
iter 310360 || Loss: 0.6955 || timer: 0.0898 sec.
iter 310370 || Loss: 0.4855 || timer: 0.1025 sec.
iter 310380 || Loss: 0.7673 || timer: 0.0895 sec.
iter 310390 || Loss: 0.4668 || timer: 0.0827 sec.
iter 310400 || Loss: 0.9347 || timer: 0.0914 sec.
iter 310410 || Loss: 0.5928 || timer: 0.0836 sec.
iter 310420 || Loss: 0.4125 || timer: 0.0170 sec.
iter 310430 || Loss: 1.0990 || timer: 0.0826 sec.
iter 310440 || Loss: 0.7056 || timer: 0.0923 sec.
iter 310450 || Loss: 1.0423 || timer: 0.0848 sec.
iter 310460 || Loss: 0.7691 || timer: 0.0892 sec.
iter 310470 || Loss: 0.6730 || timer: 0.0824 sec.
iter 310480 || Loss: 0.7367 || timer: 0.0907 sec.
iter 310490 || Loss: 1.0009 || timer: 0.0904 sec.
iter 310500 || Loss: 0.9305 || timer: 0.0826 sec.
iter 310510 || Loss: 0.5223 || timer: 0.0908 sec.
iter 310520 || Loss: 0.4600 || timer: 0.1144 sec.
iter 310530 || Loss: 0.6233 || timer: 0.0988 sec.
iter 310540 || Loss: 0.8032 || timer: 0.0878 sec.
iter 310550 || Loss: 0.7854 || timer: 0.0889 sec.
iter 310560 || Loss: 0.6643 || timer: 0.0896 sec.
iter 310570 || Loss: 0.6696 || timer: 0.1114 sec.
iter 310580 || Loss: 0.6730 || timer: 0.0924 sec.
iter 310590 || Loss: 1.1969 || timer: 0.0855 sec.
iter 310600 || Loss: 0.7171 || timer: 0.1144 sec.
iter 310610 || Loss: 0.7167 || timer: 0.0897 sec.
iter 310620 || Loss: 0.5996 || timer: 0.0824 sec.
iter 310630 || Loss: 0.5394 || timer: 0.0898 sec.
iter 310640 || Loss: 0.5722 || timer: 0.0987 sec.
iter 310650 || Loss: 0.6457 || timer: 0.0918 sec.
iter 310660 || Loss: 0.8112 || timer: 0.0920 sec.
iter 310670 || Loss: 0.9281 || timer: 0.0984 sec.
iter 310680 || Loss: 1.0270 || timer: 0.0937 sec.
iter 310690 || Loss: 0.9098 || timer: 0.0898 sec.
iter 310700 || Loss: 0.5394 || timer: 0.0832 sec.
iter 310710 || Loss: 0.5124 || timer: 0.0965 sec.
iter 310720 || Loss: 0.8429 || timer: 0.0822 sec.
iter 310730 || Loss: 0.7721 || timer: 0.0928 sec.
iter 310740 || Loss: 0.9464 || timer: 0.0912 sec.
iter 310750 || Loss: 0.7776 || timer: 0.0276 sec.
iter 310760 || Loss: 0.3036 || timer: 0.1047 sec.
iter 310770 || Loss: 0.8623 || timer: 0.0899 sec.
iter 310780 || Loss: 0.7576 || timer: 0.0907 sec.
iter 310790 || Loss: 0.6235 || timer: 0.0824 sec.
iter 310800 || Loss: 0.8753 || timer: 0.0997 sec.
iter 310810 || Loss: 0.4825 || timer: 0.0906 sec.
iter 310820 || Loss: 0.6150 || timer: 0.0895 sec.
iter 310830 || Loss: 0.4931 || timer: 0.0829 sec.
iter 310840 || Loss: 0.6351 || timer: 0.0883 sec.
iter 310850 || Loss: 0.6213 || timer: 0.0982 sec.
iter 310860 || Loss: 0.9346 || timer: 0.0902 sec.
iter 310870 || Loss: 0.7679 || timer: 0.1032 sec.
iter 310880 || Loss: 0.8657 || timer: 0.0929 sec.
iter 310890 || Loss: 0.6454 || timer: 0.0918 sec.
iter 310900 || Loss: 0.7515 || timer: 0.0908 sec.
iter 310910 || Loss: 0.7565 || timer: 0.0908 sec.
iter 310920 || Loss: 0.4881 || timer: 0.1201 sec.
iter 310930 || Loss: 0.6741 || timer: 0.0847 sec.
iter 310940 || Loss: 0.6059 || timer: 0.0916 sec.
iter 310950 || Loss: 0.6504 || timer: 0.0826 sec.
iter 310960 || Loss: 0.6685 || timer: 0.0930 sec.
iter 310970 || Loss: 0.6721 || timer: 0.0900 sec.
iter 310980 || Loss: 0.6215 || timer: 0.0908 sec.
iter 310990 || Loss: 0.7175 || timer: 0.1308 sec.
iter 311000 || Loss: 0.5856 || timer: 0.1137 sec.
iter 311010 || Loss: 0.6839 || timer: 0.0976 sec.
iter 311020 || Loss: 0.9061 || timer: 0.0922 sec.
iter 311030 || Loss: 0.6103 || timer: 0.1073 sec.
iter 311040 || Loss: 0.8554 || timer: 0.0921 sec.
iter 311050 || Loss: 0.5044 || timer: 0.1035 sec.
iter 311060 || Loss: 0.5940 || timer: 0.0906 sec.
iter 311070 || Loss: 0.7134 || timer: 0.0907 sec.
iter 311080 || Loss: 0.6913 || timer: 0.0192 sec.
iter 311090 || Loss: 0.4605 || timer: 0.1201 sec.
iter 311100 || Loss: 0.4979 || timer: 0.0926 sec.
iter 311110 || Loss: 0.9123 || timer: 0.0829 sec.
iter 311120 || Loss: 0.5823 || timer: 0.0876 sec.
iter 311130 || Loss: 0.8604 || timer: 0.1057 sec.
iter 311140 || Loss: 0.7950 || timer: 0.1028 sec.
iter 311150 || Loss: 0.7186 || timer: 0.0895 sec.
iter 311160 || Loss: 0.5077 || timer: 0.0826 sec.
iter 311170 || Loss: 0.4769 || timer: 0.0836 sec.
iter 311180 || Loss: 0.8645 || timer: 0.1099 sec.
iter 311190 || Loss: 0.4742 || timer: 0.1055 sec.
iter 311200 || Loss: 0.7683 || timer: 0.0904 sec.
iter 311210 || Loss: 0.7010 || timer: 0.0821 sec.
iter 311220 || Loss: 0.8980 || timer: 0.0828 sec.
iter 311230 || Loss: 0.6096 || timer: 0.0900 sec.
iter 311240 || Loss: 0.9659 || timer: 0.0774 sec.
iter 311250 || Loss: 0.8566 || timer: 0.1013 sec.
iter 311260 || Loss: 0.7726 || timer: 0.0899 sec.
iter 311270 || Loss: 0.8252 || timer: 0.0910 sec.
iter 311280 || Loss: 0.6044 || timer: 0.0910 sec.
iter 311290 || Loss: 0.8723 || timer: 0.1065 sec.
iter 311300 || Loss: 0.5061 || timer: 0.0838 sec.
iter 311310 || Loss: 0.4628 || timer: 0.0906 sec.
iter 311320 || Loss: 0.5765 || timer: 0.0927 sec.
iter 311330 || Loss: 0.6690 || timer: 0.0908 sec.
iter 311340 || Loss: 0.5980 || timer: 0.0921 sec.
iter 311350 || Loss: 0.7366 || timer: 0.0944 sec.
iter 311360 || Loss: 0.4149 || timer: 0.0950 sec.
iter 311370 || Loss: 0.5990 || timer: 0.0820 sec.
iter 311380 || Loss: 0.7670 || timer: 0.0765 sec.
iter 311390 || Loss: 0.6105 || timer: 0.0832 sec.
iter 311400 || Loss: 0.7980 || timer: 0.0832 sec.
iter 311410 || Loss: 0.5400 || timer: 0.0178 sec.
iter 311420 || Loss: 0.2249 || timer: 0.0855 sec.
iter 311430 || Loss: 0.6115 || timer: 0.0887 sec.
iter 311440 || Loss: 0.5665 || timer: 0.0905 sec.
iter 311450 || Loss: 0.9300 || timer: 0.0903 sec.
iter 311460 || Loss: 0.7606 || timer: 0.0899 sec.
iter 311470 || Loss: 0.5292 || timer: 0.0828 sec.
iter 311480 || Loss: 0.7682 || timer: 0.0931 sec.
iter 311490 || Loss: 1.0302 || timer: 0.0884 sec.
iter 311500 || Loss: 0.7093 || timer: 0.0883 sec.
iter 311510 || Loss: 0.5848 || timer: 0.1075 sec.
iter 311520 || Loss: 0.3914 || timer: 0.0986 sec.
iter 311530 || Loss: 0.5628 || timer: 0.1049 sec.
iter 311540 || Loss: 0.7294 || timer: 0.0832 sec.
iter 311550 || Loss: 0.7830 || timer: 0.0857 sec.
iter 311560 || Loss: 0.8117 || timer: 0.0834 sec.
iter 311570 || Loss: 0.4750 || timer: 0.0837 sec.
iter 311580 || Loss: 0.6305 || timer: 0.1013 sec.
iter 311590 || Loss: 0.8090 || timer: 0.0832 sec.
iter 311600 || Loss: 0.9793 || timer: 0.0896 sec.
iter 311610 || Loss: 0.8262 || timer: 0.0840 sec.
iter 311620 || Loss: 0.6718 || timer: 0.0832 sec.
iter 311630 || Loss: 0.7710 || timer: 0.0835 sec.
iter 311640 || Loss: 0.9670 || timer: 0.0898 sec.
iter 311650 || Loss: 0.6673 || timer: 0.0842 sec.
iter 311660 || Loss: 0.7114 || timer: 0.0933 sec.
iter 311670 || Loss: 0.6568 || timer: 0.0915 sec.
iter 311680 || Loss: 0.6138 || timer: 0.0924 sec.
iter 311690 || Loss: 1.0554 || timer: 0.0902 sec.
iter 311700 || Loss: 0.7156 || timer: 0.1064 sec.
iter 311710 || Loss: 0.5472 || timer: 0.1003 sec.
iter 311720 || Loss: 0.8076 || timer: 0.0909 sec.
iter 311730 || Loss: 0.6890 || timer: 0.0911 sec.
iter 311740 || Loss: 0.7593 || timer: 0.0262 sec.
iter 311750 || Loss: 0.4615 || timer: 0.0831 sec.
iter 311760 || Loss: 0.8458 || timer: 0.0874 sec.
iter 311770 || Loss: 0.5598 || timer: 0.0904 sec.
iter 311780 || Loss: 0.9713 || timer: 0.0832 sec.
iter 311790 || Loss: 0.6833 || timer: 0.0895 sec.
iter 311800 || Loss: 0.5667 || timer: 0.0826 sec.
iter 311810 || Loss: 0.9082 || timer: 0.0827 sec.
iter 311820 || Loss: 0.6593 || timer: 0.0934 sec.
iter 311830 || Loss: 0.6639 || timer: 0.0832 sec.
iter 311840 || Loss: 0.8519 || timer: 0.1220 sec.
iter 311850 || Loss: 0.9421 || timer: 0.1121 sec.
iter 311860 || Loss: 0.4338 || timer: 0.0929 sec.
iter 311870 || Loss: 1.0264 || timer: 0.0873 sec.
iter 311880 || Loss: 0.7344 || timer: 0.0896 sec.
iter 311890 || Loss: 0.6071 || timer: 0.1010 sec.
iter 311900 || Loss: 0.7539 || timer: 0.0832 sec.
iter 311910 || Loss: 0.7497 || timer: 0.0852 sec.
iter 311920 || Loss: 0.5829 || timer: 0.0916 sec.
iter 311930 || Loss: 0.7466 || timer: 0.0932 sec.
iter 311940 || Loss: 0.7538 || timer: 0.1070 sec.
iter 311950 || Loss: 0.7230 || timer: 0.0791 sec.
iter 311960 || Loss: 0.7992 || timer: 0.0905 sec.
iter 311970 || Loss: 0.5690 || timer: 0.0921 sec.
iter 311980 || Loss: 0.5317 || timer: 0.0937 sec.
iter 311990 || Loss: 0.6132 || timer: 0.0878 sec.
iter 312000 || Loss: 0.7617 || timer: 0.0882 sec.
iter 312010 || Loss: 0.6902 || timer: 0.0823 sec.
iter 312020 || Loss: 0.6550 || timer: 0.0921 sec.
iter 312030 || Loss: 0.7657 || timer: 0.0921 sec.
iter 312040 || Loss: 0.7522 || timer: 0.0919 sec.
iter 312050 || Loss: 0.5988 || timer: 0.0925 sec.
iter 312060 || Loss: 0.6558 || timer: 0.0837 sec.
iter 312070 || Loss: 0.6586 || timer: 0.0192 sec.
iter 312080 || Loss: 0.4442 || timer: 0.0978 sec.
iter 312090 || Loss: 0.6983 || timer: 0.0903 sec.
iter 312100 || Loss: 0.4442 || timer: 0.0838 sec.
iter 312110 || Loss: 0.5613 || timer: 0.0923 sec.
iter 312120 || Loss: 0.6800 || timer: 0.0835 sec.
iter 312130 || Loss: 0.6169 || timer: 0.0861 sec.
iter 312140 || Loss: 0.7792 || timer: 0.0907 sec.
iter 312150 || Loss: 0.7555 || timer: 0.1066 sec.
iter 312160 || Loss: 0.6416 || timer: 0.1231 sec.
iter 312170 || Loss: 1.0709 || timer: 0.0988 sec.
iter 312180 || Loss: 0.6848 || timer: 0.1041 sec.
iter 312190 || Loss: 0.7491 || timer: 0.0929 sec.
iter 312200 || Loss: 0.6027 || timer: 0.0845 sec.
iter 312210 || Loss: 0.5047 || timer: 0.1156 sec.
iter 312220 || Loss: 0.6323 || timer: 0.0957 sec.
iter 312230 || Loss: 0.7161 || timer: 0.0835 sec.
iter 312240 || Loss: 0.8905 || timer: 0.0901 sec.
iter 312250 || Loss: 0.7389 || timer: 0.1136 sec.
iter 312260 || Loss: 0.8206 || timer: 0.0896 sec.
iter 312270 || Loss: 0.8793 || timer: 0.0862 sec.
iter 312280 || Loss: 0.7672 || timer: 0.0917 sec.
iter 312290 || Loss: 0.6668 || timer: 0.0819 sec.
iter 312300 || Loss: 0.6013 || timer: 0.0739 sec.
iter 312310 || Loss: 0.9553 || timer: 0.0840 sec.
iter 312320 || Loss: 0.5653 || timer: 0.0980 sec.
iter 312330 || Loss: 0.6551 || timer: 0.0832 sec.
iter 312340 || Loss: 0.5772 || timer: 0.0846 sec.
iter 312350 || Loss: 0.5950 || timer: 0.0932 sec.
iter 312360 || Loss: 0.7362 || timer: 0.0890 sec.
iter 312370 || Loss: 0.5991 || timer: 0.0824 sec.
iter 312380 || Loss: 0.9574 || timer: 0.1051 sec.
iter 312390 || Loss: 0.8998 || timer: 0.1042 sec.
iter 312400 || Loss: 0.6661 || timer: 0.0269 sec.
iter 312410 || Loss: 0.5220 || timer: 0.1072 sec.
iter 312420 || Loss: 0.5230 || timer: 0.0977 sec.
iter 312430 || Loss: 0.7190 || timer: 0.0857 sec.
iter 312440 || Loss: 0.7411 || timer: 0.1038 sec.
iter 312450 || Loss: 0.6104 || timer: 0.0895 sec.
iter 312460 || Loss: 0.5542 || timer: 0.0889 sec.
iter 312470 || Loss: 0.5789 || timer: 0.0900 sec.
iter 312480 || Loss: 0.6458 || timer: 0.0938 sec.
iter 312490 || Loss: 0.7132 || timer: 0.0865 sec.
iter 312500 || Loss: 0.9803 || timer: 0.1190 sec.
iter 312510 || Loss: 0.4799 || timer: 0.0957 sec.
iter 312520 || Loss: 0.4318 || timer: 0.0931 sec.
iter 312530 || Loss: 0.6277 || timer: 0.0825 sec.
iter 312540 || Loss: 0.5650 || timer: 0.1089 sec.
iter 312550 || Loss: 0.7462 || timer: 0.0932 sec.
iter 312560 || Loss: 0.5652 || timer: 0.1010 sec.
iter 312570 || Loss: 0.6505 || timer: 0.0816 sec.
iter 312580 || Loss: 0.5336 || timer: 0.0897 sec.
iter 312590 || Loss: 0.7626 || timer: 0.1102 sec.
iter 312600 || Loss: 0.4104 || timer: 0.1107 sec.
iter 312610 || Loss: 0.7529 || timer: 0.0862 sec.
iter 312620 || Loss: 0.7869 || timer: 0.1088 sec.
iter 312630 || Loss: 0.8568 || timer: 0.0918 sec.
iter 312640 || Loss: 0.6986 || timer: 0.0856 sec.
iter 312650 || Loss: 0.7714 || timer: 0.1086 sec.
iter 312660 || Loss: 0.6769 || timer: 0.0952 sec.
iter 312670 || Loss: 0.7425 || timer: 0.1065 sec.
iter 312680 || Loss: 0.8507 || timer: 0.0833 sec.
iter 312690 || Loss: 0.7555 || timer: 0.0836 sec.
iter 312700 || Loss: 0.5067 || timer: 0.0918 sec.
iter 312710 || Loss: 0.6446 || timer: 0.0996 sec.
iter 312720 || Loss: 0.6217 || timer: 0.0922 sec.
iter 312730 || Loss: 0.7245 || timer: 0.0196 sec.
iter 312740 || Loss: 0.9696 || timer: 0.0839 sec.
iter 312750 || Loss: 0.5998 || timer: 0.0836 sec.
iter 312760 || Loss: 0.6857 || timer: 0.0888 sec.
iter 312770 || Loss: 0.6919 || timer: 0.0905 sec.
iter 312780 || Loss: 0.8993 || timer: 0.0922 sec.
iter 312790 || Loss: 0.4051 || timer: 0.0981 sec.
iter 312800 || Loss: 0.7696 || timer: 0.0956 sec.
iter 312810 || Loss: 0.7265 || timer: 0.0928 sec.
iter 312820 || Loss: 0.8256 || timer: 0.0938 sec.
iter 312830 || Loss: 0.5383 || timer: 0.1214 sec.
iter 312840 || Loss: 0.6984 || timer: 0.0844 sec.
iter 312850 || Loss: 0.8240 || timer: 0.0907 sec.
iter 312860 || Loss: 0.6810 || timer: 0.0921 sec.
iter 312870 || Loss: 0.4868 || timer: 0.0901 sec.
iter 312880 || Loss: 0.7392 || timer: 0.0897 sec.
iter 312890 || Loss: 0.5976 || timer: 0.1056 sec.
iter 312900 || Loss: 0.6084 || timer: 0.0906 sec.
iter 312910 || Loss: 0.8013 || timer: 0.0848 sec.
iter 312920 || Loss: 0.5261 || timer: 0.0884 sec.
iter 312930 || Loss: 0.4817 || timer: 0.1055 sec.
iter 312940 || Loss: 0.7429 || timer: 0.1004 sec.
iter 312950 || Loss: 1.1498 || timer: 0.0928 sec.
iter 312960 || Loss: 0.5954 || timer: 0.0923 sec.
iter 312970 || Loss: 0.7547 || timer: 0.0915 sec.
iter 312980 || Loss: 0.5525 || timer: 0.0893 sec.
iter 312990 || Loss: 0.7574 || timer: 0.0834 sec.
iter 313000 || Loss: 0.6403 || timer: 0.1038 sec.
iter 313010 || Loss: 0.7112 || timer: 0.1047 sec.
iter 313020 || Loss: 0.7384 || timer: 0.0918 sec.
iter 313030 || Loss: 0.5384 || timer: 0.0838 sec.
iter 313040 || Loss: 0.8303 || timer: 0.1131 sec.
iter 313050 || Loss: 0.5652 || timer: 0.0932 sec.
iter 313060 || Loss: 0.7859 || timer: 0.0241 sec.
iter 313070 || Loss: 0.3050 || timer: 0.0970 sec.
iter 313080 || Loss: 0.5937 || timer: 0.1178 sec.
iter 313090 || Loss: 1.0519 || timer: 0.0820 sec.
iter 313100 || Loss: 0.5092 || timer: 0.1079 sec.
iter 313110 || Loss: 0.5886 || timer: 0.0837 sec.
iter 313120 || Loss: 0.7050 || timer: 0.0836 sec.
iter 313130 || Loss: 0.4944 || timer: 0.0916 sec.
iter 313140 || Loss: 0.7312 || timer: 0.0883 sec.
iter 313150 || Loss: 0.6925 || timer: 0.0922 sec.
iter 313160 || Loss: 0.7000 || timer: 0.0960 sec.
iter 313170 || Loss: 0.6248 || timer: 0.0831 sec.
iter 313180 || Loss: 0.7689 || timer: 0.0895 sec.
iter 313190 || Loss: 0.5763 || timer: 0.0911 sec.
iter 313200 || Loss: 0.5207 || timer: 0.0914 sec.
iter 313210 || Loss: 0.6329 || timer: 0.0927 sec.
iter 313220 || Loss: 0.8237 || timer: 0.1099 sec.
iter 313230 || Loss: 0.8620 || timer: 0.0915 sec.
iter 313240 || Loss: 0.6861 || timer: 0.0827 sec.
iter 313250 || Loss: 0.5844 || timer: 0.1228 sec.
iter 313260 || Loss: 0.5809 || timer: 0.0902 sec.
iter 313270 || Loss: 0.5994 || timer: 0.0930 sec.
iter 313280 || Loss: 0.6933 || timer: 0.1055 sec.
iter 313290 || Loss: 0.5686 || timer: 0.0883 sec.
iter 313300 || Loss: 0.5901 || timer: 0.1211 sec.
iter 313310 || Loss: 0.6250 || timer: 0.0897 sec.
iter 313320 || Loss: 0.7377 || timer: 0.0917 sec.
iter 313330 || Loss: 0.5720 || timer: 0.0904 sec.
iter 313340 || Loss: 1.0823 || timer: 0.0916 sec.
iter 313350 || Loss: 0.6437 || timer: 0.0842 sec.
iter 313360 || Loss: 1.0241 || timer: 0.0932 sec.
iter 313370 || Loss: 0.7628 || timer: 0.1236 sec.
iter 313380 || Loss: 0.6869 || timer: 0.0894 sec.
iter 313390 || Loss: 0.5651 || timer: 0.0264 sec.
iter 313400 || Loss: 1.2523 || timer: 0.1023 sec.
iter 313410 || Loss: 0.7469 || timer: 0.1189 sec.
iter 313420 || Loss: 0.6581 || timer: 0.0904 sec.
iter 313430 || Loss: 0.7858 || timer: 0.0834 sec.
iter 313440 || Loss: 0.4922 || timer: 0.0914 sec.
iter 313450 || Loss: 0.7201 || timer: 0.1008 sec.
iter 313460 || Loss: 0.6374 || timer: 0.0872 sec.
iter 313470 || Loss: 0.6081 || timer: 0.0934 sec.
iter 313480 || Loss: 0.7720 || timer: 0.1158 sec.
iter 313490 || Loss: 0.4770 || timer: 0.0990 sec.
iter 313500 || Loss: 0.7751 || timer: 0.0834 sec.
iter 313510 || Loss: 1.0788 || timer: 0.0881 sec.
iter 313520 || Loss: 0.7589 || timer: 0.0908 sec.
iter 313530 || Loss: 0.7888 || timer: 0.0826 sec.
iter 313540 || Loss: 0.6154 || timer: 0.0901 sec.
iter 313550 || Loss: 0.5388 || timer: 0.0842 sec.
iter 313560 || Loss: 0.8819 || timer: 0.0995 sec.
iter 313570 || Loss: 0.7641 || timer: 0.0837 sec.
iter 313580 || Loss: 0.8080 || timer: 0.0839 sec.
iter 313590 || Loss: 0.5825 || timer: 0.0945 sec.
iter 313600 || Loss: 0.5372 || timer: 0.1085 sec.
iter 313610 || Loss: 0.8557 || timer: 0.0833 sec.
iter 313620 || Loss: 1.0216 || timer: 0.0902 sec.
iter 313630 || Loss: 0.7715 || timer: 0.1046 sec.
iter 313640 || Loss: 0.5142 || timer: 0.0825 sec.
iter 313650 || Loss: 0.8652 || timer: 0.0918 sec.
iter 313660 || Loss: 0.6839 || timer: 0.1015 sec.
iter 313670 || Loss: 0.8271 || timer: 0.0836 sec.
iter 313680 || Loss: 0.7848 || timer: 0.1123 sec.
iter 313690 || Loss: 0.7026 || timer: 0.0829 sec.
iter 313700 || Loss: 0.8254 || timer: 0.0954 sec.
iter 313710 || Loss: 0.8475 || timer: 0.0842 sec.
iter 313720 || Loss: 0.6655 || timer: 0.0265 sec.
iter 313730 || Loss: 1.2422 || timer: 0.0903 sec.
iter 313740 || Loss: 0.8191 || timer: 0.0846 sec.
iter 313750 || Loss: 0.7860 || timer: 0.0894 sec.
iter 313760 || Loss: 0.6066 || timer: 0.0988 sec.
iter 313770 || Loss: 0.8383 || timer: 0.0893 sec.
iter 313780 || Loss: 0.5413 || timer: 0.0865 sec.
iter 313790 || Loss: 0.7936 || timer: 0.0858 sec.
iter 313800 || Loss: 0.6835 || timer: 0.0893 sec.
iter 313810 || Loss: 0.7087 || timer: 0.1424 sec.
iter 313820 || Loss: 0.8704 || timer: 0.1030 sec.
iter 313830 || Loss: 0.8039 || timer: 0.1104 sec.
iter 313840 || Loss: 0.5412 || timer: 0.0884 sec.
iter 313850 || Loss: 0.6749 || timer: 0.0910 sec.
iter 313860 || Loss: 0.6526 || timer: 0.0839 sec.
iter 313870 || Loss: 0.7577 || timer: 0.0911 sec.
iter 313880 || Loss: 0.6037 || timer: 0.0912 sec.
iter 313890 || Loss: 0.9944 || timer: 0.0825 sec.
iter 313900 || Loss: 0.7162 || timer: 0.0810 sec.
iter 313910 || Loss: 0.6521 || timer: 0.0906 sec.
iter 313920 || Loss: 0.7306 || timer: 0.0929 sec.
iter 313930 || Loss: 0.6433 || timer: 0.0891 sec.
iter 313940 || Loss: 0.7540 || timer: 0.1014 sec.
iter 313950 || Loss: 0.5962 || timer: 0.0853 sec.
iter 313960 || Loss: 0.7954 || timer: 0.0944 sec.
iter 313970 || Loss: 0.8033 || timer: 0.1054 sec.
iter 313980 || Loss: 0.6753 || timer: 0.1075 sec.
iter 313990 || Loss: 0.7967 || timer: 0.0937 sec.
iter 314000 || Loss: 0.6089 || timer: 0.0841 sec.
iter 314010 || Loss: 0.5336 || timer: 0.0833 sec.
iter 314020 || Loss: 0.7162 || timer: 0.1097 sec.
iter 314030 || Loss: 0.9604 || timer: 0.0924 sec.
iter 314040 || Loss: 0.5826 || timer: 0.0835 sec.
iter 314050 || Loss: 0.7644 || timer: 0.0286 sec.
iter 314060 || Loss: 0.2595 || timer: 0.1021 sec.
iter 314070 || Loss: 0.8081 || timer: 0.0907 sec.
iter 314080 || Loss: 0.7445 || timer: 0.0836 sec.
iter 314090 || Loss: 0.6042 || timer: 0.0823 sec.
iter 314100 || Loss: 0.5243 || timer: 0.1213 sec.
iter 314110 || Loss: 0.6002 || timer: 0.0896 sec.
iter 314120 || Loss: 1.1076 || timer: 0.0920 sec.
iter 314130 || Loss: 0.5690 || timer: 0.0895 sec.
iter 314140 || Loss: 0.6411 || timer: 0.0894 sec.
iter 314150 || Loss: 0.6023 || timer: 0.1034 sec.
iter 314160 || Loss: 0.7605 || timer: 0.0918 sec.
iter 314170 || Loss: 0.5823 || timer: 0.0896 sec.
iter 314180 || Loss: 0.7492 || timer: 0.0942 sec.
iter 314190 || Loss: 0.8990 || timer: 0.0845 sec.
iter 314200 || Loss: 0.6088 || timer: 0.0845 sec.
iter 314210 || Loss: 0.6808 || timer: 0.0834 sec.
iter 314220 || Loss: 0.8654 || timer: 0.0831 sec.
iter 314230 || Loss: 0.9409 || timer: 0.1137 sec.
iter 314240 || Loss: 0.4700 || timer: 0.1013 sec.
iter 314250 || Loss: 0.7630 || timer: 0.0831 sec.
iter 314260 || Loss: 0.5757 || timer: 0.0835 sec.
iter 314270 || Loss: 0.6052 || timer: 0.0925 sec.
iter 314280 || Loss: 0.9087 || timer: 0.0912 sec.
iter 314290 || Loss: 0.6550 || timer: 0.0837 sec.
iter 314300 || Loss: 0.7875 || timer: 0.1062 sec.
iter 314310 || Loss: 0.8509 || timer: 0.0754 sec.
iter 314320 || Loss: 0.7874 || timer: 0.0828 sec.
iter 314330 || Loss: 0.6512 || timer: 0.0827 sec.
iter 314340 || Loss: 0.6712 || timer: 0.0899 sec.
iter 314350 || Loss: 0.8838 || timer: 0.0902 sec.
iter 314360 || Loss: 0.7851 || timer: 0.0952 sec.
iter 314370 || Loss: 0.8306 || timer: 0.0907 sec.
iter 314380 || Loss: 0.7673 || timer: 0.0246 sec.
iter 314390 || Loss: 1.1051 || timer: 0.0864 sec.
iter 314400 || Loss: 0.4999 || timer: 0.1169 sec.
iter 314410 || Loss: 0.8899 || timer: 0.0837 sec.
iter 314420 || Loss: 0.6047 || timer: 0.0901 sec.
iter 314430 || Loss: 0.8252 || timer: 0.0859 sec.
iter 314440 || Loss: 0.7028 || timer: 0.0894 sec.
iter 314450 || Loss: 0.6353 || timer: 0.0921 sec.
iter 314460 || Loss: 0.7795 || timer: 0.0926 sec.
iter 314470 || Loss: 0.6312 || timer: 0.0851 sec.
iter 314480 || Loss: 0.6773 || timer: 0.0932 sec.
iter 314490 || Loss: 0.6374 || timer: 0.0865 sec.
iter 314500 || Loss: 1.1625 || timer: 0.0821 sec.
iter 314510 || Loss: 0.3813 || timer: 0.0839 sec.
iter 314520 || Loss: 0.5721 || timer: 0.0918 sec.
iter 314530 || Loss: 0.6236 || timer: 0.1004 sec.
iter 314540 || Loss: 0.6475 || timer: 0.1165 sec.
iter 314550 || Loss: 0.5629 || timer: 0.0854 sec.
iter 314560 || Loss: 0.6831 || timer: 0.0869 sec.
iter 314570 || Loss: 0.6365 || timer: 0.0847 sec.
iter 314580 || Loss: 0.6987 || timer: 0.0992 sec.
iter 314590 || Loss: 0.9638 || timer: 0.0827 sec.
iter 314600 || Loss: 0.5302 || timer: 0.0917 sec.
iter 314610 || Loss: 0.7586 || timer: 0.0905 sec.
iter 314620 || Loss: 0.6123 || timer: 0.0901 sec.
iter 314630 || Loss: 0.5831 || timer: 0.0892 sec.
iter 314640 || Loss: 0.8314 || timer: 0.0952 sec.
iter 314650 || Loss: 0.6559 || timer: 0.0827 sec.
iter 314660 || Loss: 0.4776 || timer: 0.0841 sec.
iter 314670 || Loss: 0.7931 || timer: 0.0923 sec.
iter 314680 || Loss: 0.6007 || timer: 0.0826 sec.
iter 314690 || Loss: 0.9210 || timer: 0.0920 sec.
iter 314700 || Loss: 0.6017 || timer: 0.0896 sec.
iter 314710 || Loss: 0.7584 || timer: 0.0203 sec.
iter 314720 || Loss: 0.5366 || timer: 0.0937 sec.
iter 314730 || Loss: 0.7904 || timer: 0.0828 sec.
iter 314740 || Loss: 0.7383 || timer: 0.0836 sec.
iter 314750 || Loss: 0.8076 || timer: 0.0829 sec.
iter 314760 || Loss: 0.5940 || timer: 0.1087 sec.
iter 314770 || Loss: 0.6213 || timer: 0.0902 sec.
iter 314780 || Loss: 0.7369 || timer: 0.0854 sec.
iter 314790 || Loss: 0.9471 || timer: 0.0836 sec.
iter 314800 || Loss: 0.7756 || timer: 0.0827 sec.
iter 314810 || Loss: 0.7671 || timer: 0.0997 sec.
iter 314820 || Loss: 0.9290 || timer: 0.1327 sec.
iter 314830 || Loss: 1.3226 || timer: 0.0903 sec.
iter 314840 || Loss: 0.6034 || timer: 0.0852 sec.
iter 314850 || Loss: 0.8128 || timer: 0.1016 sec.
iter 314860 || Loss: 0.6052 || timer: 0.0909 sec.
iter 314870 || Loss: 0.5057 || timer: 0.0841 sec.
iter 314880 || Loss: 0.8143 || timer: 0.1083 sec.
iter 314890 || Loss: 0.7766 || timer: 0.0906 sec.
iter 314900 || Loss: 0.8274 || timer: 0.0922 sec.
iter 314910 || Loss: 0.6914 || timer: 0.0826 sec.
iter 314920 || Loss: 0.7144 || timer: 0.0927 sec.
iter 314930 || Loss: 0.6795 || timer: 0.0856 sec.
iter 314940 || Loss: 0.6791 || timer: 0.0935 sec.
iter 314950 || Loss: 0.6482 || timer: 0.0941 sec.
iter 314960 || Loss: 0.7351 || timer: 0.0830 sec.
iter 314970 || Loss: 0.7213 || timer: 0.0888 sec.
iter 314980 || Loss: 0.5232 || timer: 0.0823 sec.
iter 314990 || Loss: 0.6032 || timer: 0.1015 sec.
iter 315000 || Loss: 0.8835 || Saving state, iter: 315000
timer: 0.0898 sec.
iter 315010 || Loss: 0.7138 || timer: 0.0876 sec.
iter 315020 || Loss: 0.7825 || timer: 0.1001 sec.
iter 315030 || Loss: 0.6952 || timer: 0.0898 sec.
iter 315040 || Loss: 0.6408 || timer: 0.0175 sec.
iter 315050 || Loss: 0.6794 || timer: 0.0829 sec.
iter 315060 || Loss: 0.5208 || timer: 0.1067 sec.
iter 315070 || Loss: 0.7177 || timer: 0.0925 sec.
iter 315080 || Loss: 0.6619 || timer: 0.1328 sec.
iter 315090 || Loss: 0.5650 || timer: 0.1336 sec.
iter 315100 || Loss: 1.1197 || timer: 0.1145 sec.
iter 315110 || Loss: 0.7924 || timer: 0.1073 sec.
iter 315120 || Loss: 0.5522 || timer: 0.0929 sec.
iter 315130 || Loss: 0.7524 || timer: 0.0891 sec.
iter 315140 || Loss: 0.5082 || timer: 0.0980 sec.
iter 315150 || Loss: 0.7840 || timer: 0.0876 sec.
iter 315160 || Loss: 0.5641 || timer: 0.0922 sec.
iter 315170 || Loss: 0.7007 || timer: 0.0958 sec.
iter 315180 || Loss: 0.7467 || timer: 0.1016 sec.
iter 315190 || Loss: 0.6668 || timer: 0.0900 sec.
iter 315200 || Loss: 0.6082 || timer: 0.0831 sec.
iter 315210 || Loss: 0.8751 || timer: 0.0970 sec.
iter 315220 || Loss: 0.5946 || timer: 0.1130 sec.
iter 315230 || Loss: 0.6393 || timer: 0.0841 sec.
iter 315240 || Loss: 0.6480 || timer: 0.0822 sec.
iter 315250 || Loss: 0.7497 || timer: 0.0972 sec.
iter 315260 || Loss: 0.7550 || timer: 0.0850 sec.
iter 315270 || Loss: 0.7056 || timer: 0.0840 sec.
iter 315280 || Loss: 0.7069 || timer: 0.0910 sec.
iter 315290 || Loss: 1.1288 || timer: 0.0892 sec.
iter 315300 || Loss: 1.0137 || timer: 0.0909 sec.
iter 315310 || Loss: 0.4447 || timer: 0.0988 sec.
iter 315320 || Loss: 0.6394 || timer: 0.0870 sec.
iter 315330 || Loss: 0.4641 || timer: 0.0828 sec.
iter 315340 || Loss: 0.6972 || timer: 0.0771 sec.
iter 315350 || Loss: 0.6933 || timer: 0.0902 sec.
iter 315360 || Loss: 0.6361 || timer: 0.0924 sec.
iter 315370 || Loss: 0.6329 || timer: 0.0288 sec.
iter 315380 || Loss: 0.3885 || timer: 0.0834 sec.
iter 315390 || Loss: 0.7831 || timer: 0.1073 sec.
iter 315400 || Loss: 0.9679 || timer: 0.0890 sec.
iter 315410 || Loss: 1.1246 || timer: 0.0830 sec.
iter 315420 || Loss: 0.6237 || timer: 0.0852 sec.
iter 315430 || Loss: 0.7139 || timer: 0.0903 sec.
iter 315440 || Loss: 0.4575 || timer: 0.0932 sec.
iter 315450 || Loss: 0.9525 || timer: 0.0897 sec.
iter 315460 || Loss: 0.9312 || timer: 0.0898 sec.
iter 315470 || Loss: 0.6890 || timer: 0.1024 sec.
iter 315480 || Loss: 0.7555 || timer: 0.0839 sec.
iter 315490 || Loss: 0.8941 || timer: 0.0902 sec.
iter 315500 || Loss: 0.8512 || timer: 0.0841 sec.
iter 315510 || Loss: 0.5388 || timer: 0.0901 sec.
iter 315520 || Loss: 0.9222 || timer: 0.0907 sec.
iter 315530 || Loss: 0.5793 || timer: 0.0827 sec.
iter 315540 || Loss: 0.9638 || timer: 0.0899 sec.
iter 315550 || Loss: 0.9276 || timer: 0.1022 sec.
iter 315560 || Loss: 0.5981 || timer: 0.0914 sec.
iter 315570 || Loss: 0.3908 || timer: 0.0956 sec.
iter 315580 || Loss: 0.4940 || timer: 0.0892 sec.
iter 315590 || Loss: 0.5791 || timer: 0.0850 sec.
iter 315600 || Loss: 0.7614 || timer: 0.0883 sec.
iter 315610 || Loss: 0.6646 || timer: 0.0899 sec.
iter 315620 || Loss: 0.7782 || timer: 0.0911 sec.
iter 315630 || Loss: 0.5502 || timer: 0.1124 sec.
iter 315640 || Loss: 0.9098 || timer: 0.0836 sec.
iter 315650 || Loss: 0.9636 || timer: 0.0913 sec.
iter 315660 || Loss: 0.7498 || timer: 0.0910 sec.
iter 315670 || Loss: 0.6169 || timer: 0.0860 sec.
iter 315680 || Loss: 0.5105 || timer: 0.1032 sec.
iter 315690 || Loss: 0.6405 || timer: 0.0917 sec.
iter 315700 || Loss: 0.8186 || timer: 0.0293 sec.
iter 315710 || Loss: 0.5468 || timer: 0.0940 sec.
iter 315720 || Loss: 0.5767 || timer: 0.0926 sec.
iter 315730 || Loss: 0.7554 || timer: 0.1097 sec.
iter 315740 || Loss: 0.5130 || timer: 0.0837 sec.
iter 315750 || Loss: 0.4390 || timer: 0.0923 sec.
iter 315760 || Loss: 0.5518 || timer: 0.1099 sec.
iter 315770 || Loss: 0.8280 || timer: 0.0922 sec.
iter 315780 || Loss: 0.6825 || timer: 0.1052 sec.
iter 315790 || Loss: 0.5633 || timer: 0.0890 sec.
iter 315800 || Loss: 0.9285 || timer: 0.1246 sec.
iter 315810 || Loss: 0.3979 || timer: 0.0923 sec.
iter 315820 || Loss: 0.5527 || timer: 0.0878 sec.
iter 315830 || Loss: 0.5508 || timer: 0.0890 sec.
iter 315840 || Loss: 1.2552 || timer: 0.0840 sec.
iter 315850 || Loss: 0.6409 || timer: 0.0837 sec.
iter 315860 || Loss: 0.6095 || timer: 0.0832 sec.
iter 315870 || Loss: 0.4543 || timer: 0.0885 sec.
iter 315880 || Loss: 0.5384 || timer: 0.0838 sec.
iter 315890 || Loss: 0.6884 || timer: 0.1182 sec.
iter 315900 || Loss: 0.8847 || timer: 0.1142 sec.
iter 315910 || Loss: 0.5992 || timer: 0.0973 sec.
iter 315920 || Loss: 0.6859 || timer: 0.0886 sec.
iter 315930 || Loss: 0.8252 || timer: 0.0835 sec.
iter 315940 || Loss: 0.9115 || timer: 0.0843 sec.
iter 315950 || Loss: 0.7328 || timer: 0.0887 sec.
iter 315960 || Loss: 0.6332 || timer: 0.0896 sec.
iter 315970 || Loss: 0.9305 || timer: 0.0911 sec.
iter 315980 || Loss: 0.6111 || timer: 0.0838 sec.
iter 315990 || Loss: 0.5896 || timer: 0.0912 sec.
iter 316000 || Loss: 0.8183 || timer: 0.0873 sec.
iter 316010 || Loss: 0.8880 || timer: 0.1070 sec.
iter 316020 || Loss: 0.6777 || timer: 0.0998 sec.
iter 316030 || Loss: 0.5789 || timer: 0.0309 sec.
iter 316040 || Loss: 0.0799 || timer: 0.1089 sec.
iter 316050 || Loss: 0.8978 || timer: 0.1086 sec.
iter 316060 || Loss: 0.5250 || timer: 0.0930 sec.
iter 316070 || Loss: 0.6069 || timer: 0.0865 sec.
iter 316080 || Loss: 0.5983 || timer: 0.0767 sec.
iter 316090 || Loss: 0.6256 || timer: 0.0916 sec.
iter 316100 || Loss: 0.5898 || timer: 0.1016 sec.
iter 316110 || Loss: 0.7180 || timer: 0.1059 sec.
iter 316120 || Loss: 0.6198 || timer: 0.1099 sec.
iter 316130 || Loss: 0.6617 || timer: 0.1107 sec.
iter 316140 || Loss: 0.6527 || timer: 0.1110 sec.
iter 316150 || Loss: 0.7596 || timer: 0.0847 sec.
iter 316160 || Loss: 0.6869 || timer: 0.1305 sec.
iter 316170 || Loss: 0.8878 || timer: 0.0905 sec.
iter 316180 || Loss: 0.9833 || timer: 0.0916 sec.
iter 316190 || Loss: 0.5877 || timer: 0.0837 sec.
iter 316200 || Loss: 0.6395 || timer: 0.0964 sec.
iter 316210 || Loss: 0.7828 || timer: 0.0944 sec.
iter 316220 || Loss: 0.7809 || timer: 0.0915 sec.
iter 316230 || Loss: 0.9328 || timer: 0.0897 sec.
iter 316240 || Loss: 0.9392 || timer: 0.0929 sec.
iter 316250 || Loss: 0.8714 || timer: 0.0897 sec.
iter 316260 || Loss: 0.8155 || timer: 0.0798 sec.
iter 316270 || Loss: 0.6735 || timer: 0.0853 sec.
iter 316280 || Loss: 0.6046 || timer: 0.0865 sec.
iter 316290 || Loss: 0.8583 || timer: 0.1442 sec.
iter 316300 || Loss: 1.0032 || timer: 0.0819 sec.
iter 316310 || Loss: 0.6248 || timer: 0.0822 sec.
iter 316320 || Loss: 0.7016 || timer: 0.0930 sec.
iter 316330 || Loss: 0.7808 || timer: 0.0825 sec.
iter 316340 || Loss: 0.6376 || timer: 0.0909 sec.
iter 316350 || Loss: 0.7397 || timer: 0.0911 sec.
iter 316360 || Loss: 0.8081 || timer: 0.0201 sec.
iter 316370 || Loss: 0.4658 || timer: 0.0847 sec.
iter 316380 || Loss: 0.5730 || timer: 0.0934 sec.
iter 316390 || Loss: 0.6987 || timer: 0.0940 sec.
iter 316400 || Loss: 0.6359 || timer: 0.0970 sec.
iter 316410 || Loss: 0.7240 || timer: 0.0932 sec.
iter 316420 || Loss: 0.6599 || timer: 0.0952 sec.
iter 316430 || Loss: 1.0418 || timer: 0.0946 sec.
iter 316440 || Loss: 0.6767 || timer: 0.0937 sec.
iter 316450 || Loss: 0.6567 || timer: 0.0927 sec.
iter 316460 || Loss: 0.9600 || timer: 0.1263 sec.
iter 316470 || Loss: 0.6477 || timer: 0.0877 sec.
iter 316480 || Loss: 0.5738 || timer: 0.0900 sec.
iter 316490 || Loss: 0.6719 || timer: 0.0831 sec.
iter 316500 || Loss: 0.8951 || timer: 0.0941 sec.
iter 316510 || Loss: 0.5413 || timer: 0.0925 sec.
iter 316520 || Loss: 0.6798 || timer: 0.0885 sec.
iter 316530 || Loss: 0.7137 || timer: 0.1001 sec.
iter 316540 || Loss: 0.5902 || timer: 0.0904 sec.
iter 316550 || Loss: 0.6225 || timer: 0.1049 sec.
iter 316560 || Loss: 0.8846 || timer: 0.0840 sec.
iter 316570 || Loss: 0.5870 || timer: 0.0897 sec.
iter 316580 || Loss: 0.9486 || timer: 0.0782 sec.
iter 316590 || Loss: 0.5394 || timer: 0.0842 sec.
iter 316600 || Loss: 0.5944 || timer: 0.0929 sec.
iter 316610 || Loss: 0.8251 || timer: 0.0828 sec.
iter 316620 || Loss: 0.7141 || timer: 0.0847 sec.
iter 316630 || Loss: 0.9844 || timer: 0.0874 sec.
iter 316640 || Loss: 0.5758 || timer: 0.0902 sec.
iter 316650 || Loss: 0.6745 || timer: 0.0898 sec.
iter 316660 || Loss: 0.4481 || timer: 0.0918 sec.
iter 316670 || Loss: 0.5827 || timer: 0.1032 sec.
iter 316680 || Loss: 0.6639 || timer: 0.0911 sec.
iter 316690 || Loss: 0.9193 || timer: 0.0152 sec.
iter 316700 || Loss: 1.0337 || timer: 0.0919 sec.
iter 316710 || Loss: 0.5762 || timer: 0.0825 sec.
iter 316720 || Loss: 0.4478 || timer: 0.1089 sec.
iter 316730 || Loss: 0.7739 || timer: 0.1306 sec.
iter 316740 || Loss: 0.5807 || timer: 0.1023 sec.
iter 316750 || Loss: 0.5471 || timer: 0.0767 sec.
iter 316760 || Loss: 0.5489 || timer: 0.0928 sec.
iter 316770 || Loss: 0.6274 || timer: 0.0824 sec.
iter 316780 || Loss: 0.4933 || timer: 0.0985 sec.
iter 316790 || Loss: 0.6015 || timer: 0.1056 sec.
iter 316800 || Loss: 0.7668 || timer: 0.1090 sec.
iter 316810 || Loss: 0.8024 || timer: 0.1050 sec.
iter 316820 || Loss: 0.5055 || timer: 0.0855 sec.
iter 316830 || Loss: 0.8587 || timer: 0.1193 sec.
iter 316840 || Loss: 0.5388 || timer: 0.1282 sec.
iter 316850 || Loss: 0.7828 || timer: 0.0770 sec.
iter 316860 || Loss: 0.7398 || timer: 0.0890 sec.
iter 316870 || Loss: 0.7209 || timer: 0.1232 sec.
iter 316880 || Loss: 0.5826 || timer: 0.0921 sec.
iter 316890 || Loss: 0.7916 || timer: 0.0824 sec.
iter 316900 || Loss: 0.6209 || timer: 0.1079 sec.
iter 316910 || Loss: 0.6406 || timer: 0.0821 sec.
iter 316920 || Loss: 0.6499 || timer: 0.0828 sec.
iter 316930 || Loss: 0.5898 || timer: 0.1055 sec.
iter 316940 || Loss: 0.6290 || timer: 0.0851 sec.
iter 316950 || Loss: 0.6356 || timer: 0.0908 sec.
iter 316960 || Loss: 0.7760 || timer: 0.1078 sec.
iter 316970 || Loss: 0.7134 || timer: 0.0893 sec.
iter 316980 || Loss: 0.6871 || timer: 0.1125 sec.
iter 316990 || Loss: 0.6818 || timer: 0.0864 sec.
iter 317000 || Loss: 0.4862 || timer: 0.0887 sec.
iter 317010 || Loss: 0.7868 || timer: 0.0823 sec.
iter 317020 || Loss: 0.6459 || timer: 0.0203 sec.
iter 317030 || Loss: 1.1707 || timer: 0.0928 sec.
iter 317040 || Loss: 0.6842 || timer: 0.0987 sec.
iter 317050 || Loss: 0.7900 || timer: 0.0874 sec.
iter 317060 || Loss: 0.6419 || timer: 0.0879 sec.
iter 317070 || Loss: 0.5476 || timer: 0.0834 sec.
iter 317080 || Loss: 0.5639 || timer: 0.0827 sec.
iter 317090 || Loss: 0.6225 || timer: 0.0832 sec.
iter 317100 || Loss: 0.5385 || timer: 0.0828 sec.
iter 317110 || Loss: 0.8214 || timer: 0.0907 sec.
iter 317120 || Loss: 0.8318 || timer: 0.1208 sec.
iter 317130 || Loss: 0.7908 || timer: 0.0862 sec.
iter 317140 || Loss: 0.7391 || timer: 0.0819 sec.
iter 317150 || Loss: 0.9437 || timer: 0.0905 sec.
iter 317160 || Loss: 0.8062 || timer: 0.0825 sec.
iter 317170 || Loss: 0.5275 || timer: 0.0755 sec.
iter 317180 || Loss: 0.6000 || timer: 0.1017 sec.
iter 317190 || Loss: 0.6540 || timer: 0.0961 sec.
iter 317200 || Loss: 0.7736 || timer: 0.1082 sec.
iter 317210 || Loss: 0.6273 || timer: 0.0832 sec.
iter 317220 || Loss: 0.7734 || timer: 0.0951 sec.
iter 317230 || Loss: 0.6618 || timer: 0.0925 sec.
iter 317240 || Loss: 0.7119 || timer: 0.0896 sec.
iter 317250 || Loss: 0.8689 || timer: 0.0931 sec.
iter 317260 || Loss: 0.7924 || timer: 0.1000 sec.
iter 317270 || Loss: 0.6127 || timer: 0.0840 sec.
iter 317280 || Loss: 0.6992 || timer: 0.0899 sec.
iter 317290 || Loss: 0.8067 || timer: 0.0906 sec.
iter 317300 || Loss: 0.6401 || timer: 0.0969 sec.
iter 317310 || Loss: 0.6043 || timer: 0.0836 sec.
iter 317320 || Loss: 0.7695 || timer: 0.0892 sec.
iter 317330 || Loss: 0.6777 || timer: 0.0981 sec.
iter 317340 || Loss: 0.6291 || timer: 0.0936 sec.
iter 317350 || Loss: 0.7412 || timer: 0.0273 sec.
iter 317360 || Loss: 0.4799 || timer: 0.0744 sec.
iter 317370 || Loss: 1.0207 || timer: 0.0932 sec.
iter 317380 || Loss: 0.7241 || timer: 0.0870 sec.
iter 317390 || Loss: 0.6244 || timer: 0.0904 sec.
iter 317400 || Loss: 0.7194 || timer: 0.0841 sec.
iter 317410 || Loss: 0.6893 || timer: 0.0921 sec.
iter 317420 || Loss: 0.6433 || timer: 0.0759 sec.
iter 317430 || Loss: 0.8218 || timer: 0.0894 sec.
iter 317440 || Loss: 0.7743 || timer: 0.0876 sec.
iter 317450 || Loss: 0.5651 || timer: 0.0991 sec.
iter 317460 || Loss: 0.8159 || timer: 0.0855 sec.
iter 317470 || Loss: 0.7398 || timer: 0.0892 sec.
iter 317480 || Loss: 0.6137 || timer: 0.0922 sec.
iter 317490 || Loss: 0.8805 || timer: 0.0853 sec.
iter 317500 || Loss: 0.6869 || timer: 0.0881 sec.
iter 317510 || Loss: 0.6369 || timer: 0.0930 sec.
iter 317520 || Loss: 0.8926 || timer: 0.0818 sec.
iter 317530 || Loss: 0.5453 || timer: 0.0895 sec.
iter 317540 || Loss: 0.3611 || timer: 0.0833 sec.
iter 317550 || Loss: 0.5467 || timer: 0.0760 sec.
iter 317560 || Loss: 0.5088 || timer: 0.0815 sec.
iter 317570 || Loss: 0.5166 || timer: 0.0823 sec.
iter 317580 || Loss: 0.5913 || timer: 0.0835 sec.
iter 317590 || Loss: 0.6377 || timer: 0.0899 sec.
iter 317600 || Loss: 0.6141 || timer: 0.0818 sec.
iter 317610 || Loss: 0.8022 || timer: 0.0905 sec.
iter 317620 || Loss: 0.8365 || timer: 0.0922 sec.
iter 317630 || Loss: 1.1683 || timer: 0.0912 sec.
iter 317640 || Loss: 0.6158 || timer: 0.0886 sec.
iter 317650 || Loss: 0.4557 || timer: 0.0752 sec.
iter 317660 || Loss: 0.6285 || timer: 0.1248 sec.
iter 317670 || Loss: 0.8641 || timer: 0.1014 sec.
iter 317680 || Loss: 0.5972 || timer: 0.0168 sec.
iter 317690 || Loss: 0.5541 || timer: 0.0808 sec.
iter 317700 || Loss: 0.6088 || timer: 0.0986 sec.
iter 317710 || Loss: 0.5757 || timer: 0.0890 sec.
iter 317720 || Loss: 0.6532 || timer: 0.0782 sec.
iter 317730 || Loss: 0.7997 || timer: 0.0937 sec.
iter 317740 || Loss: 0.6782 || timer: 0.0820 sec.
iter 317750 || Loss: 0.6398 || timer: 0.0796 sec.
iter 317760 || Loss: 0.6431 || timer: 0.0852 sec.
iter 317770 || Loss: 0.5812 || timer: 0.0911 sec.
iter 317780 || Loss: 0.7027 || timer: 0.1355 sec.
iter 317790 || Loss: 0.6651 || timer: 0.0914 sec.
iter 317800 || Loss: 0.6495 || timer: 0.1291 sec.
iter 317810 || Loss: 0.5740 || timer: 0.1037 sec.
iter 317820 || Loss: 0.5551 || timer: 0.0950 sec.
iter 317830 || Loss: 0.6864 || timer: 0.0929 sec.
iter 317840 || Loss: 1.1521 || timer: 0.0902 sec.
iter 317850 || Loss: 0.5859 || timer: 0.1097 sec.
iter 317860 || Loss: 0.7138 || timer: 0.1019 sec.
iter 317870 || Loss: 0.6448 || timer: 0.0913 sec.
iter 317880 || Loss: 0.6478 || timer: 0.0894 sec.
iter 317890 || Loss: 0.5717 || timer: 0.1104 sec.
iter 317900 || Loss: 0.6388 || timer: 0.0897 sec.
iter 317910 || Loss: 0.5558 || timer: 0.0870 sec.
iter 317920 || Loss: 0.9171 || timer: 0.0894 sec.
iter 317930 || Loss: 0.6013 || timer: 0.0913 sec.
iter 317940 || Loss: 0.7773 || timer: 0.0905 sec.
iter 317950 || Loss: 0.8222 || timer: 0.0915 sec.
iter 317960 || Loss: 0.7414 || timer: 0.0835 sec.
iter 317970 || Loss: 0.6700 || timer: 0.0921 sec.
iter 317980 || Loss: 0.7045 || timer: 0.0918 sec.
iter 317990 || Loss: 0.6028 || timer: 0.0954 sec.
iter 318000 || Loss: 0.8235 || timer: 0.1050 sec.
iter 318010 || Loss: 0.9713 || timer: 0.0261 sec.
iter 318020 || Loss: 0.3830 || timer: 0.1073 sec.
iter 318030 || Loss: 0.5605 || timer: 0.0949 sec.
iter 318040 || Loss: 0.7369 || timer: 0.0871 sec.
iter 318050 || Loss: 0.7430 || timer: 0.0829 sec.
iter 318060 || Loss: 0.7305 || timer: 0.0901 sec.
iter 318070 || Loss: 0.7685 || timer: 0.0909 sec.
iter 318080 || Loss: 0.7975 || timer: 0.0906 sec.
iter 318090 || Loss: 0.8311 || timer: 0.0847 sec.
iter 318100 || Loss: 0.5212 || timer: 0.0846 sec.
iter 318110 || Loss: 0.5332 || timer: 0.0984 sec.
iter 318120 || Loss: 0.5479 || timer: 0.0824 sec.
iter 318130 || Loss: 0.8200 || timer: 0.0859 sec.
iter 318140 || Loss: 0.8407 || timer: 0.0929 sec.
iter 318150 || Loss: 0.6214 || timer: 0.0915 sec.
iter 318160 || Loss: 0.8235 || timer: 0.0893 sec.
iter 318170 || Loss: 0.8505 || timer: 0.1109 sec.
iter 318180 || Loss: 0.6941 || timer: 0.0990 sec.
iter 318190 || Loss: 0.6063 || timer: 0.1012 sec.
iter 318200 || Loss: 0.8103 || timer: 0.0929 sec.
iter 318210 || Loss: 0.5596 || timer: 0.0893 sec.
iter 318220 || Loss: 0.4731 || timer: 0.0882 sec.
iter 318230 || Loss: 0.5651 || timer: 0.1179 sec.
iter 318240 || Loss: 0.5782 || timer: 0.0925 sec.
iter 318250 || Loss: 0.5393 || timer: 0.1066 sec.
iter 318260 || Loss: 0.7125 || timer: 0.0913 sec.
iter 318270 || Loss: 0.8003 || timer: 0.1035 sec.
iter 318280 || Loss: 0.6556 || timer: 0.0846 sec.
iter 318290 || Loss: 1.0045 || timer: 0.0928 sec.
iter 318300 || Loss: 0.6358 || timer: 0.0937 sec.
iter 318310 || Loss: 0.8058 || timer: 0.0910 sec.
iter 318320 || Loss: 0.6804 || timer: 0.0966 sec.
iter 318330 || Loss: 0.9093 || timer: 0.0842 sec.
iter 318340 || Loss: 1.0617 || timer: 0.0317 sec.
iter 318350 || Loss: 0.7371 || timer: 0.0827 sec.
iter 318360 || Loss: 0.5136 || timer: 0.0918 sec.
iter 318370 || Loss: 0.6438 || timer: 0.0843 sec.
iter 318380 || Loss: 0.7430 || timer: 0.0917 sec.
iter 318390 || Loss: 0.6922 || timer: 0.0770 sec.
iter 318400 || Loss: 0.6293 || timer: 0.0827 sec.
iter 318410 || Loss: 0.6089 || timer: 0.1047 sec.
iter 318420 || Loss: 0.7521 || timer: 0.0818 sec.
iter 318430 || Loss: 0.9654 || timer: 0.0913 sec.
iter 318440 || Loss: 0.7356 || timer: 0.0976 sec.
iter 318450 || Loss: 0.9404 || timer: 0.0848 sec.
iter 318460 || Loss: 0.7145 || timer: 0.0981 sec.
iter 318470 || Loss: 0.6892 || timer: 0.0814 sec.
iter 318480 || Loss: 0.7910 || timer: 0.0765 sec.
iter 318490 || Loss: 0.5980 || timer: 0.1024 sec.
iter 318500 || Loss: 0.8867 || timer: 0.0916 sec.
iter 318510 || Loss: 0.8583 || timer: 0.1089 sec.
iter 318520 || Loss: 0.9611 || timer: 0.0898 sec.
iter 318530 || Loss: 0.5494 || timer: 0.1123 sec.
iter 318540 || Loss: 0.5977 || timer: 0.0920 sec.
iter 318550 || Loss: 0.4822 || timer: 0.1061 sec.
iter 318560 || Loss: 0.5835 || timer: 0.0930 sec.
iter 318570 || Loss: 0.7560 || timer: 0.0892 sec.
iter 318580 || Loss: 0.7520 || timer: 0.0930 sec.
iter 318590 || Loss: 0.6821 || timer: 0.0981 sec.
iter 318600 || Loss: 0.8122 || timer: 0.1229 sec.
iter 318610 || Loss: 0.6107 || timer: 0.0837 sec.
iter 318620 || Loss: 0.5538 || timer: 0.0911 sec.
iter 318630 || Loss: 0.7394 || timer: 0.0859 sec.
iter 318640 || Loss: 0.7105 || timer: 0.0841 sec.
iter 318650 || Loss: 0.7239 || timer: 0.0870 sec.
iter 318660 || Loss: 0.5332 || timer: 0.0848 sec.
iter 318670 || Loss: 0.8124 || timer: 0.0155 sec.
iter 318680 || Loss: 0.4117 || timer: 0.0967 sec.
iter 318690 || Loss: 0.6180 || timer: 0.0917 sec.
iter 318700 || Loss: 0.9065 || timer: 0.0862 sec.
iter 318710 || Loss: 0.6783 || timer: 0.0923 sec.
iter 318720 || Loss: 0.7509 || timer: 0.0904 sec.
iter 318730 || Loss: 0.7282 || timer: 0.0828 sec.
iter 318740 || Loss: 0.6465 || timer: 0.0926 sec.
iter 318750 || Loss: 0.6340 || timer: 0.0819 sec.
iter 318760 || Loss: 0.6251 || timer: 0.0830 sec.
iter 318770 || Loss: 0.5424 || timer: 0.0945 sec.
iter 318780 || Loss: 0.6961 || timer: 0.0916 sec.
iter 318790 || Loss: 0.6628 || timer: 0.0890 sec.
iter 318800 || Loss: 0.8341 || timer: 0.0945 sec.
iter 318810 || Loss: 0.9803 || timer: 0.0901 sec.
iter 318820 || Loss: 0.8867 || timer: 0.1238 sec.
iter 318830 || Loss: 0.6111 || timer: 0.1019 sec.
iter 318840 || Loss: 0.8111 || timer: 0.0893 sec.
iter 318850 || Loss: 0.5327 || timer: 0.0925 sec.
iter 318860 || Loss: 0.5399 || timer: 0.0893 sec.
iter 318870 || Loss: 0.5421 || timer: 0.1095 sec.
iter 318880 || Loss: 0.7378 || timer: 0.1268 sec.
iter 318890 || Loss: 0.6586 || timer: 0.1224 sec.
iter 318900 || Loss: 0.7157 || timer: 0.0878 sec.
iter 318910 || Loss: 0.7420 || timer: 0.1017 sec.
iter 318920 || Loss: 0.6636 || timer: 0.0891 sec.
iter 318930 || Loss: 0.8721 || timer: 0.0937 sec.
iter 318940 || Loss: 0.7580 || timer: 0.0901 sec.
iter 318950 || Loss: 0.8844 || timer: 0.1017 sec.
iter 318960 || Loss: 0.7493 || timer: 0.1078 sec.
iter 318970 || Loss: 0.7105 || timer: 0.0820 sec.
iter 318980 || Loss: 0.6878 || timer: 0.0921 sec.
iter 318990 || Loss: 0.8044 || timer: 0.0898 sec.
iter 319000 || Loss: 0.7742 || timer: 0.0179 sec.
iter 319010 || Loss: 1.2601 || timer: 0.1058 sec.
iter 319020 || Loss: 0.6038 || timer: 0.0901 sec.
iter 319030 || Loss: 0.7764 || timer: 0.1154 sec.
iter 319040 || Loss: 1.0028 || timer: 0.0903 sec.
iter 319050 || Loss: 0.5853 || timer: 0.0845 sec.
iter 319060 || Loss: 0.6618 || timer: 0.1092 sec.
iter 319070 || Loss: 0.7287 || timer: 0.0929 sec.
iter 319080 || Loss: 0.4900 || timer: 0.1121 sec.
iter 319090 || Loss: 0.9003 || timer: 0.0899 sec.
iter 319100 || Loss: 0.9440 || timer: 0.1103 sec.
iter 319110 || Loss: 1.3174 || timer: 0.1342 sec.
iter 319120 || Loss: 0.6520 || timer: 0.0886 sec.
iter 319130 || Loss: 0.8076 || timer: 0.0828 sec.
iter 319140 || Loss: 0.5204 || timer: 0.0867 sec.
iter 319150 || Loss: 0.5573 || timer: 0.1190 sec.
iter 319160 || Loss: 0.7164 || timer: 0.0832 sec.
iter 319170 || Loss: 0.6933 || timer: 0.0925 sec.
iter 319180 || Loss: 0.6523 || timer: 0.0824 sec.
iter 319190 || Loss: 0.9796 || timer: 0.0911 sec.
iter 319200 || Loss: 0.8005 || timer: 0.0927 sec.
iter 319210 || Loss: 0.8479 || timer: 0.0810 sec.
iter 319220 || Loss: 0.9320 || timer: 0.0917 sec.
iter 319230 || Loss: 0.8304 || timer: 0.0833 sec.
iter 319240 || Loss: 0.8637 || timer: 0.0931 sec.
iter 319250 || Loss: 0.5795 || timer: 0.0892 sec.
iter 319260 || Loss: 0.4648 || timer: 0.0864 sec.
iter 319270 || Loss: 0.4807 || timer: 0.0935 sec.
iter 319280 || Loss: 0.8039 || timer: 0.1009 sec.
iter 319290 || Loss: 0.6272 || timer: 0.0960 sec.
iter 319300 || Loss: 0.6360 || timer: 0.0886 sec.
iter 319310 || Loss: 0.7660 || timer: 0.0883 sec.
iter 319320 || Loss: 0.7985 || timer: 0.0811 sec.
iter 319330 || Loss: 0.5798 || timer: 0.0243 sec.
iter 319340 || Loss: 0.2696 || timer: 0.0892 sec.
iter 319350 || Loss: 0.6569 || timer: 0.1082 sec.
iter 319360 || Loss: 0.9550 || timer: 0.0811 sec.
iter 319370 || Loss: 0.7149 || timer: 0.0874 sec.
iter 319380 || Loss: 0.7109 || timer: 0.0853 sec.
iter 319390 || Loss: 0.5772 || timer: 0.0918 sec.
iter 319400 || Loss: 0.7298 || timer: 0.0974 sec.
iter 319410 || Loss: 0.7032 || timer: 0.0904 sec.
iter 319420 || Loss: 0.7530 || timer: 0.0923 sec.
iter 319430 || Loss: 0.6994 || timer: 0.1039 sec.
iter 319440 || Loss: 0.6086 || timer: 0.0834 sec.
iter 319450 || Loss: 0.5100 || timer: 0.0883 sec.
iter 319460 || Loss: 0.8923 || timer: 0.1237 sec.
iter 319470 || Loss: 0.6642 || timer: 0.0808 sec.
iter 319480 || Loss: 0.6264 || timer: 0.0878 sec.
iter 319490 || Loss: 0.5873 || timer: 0.0847 sec.
iter 319500 || Loss: 1.0613 || timer: 0.1303 sec.
iter 319510 || Loss: 0.6636 || timer: 0.1096 sec.
iter 319520 || Loss: 0.5410 || timer: 0.0961 sec.
iter 319530 || Loss: 0.5866 || timer: 0.1023 sec.
iter 319540 || Loss: 0.7201 || timer: 0.0925 sec.
iter 319550 || Loss: 0.6977 || timer: 0.0896 sec.
iter 319560 || Loss: 0.5605 || timer: 0.0828 sec.
iter 319570 || Loss: 0.5899 || timer: 0.0890 sec.
iter 319580 || Loss: 0.5744 || timer: 0.1108 sec.
iter 319590 || Loss: 0.7563 || timer: 0.0918 sec.
iter 319600 || Loss: 0.6479 || timer: 0.0901 sec.
iter 319610 || Loss: 0.6454 || timer: 0.0885 sec.
iter 319620 || Loss: 0.8047 || timer: 0.0958 sec.
iter 319630 || Loss: 0.5715 || timer: 0.0909 sec.
iter 319640 || Loss: 0.8089 || timer: 0.0919 sec.
iter 319650 || Loss: 0.4884 || timer: 0.0812 sec.
iter 319660 || Loss: 0.6419 || timer: 0.0184 sec.
iter 319670 || Loss: 0.8078 || timer: 0.0847 sec.
iter 319680 || Loss: 0.6549 || timer: 0.0849 sec.
iter 319690 || Loss: 0.8695 || timer: 0.0843 sec.
iter 319700 || Loss: 0.5683 || timer: 0.0921 sec.
iter 319710 || Loss: 0.7430 || timer: 0.0914 sec.
iter 319720 || Loss: 0.6182 || timer: 0.0870 sec.
iter 319730 || Loss: 0.6367 || timer: 0.0911 sec.
iter 319740 || Loss: 0.6413 || timer: 0.0809 sec.
iter 319750 || Loss: 0.6390 || timer: 0.0828 sec.
iter 319760 || Loss: 0.8413 || timer: 0.1373 sec.
iter 319770 || Loss: 0.7228 || timer: 0.0833 sec.
iter 319780 || Loss: 0.4458 || timer: 0.0845 sec.
iter 319790 || Loss: 0.8540 || timer: 0.0884 sec.
iter 319800 || Loss: 0.3944 || timer: 0.0957 sec.
iter 319810 || Loss: 0.6233 || timer: 0.0943 sec.
iter 319820 || Loss: 0.4711 || timer: 0.0859 sec.
iter 319830 || Loss: 1.0194 || timer: 0.0894 sec.
iter 319840 || Loss: 0.9511 || timer: 0.1067 sec.
iter 319850 || Loss: 0.5946 || timer: 0.0813 sec.
iter 319860 || Loss: 0.7824 || timer: 0.0848 sec.
iter 319870 || Loss: 0.7321 || timer: 0.1123 sec.
iter 319880 || Loss: 1.2023 || timer: 0.0912 sec.
iter 319890 || Loss: 0.7320 || timer: 0.0922 sec.
iter 319900 || Loss: 0.7388 || timer: 0.0970 sec.
iter 319910 || Loss: 0.7348 || timer: 0.0917 sec.
iter 319920 || Loss: 0.5525 || timer: 0.0923 sec.
iter 319930 || Loss: 0.5961 || timer: 0.0846 sec.
iter 319940 || Loss: 0.5531 || timer: 0.0832 sec.
iter 319950 || Loss: 0.7998 || timer: 0.0972 sec.
iter 319960 || Loss: 0.7825 || timer: 0.1016 sec.
iter 319970 || Loss: 0.9732 || timer: 0.0805 sec.
iter 319980 || Loss: 0.7770 || timer: 0.1073 sec.
iter 319990 || Loss: 0.7042 || timer: 0.0201 sec.
iter 320000 || Loss: 0.4903 || Saving state, iter: 320000
timer: 0.0840 sec.
iter 320010 || Loss: 0.6663 || timer: 0.0937 sec.
iter 320020 || Loss: 0.7152 || timer: 0.0907 sec.
iter 320030 || Loss: 0.7260 || timer: 0.0900 sec.
iter 320040 || Loss: 0.9706 || timer: 0.0907 sec.
iter 320050 || Loss: 0.5305 || timer: 0.0839 sec.
iter 320060 || Loss: 0.9114 || timer: 0.1042 sec.
iter 320070 || Loss: 0.6420 || timer: 0.1031 sec.
iter 320080 || Loss: 0.5565 || timer: 0.0823 sec.
iter 320090 || Loss: 0.7607 || timer: 0.1194 sec.
iter 320100 || Loss: 0.5838 || timer: 0.0882 sec.
iter 320110 || Loss: 0.5882 || timer: 0.0904 sec.
iter 320120 || Loss: 0.5570 || timer: 0.0844 sec.
iter 320130 || Loss: 0.6890 || timer: 0.0887 sec.
iter 320140 || Loss: 1.0722 || timer: 0.1016 sec.
iter 320150 || Loss: 0.7148 || timer: 0.0744 sec.
iter 320160 || Loss: 0.7284 || timer: 0.1016 sec.
iter 320170 || Loss: 0.5506 || timer: 0.0987 sec.
iter 320180 || Loss: 0.8669 || timer: 0.0893 sec.
iter 320190 || Loss: 0.9015 || timer: 0.0809 sec.
iter 320200 || Loss: 0.6615 || timer: 0.0921 sec.
iter 320210 || Loss: 0.6842 || timer: 0.0905 sec.
iter 320220 || Loss: 0.8532 || timer: 0.0917 sec.
iter 320230 || Loss: 0.7315 || timer: 0.0860 sec.
iter 320240 || Loss: 0.5963 || timer: 0.0865 sec.
iter 320250 || Loss: 0.9897 || timer: 0.0996 sec.
iter 320260 || Loss: 0.4289 || timer: 0.0813 sec.
iter 320270 || Loss: 0.8929 || timer: 0.0736 sec.
iter 320280 || Loss: 0.6073 || timer: 0.0877 sec.
iter 320290 || Loss: 0.8824 || timer: 0.0801 sec.
iter 320300 || Loss: 0.4511 || timer: 0.0901 sec.
iter 320310 || Loss: 0.8251 || timer: 0.0856 sec.
iter 320320 || Loss: 0.5814 || timer: 0.0238 sec.
iter 320330 || Loss: 0.7578 || timer: 0.0811 sec.
iter 320340 || Loss: 0.7212 || timer: 0.0831 sec.
iter 320350 || Loss: 0.8764 || timer: 0.0884 sec.
iter 320360 || Loss: 0.5577 || timer: 0.0887 sec.
iter 320370 || Loss: 0.5696 || timer: 0.0842 sec.
iter 320380 || Loss: 0.6625 || timer: 0.0881 sec.
iter 320390 || Loss: 1.0184 || timer: 0.0895 sec.
iter 320400 || Loss: 0.7387 || timer: 0.0866 sec.
iter 320410 || Loss: 0.7427 || timer: 0.0869 sec.
iter 320420 || Loss: 0.4443 || timer: 0.1049 sec.
iter 320430 || Loss: 0.7363 || timer: 0.0956 sec.
iter 320440 || Loss: 0.9781 || timer: 0.0891 sec.
iter 320450 || Loss: 0.7630 || timer: 0.0879 sec.
iter 320460 || Loss: 0.5992 || timer: 0.0821 sec.
iter 320470 || Loss: 0.5861 || timer: 0.0908 sec.
iter 320480 || Loss: 0.8987 || timer: 0.0842 sec.
iter 320490 || Loss: 0.6809 || timer: 0.0811 sec.
iter 320500 || Loss: 0.7727 || timer: 0.0918 sec.
iter 320510 || Loss: 0.7351 || timer: 0.0880 sec.
iter 320520 || Loss: 0.6752 || timer: 0.0813 sec.
iter 320530 || Loss: 0.5396 || timer: 0.1066 sec.
iter 320540 || Loss: 0.9556 || timer: 0.0883 sec.
iter 320550 || Loss: 0.5669 || timer: 0.0807 sec.
iter 320560 || Loss: 0.6148 || timer: 0.0898 sec.
iter 320570 || Loss: 0.7217 || timer: 0.0825 sec.
iter 320580 || Loss: 0.9487 || timer: 0.0990 sec.
iter 320590 || Loss: 0.6428 || timer: 0.0814 sec.
iter 320600 || Loss: 0.5354 || timer: 0.0807 sec.
iter 320610 || Loss: 0.7788 || timer: 0.0884 sec.
iter 320620 || Loss: 0.7947 || timer: 0.0911 sec.
iter 320630 || Loss: 0.6464 || timer: 0.0873 sec.
iter 320640 || Loss: 0.6558 || timer: 0.0867 sec.
iter 320650 || Loss: 0.7998 || timer: 0.0230 sec.
iter 320660 || Loss: 0.3467 || timer: 0.0902 sec.
iter 320670 || Loss: 0.4978 || timer: 0.0907 sec.
iter 320680 || Loss: 0.7667 || timer: 0.1086 sec.
iter 320690 || Loss: 0.7425 || timer: 0.0830 sec.
iter 320700 || Loss: 0.6275 || timer: 0.0888 sec.
iter 320710 || Loss: 0.6267 || timer: 0.0865 sec.
iter 320720 || Loss: 0.7654 || timer: 0.0822 sec.
iter 320730 || Loss: 0.7628 || timer: 0.0846 sec.
iter 320740 || Loss: 0.4972 || timer: 0.0890 sec.
iter 320750 || Loss: 0.9889 || timer: 0.1090 sec.
iter 320760 || Loss: 0.7844 || timer: 0.0815 sec.
iter 320770 || Loss: 0.8030 || timer: 0.0857 sec.
iter 320780 || Loss: 0.9820 || timer: 0.0815 sec.
iter 320790 || Loss: 0.6413 || timer: 0.0904 sec.
iter 320800 || Loss: 0.7491 || timer: 0.0817 sec.
iter 320810 || Loss: 0.6503 || timer: 0.0833 sec.
iter 320820 || Loss: 0.8013 || timer: 0.0867 sec.
iter 320830 || Loss: 0.5309 || timer: 0.0875 sec.
iter 320840 || Loss: 0.6841 || timer: 0.0848 sec.
iter 320850 || Loss: 0.7252 || timer: 0.0960 sec.
iter 320860 || Loss: 0.8079 || timer: 0.0826 sec.
iter 320870 || Loss: 0.8906 || timer: 0.0904 sec.
iter 320880 || Loss: 0.4883 || timer: 0.0896 sec.
iter 320890 || Loss: 0.6693 || timer: 0.0925 sec.
iter 320900 || Loss: 0.7536 || timer: 0.0904 sec.
iter 320910 || Loss: 0.7397 || timer: 0.0915 sec.
iter 320920 || Loss: 1.0301 || timer: 0.0912 sec.
iter 320930 || Loss: 0.9707 || timer: 0.1015 sec.
iter 320940 || Loss: 0.6554 || timer: 0.0883 sec.
iter 320950 || Loss: 0.7116 || timer: 0.0808 sec.
iter 320960 || Loss: 0.7132 || timer: 0.0893 sec.
iter 320970 || Loss: 0.5878 || timer: 0.0899 sec.
iter 320980 || Loss: 0.7713 || timer: 0.0155 sec.
iter 320990 || Loss: 0.7383 || timer: 0.0886 sec.
iter 321000 || Loss: 0.5505 || timer: 0.0916 sec.
iter 321010 || Loss: 0.7736 || timer: 0.0895 sec.
iter 321020 || Loss: 0.8467 || timer: 0.0838 sec.
iter 321030 || Loss: 0.7131 || timer: 0.0887 sec.
iter 321040 || Loss: 0.6769 || timer: 0.0814 sec.
iter 321050 || Loss: 0.6329 || timer: 0.0924 sec.
iter 321060 || Loss: 0.5266 || timer: 0.0869 sec.
iter 321070 || Loss: 0.7535 || timer: 0.0917 sec.
iter 321080 || Loss: 0.5427 || timer: 0.1165 sec.
iter 321090 || Loss: 0.8355 || timer: 0.0895 sec.
iter 321100 || Loss: 0.7227 || timer: 0.0938 sec.
iter 321110 || Loss: 0.7501 || timer: 0.0878 sec.
iter 321120 || Loss: 0.8241 || timer: 0.0842 sec.
iter 321130 || Loss: 1.0092 || timer: 0.1075 sec.
iter 321140 || Loss: 0.8497 || timer: 0.0884 sec.
iter 321150 || Loss: 1.1648 || timer: 0.0931 sec.
iter 321160 || Loss: 0.7850 || timer: 0.0810 sec.
iter 321170 || Loss: 0.6082 || timer: 0.0951 sec.
iter 321180 || Loss: 0.7352 || timer: 0.0905 sec.
iter 321190 || Loss: 0.8387 || timer: 0.1053 sec.
iter 321200 || Loss: 0.5354 || timer: 0.0884 sec.
iter 321210 || Loss: 0.5554 || timer: 0.0811 sec.
iter 321220 || Loss: 0.7088 || timer: 0.0819 sec.
iter 321230 || Loss: 0.9767 || timer: 0.0820 sec.
iter 321240 || Loss: 0.9623 || timer: 0.0909 sec.
iter 321250 || Loss: 0.5194 || timer: 0.0833 sec.
iter 321260 || Loss: 0.5836 || timer: 0.0820 sec.
iter 321270 || Loss: 0.5587 || timer: 0.0872 sec.
iter 321280 || Loss: 0.6824 || timer: 0.0900 sec.
iter 321290 || Loss: 0.7846 || timer: 0.0968 sec.
iter 321300 || Loss: 0.8489 || timer: 0.0945 sec.
iter 321310 || Loss: 0.6041 || timer: 0.0266 sec.
iter 321320 || Loss: 0.0656 || timer: 0.0927 sec.
iter 321330 || Loss: 0.8583 || timer: 0.1048 sec.
iter 321340 || Loss: 0.8596 || timer: 0.0919 sec.
iter 321350 || Loss: 0.9812 || timer: 0.0758 sec.
iter 321360 || Loss: 0.5861 || timer: 0.0839 sec.
iter 321370 || Loss: 0.6380 || timer: 0.0918 sec.
iter 321380 || Loss: 0.6544 || timer: 0.0822 sec.
iter 321390 || Loss: 0.6695 || timer: 0.0818 sec.
iter 321400 || Loss: 0.6173 || timer: 0.0848 sec.
iter 321410 || Loss: 0.6502 || timer: 0.0961 sec.
iter 321420 || Loss: 0.8082 || timer: 0.0865 sec.
iter 321430 || Loss: 0.6367 || timer: 0.0927 sec.
iter 321440 || Loss: 0.7331 || timer: 0.0811 sec.
iter 321450 || Loss: 0.4784 || timer: 0.0826 sec.
iter 321460 || Loss: 0.8413 || timer: 0.0857 sec.
iter 321470 || Loss: 0.7553 || timer: 0.0909 sec.
iter 321480 || Loss: 0.7041 || timer: 0.1207 sec.
iter 321490 || Loss: 0.7189 || timer: 0.0979 sec.
iter 321500 || Loss: 0.8929 || timer: 0.0915 sec.
iter 321510 || Loss: 0.5908 || timer: 0.0771 sec.
iter 321520 || Loss: 0.4441 || timer: 0.0892 sec.
iter 321530 || Loss: 0.4856 || timer: 0.0902 sec.
iter 321540 || Loss: 0.7031 || timer: 0.0927 sec.
iter 321550 || Loss: 0.8946 || timer: 0.0892 sec.
iter 321560 || Loss: 0.7175 || timer: 0.0881 sec.
iter 321570 || Loss: 0.7395 || timer: 0.0873 sec.
iter 321580 || Loss: 0.5609 || timer: 0.0838 sec.
iter 321590 || Loss: 0.6986 || timer: 0.0981 sec.
iter 321600 || Loss: 0.7085 || timer: 0.0831 sec.
iter 321610 || Loss: 0.5701 || timer: 0.0838 sec.
iter 321620 || Loss: 0.8056 || timer: 0.1119 sec.
iter 321630 || Loss: 0.8823 || timer: 0.0980 sec.
iter 321640 || Loss: 0.8533 || timer: 0.0273 sec.
iter 321650 || Loss: 1.9358 || timer: 0.0880 sec.
iter 321660 || Loss: 0.7016 || timer: 0.0893 sec.
iter 321670 || Loss: 0.6482 || timer: 0.0899 sec.
iter 321680 || Loss: 0.5024 || timer: 0.0895 sec.
iter 321690 || Loss: 0.6717 || timer: 0.0846 sec.
iter 321700 || Loss: 0.8612 || timer: 0.0923 sec.
iter 321710 || Loss: 0.7589 || timer: 0.1173 sec.
iter 321720 || Loss: 0.6950 || timer: 0.0950 sec.
iter 321730 || Loss: 0.5400 || timer: 0.0840 sec.
iter 321740 || Loss: 0.5724 || timer: 0.1170 sec.
iter 321750 || Loss: 0.5289 || timer: 0.0846 sec.
iter 321760 || Loss: 0.6837 || timer: 0.0884 sec.
iter 321770 || Loss: 0.6190 || timer: 0.0928 sec.
iter 321780 || Loss: 0.7948 || timer: 0.0934 sec.
iter 321790 || Loss: 0.6676 || timer: 0.0858 sec.
iter 321800 || Loss: 0.9129 || timer: 0.0902 sec.
iter 321810 || Loss: 0.6824 || timer: 0.0908 sec.
iter 321820 || Loss: 0.9611 || timer: 0.0903 sec.
iter 321830 || Loss: 0.9826 || timer: 0.0897 sec.
iter 321840 || Loss: 0.4520 || timer: 0.0921 sec.
iter 321850 || Loss: 0.7707 || timer: 0.0911 sec.
iter 321860 || Loss: 0.6471 || timer: 0.0899 sec.
iter 321870 || Loss: 0.4227 || timer: 0.0917 sec.
iter 321880 || Loss: 0.7045 || timer: 0.0887 sec.
iter 321890 || Loss: 0.9438 || timer: 0.0939 sec.
iter 321900 || Loss: 0.5005 || timer: 0.0897 sec.
iter 321910 || Loss: 0.9437 || timer: 0.0910 sec.
iter 321920 || Loss: 0.7741 || timer: 0.0919 sec.
iter 321930 || Loss: 0.8917 || timer: 0.0822 sec.
iter 321940 || Loss: 0.5717 || timer: 0.0872 sec.
iter 321950 || Loss: 0.4700 || timer: 0.0908 sec.
iter 321960 || Loss: 0.6858 || timer: 0.0865 sec.
iter 321970 || Loss: 0.5663 || timer: 0.0167 sec.
iter 321980 || Loss: 0.1314 || timer: 0.0811 sec.
iter 321990 || Loss: 0.5955 || timer: 0.0891 sec.
iter 322000 || Loss: 0.6931 || timer: 0.0864 sec.
iter 322010 || Loss: 0.8519 || timer: 0.0818 sec.
iter 322020 || Loss: 0.8867 || timer: 0.0817 sec.
iter 322030 || Loss: 0.5748 || timer: 0.1046 sec.
iter 322040 || Loss: 1.0523 || timer: 0.0880 sec.
iter 322050 || Loss: 0.9233 || timer: 0.1024 sec.
iter 322060 || Loss: 0.7241 || timer: 0.0875 sec.
iter 322070 || Loss: 0.6731 || timer: 0.0982 sec.
iter 322080 || Loss: 0.6763 || timer: 0.0821 sec.
iter 322090 || Loss: 0.6708 || timer: 0.0880 sec.
iter 322100 || Loss: 0.7555 || timer: 0.0899 sec.
iter 322110 || Loss: 0.9564 || timer: 0.0841 sec.
iter 322120 || Loss: 0.6154 || timer: 0.0904 sec.
iter 322130 || Loss: 0.4773 || timer: 0.1064 sec.
iter 322140 || Loss: 0.5238 || timer: 0.0900 sec.
iter 322150 || Loss: 0.5830 || timer: 0.1046 sec.
iter 322160 || Loss: 0.6300 || timer: 0.1071 sec.
iter 322170 || Loss: 0.6495 || timer: 0.0838 sec.
iter 322180 || Loss: 0.7091 || timer: 0.1025 sec.
iter 322190 || Loss: 0.5135 || timer: 0.0736 sec.
iter 322200 || Loss: 0.8332 || timer: 0.0878 sec.
iter 322210 || Loss: 0.9197 || timer: 0.0839 sec.
iter 322220 || Loss: 0.5950 || timer: 0.1178 sec.
iter 322230 || Loss: 0.5667 || timer: 0.0973 sec.
iter 322240 || Loss: 0.7351 || timer: 0.0883 sec.
iter 322250 || Loss: 0.7776 || timer: 0.0923 sec.
iter 322260 || Loss: 0.5907 || timer: 0.0826 sec.
iter 322270 || Loss: 0.8655 || timer: 0.0858 sec.
iter 322280 || Loss: 0.7312 || timer: 0.0910 sec.
iter 322290 || Loss: 0.6167 || timer: 0.0767 sec.
iter 322300 || Loss: 0.7022 || timer: 0.0198 sec.
iter 322310 || Loss: 0.3520 || timer: 0.0861 sec.
iter 322320 || Loss: 0.6828 || timer: 0.0827 sec.
iter 322330 || Loss: 0.6999 || timer: 0.0864 sec.
iter 322340 || Loss: 1.0270 || timer: 0.0954 sec.
iter 322350 || Loss: 0.7360 || timer: 0.0895 sec.
iter 322360 || Loss: 0.7697 || timer: 0.1059 sec.
iter 322370 || Loss: 0.4839 || timer: 0.0771 sec.
iter 322380 || Loss: 0.4908 || timer: 0.0866 sec.
iter 322390 || Loss: 0.8164 || timer: 0.0757 sec.
iter 322400 || Loss: 0.7744 || timer: 0.0962 sec.
iter 322410 || Loss: 0.5199 || timer: 0.0923 sec.
iter 322420 || Loss: 0.8012 || timer: 0.0908 sec.
iter 322430 || Loss: 0.7674 || timer: 0.0852 sec.
iter 322440 || Loss: 0.8166 || timer: 0.1037 sec.
iter 322450 || Loss: 0.8398 || timer: 0.0825 sec.
iter 322460 || Loss: 0.7700 || timer: 0.0979 sec.
iter 322470 || Loss: 0.7582 || timer: 0.0829 sec.
iter 322480 || Loss: 0.4938 || timer: 0.0915 sec.
iter 322490 || Loss: 0.4276 || timer: 0.1232 sec.
iter 322500 || Loss: 0.8784 || timer: 0.0870 sec.
iter 322510 || Loss: 0.6292 || timer: 0.1028 sec.
iter 322520 || Loss: 0.7595 || timer: 0.0900 sec.
iter 322530 || Loss: 0.7345 || timer: 0.0959 sec.
iter 322540 || Loss: 0.6180 || timer: 0.0910 sec.
iter 322550 || Loss: 0.9173 || timer: 0.0905 sec.
iter 322560 || Loss: 0.7263 || timer: 0.1064 sec.
iter 322570 || Loss: 0.5899 || timer: 0.0843 sec.
iter 322580 || Loss: 0.5430 || timer: 0.0856 sec.
iter 322590 || Loss: 0.7303 || timer: 0.0929 sec.
iter 322600 || Loss: 0.6486 || timer: 0.1342 sec.
iter 322610 || Loss: 0.6711 || timer: 0.0934 sec.
iter 322620 || Loss: 0.3527 || timer: 0.0992 sec.
iter 322630 || Loss: 0.6086 || timer: 0.0288 sec.
iter 322640 || Loss: 0.5057 || timer: 0.0894 sec.
iter 322650 || Loss: 0.5111 || timer: 0.0860 sec.
iter 322660 || Loss: 0.6502 || timer: 0.0981 sec.
iter 322670 || Loss: 0.6607 || timer: 0.0827 sec.
iter 322680 || Loss: 0.8504 || timer: 0.0909 sec.
iter 322690 || Loss: 0.6873 || timer: 0.0891 sec.
iter 322700 || Loss: 0.6094 || timer: 0.0912 sec.
iter 322710 || Loss: 0.8606 || timer: 0.0887 sec.
iter 322720 || Loss: 0.5919 || timer: 0.0909 sec.
iter 322730 || Loss: 0.6545 || timer: 0.1167 sec.
iter 322740 || Loss: 0.5799 || timer: 0.0967 sec.
iter 322750 || Loss: 0.8883 || timer: 0.0920 sec.
iter 322760 || Loss: 0.7846 || timer: 0.0749 sec.
iter 322770 || Loss: 1.0383 || timer: 0.0762 sec.
iter 322780 || Loss: 0.8161 || timer: 0.0823 sec.
iter 322790 || Loss: 0.5131 || timer: 0.0902 sec.
iter 322800 || Loss: 0.5735 || timer: 0.1096 sec.
iter 322810 || Loss: 0.5967 || timer: 0.0902 sec.
iter 322820 || Loss: 0.6658 || timer: 0.0823 sec.
iter 322830 || Loss: 0.7873 || timer: 0.0848 sec.
iter 322840 || Loss: 0.5719 || timer: 0.0907 sec.
iter 322850 || Loss: 0.5628 || timer: 0.0911 sec.
iter 322860 || Loss: 0.8469 || timer: 0.0936 sec.
iter 322870 || Loss: 0.7171 || timer: 0.0876 sec.
iter 322880 || Loss: 0.6619 || timer: 0.0944 sec.
iter 322890 || Loss: 0.8478 || timer: 0.0924 sec.
iter 322900 || Loss: 0.9348 || timer: 0.1126 sec.
iter 322910 || Loss: 0.8830 || timer: 0.1131 sec.
iter 322920 || Loss: 0.9107 || timer: 0.0910 sec.
iter 322930 || Loss: 0.6539 || timer: 0.0871 sec.
iter 322940 || Loss: 0.8306 || timer: 0.0888 sec.
iter 322950 || Loss: 0.6785 || timer: 0.0940 sec.
iter 322960 || Loss: 0.5517 || timer: 0.0237 sec.
iter 322970 || Loss: 0.5347 || timer: 0.0837 sec.
iter 322980 || Loss: 0.7528 || timer: 0.1017 sec.
iter 322990 || Loss: 0.6575 || timer: 0.0914 sec.
iter 323000 || Loss: 0.7378 || timer: 0.0768 sec.
iter 323010 || Loss: 0.7232 || timer: 0.1032 sec.
iter 323020 || Loss: 0.5581 || timer: 0.0837 sec.
iter 323030 || Loss: 0.6952 || timer: 0.0826 sec.
iter 323040 || Loss: 0.7822 || timer: 0.0842 sec.
iter 323050 || Loss: 0.4960 || timer: 0.1064 sec.
iter 323060 || Loss: 0.5762 || timer: 0.1270 sec.
iter 323070 || Loss: 0.8084 || timer: 0.1098 sec.
iter 323080 || Loss: 0.5940 || timer: 0.0894 sec.
iter 323090 || Loss: 0.7409 || timer: 0.0842 sec.
iter 323100 || Loss: 1.2118 || timer: 0.0829 sec.
iter 323110 || Loss: 0.5658 || timer: 0.0931 sec.
iter 323120 || Loss: 0.8091 || timer: 0.0903 sec.
iter 323130 || Loss: 0.6500 || timer: 0.0923 sec.
iter 323140 || Loss: 0.4654 || timer: 0.0829 sec.
iter 323150 || Loss: 0.5988 || timer: 0.0939 sec.
iter 323160 || Loss: 1.1513 || timer: 0.0940 sec.
iter 323170 || Loss: 0.9061 || timer: 0.0936 sec.
iter 323180 || Loss: 0.5211 || timer: 0.0913 sec.
iter 323190 || Loss: 0.6786 || timer: 0.0831 sec.
iter 323200 || Loss: 0.8879 || timer: 0.0870 sec.
iter 323210 || Loss: 0.7358 || timer: 0.0922 sec.
iter 323220 || Loss: 0.6587 || timer: 0.1057 sec.
iter 323230 || Loss: 0.9897 || timer: 0.0832 sec.
iter 323240 || Loss: 0.7348 || timer: 0.0848 sec.
iter 323250 || Loss: 0.6490 || timer: 0.0835 sec.
iter 323260 || Loss: 0.5718 || timer: 0.0898 sec.
iter 323270 || Loss: 0.6266 || timer: 0.1162 sec.
iter 323280 || Loss: 0.7866 || timer: 0.0886 sec.
iter 323290 || Loss: 0.5915 || timer: 0.0254 sec.
iter 323300 || Loss: 1.4404 || timer: 0.0862 sec.
iter 323310 || Loss: 0.7075 || timer: 0.0902 sec.
iter 323320 || Loss: 0.3999 || timer: 0.0893 sec.
iter 323330 || Loss: 0.5559 || timer: 0.0824 sec.
iter 323340 || Loss: 0.4871 || timer: 0.0943 sec.
iter 323350 || Loss: 0.9268 || timer: 0.0831 sec.
iter 323360 || Loss: 0.7000 || timer: 0.0829 sec.
iter 323370 || Loss: 1.0406 || timer: 0.0975 sec.
iter 323380 || Loss: 0.6856 || timer: 0.1035 sec.
iter 323390 || Loss: 0.5250 || timer: 0.0942 sec.
iter 323400 || Loss: 1.0058 || timer: 0.1046 sec.
iter 323410 || Loss: 0.7794 || timer: 0.0916 sec.
iter 323420 || Loss: 0.6640 || timer: 0.0824 sec.
iter 323430 || Loss: 0.6181 || timer: 0.0901 sec.
iter 323440 || Loss: 0.7390 || timer: 0.1059 sec.
iter 323450 || Loss: 0.5873 || timer: 0.0881 sec.
iter 323460 || Loss: 0.8165 || timer: 0.0927 sec.
iter 323470 || Loss: 0.7092 || timer: 0.0840 sec.
iter 323480 || Loss: 0.6805 || timer: 0.0950 sec.
iter 323490 || Loss: 0.5746 || timer: 0.0906 sec.
iter 323500 || Loss: 0.5458 || timer: 0.0919 sec.
iter 323510 || Loss: 1.1427 || timer: 0.0927 sec.
iter 323520 || Loss: 0.5958 || timer: 0.0900 sec.
iter 323530 || Loss: 0.9639 || timer: 0.0905 sec.
iter 323540 || Loss: 0.7120 || timer: 0.0897 sec.
iter 323550 || Loss: 0.5868 || timer: 0.0980 sec.
iter 323560 || Loss: 0.6205 || timer: 0.0884 sec.
iter 323570 || Loss: 1.1331 || timer: 0.0884 sec.
iter 323580 || Loss: 0.6304 || timer: 0.0970 sec.
iter 323590 || Loss: 0.4979 || timer: 0.1030 sec.
iter 323600 || Loss: 0.7994 || timer: 0.0920 sec.
iter 323610 || Loss: 0.5408 || timer: 0.0850 sec.
iter 323620 || Loss: 0.6742 || timer: 0.0168 sec.
iter 323630 || Loss: 0.0667 || timer: 0.0902 sec.
iter 323640 || Loss: 0.6262 || timer: 0.0830 sec.
iter 323650 || Loss: 0.5940 || timer: 0.0850 sec.
iter 323660 || Loss: 0.5658 || timer: 0.1129 sec.
iter 323670 || Loss: 0.8346 || timer: 0.1008 sec.
iter 323680 || Loss: 0.7002 || timer: 0.0835 sec.
iter 323690 || Loss: 1.2351 || timer: 0.0914 sec.
iter 323700 || Loss: 0.6852 || timer: 0.0924 sec.
iter 323710 || Loss: 0.9355 || timer: 0.0928 sec.
iter 323720 || Loss: 0.6150 || timer: 0.1249 sec.
iter 323730 || Loss: 0.5592 || timer: 0.0830 sec.
iter 323740 || Loss: 0.6941 || timer: 0.0878 sec.
iter 323750 || Loss: 0.6880 || timer: 0.0916 sec.
iter 323760 || Loss: 0.5250 || timer: 0.0836 sec.
iter 323770 || Loss: 0.8134 || timer: 0.0875 sec.
iter 323780 || Loss: 0.6391 || timer: 0.1107 sec.
iter 323790 || Loss: 0.6098 || timer: 0.0925 sec.
iter 323800 || Loss: 0.5396 || timer: 0.0943 sec.
iter 323810 || Loss: 0.7028 || timer: 0.0911 sec.
iter 323820 || Loss: 0.6515 || timer: 0.0911 sec.
iter 323830 || Loss: 0.4968 || timer: 0.0840 sec.
iter 323840 || Loss: 0.5910 || timer: 0.0904 sec.
iter 323850 || Loss: 0.7192 || timer: 0.0910 sec.
iter 323860 || Loss: 0.6859 || timer: 0.1179 sec.
iter 323870 || Loss: 0.8633 || timer: 0.0914 sec.
iter 323880 || Loss: 0.6400 || timer: 0.1140 sec.
iter 323890 || Loss: 0.6403 || timer: 0.0951 sec.
iter 323900 || Loss: 0.9312 || timer: 0.0928 sec.
iter 323910 || Loss: 0.4864 || timer: 0.0913 sec.
iter 323920 || Loss: 0.7229 || timer: 0.0881 sec.
iter 323930 || Loss: 0.8071 || timer: 0.0923 sec.
iter 323940 || Loss: 0.5827 || timer: 0.0907 sec.
iter 323950 || Loss: 0.9156 || timer: 0.0306 sec.
iter 323960 || Loss: 0.6420 || timer: 0.1143 sec.
iter 323970 || Loss: 0.4861 || timer: 0.0837 sec.
iter 323980 || Loss: 0.6961 || timer: 0.1087 sec.
iter 323990 || Loss: 0.6844 || timer: 0.0959 sec.
iter 324000 || Loss: 0.7437 || timer: 0.0911 sec.
iter 324010 || Loss: 0.8160 || timer: 0.1174 sec.
iter 324020 || Loss: 0.5812 || timer: 0.0930 sec.
iter 324030 || Loss: 0.9628 || timer: 0.0855 sec.
iter 324040 || Loss: 0.5902 || timer: 0.0918 sec.
iter 324050 || Loss: 0.7234 || timer: 0.0977 sec.
iter 324060 || Loss: 0.6891 || timer: 0.0788 sec.
iter 324070 || Loss: 0.3695 || timer: 0.0771 sec.
iter 324080 || Loss: 0.6010 || timer: 0.0895 sec.
iter 324090 || Loss: 0.6517 || timer: 0.0926 sec.
iter 324100 || Loss: 0.5871 || timer: 0.1065 sec.
iter 324110 || Loss: 0.7785 || timer: 0.0840 sec.
iter 324120 || Loss: 0.7249 || timer: 0.0896 sec.
iter 324130 || Loss: 0.9846 || timer: 0.1097 sec.
iter 324140 || Loss: 0.6563 || timer: 0.0921 sec.
iter 324150 || Loss: 0.7238 || timer: 0.0861 sec.
iter 324160 || Loss: 0.5639 || timer: 0.0893 sec.
iter 324170 || Loss: 1.0357 || timer: 0.0918 sec.
iter 324180 || Loss: 0.7206 || timer: 0.0926 sec.
iter 324190 || Loss: 0.5144 || timer: 0.0895 sec.
iter 324200 || Loss: 0.6740 || timer: 0.0979 sec.
iter 324210 || Loss: 0.6011 || timer: 0.0929 sec.
iter 324220 || Loss: 0.6486 || timer: 0.1062 sec.
iter 324230 || Loss: 0.6982 || timer: 0.0905 sec.
iter 324240 || Loss: 0.6480 || timer: 0.0824 sec.
iter 324250 || Loss: 0.8481 || timer: 0.0914 sec.
iter 324260 || Loss: 0.5954 || timer: 0.0892 sec.
iter 324270 || Loss: 0.7771 || timer: 0.1145 sec.
iter 324280 || Loss: 0.6893 || timer: 0.0170 sec.
iter 324290 || Loss: 0.4078 || timer: 0.0843 sec.
iter 324300 || Loss: 0.8989 || timer: 0.0846 sec.
iter 324310 || Loss: 0.7932 || timer: 0.0833 sec.
iter 324320 || Loss: 0.5660 || timer: 0.0899 sec.
iter 324330 || Loss: 0.6173 || timer: 0.1051 sec.
iter 324340 || Loss: 0.5741 || timer: 0.0922 sec.
iter 324350 || Loss: 0.7217 || timer: 0.0841 sec.
iter 324360 || Loss: 0.8070 || timer: 0.0919 sec.
iter 324370 || Loss: 0.7184 || timer: 0.1020 sec.
iter 324380 || Loss: 0.5692 || timer: 0.1005 sec.
iter 324390 || Loss: 0.7567 || timer: 0.0833 sec.
iter 324400 || Loss: 0.7379 || timer: 0.0896 sec.
iter 324410 || Loss: 0.7836 || timer: 0.0833 sec.
iter 324420 || Loss: 0.6852 || timer: 0.0970 sec.
iter 324430 || Loss: 0.7668 || timer: 0.0859 sec.
iter 324440 || Loss: 0.8135 || timer: 0.0915 sec.
iter 324450 || Loss: 0.5211 || timer: 0.0909 sec.
iter 324460 || Loss: 0.5385 || timer: 0.0929 sec.
iter 324470 || Loss: 0.7957 || timer: 0.1047 sec.
iter 324480 || Loss: 0.8811 || timer: 0.0907 sec.
iter 324490 || Loss: 0.6847 || timer: 0.0925 sec.
iter 324500 || Loss: 0.8819 || timer: 0.0889 sec.
iter 324510 || Loss: 0.6540 || timer: 0.0906 sec.
iter 324520 || Loss: 0.6981 || timer: 0.0926 sec.
iter 324530 || Loss: 0.4760 || timer: 0.0891 sec.
iter 324540 || Loss: 0.5844 || timer: 0.0945 sec.
iter 324550 || Loss: 0.6151 || timer: 0.0770 sec.
iter 324560 || Loss: 0.7492 || timer: 0.0768 sec.
iter 324570 || Loss: 0.5058 || timer: 0.0897 sec.
iter 324580 || Loss: 0.7275 || timer: 0.0895 sec.
iter 324590 || Loss: 0.5016 || timer: 0.0846 sec.
iter 324600 || Loss: 0.5914 || timer: 0.1127 sec.
iter 324610 || Loss: 0.6898 || timer: 0.0279 sec.
iter 324620 || Loss: 0.4012 || timer: 0.0917 sec.
iter 324630 || Loss: 0.6827 || timer: 0.0905 sec.
iter 324640 || Loss: 0.6149 || timer: 0.0932 sec.
iter 324650 || Loss: 0.8200 || timer: 0.0913 sec.
iter 324660 || Loss: 0.8497 || timer: 0.0913 sec.
iter 324670 || Loss: 0.9038 || timer: 0.0758 sec.
iter 324680 || Loss: 0.6912 || timer: 0.0934 sec.
iter 324690 || Loss: 0.8810 || timer: 0.0896 sec.
iter 324700 || Loss: 0.8181 || timer: 0.0854 sec.
iter 324710 || Loss: 0.7693 || timer: 0.1427 sec.
iter 324720 || Loss: 0.7751 || timer: 0.0840 sec.
iter 324730 || Loss: 0.7968 || timer: 0.0847 sec.
iter 324740 || Loss: 0.8612 || timer: 0.0884 sec.
iter 324750 || Loss: 0.6563 || timer: 0.0934 sec.
iter 324760 || Loss: 0.8465 || timer: 0.0844 sec.
iter 324770 || Loss: 0.6166 || timer: 0.0848 sec.
iter 324780 || Loss: 0.4898 || timer: 0.0942 sec.
iter 324790 || Loss: 0.6343 || timer: 0.0838 sec.
iter 324800 || Loss: 0.6223 || timer: 0.1075 sec.
iter 324810 || Loss: 0.5804 || timer: 0.0933 sec.
iter 324820 || Loss: 0.8345 || timer: 0.0949 sec.
iter 324830 || Loss: 0.7106 || timer: 0.0836 sec.
iter 324840 || Loss: 0.7827 || timer: 0.0917 sec.
iter 324850 || Loss: 0.5532 || timer: 0.0931 sec.
iter 324860 || Loss: 0.6214 || timer: 0.0859 sec.
iter 324870 || Loss: 0.7836 || timer: 0.1030 sec.
iter 324880 || Loss: 0.5981 || timer: 0.0916 sec.
iter 324890 || Loss: 0.7581 || timer: 0.0868 sec.
iter 324900 || Loss: 0.7569 || timer: 0.0766 sec.
iter 324910 || Loss: 0.6742 || timer: 0.0883 sec.
iter 324920 || Loss: 0.6404 || timer: 0.1055 sec.
iter 324930 || Loss: 0.8555 || timer: 0.0901 sec.
iter 324940 || Loss: 0.8024 || timer: 0.0258 sec.
iter 324950 || Loss: 1.5095 || timer: 0.1242 sec.
iter 324960 || Loss: 0.5651 || timer: 0.0921 sec.
iter 324970 || Loss: 0.5826 || timer: 0.0924 sec.
iter 324980 || Loss: 0.6048 || timer: 0.0844 sec.
iter 324990 || Loss: 0.5847 || timer: 0.0921 sec.
iter 325000 || Loss: 0.7014 || Saving state, iter: 325000
timer: 0.0862 sec.
iter 325010 || Loss: 0.4995 || timer: 0.0788 sec.
iter 325020 || Loss: 0.5933 || timer: 0.0822 sec.
iter 325030 || Loss: 0.6805 || timer: 0.0892 sec.
iter 325040 || Loss: 0.6714 || timer: 0.0977 sec.
iter 325050 || Loss: 0.7691 || timer: 0.0890 sec.
iter 325060 || Loss: 0.6072 || timer: 0.0967 sec.
iter 325070 || Loss: 1.1415 || timer: 0.0832 sec.
iter 325080 || Loss: 0.6686 || timer: 0.0896 sec.
iter 325090 || Loss: 0.8438 || timer: 0.0869 sec.
iter 325100 || Loss: 0.7299 || timer: 0.0813 sec.
iter 325110 || Loss: 0.8476 || timer: 0.0926 sec.
iter 325120 || Loss: 0.8069 || timer: 0.0810 sec.
iter 325130 || Loss: 0.7150 || timer: 0.1163 sec.
iter 325140 || Loss: 0.6287 || timer: 0.0810 sec.
iter 325150 || Loss: 0.6405 || timer: 0.0917 sec.
iter 325160 || Loss: 0.7085 || timer: 0.1277 sec.
iter 325170 || Loss: 0.5053 || timer: 0.0816 sec.
iter 325180 || Loss: 0.8050 || timer: 0.0888 sec.
iter 325190 || Loss: 0.5707 || timer: 0.0829 sec.
iter 325200 || Loss: 0.6898 || timer: 0.0863 sec.
iter 325210 || Loss: 0.6199 || timer: 0.0814 sec.
iter 325220 || Loss: 0.6538 || timer: 0.0926 sec.
iter 325230 || Loss: 0.7434 || timer: 0.1069 sec.
iter 325240 || Loss: 0.6612 || timer: 0.0805 sec.
iter 325250 || Loss: 0.7544 || timer: 0.0890 sec.
iter 325260 || Loss: 0.5956 || timer: 0.0897 sec.
iter 325270 || Loss: 0.6076 || timer: 0.0267 sec.
iter 325280 || Loss: 2.6345 || timer: 0.0918 sec.
iter 325290 || Loss: 0.6886 || timer: 0.0820 sec.
iter 325300 || Loss: 0.5602 || timer: 0.0908 sec.
iter 325310 || Loss: 1.1213 || timer: 0.0920 sec.
iter 325320 || Loss: 0.6602 || timer: 0.0872 sec.
iter 325330 || Loss: 0.7292 || timer: 0.0893 sec.
iter 325340 || Loss: 0.5070 || timer: 0.0934 sec.
iter 325350 || Loss: 0.7509 || timer: 0.0914 sec.
iter 325360 || Loss: 0.5746 || timer: 0.0828 sec.
iter 325370 || Loss: 0.7802 || timer: 0.1146 sec.
iter 325380 || Loss: 0.6744 || timer: 0.0856 sec.
iter 325390 || Loss: 0.5375 || timer: 0.0894 sec.
iter 325400 || Loss: 0.5016 || timer: 0.0914 sec.
iter 325410 || Loss: 0.7030 || timer: 0.0867 sec.
iter 325420 || Loss: 0.9841 || timer: 0.0886 sec.
iter 325430 || Loss: 0.7069 || timer: 0.0811 sec.
iter 325440 || Loss: 0.7222 || timer: 0.0793 sec.
iter 325450 || Loss: 0.7181 || timer: 0.0899 sec.
iter 325460 || Loss: 0.5380 || timer: 0.0844 sec.
iter 325470 || Loss: 0.9738 || timer: 0.1098 sec.
iter 325480 || Loss: 0.7577 || timer: 0.0893 sec.
iter 325490 || Loss: 0.5913 || timer: 0.0846 sec.
iter 325500 || Loss: 0.5956 || timer: 0.0880 sec.
iter 325510 || Loss: 0.8412 || timer: 0.0817 sec.
iter 325520 || Loss: 0.5616 || timer: 0.0918 sec.
iter 325530 || Loss: 0.8355 || timer: 0.0911 sec.
iter 325540 || Loss: 0.9955 || timer: 0.0890 sec.
iter 325550 || Loss: 0.4305 || timer: 0.1243 sec.
iter 325560 || Loss: 0.7381 || timer: 0.0814 sec.
iter 325570 || Loss: 0.7061 || timer: 0.0902 sec.
iter 325580 || Loss: 0.6571 || timer: 0.0874 sec.
iter 325590 || Loss: 0.5824 || timer: 0.0818 sec.
iter 325600 || Loss: 0.7096 || timer: 0.0243 sec.
iter 325610 || Loss: 0.0960 || timer: 0.1016 sec.
iter 325620 || Loss: 0.8543 || timer: 0.1078 sec.
iter 325630 || Loss: 0.6305 || timer: 0.0851 sec.
iter 325640 || Loss: 1.0338 || timer: 0.0885 sec.
iter 325650 || Loss: 0.7700 || timer: 0.0903 sec.
iter 325660 || Loss: 0.8946 || timer: 0.0894 sec.
iter 325670 || Loss: 0.8183 || timer: 0.0892 sec.
iter 325680 || Loss: 0.7162 || timer: 0.0907 sec.
iter 325690 || Loss: 0.9849 || timer: 0.1034 sec.
iter 325700 || Loss: 0.6150 || timer: 0.1200 sec.
iter 325710 || Loss: 0.6989 || timer: 0.0821 sec.
iter 325720 || Loss: 0.8672 || timer: 0.1101 sec.
iter 325730 || Loss: 0.6319 || timer: 0.0908 sec.
iter 325740 || Loss: 0.6436 || timer: 0.0884 sec.
iter 325750 || Loss: 0.8566 || timer: 0.0904 sec.
iter 325760 || Loss: 0.5619 || timer: 0.1022 sec.
iter 325770 || Loss: 0.8300 || timer: 0.0905 sec.
iter 325780 || Loss: 0.7668 || timer: 0.0911 sec.
iter 325790 || Loss: 0.7209 || timer: 0.0810 sec.
iter 325800 || Loss: 0.6517 || timer: 0.1027 sec.
iter 325810 || Loss: 0.5362 || timer: 0.0891 sec.
iter 325820 || Loss: 0.7524 || timer: 0.1057 sec.
iter 325830 || Loss: 0.6983 || timer: 0.0817 sec.
iter 325840 || Loss: 0.8278 || timer: 0.1013 sec.
iter 325850 || Loss: 0.5930 || timer: 0.1054 sec.
iter 325860 || Loss: 0.8119 || timer: 0.0928 sec.
iter 325870 || Loss: 0.7078 || timer: 0.0890 sec.
iter 325880 || Loss: 0.6117 || timer: 0.1129 sec.
iter 325890 || Loss: 0.5731 || timer: 0.1007 sec.
iter 325900 || Loss: 0.5952 || timer: 0.0867 sec.
iter 325910 || Loss: 0.6648 || timer: 0.0845 sec.
iter 325920 || Loss: 0.5047 || timer: 0.0891 sec.
iter 325930 || Loss: 0.5692 || timer: 0.0168 sec.
iter 325940 || Loss: 0.2839 || timer: 0.0893 sec.
iter 325950 || Loss: 0.5984 || timer: 0.0832 sec.
iter 325960 || Loss: 0.7371 || timer: 0.0916 sec.
iter 325970 || Loss: 1.1691 || timer: 0.0818 sec.
iter 325980 || Loss: 0.6086 || timer: 0.1008 sec.
iter 325990 || Loss: 0.6553 || timer: 0.0860 sec.
iter 326000 || Loss: 0.7668 || timer: 0.0921 sec.
iter 326010 || Loss: 0.5830 || timer: 0.0917 sec.
iter 326020 || Loss: 0.6720 || timer: 0.0901 sec.
iter 326030 || Loss: 0.7260 || timer: 0.0957 sec.
iter 326040 || Loss: 0.6979 || timer: 0.0822 sec.
iter 326050 || Loss: 0.5501 || timer: 0.0901 sec.
iter 326060 || Loss: 0.8229 || timer: 0.0817 sec.
iter 326070 || Loss: 0.6289 || timer: 0.0914 sec.
iter 326080 || Loss: 0.6277 || timer: 0.0819 sec.
iter 326090 || Loss: 0.6150 || timer: 0.0811 sec.
iter 326100 || Loss: 0.7587 || timer: 0.0848 sec.
iter 326110 || Loss: 0.8116 || timer: 0.0823 sec.
iter 326120 || Loss: 0.7632 || timer: 0.0981 sec.
iter 326130 || Loss: 0.8821 || timer: 0.1090 sec.
iter 326140 || Loss: 0.9781 || timer: 0.0943 sec.
iter 326150 || Loss: 0.8740 || timer: 0.1181 sec.
iter 326160 || Loss: 0.6373 || timer: 0.0893 sec.
iter 326170 || Loss: 0.7955 || timer: 0.0921 sec.
iter 326180 || Loss: 0.6680 || timer: 0.0886 sec.
iter 326190 || Loss: 0.5566 || timer: 0.0903 sec.
iter 326200 || Loss: 0.8816 || timer: 0.0891 sec.
iter 326210 || Loss: 0.7514 || timer: 0.1053 sec.
iter 326220 || Loss: 0.6953 || timer: 0.0826 sec.
iter 326230 || Loss: 0.7299 || timer: 0.0924 sec.
iter 326240 || Loss: 0.7257 || timer: 0.0884 sec.
iter 326250 || Loss: 0.6070 || timer: 0.0825 sec.
iter 326260 || Loss: 0.8780 || timer: 0.0189 sec.
iter 326270 || Loss: 0.4177 || timer: 0.0814 sec.
iter 326280 || Loss: 0.8878 || timer: 0.0904 sec.
iter 326290 || Loss: 0.8126 || timer: 0.1072 sec.
iter 326300 || Loss: 0.6488 || timer: 0.0905 sec.
iter 326310 || Loss: 0.6293 || timer: 0.0881 sec.
iter 326320 || Loss: 0.6382 || timer: 0.1032 sec.
iter 326330 || Loss: 0.7766 || timer: 0.1058 sec.
iter 326340 || Loss: 0.5923 || timer: 0.0882 sec.
iter 326350 || Loss: 1.1362 || timer: 0.0905 sec.
iter 326360 || Loss: 0.5820 || timer: 0.0944 sec.
iter 326370 || Loss: 0.6549 || timer: 0.0861 sec.
iter 326380 || Loss: 0.8086 || timer: 0.0996 sec.
iter 326390 || Loss: 0.7356 || timer: 0.0900 sec.
iter 326400 || Loss: 0.6499 || timer: 0.0884 sec.
iter 326410 || Loss: 0.7293 || timer: 0.0900 sec.
iter 326420 || Loss: 0.8204 || timer: 0.0948 sec.
iter 326430 || Loss: 0.5731 || timer: 0.1273 sec.
iter 326440 || Loss: 0.5479 || timer: 0.0818 sec.
iter 326450 || Loss: 0.7814 || timer: 0.0928 sec.
iter 326460 || Loss: 0.6999 || timer: 0.0811 sec.
iter 326470 || Loss: 0.6476 || timer: 0.1289 sec.
iter 326480 || Loss: 0.7056 || timer: 0.0827 sec.
iter 326490 || Loss: 0.7376 || timer: 0.0832 sec.
iter 326500 || Loss: 0.6529 || timer: 0.0893 sec.
iter 326510 || Loss: 0.6000 || timer: 0.0885 sec.
iter 326520 || Loss: 0.5389 || timer: 0.0915 sec.
iter 326530 || Loss: 1.1179 || timer: 0.0848 sec.
iter 326540 || Loss: 0.9541 || timer: 0.0816 sec.
iter 326550 || Loss: 0.6649 || timer: 0.0945 sec.
iter 326560 || Loss: 0.8433 || timer: 0.0892 sec.
iter 326570 || Loss: 0.7075 || timer: 0.0882 sec.
iter 326580 || Loss: 0.7880 || timer: 0.0822 sec.
iter 326590 || Loss: 0.5917 || timer: 0.0300 sec.
iter 326600 || Loss: 1.0489 || timer: 0.0894 sec.
iter 326610 || Loss: 0.6414 || timer: 0.1003 sec.
iter 326620 || Loss: 1.2136 || timer: 0.0917 sec.
iter 326630 || Loss: 0.4391 || timer: 0.0815 sec.
iter 326640 || Loss: 0.6771 || timer: 0.0884 sec.
iter 326650 || Loss: 0.5904 || timer: 0.0804 sec.
iter 326660 || Loss: 0.6107 || timer: 0.0957 sec.
iter 326670 || Loss: 0.4811 || timer: 0.0975 sec.
iter 326680 || Loss: 0.6442 || timer: 0.0981 sec.
iter 326690 || Loss: 0.8443 || timer: 0.1255 sec.
iter 326700 || Loss: 0.6485 || timer: 0.0830 sec.
iter 326710 || Loss: 0.6389 || timer: 0.0832 sec.
iter 326720 || Loss: 0.7525 || timer: 0.0885 sec.
iter 326730 || Loss: 0.7365 || timer: 0.0857 sec.
iter 326740 || Loss: 0.7355 || timer: 0.0756 sec.
iter 326750 || Loss: 0.7498 || timer: 0.1069 sec.
iter 326760 || Loss: 0.6578 || timer: 0.0923 sec.
iter 326770 || Loss: 0.7650 || timer: 0.0891 sec.
iter 326780 || Loss: 0.8265 || timer: 0.0876 sec.
iter 326790 || Loss: 0.6793 || timer: 0.0923 sec.
iter 326800 || Loss: 0.8993 || timer: 0.0829 sec.
iter 326810 || Loss: 0.7301 || timer: 0.0919 sec.
iter 326820 || Loss: 0.4915 || timer: 0.0826 sec.
iter 326830 || Loss: 0.8754 || timer: 0.0898 sec.
iter 326840 || Loss: 0.6776 || timer: 0.0899 sec.
iter 326850 || Loss: 0.8297 || timer: 0.0887 sec.
iter 326860 || Loss: 0.7547 || timer: 0.0904 sec.
iter 326870 || Loss: 0.6218 || timer: 0.0993 sec.
iter 326880 || Loss: 0.7864 || timer: 0.0898 sec.
iter 326890 || Loss: 0.6920 || timer: 0.0899 sec.
iter 326900 || Loss: 0.8873 || timer: 0.0814 sec.
iter 326910 || Loss: 0.6687 || timer: 0.0843 sec.
iter 326920 || Loss: 0.7702 || timer: 0.0306 sec.
iter 326930 || Loss: 0.5762 || timer: 0.0821 sec.
iter 326940 || Loss: 0.7433 || timer: 0.0911 sec.
iter 326950 || Loss: 0.8822 || timer: 0.0891 sec.
iter 326960 || Loss: 0.7599 || timer: 0.0916 sec.
iter 326970 || Loss: 0.6731 || timer: 0.1038 sec.
iter 326980 || Loss: 0.7334 || timer: 0.0877 sec.
iter 326990 || Loss: 0.4766 || timer: 0.0939 sec.
iter 327000 || Loss: 0.7974 || timer: 0.0897 sec.
iter 327010 || Loss: 0.5254 || timer: 0.0896 sec.
iter 327020 || Loss: 0.5850 || timer: 0.1168 sec.
iter 327030 || Loss: 0.5492 || timer: 0.0889 sec.
iter 327040 || Loss: 0.4731 || timer: 0.0927 sec.
iter 327050 || Loss: 0.6458 || timer: 0.0898 sec.
iter 327060 || Loss: 1.0354 || timer: 0.0903 sec.
iter 327070 || Loss: 1.1144 || timer: 0.0902 sec.
iter 327080 || Loss: 0.5696 || timer: 0.0856 sec.
iter 327090 || Loss: 0.7181 || timer: 0.1039 sec.
iter 327100 || Loss: 0.8295 || timer: 0.1078 sec.
iter 327110 || Loss: 0.5770 || timer: 0.0940 sec.
iter 327120 || Loss: 0.4692 || timer: 0.0939 sec.
iter 327130 || Loss: 0.5841 || timer: 0.0928 sec.
iter 327140 || Loss: 0.8607 || timer: 0.0919 sec.
iter 327150 || Loss: 0.9904 || timer: 0.0826 sec.
iter 327160 || Loss: 0.6031 || timer: 0.0919 sec.
iter 327170 || Loss: 0.6150 || timer: 0.0913 sec.
iter 327180 || Loss: 0.6377 || timer: 0.1042 sec.
iter 327190 || Loss: 0.7423 || timer: 0.0865 sec.
iter 327200 || Loss: 0.3812 || timer: 0.0832 sec.
iter 327210 || Loss: 0.6284 || timer: 0.0829 sec.
iter 327220 || Loss: 0.6134 || timer: 0.0950 sec.
iter 327230 || Loss: 0.7798 || timer: 0.1100 sec.
iter 327240 || Loss: 0.6249 || timer: 0.0866 sec.
iter 327250 || Loss: 0.4574 || timer: 0.0196 sec.
iter 327260 || Loss: 0.8457 || timer: 0.0940 sec.
iter 327270 || Loss: 0.4426 || timer: 0.0930 sec.
iter 327280 || Loss: 0.6879 || timer: 0.0880 sec.
iter 327290 || Loss: 0.8039 || timer: 0.0906 sec.
iter 327300 || Loss: 0.6437 || timer: 0.0820 sec.
iter 327310 || Loss: 0.6883 || timer: 0.0926 sec.
iter 327320 || Loss: 0.4179 || timer: 0.0910 sec.
iter 327330 || Loss: 0.9249 || timer: 0.0917 sec.
iter 327340 || Loss: 0.6350 || timer: 0.0845 sec.
iter 327350 || Loss: 0.6293 || timer: 0.0974 sec.
iter 327360 || Loss: 0.6669 || timer: 0.0834 sec.
iter 327370 || Loss: 0.6191 || timer: 0.0775 sec.
iter 327380 || Loss: 0.6303 || timer: 0.0914 sec.
iter 327390 || Loss: 0.7388 || timer: 0.0921 sec.
iter 327400 || Loss: 0.6469 || timer: 0.1031 sec.
iter 327410 || Loss: 0.7906 || timer: 0.1013 sec.
iter 327420 || Loss: 0.5801 || timer: 0.1084 sec.
iter 327430 || Loss: 0.9555 || timer: 0.0904 sec.
iter 327440 || Loss: 0.8087 || timer: 0.0821 sec.
iter 327450 || Loss: 0.8032 || timer: 0.0863 sec.
iter 327460 || Loss: 0.8230 || timer: 0.0836 sec.
iter 327470 || Loss: 0.6317 || timer: 0.0812 sec.
iter 327480 || Loss: 0.4484 || timer: 0.0818 sec.
iter 327490 || Loss: 0.5878 || timer: 0.0823 sec.
iter 327500 || Loss: 0.7609 || timer: 0.0908 sec.
iter 327510 || Loss: 0.4624 || timer: 0.0868 sec.
iter 327520 || Loss: 0.4998 || timer: 0.0878 sec.
iter 327530 || Loss: 0.5398 || timer: 0.0911 sec.
iter 327540 || Loss: 0.4714 || timer: 0.0901 sec.
iter 327550 || Loss: 0.5934 || timer: 0.0870 sec.
iter 327560 || Loss: 0.7925 || timer: 0.0890 sec.
iter 327570 || Loss: 0.6874 || timer: 0.1030 sec.
iter 327580 || Loss: 0.4824 || timer: 0.0193 sec.
iter 327590 || Loss: 1.6886 || timer: 0.0948 sec.
iter 327600 || Loss: 0.8024 || timer: 0.1526 sec.
iter 327610 || Loss: 0.5441 || timer: 0.0819 sec.
iter 327620 || Loss: 0.5601 || timer: 0.0915 sec.
iter 327630 || Loss: 0.7878 || timer: 0.0901 sec.
iter 327640 || Loss: 0.6452 || timer: 0.0892 sec.
iter 327650 || Loss: 0.6971 || timer: 0.0901 sec.
iter 327660 || Loss: 0.7490 || timer: 0.0917 sec.
iter 327670 || Loss: 0.5152 || timer: 0.0908 sec.
iter 327680 || Loss: 1.0028 || timer: 0.1162 sec.
iter 327690 || Loss: 0.7337 || timer: 0.0898 sec.
iter 327700 || Loss: 0.6701 || timer: 0.0822 sec.
iter 327710 || Loss: 0.8430 || timer: 0.1187 sec.
iter 327720 || Loss: 0.8042 || timer: 0.0865 sec.
iter 327730 || Loss: 0.6246 || timer: 0.0898 sec.
iter 327740 || Loss: 0.6885 || timer: 0.0909 sec.
iter 327750 || Loss: 0.7547 || timer: 0.0875 sec.
iter 327760 || Loss: 0.8360 || timer: 0.1373 sec.
iter 327770 || Loss: 0.7571 || timer: 0.0906 sec.
iter 327780 || Loss: 0.7134 || timer: 0.0904 sec.
iter 327790 || Loss: 0.6317 || timer: 0.0829 sec.
iter 327800 || Loss: 0.4699 || timer: 0.0919 sec.
iter 327810 || Loss: 0.7169 || timer: 0.0820 sec.
iter 327820 || Loss: 0.4685 || timer: 0.0828 sec.
iter 327830 || Loss: 0.6006 || timer: 0.0919 sec.
iter 327840 || Loss: 0.7039 || timer: 0.0923 sec.
iter 327850 || Loss: 0.7664 || timer: 0.0888 sec.
iter 327860 || Loss: 0.5612 || timer: 0.0819 sec.
iter 327870 || Loss: 0.6264 || timer: 0.0828 sec.
iter 327880 || Loss: 0.6115 || timer: 0.0908 sec.
iter 327890 || Loss: 0.5870 || timer: 0.0835 sec.
iter 327900 || Loss: 0.6017 || timer: 0.1286 sec.
iter 327910 || Loss: 0.6376 || timer: 0.0190 sec.
iter 327920 || Loss: 0.9411 || timer: 0.0825 sec.
iter 327930 || Loss: 0.6271 || timer: 0.0918 sec.
iter 327940 || Loss: 0.7942 || timer: 0.0831 sec.
iter 327950 || Loss: 0.6745 || timer: 0.0913 sec.
iter 327960 || Loss: 0.5735 || timer: 0.0913 sec.
iter 327970 || Loss: 0.4778 || timer: 0.0921 sec.
iter 327980 || Loss: 0.6198 || timer: 0.0900 sec.
iter 327990 || Loss: 0.7418 || timer: 0.0925 sec.
iter 328000 || Loss: 0.6761 || timer: 0.0900 sec.
iter 328010 || Loss: 0.6211 || timer: 0.0945 sec.
iter 328020 || Loss: 0.6080 || timer: 0.0933 sec.
iter 328030 || Loss: 1.0931 || timer: 0.0898 sec.
iter 328040 || Loss: 0.6571 || timer: 0.0915 sec.
iter 328050 || Loss: 0.6049 || timer: 0.0892 sec.
iter 328060 || Loss: 0.7433 || timer: 0.0941 sec.
iter 328070 || Loss: 0.4559 || timer: 0.1131 sec.
iter 328080 || Loss: 0.4921 || timer: 0.1150 sec.
iter 328090 || Loss: 0.9709 || timer: 0.0871 sec.
iter 328100 || Loss: 1.1388 || timer: 0.0909 sec.
iter 328110 || Loss: 0.6817 || timer: 0.0875 sec.
iter 328120 || Loss: 1.1800 || timer: 0.0880 sec.
iter 328130 || Loss: 0.9142 || timer: 0.0907 sec.
iter 328140 || Loss: 0.7280 || timer: 0.0908 sec.
iter 328150 || Loss: 0.8527 || timer: 0.1043 sec.
iter 328160 || Loss: 0.6363 || timer: 0.0899 sec.
iter 328170 || Loss: 0.6867 || timer: 0.0930 sec.
iter 328180 || Loss: 0.6076 || timer: 0.1061 sec.
iter 328190 || Loss: 0.8337 || timer: 0.0940 sec.
iter 328200 || Loss: 0.4710 || timer: 0.0939 sec.
iter 328210 || Loss: 0.5722 || timer: 0.0930 sec.
iter 328220 || Loss: 0.5304 || timer: 0.1102 sec.
iter 328230 || Loss: 0.8603 || timer: 0.0823 sec.
iter 328240 || Loss: 0.6757 || timer: 0.0293 sec.
iter 328250 || Loss: 0.3364 || timer: 0.0930 sec.
iter 328260 || Loss: 0.7292 || timer: 0.0906 sec.
iter 328270 || Loss: 0.8711 || timer: 0.0936 sec.
iter 328280 || Loss: 0.8200 || timer: 0.0920 sec.
iter 328290 || Loss: 0.6400 || timer: 0.0887 sec.
iter 328300 || Loss: 0.5809 || timer: 0.0842 sec.
iter 328310 || Loss: 0.5582 || timer: 0.0886 sec.
iter 328320 || Loss: 0.6809 || timer: 0.0984 sec.
iter 328330 || Loss: 0.4317 || timer: 0.0825 sec.
iter 328340 || Loss: 0.5457 || timer: 0.1184 sec.
iter 328350 || Loss: 0.6349 || timer: 0.0931 sec.
iter 328360 || Loss: 0.7814 || timer: 0.0926 sec.
iter 328370 || Loss: 0.7160 || timer: 0.0910 sec.
iter 328380 || Loss: 0.7401 || timer: 0.0910 sec.
iter 328390 || Loss: 0.8323 || timer: 0.0908 sec.
iter 328400 || Loss: 0.8899 || timer: 0.1021 sec.
iter 328410 || Loss: 0.8916 || timer: 0.0907 sec.
iter 328420 || Loss: 0.5554 || timer: 0.0911 sec.
iter 328430 || Loss: 0.6777 || timer: 0.1024 sec.
iter 328440 || Loss: 0.8591 || timer: 0.0858 sec.
iter 328450 || Loss: 0.9247 || timer: 0.0872 sec.
iter 328460 || Loss: 0.6632 || timer: 0.0890 sec.
iter 328470 || Loss: 1.0410 || timer: 0.0917 sec.
iter 328480 || Loss: 0.6199 || timer: 0.1058 sec.
iter 328490 || Loss: 0.8561 || timer: 0.0910 sec.
iter 328500 || Loss: 0.6177 || timer: 0.0842 sec.
iter 328510 || Loss: 0.6482 || timer: 0.0900 sec.
iter 328520 || Loss: 0.7283 || timer: 0.0835 sec.
iter 328530 || Loss: 0.4313 || timer: 0.0839 sec.
iter 328540 || Loss: 0.8027 || timer: 0.0843 sec.
iter 328550 || Loss: 0.6191 || timer: 0.0865 sec.
iter 328560 || Loss: 0.8975 || timer: 0.0909 sec.
iter 328570 || Loss: 0.6508 || timer: 0.0242 sec.
iter 328580 || Loss: 0.7127 || timer: 0.0917 sec.
iter 328590 || Loss: 0.9716 || timer: 0.0897 sec.
iter 328600 || Loss: 0.7210 || timer: 0.0909 sec.
iter 328610 || Loss: 0.8309 || timer: 0.0864 sec.
iter 328620 || Loss: 0.9030 || timer: 0.0835 sec.
iter 328630 || Loss: 0.6186 || timer: 0.0850 sec.
iter 328640 || Loss: 0.6770 || timer: 0.1162 sec.
iter 328650 || Loss: 0.6715 || timer: 0.1024 sec.
iter 328660 || Loss: 0.4517 || timer: 0.0849 sec.
iter 328670 || Loss: 0.6365 || timer: 0.0952 sec.
iter 328680 || Loss: 0.8216 || timer: 0.0833 sec.
iter 328690 || Loss: 0.7521 || timer: 0.1117 sec.
iter 328700 || Loss: 1.1619 || timer: 0.0899 sec.
iter 328710 || Loss: 0.8102 || timer: 0.0898 sec.
iter 328720 || Loss: 0.6334 || timer: 0.0917 sec.
iter 328730 || Loss: 0.7663 || timer: 0.1063 sec.
iter 328740 || Loss: 0.6202 || timer: 0.0911 sec.
iter 328750 || Loss: 1.1268 || timer: 0.0916 sec.
iter 328760 || Loss: 0.8475 || timer: 0.0878 sec.
iter 328770 || Loss: 0.6350 || timer: 0.0864 sec.
iter 328780 || Loss: 0.5417 || timer: 0.0924 sec.
iter 328790 || Loss: 0.9090 || timer: 0.0915 sec.
iter 328800 || Loss: 1.0011 || timer: 0.0879 sec.
iter 328810 || Loss: 0.9004 || timer: 0.0828 sec.
iter 328820 || Loss: 0.8023 || timer: 0.0891 sec.
iter 328830 || Loss: 0.7812 || timer: 0.0935 sec.
iter 328840 || Loss: 0.4561 || timer: 0.0921 sec.
iter 328850 || Loss: 0.4960 || timer: 0.0838 sec.
iter 328860 || Loss: 0.9248 || timer: 0.0846 sec.
iter 328870 || Loss: 0.7905 || timer: 0.0916 sec.
iter 328880 || Loss: 0.7818 || timer: 0.0833 sec.
iter 328890 || Loss: 0.6314 || timer: 0.1030 sec.
iter 328900 || Loss: 0.5066 || timer: 0.0263 sec.
iter 328910 || Loss: 1.1054 || timer: 0.0863 sec.
iter 328920 || Loss: 0.5595 || timer: 0.0904 sec.
iter 328930 || Loss: 0.6980 || timer: 0.0909 sec.
iter 328940 || Loss: 1.1635 || timer: 0.0895 sec.
iter 328950 || Loss: 0.6606 || timer: 0.0858 sec.
iter 328960 || Loss: 0.5501 || timer: 0.0824 sec.
iter 328970 || Loss: 0.6305 || timer: 0.0897 sec.
iter 328980 || Loss: 0.5734 || timer: 0.0911 sec.
iter 328990 || Loss: 0.5566 || timer: 0.0996 sec.
iter 329000 || Loss: 0.7051 || timer: 0.1084 sec.
iter 329010 || Loss: 0.6099 || timer: 0.0916 sec.
iter 329020 || Loss: 0.8196 || timer: 0.0898 sec.
iter 329030 || Loss: 0.5868 || timer: 0.0837 sec.
iter 329040 || Loss: 0.5489 || timer: 0.0834 sec.
iter 329050 || Loss: 0.8035 || timer: 0.0906 sec.
iter 329060 || Loss: 0.5853 || timer: 0.0909 sec.
iter 329070 || Loss: 0.7424 || timer: 0.0895 sec.
iter 329080 || Loss: 0.8647 || timer: 0.1180 sec.
iter 329090 || Loss: 0.6394 || timer: 0.0899 sec.
iter 329100 || Loss: 0.8099 || timer: 0.0850 sec.
iter 329110 || Loss: 0.6347 || timer: 0.0902 sec.
iter 329120 || Loss: 0.7440 || timer: 0.0901 sec.
iter 329130 || Loss: 0.5343 || timer: 0.0914 sec.
iter 329140 || Loss: 0.8393 || timer: 0.0946 sec.
iter 329150 || Loss: 0.6236 || timer: 0.0908 sec.
iter 329160 || Loss: 0.6374 || timer: 0.0895 sec.
iter 329170 || Loss: 0.9188 || timer: 0.0893 sec.
iter 329180 || Loss: 0.5742 || timer: 0.0942 sec.
iter 329190 || Loss: 0.5958 || timer: 0.0837 sec.
iter 329200 || Loss: 0.7328 || timer: 0.0827 sec.
iter 329210 || Loss: 0.8259 || timer: 0.0824 sec.
iter 329220 || Loss: 0.7795 || timer: 0.0922 sec.
iter 329230 || Loss: 0.6666 || timer: 0.0257 sec.
iter 329240 || Loss: 1.5392 || timer: 0.0882 sec.
iter 329250 || Loss: 0.6714 || timer: 0.0825 sec.
iter 329260 || Loss: 0.6350 || timer: 0.0830 sec.
iter 329270 || Loss: 0.7455 || timer: 0.0949 sec.
iter 329280 || Loss: 0.8890 || timer: 0.1069 sec.
iter 329290 || Loss: 0.7749 || timer: 0.0911 sec.
iter 329300 || Loss: 1.0535 || timer: 0.0990 sec.
iter 329310 || Loss: 0.6773 || timer: 0.0896 sec.
iter 329320 || Loss: 0.7115 || timer: 0.0975 sec.
iter 329330 || Loss: 0.4938 || timer: 0.1067 sec.
iter 329340 || Loss: 0.7707 || timer: 0.0845 sec.
iter 329350 || Loss: 0.5734 || timer: 0.0912 sec.
iter 329360 || Loss: 0.8043 || timer: 0.0883 sec.
iter 329370 || Loss: 0.9675 || timer: 0.0947 sec.
iter 329380 || Loss: 0.6636 || timer: 0.0896 sec.
iter 329390 || Loss: 0.8658 || timer: 0.0834 sec.
iter 329400 || Loss: 0.5300 || timer: 0.0897 sec.
iter 329410 || Loss: 0.8523 || timer: 0.0884 sec.
iter 329420 || Loss: 0.5994 || timer: 0.0977 sec.
iter 329430 || Loss: 0.6934 || timer: 0.0922 sec.
iter 329440 || Loss: 0.5457 || timer: 0.0937 sec.
iter 329450 || Loss: 0.6912 || timer: 0.0893 sec.
iter 329460 || Loss: 0.7492 || timer: 0.0907 sec.
iter 329470 || Loss: 0.8211 || timer: 0.0914 sec.
iter 329480 || Loss: 0.7470 || timer: 0.1020 sec.
iter 329490 || Loss: 0.5959 || timer: 0.1076 sec.
iter 329500 || Loss: 0.7877 || timer: 0.0960 sec.
iter 329510 || Loss: 0.9897 || timer: 0.0914 sec.
iter 329520 || Loss: 0.8123 || timer: 0.0902 sec.
iter 329530 || Loss: 0.5898 || timer: 0.0839 sec.
iter 329540 || Loss: 0.6311 || timer: 0.0930 sec.
iter 329550 || Loss: 0.9777 || timer: 0.0885 sec.
iter 329560 || Loss: 0.6531 || timer: 0.0266 sec.
iter 329570 || Loss: 1.6034 || timer: 0.0895 sec.
iter 329580 || Loss: 0.8818 || timer: 0.0904 sec.
iter 329590 || Loss: 0.8315 || timer: 0.0840 sec.
iter 329600 || Loss: 0.7091 || timer: 0.0830 sec.
iter 329610 || Loss: 0.8131 || timer: 0.1089 sec.
iter 329620 || Loss: 0.5412 || timer: 0.0922 sec.
iter 329630 || Loss: 0.7144 || timer: 0.0828 sec.
iter 329640 || Loss: 0.6352 || timer: 0.0908 sec.
iter 329650 || Loss: 0.9353 || timer: 0.0837 sec.
iter 329660 || Loss: 0.8900 || timer: 0.1246 sec.
iter 329670 || Loss: 0.6532 || timer: 0.0827 sec.
iter 329680 || Loss: 0.6561 || timer: 0.1039 sec.
iter 329690 || Loss: 0.6162 || timer: 0.0852 sec.
iter 329700 || Loss: 0.5016 || timer: 0.0904 sec.
iter 329710 || Loss: 0.7481 || timer: 0.0908 sec.
iter 329720 || Loss: 0.7529 || timer: 0.0902 sec.
iter 329730 || Loss: 0.6069 || timer: 0.0833 sec.
iter 329740 || Loss: 0.8216 || timer: 0.0875 sec.
iter 329750 || Loss: 0.8610 || timer: 0.0825 sec.
iter 329760 || Loss: 0.4431 || timer: 0.1033 sec.
iter 329770 || Loss: 1.1879 || timer: 0.0981 sec.
iter 329780 || Loss: 0.4659 || timer: 0.0920 sec.
iter 329790 || Loss: 0.7568 || timer: 0.0879 sec.
iter 329800 || Loss: 0.7873 || timer: 0.0815 sec.
iter 329810 || Loss: 0.7913 || timer: 0.0915 sec.
iter 329820 || Loss: 0.7508 || timer: 0.1057 sec.
iter 329830 || Loss: 0.8776 || timer: 0.0905 sec.
iter 329840 || Loss: 0.5261 || timer: 0.0827 sec.
iter 329850 || Loss: 0.7535 || timer: 0.0920 sec.
iter 329860 || Loss: 0.6506 || timer: 0.0888 sec.
iter 329870 || Loss: 0.6413 || timer: 0.0904 sec.
iter 329880 || Loss: 0.4893 || timer: 0.0839 sec.
iter 329890 || Loss: 0.6868 || timer: 0.0170 sec.
iter 329900 || Loss: 0.5040 || timer: 0.0837 sec.
iter 329910 || Loss: 0.8411 || timer: 0.0905 sec.
iter 329920 || Loss: 0.7582 || timer: 0.0839 sec.
iter 329930 || Loss: 0.6575 || timer: 0.0960 sec.
iter 329940 || Loss: 0.6396 || timer: 0.0906 sec.
iter 329950 || Loss: 0.8664 || timer: 0.1093 sec.
iter 329960 || Loss: 0.9483 || timer: 0.0833 sec.
iter 329970 || Loss: 0.5274 || timer: 0.1008 sec.
iter 329980 || Loss: 0.7616 || timer: 0.0908 sec.
iter 329990 || Loss: 0.8386 || timer: 0.0961 sec.
iter 330000 || Loss: 0.7914 || Saving state, iter: 330000
timer: 0.0847 sec.
iter 330010 || Loss: 0.8662 || timer: 0.0824 sec.
iter 330020 || Loss: 0.5471 || timer: 0.0879 sec.
iter 330030 || Loss: 0.6548 || timer: 0.0909 sec.
iter 330040 || Loss: 0.5259 || timer: 0.1041 sec.
iter 330050 || Loss: 0.7795 || timer: 0.0876 sec.
iter 330060 || Loss: 0.6689 || timer: 0.0904 sec.
iter 330070 || Loss: 0.3932 || timer: 0.0832 sec.
iter 330080 || Loss: 0.7317 || timer: 0.0901 sec.
iter 330090 || Loss: 0.6776 || timer: 0.0848 sec.
iter 330100 || Loss: 0.7573 || timer: 0.0879 sec.
iter 330110 || Loss: 1.1038 || timer: 0.0884 sec.
iter 330120 || Loss: 0.6822 || timer: 0.0847 sec.
iter 330130 || Loss: 0.6503 || timer: 0.0941 sec.
iter 330140 || Loss: 0.6968 || timer: 0.0835 sec.
iter 330150 || Loss: 0.7953 || timer: 0.0826 sec.
iter 330160 || Loss: 0.9032 || timer: 0.0929 sec.
iter 330170 || Loss: 1.0222 || timer: 0.1032 sec.
iter 330180 || Loss: 0.8240 || timer: 0.0837 sec.
iter 330190 || Loss: 0.3974 || timer: 0.0842 sec.
iter 330200 || Loss: 0.6263 || timer: 0.0885 sec.
iter 330210 || Loss: 0.8012 || timer: 0.0912 sec.
iter 330220 || Loss: 0.8136 || timer: 0.0203 sec.
iter 330230 || Loss: 0.1569 || timer: 0.1007 sec.
iter 330240 || Loss: 0.5549 || timer: 0.0933 sec.
iter 330250 || Loss: 0.8510 || timer: 0.1200 sec.
iter 330260 || Loss: 0.7047 || timer: 0.1021 sec.
iter 330270 || Loss: 0.7433 || timer: 0.0814 sec.
iter 330280 || Loss: 0.8710 || timer: 0.0916 sec.
iter 330290 || Loss: 0.4457 || timer: 0.1107 sec.
iter 330300 || Loss: 0.6067 || timer: 0.1062 sec.
iter 330310 || Loss: 0.7035 || timer: 0.0956 sec.
iter 330320 || Loss: 0.7341 || timer: 0.0966 sec.
iter 330330 || Loss: 0.6013 || timer: 0.0912 sec.
iter 330340 || Loss: 0.5826 || timer: 0.1004 sec.
iter 330350 || Loss: 0.5014 || timer: 0.0890 sec.
iter 330360 || Loss: 0.7684 || timer: 0.0903 sec.
iter 330370 || Loss: 0.6444 || timer: 0.0919 sec.
iter 330380 || Loss: 0.4469 || timer: 0.1082 sec.
iter 330390 || Loss: 0.6800 || timer: 0.0931 sec.
iter 330400 || Loss: 0.8412 || timer: 0.0930 sec.
iter 330410 || Loss: 0.7233 || timer: 0.1049 sec.
iter 330420 || Loss: 0.7219 || timer: 0.0824 sec.
iter 330430 || Loss: 0.7529 || timer: 0.1032 sec.
iter 330440 || Loss: 0.6263 || timer: 0.0905 sec.
iter 330450 || Loss: 0.9332 || timer: 0.0831 sec.
iter 330460 || Loss: 0.5812 || timer: 0.0841 sec.
iter 330470 || Loss: 0.4334 || timer: 0.1106 sec.
iter 330480 || Loss: 0.6998 || timer: 0.1139 sec.
iter 330490 || Loss: 0.8351 || timer: 0.0921 sec.
iter 330500 || Loss: 0.6403 || timer: 0.0900 sec.
iter 330510 || Loss: 0.7315 || timer: 0.1077 sec.
iter 330520 || Loss: 0.9235 || timer: 0.1334 sec.
iter 330530 || Loss: 0.4420 || timer: 0.0877 sec.
iter 330540 || Loss: 0.5502 || timer: 0.0882 sec.
iter 330550 || Loss: 0.6868 || timer: 0.0194 sec.
iter 330560 || Loss: 0.2576 || timer: 0.0827 sec.
iter 330570 || Loss: 0.7950 || timer: 0.0906 sec.
iter 330580 || Loss: 0.6313 || timer: 0.0898 sec.
iter 330590 || Loss: 1.0200 || timer: 0.0877 sec.
iter 330600 || Loss: 0.7818 || timer: 0.0912 sec.
iter 330610 || Loss: 0.5540 || timer: 0.0891 sec.
iter 330620 || Loss: 0.6570 || timer: 0.0922 sec.
iter 330630 || Loss: 0.6226 || timer: 0.0871 sec.
iter 330640 || Loss: 0.7924 || timer: 0.0833 sec.
iter 330650 || Loss: 0.4966 || timer: 0.1134 sec.
iter 330660 || Loss: 0.7135 || timer: 0.0813 sec.
iter 330670 || Loss: 0.8081 || timer: 0.0822 sec.
iter 330680 || Loss: 0.7260 || timer: 0.0917 sec.
iter 330690 || Loss: 0.5149 || timer: 0.0880 sec.
iter 330700 || Loss: 0.5817 || timer: 0.0826 sec.
iter 330710 || Loss: 0.6050 || timer: 0.0857 sec.
iter 330720 || Loss: 0.6654 || timer: 0.1041 sec.
iter 330730 || Loss: 0.8766 || timer: 0.0929 sec.
iter 330740 || Loss: 1.1126 || timer: 0.0917 sec.
iter 330750 || Loss: 0.6268 || timer: 0.0963 sec.
iter 330760 || Loss: 0.6846 || timer: 0.0929 sec.
iter 330770 || Loss: 0.6637 || timer: 0.0867 sec.
iter 330780 || Loss: 0.6722 || timer: 0.0828 sec.
iter 330790 || Loss: 0.7777 || timer: 0.0926 sec.
iter 330800 || Loss: 0.7948 || timer: 0.1089 sec.
iter 330810 || Loss: 0.6103 || timer: 0.0974 sec.
iter 330820 || Loss: 0.5754 || timer: 0.0911 sec.
iter 330830 || Loss: 0.7045 || timer: 0.0915 sec.
iter 330840 || Loss: 0.5823 || timer: 0.0909 sec.
iter 330850 || Loss: 0.7261 || timer: 0.0887 sec.
iter 330860 || Loss: 1.0715 || timer: 0.0895 sec.
iter 330870 || Loss: 0.9445 || timer: 0.0875 sec.
iter 330880 || Loss: 0.6144 || timer: 0.0204 sec.
iter 330890 || Loss: 0.9834 || timer: 0.0944 sec.
iter 330900 || Loss: 1.0371 || timer: 0.0820 sec.
iter 330910 || Loss: 0.5258 || timer: 0.0905 sec.
iter 330920 || Loss: 0.7675 || timer: 0.0905 sec.
iter 330930 || Loss: 0.5039 || timer: 0.0930 sec.
iter 330940 || Loss: 0.6525 || timer: 0.0843 sec.
iter 330950 || Loss: 0.7449 || timer: 0.0916 sec.
iter 330960 || Loss: 0.5660 || timer: 0.0989 sec.
iter 330970 || Loss: 0.9428 || timer: 0.0894 sec.
iter 330980 || Loss: 0.6133 || timer: 0.1124 sec.
iter 330990 || Loss: 0.6354 || timer: 0.1077 sec.
iter 331000 || Loss: 0.5836 || timer: 0.1059 sec.
iter 331010 || Loss: 0.5481 || timer: 0.0837 sec.
iter 331020 || Loss: 1.0582 || timer: 0.0922 sec.
iter 331030 || Loss: 0.7323 || timer: 0.0920 sec.
iter 331040 || Loss: 0.7532 || timer: 0.0903 sec.
iter 331050 || Loss: 0.6507 || timer: 0.0899 sec.
iter 331060 || Loss: 0.5595 || timer: 0.0907 sec.
iter 331070 || Loss: 0.8500 || timer: 0.0899 sec.
iter 331080 || Loss: 0.9791 || timer: 0.0942 sec.
iter 331090 || Loss: 0.5140 || timer: 0.0930 sec.
iter 331100 || Loss: 0.5871 || timer: 0.1110 sec.
iter 331110 || Loss: 0.6436 || timer: 0.0929 sec.
iter 331120 || Loss: 0.5942 || timer: 0.0918 sec.
iter 331130 || Loss: 0.6713 || timer: 0.1062 sec.
iter 331140 || Loss: 0.6533 || timer: 0.1111 sec.
iter 331150 || Loss: 0.5904 || timer: 0.0916 sec.
iter 331160 || Loss: 0.7740 || timer: 0.0933 sec.
iter 331170 || Loss: 0.8709 || timer: 0.1040 sec.
iter 331180 || Loss: 0.5103 || timer: 0.0945 sec.
iter 331190 || Loss: 0.6025 || timer: 0.0909 sec.
iter 331200 || Loss: 0.6744 || timer: 0.0923 sec.
iter 331210 || Loss: 0.4593 || timer: 0.0277 sec.
iter 331220 || Loss: 2.1147 || timer: 0.0902 sec.
iter 331230 || Loss: 0.5057 || timer: 0.1550 sec.
iter 331240 || Loss: 0.7233 || timer: 0.0838 sec.
iter 331250 || Loss: 0.6534 || timer: 0.0865 sec.
iter 331260 || Loss: 0.7912 || timer: 0.1056 sec.
iter 331270 || Loss: 0.8936 || timer: 0.1115 sec.
iter 331280 || Loss: 0.6341 || timer: 0.0909 sec.
iter 331290 || Loss: 0.5662 || timer: 0.0877 sec.
iter 331300 || Loss: 0.5012 || timer: 0.0957 sec.
iter 331310 || Loss: 0.5543 || timer: 0.0977 sec.
iter 331320 || Loss: 0.6265 || timer: 0.0994 sec.
iter 331330 || Loss: 0.5702 || timer: 0.0933 sec.
iter 331340 || Loss: 0.6708 || timer: 0.1084 sec.
iter 331350 || Loss: 0.5423 || timer: 0.0914 sec.
iter 331360 || Loss: 0.6161 || timer: 0.0910 sec.
iter 331370 || Loss: 0.6077 || timer: 0.0937 sec.
iter 331380 || Loss: 0.6128 || timer: 0.0908 sec.
iter 331390 || Loss: 0.8263 || timer: 0.0834 sec.
iter 331400 || Loss: 0.7768 || timer: 0.0916 sec.
iter 331410 || Loss: 0.6913 || timer: 0.0932 sec.
iter 331420 || Loss: 0.8250 || timer: 0.0834 sec.
iter 331430 || Loss: 0.6073 || timer: 0.1001 sec.
iter 331440 || Loss: 0.5851 || timer: 0.1106 sec.
iter 331450 || Loss: 0.6076 || timer: 0.0987 sec.
iter 331460 || Loss: 0.7726 || timer: 0.0954 sec.
iter 331470 || Loss: 0.8750 || timer: 0.0922 sec.
iter 331480 || Loss: 0.5700 || timer: 0.0908 sec.
iter 331490 || Loss: 0.6558 || timer: 0.0902 sec.
iter 331500 || Loss: 0.7492 || timer: 0.0903 sec.
iter 331510 || Loss: 0.5572 || timer: 0.0923 sec.
iter 331520 || Loss: 0.4450 || timer: 0.1033 sec.
iter 331530 || Loss: 0.8374 || timer: 0.0925 sec.
iter 331540 || Loss: 0.7521 || timer: 0.0202 sec.
iter 331550 || Loss: 1.1317 || timer: 0.0843 sec.
iter 331560 || Loss: 0.6799 || timer: 0.0927 sec.
iter 331570 || Loss: 0.7407 || timer: 0.0936 sec.
iter 331580 || Loss: 0.7641 || timer: 0.0927 sec.
iter 331590 || Loss: 0.7994 || timer: 0.0832 sec.
iter 331600 || Loss: 0.7277 || timer: 0.0911 sec.
iter 331610 || Loss: 0.7674 || timer: 0.0831 sec.
iter 331620 || Loss: 0.7950 || timer: 0.1152 sec.
iter 331630 || Loss: 0.9573 || timer: 0.0950 sec.
iter 331640 || Loss: 0.8801 || timer: 0.1199 sec.
iter 331650 || Loss: 0.6285 || timer: 0.0909 sec.
iter 331660 || Loss: 1.1190 || timer: 0.0986 sec.
iter 331670 || Loss: 0.5637 || timer: 0.0833 sec.
iter 331680 || Loss: 0.4856 || timer: 0.0836 sec.
iter 331690 || Loss: 0.8241 || timer: 0.0910 sec.
iter 331700 || Loss: 0.4868 || timer: 0.0836 sec.
iter 331710 || Loss: 0.7873 || timer: 0.0856 sec.
iter 331720 || Loss: 0.5552 || timer: 0.0872 sec.
iter 331730 || Loss: 0.6454 || timer: 0.0901 sec.
iter 331740 || Loss: 0.6977 || timer: 0.0888 sec.
iter 331750 || Loss: 0.8915 || timer: 0.1281 sec.
iter 331760 || Loss: 0.9327 || timer: 0.0839 sec.
iter 331770 || Loss: 0.7044 || timer: 0.0936 sec.
iter 331780 || Loss: 0.6013 || timer: 0.1051 sec.
iter 331790 || Loss: 0.4500 || timer: 0.1260 sec.
iter 331800 || Loss: 0.4844 || timer: 0.1162 sec.
iter 331810 || Loss: 0.6211 || timer: 0.0894 sec.
iter 331820 || Loss: 0.6922 || timer: 0.0899 sec.
iter 331830 || Loss: 0.7009 || timer: 0.1128 sec.
iter 331840 || Loss: 0.8255 || timer: 0.0903 sec.
iter 331850 || Loss: 0.7081 || timer: 0.0913 sec.
iter 331860 || Loss: 0.6087 || timer: 0.0907 sec.
iter 331870 || Loss: 0.8197 || timer: 0.0257 sec.
iter 331880 || Loss: 1.0091 || timer: 0.0911 sec.
iter 331890 || Loss: 0.7557 || timer: 0.0892 sec.
iter 331900 || Loss: 0.8238 || timer: 0.0894 sec.
iter 331910 || Loss: 0.5666 || timer: 0.0878 sec.
iter 331920 || Loss: 0.6330 || timer: 0.0909 sec.
iter 331930 || Loss: 0.9826 || timer: 0.1029 sec.
iter 331940 || Loss: 0.7004 || timer: 0.0824 sec.
iter 331950 || Loss: 0.7364 || timer: 0.1095 sec.
iter 331960 || Loss: 0.7508 || timer: 0.0923 sec.
iter 331970 || Loss: 0.6581 || timer: 0.1001 sec.
iter 331980 || Loss: 0.7974 || timer: 0.0836 sec.
iter 331990 || Loss: 0.7357 || timer: 0.0833 sec.
iter 332000 || Loss: 0.5313 || timer: 0.0913 sec.
iter 332010 || Loss: 0.9672 || timer: 0.0897 sec.
iter 332020 || Loss: 0.6430 || timer: 0.0926 sec.
iter 332030 || Loss: 0.8271 || timer: 0.0829 sec.
iter 332040 || Loss: 0.9038 || timer: 0.1045 sec.
iter 332050 || Loss: 0.5623 || timer: 0.0834 sec.
iter 332060 || Loss: 0.7392 || timer: 0.0911 sec.
iter 332070 || Loss: 0.6133 || timer: 0.0950 sec.
iter 332080 || Loss: 0.7621 || timer: 0.0907 sec.
iter 332090 || Loss: 0.8280 || timer: 0.1053 sec.
iter 332100 || Loss: 0.4545 || timer: 0.0827 sec.
iter 332110 || Loss: 0.9141 || timer: 0.1021 sec.
iter 332120 || Loss: 0.5715 || timer: 0.0825 sec.
iter 332130 || Loss: 0.5715 || timer: 0.0909 sec.
iter 332140 || Loss: 0.6213 || timer: 0.0910 sec.
iter 332150 || Loss: 0.6783 || timer: 0.0907 sec.
iter 332160 || Loss: 0.6290 || timer: 0.0916 sec.
iter 332170 || Loss: 0.7747 || timer: 0.0823 sec.
iter 332180 || Loss: 0.6131 || timer: 0.0925 sec.
iter 332190 || Loss: 0.5566 || timer: 0.0907 sec.
iter 332200 || Loss: 0.6054 || timer: 0.0263 sec.
iter 332210 || Loss: 0.6068 || timer: 0.0841 sec.
iter 332220 || Loss: 0.7222 || timer: 0.0832 sec.
iter 332230 || Loss: 0.7431 || timer: 0.0908 sec.
iter 332240 || Loss: 0.7276 || timer: 0.0921 sec.
iter 332250 || Loss: 1.0117 || timer: 0.0919 sec.
iter 332260 || Loss: 0.4500 || timer: 0.0911 sec.
iter 332270 || Loss: 0.5418 || timer: 0.0831 sec.
iter 332280 || Loss: 0.6769 || timer: 0.1106 sec.
iter 332290 || Loss: 0.6159 || timer: 0.0919 sec.
iter 332300 || Loss: 0.5121 || timer: 0.1016 sec.
iter 332310 || Loss: 0.6067 || timer: 0.0935 sec.
iter 332320 || Loss: 0.4605 || timer: 0.0900 sec.
iter 332330 || Loss: 0.7841 || timer: 0.0847 sec.
iter 332340 || Loss: 0.6954 || timer: 0.0927 sec.
iter 332350 || Loss: 0.5617 || timer: 0.0961 sec.
iter 332360 || Loss: 0.4973 || timer: 0.0868 sec.
iter 332370 || Loss: 0.7768 || timer: 0.0845 sec.
iter 332380 || Loss: 0.6337 || timer: 0.0839 sec.
iter 332390 || Loss: 0.5745 || timer: 0.0925 sec.
iter 332400 || Loss: 0.8708 || timer: 0.0909 sec.
iter 332410 || Loss: 0.6284 || timer: 0.1065 sec.
iter 332420 || Loss: 0.9078 || timer: 0.0987 sec.
iter 332430 || Loss: 0.9039 || timer: 0.0832 sec.
iter 332440 || Loss: 0.6004 || timer: 0.0950 sec.
iter 332450 || Loss: 0.6266 || timer: 0.1005 sec.
iter 332460 || Loss: 0.9033 || timer: 0.0947 sec.
iter 332470 || Loss: 0.5864 || timer: 0.0893 sec.
iter 332480 || Loss: 0.6970 || timer: 0.1075 sec.
iter 332490 || Loss: 0.6908 || timer: 0.0902 sec.
iter 332500 || Loss: 1.0460 || timer: 0.0907 sec.
iter 332510 || Loss: 0.7457 || timer: 0.0860 sec.
iter 332520 || Loss: 0.6252 || timer: 0.1049 sec.
iter 332530 || Loss: 0.9274 || timer: 0.0286 sec.
iter 332540 || Loss: 0.6416 || timer: 0.0875 sec.
iter 332550 || Loss: 0.8155 || timer: 0.0926 sec.
iter 332560 || Loss: 0.5401 || timer: 0.0884 sec.
iter 332570 || Loss: 0.7666 || timer: 0.0867 sec.
iter 332580 || Loss: 0.6465 || timer: 0.0815 sec.
iter 332590 || Loss: 0.5864 || timer: 0.0898 sec.
iter 332600 || Loss: 0.6748 || timer: 0.0915 sec.
iter 332610 || Loss: 0.6802 || timer: 0.1056 sec.
iter 332620 || Loss: 0.6933 || timer: 0.1145 sec.
iter 332630 || Loss: 0.5973 || timer: 0.0962 sec.
iter 332640 || Loss: 0.7070 || timer: 0.0878 sec.
iter 332650 || Loss: 0.7123 || timer: 0.1329 sec.
iter 332660 || Loss: 0.6994 || timer: 0.0917 sec.
iter 332670 || Loss: 0.6470 || timer: 0.0962 sec.
iter 332680 || Loss: 0.5602 || timer: 0.0904 sec.
iter 332690 || Loss: 0.6205 || timer: 0.0920 sec.
iter 332700 || Loss: 0.7768 || timer: 0.0902 sec.
iter 332710 || Loss: 0.7613 || timer: 0.0914 sec.
iter 332720 || Loss: 0.7779 || timer: 0.0891 sec.
iter 332730 || Loss: 0.6982 || timer: 0.1069 sec.
iter 332740 || Loss: 0.4818 || timer: 0.0835 sec.
iter 332750 || Loss: 0.7506 || timer: 0.0903 sec.
iter 332760 || Loss: 0.8242 || timer: 0.0931 sec.
iter 332770 || Loss: 0.8004 || timer: 0.0907 sec.
iter 332780 || Loss: 1.1028 || timer: 0.0907 sec.
iter 332790 || Loss: 0.6660 || timer: 0.0887 sec.
iter 332800 || Loss: 0.4993 || timer: 0.0919 sec.
iter 332810 || Loss: 0.6613 || timer: 0.0924 sec.
iter 332820 || Loss: 0.5667 || timer: 0.0893 sec.
iter 332830 || Loss: 0.5822 || timer: 0.0924 sec.
iter 332840 || Loss: 0.5549 || timer: 0.0896 sec.
iter 332850 || Loss: 0.6723 || timer: 0.0827 sec.
iter 332860 || Loss: 0.7402 || timer: 0.0261 sec.
iter 332870 || Loss: 5.9639 || timer: 0.0890 sec.
iter 332880 || Loss: 0.5079 || timer: 0.0935 sec.
iter 332890 || Loss: 0.6580 || timer: 0.0837 sec.
iter 332900 || Loss: 0.6179 || timer: 0.0904 sec.
iter 332910 || Loss: 1.0098 || timer: 0.0892 sec.
iter 332920 || Loss: 0.5986 || timer: 0.0821 sec.
iter 332930 || Loss: 0.6443 || timer: 0.0890 sec.
iter 332940 || Loss: 1.1664 || timer: 0.0932 sec.
iter 332950 || Loss: 0.4541 || timer: 0.0916 sec.
iter 332960 || Loss: 0.7181 || timer: 0.1210 sec.
iter 332970 || Loss: 0.6456 || timer: 0.0830 sec.
iter 332980 || Loss: 0.6089 || timer: 0.0985 sec.
iter 332990 || Loss: 0.9989 || timer: 0.0910 sec.
iter 333000 || Loss: 0.6572 || timer: 0.0901 sec.
iter 333010 || Loss: 0.5725 || timer: 0.0981 sec.
iter 333020 || Loss: 0.4863 || timer: 0.0837 sec.
iter 333030 || Loss: 0.9906 || timer: 0.1122 sec.
iter 333040 || Loss: 0.7002 || timer: 0.0834 sec.
iter 333050 || Loss: 0.5632 || timer: 0.1080 sec.
iter 333060 || Loss: 0.6731 || timer: 0.0921 sec.
iter 333070 || Loss: 0.6361 || timer: 0.1140 sec.
iter 333080 || Loss: 0.6923 || timer: 0.0870 sec.
iter 333090 || Loss: 0.6338 || timer: 0.0925 sec.
iter 333100 || Loss: 0.8218 || timer: 0.0906 sec.
iter 333110 || Loss: 0.6156 || timer: 0.1047 sec.
iter 333120 || Loss: 0.8968 || timer: 0.0899 sec.
iter 333130 || Loss: 0.4795 || timer: 0.0970 sec.
iter 333140 || Loss: 0.7318 || timer: 0.0959 sec.
iter 333150 || Loss: 0.8137 || timer: 0.0902 sec.
iter 333160 || Loss: 0.6345 || timer: 0.0871 sec.
iter 333170 || Loss: 0.7124 || timer: 0.0919 sec.
iter 333180 || Loss: 0.8283 || timer: 0.0918 sec.
iter 333190 || Loss: 0.7166 || timer: 0.0237 sec.
iter 333200 || Loss: 1.0670 || timer: 0.1105 sec.
iter 333210 || Loss: 0.6418 || timer: 0.1045 sec.
iter 333220 || Loss: 0.6945 || timer: 0.0907 sec.
iter 333230 || Loss: 0.6379 || timer: 0.0825 sec.
iter 333240 || Loss: 0.8038 || timer: 0.0912 sec.
iter 333250 || Loss: 0.7685 || timer: 0.0835 sec.
iter 333260 || Loss: 0.5602 || timer: 0.0842 sec.
iter 333270 || Loss: 0.9220 || timer: 0.0912 sec.
iter 333280 || Loss: 0.6180 || timer: 0.0904 sec.
iter 333290 || Loss: 0.8448 || timer: 0.0956 sec.
iter 333300 || Loss: 0.5912 || timer: 0.0905 sec.
iter 333310 || Loss: 0.6083 || timer: 0.0916 sec.
iter 333320 || Loss: 0.6745 || timer: 0.0923 sec.
iter 333330 || Loss: 0.7757 || timer: 0.0809 sec.
iter 333340 || Loss: 0.8286 || timer: 0.0900 sec.
iter 333350 || Loss: 0.5914 || timer: 0.0907 sec.
iter 333360 || Loss: 0.8468 || timer: 0.0920 sec.
iter 333370 || Loss: 0.5655 || timer: 0.0828 sec.
iter 333380 || Loss: 0.5030 || timer: 0.0911 sec.
iter 333390 || Loss: 0.7174 || timer: 0.0895 sec.
iter 333400 || Loss: 0.6970 || timer: 0.0956 sec.
iter 333410 || Loss: 0.6703 || timer: 0.0911 sec.
iter 333420 || Loss: 0.7518 || timer: 0.0919 sec.
iter 333430 || Loss: 0.7423 || timer: 0.0906 sec.
iter 333440 || Loss: 0.6381 || timer: 0.1192 sec.
iter 333450 || Loss: 0.6686 || timer: 0.0844 sec.
iter 333460 || Loss: 0.7354 || timer: 0.0831 sec.
iter 333470 || Loss: 0.7099 || timer: 0.0922 sec.
iter 333480 || Loss: 1.0687 || timer: 0.0827 sec.
iter 333490 || Loss: 0.9909 || timer: 0.0906 sec.
iter 333500 || Loss: 0.7306 || timer: 0.0817 sec.
iter 333510 || Loss: 0.4642 || timer: 0.0906 sec.
iter 333520 || Loss: 0.7467 || timer: 0.0167 sec.
iter 333530 || Loss: 0.3739 || timer: 0.0863 sec.
iter 333540 || Loss: 0.7681 || timer: 0.1065 sec.
iter 333550 || Loss: 0.6914 || timer: 0.0928 sec.
iter 333560 || Loss: 1.1263 || timer: 0.0899 sec.
iter 333570 || Loss: 0.6688 || timer: 0.0904 sec.
iter 333580 || Loss: 0.9942 || timer: 0.1255 sec.
iter 333590 || Loss: 0.8570 || timer: 0.0904 sec.
iter 333600 || Loss: 0.5957 || timer: 0.0832 sec.
iter 333610 || Loss: 0.9497 || timer: 0.0907 sec.
iter 333620 || Loss: 0.8537 || timer: 0.1226 sec.
iter 333630 || Loss: 0.6552 || timer: 0.0894 sec.
iter 333640 || Loss: 0.6850 || timer: 0.0903 sec.
iter 333650 || Loss: 0.6600 || timer: 0.0845 sec.
iter 333660 || Loss: 0.6075 || timer: 0.0842 sec.
iter 333670 || Loss: 0.9537 || timer: 0.0841 sec.
iter 333680 || Loss: 0.6817 || timer: 0.0761 sec.
iter 333690 || Loss: 0.6850 || timer: 0.0902 sec.
iter 333700 || Loss: 0.6477 || timer: 0.0837 sec.
iter 333710 || Loss: 0.5691 || timer: 0.0923 sec.
iter 333720 || Loss: 0.7260 || timer: 0.0836 sec.
iter 333730 || Loss: 0.6971 || timer: 0.0928 sec.
iter 333740 || Loss: 0.5393 || timer: 0.0898 sec.
iter 333750 || Loss: 0.7718 || timer: 0.0838 sec.
iter 333760 || Loss: 0.6146 || timer: 0.0921 sec.
iter 333770 || Loss: 0.6306 || timer: 0.0885 sec.
iter 333780 || Loss: 0.6463 || timer: 0.1054 sec.
iter 333790 || Loss: 0.6098 || timer: 0.0911 sec.
iter 333800 || Loss: 0.9325 || timer: 0.0885 sec.
iter 333810 || Loss: 0.4867 || timer: 0.0906 sec.
iter 333820 || Loss: 0.6291 || timer: 0.0830 sec.
iter 333830 || Loss: 0.8270 || timer: 0.1004 sec.
iter 333840 || Loss: 0.8260 || timer: 0.0954 sec.
iter 333850 || Loss: 0.6513 || timer: 0.0215 sec.
iter 333860 || Loss: 0.0881 || timer: 0.0915 sec.
iter 333870 || Loss: 0.6514 || timer: 0.1005 sec.
iter 333880 || Loss: 0.6590 || timer: 0.0904 sec.
iter 333890 || Loss: 0.6396 || timer: 0.0928 sec.
iter 333900 || Loss: 0.5963 || timer: 0.0888 sec.
iter 333910 || Loss: 1.0778 || timer: 0.0926 sec.
iter 333920 || Loss: 0.6825 || timer: 0.0829 sec.
iter 333930 || Loss: 0.4693 || timer: 0.1309 sec.
iter 333940 || Loss: 0.7093 || timer: 0.0829 sec.
iter 333950 || Loss: 0.6053 || timer: 0.1038 sec.
iter 333960 || Loss: 0.6255 || timer: 0.0829 sec.
iter 333970 || Loss: 0.8190 || timer: 0.0837 sec.
iter 333980 || Loss: 0.6787 || timer: 0.0893 sec.
iter 333990 || Loss: 0.5947 || timer: 0.1039 sec.
iter 334000 || Loss: 0.6395 || timer: 0.0879 sec.
iter 334010 || Loss: 0.7431 || timer: 0.0908 sec.
iter 334020 || Loss: 0.6107 || timer: 0.0902 sec.
iter 334030 || Loss: 0.6966 || timer: 0.0903 sec.
iter 334040 || Loss: 0.9801 || timer: 0.0881 sec.
iter 334050 || Loss: 0.7520 || timer: 0.1048 sec.
iter 334060 || Loss: 0.9316 || timer: 0.1108 sec.
iter 334070 || Loss: 0.6886 || timer: 0.1038 sec.
iter 334080 || Loss: 0.5400 || timer: 0.0836 sec.
iter 334090 || Loss: 0.9003 || timer: 0.1057 sec.
iter 334100 || Loss: 0.7297 || timer: 0.0972 sec.
iter 334110 || Loss: 0.5870 || timer: 0.1067 sec.
iter 334120 || Loss: 0.6798 || timer: 0.0905 sec.
iter 334130 || Loss: 0.9815 || timer: 0.0901 sec.
iter 334140 || Loss: 0.6956 || timer: 0.0898 sec.
iter 334150 || Loss: 0.6995 || timer: 0.0942 sec.
iter 334160 || Loss: 0.7076 || timer: 0.0891 sec.
iter 334170 || Loss: 0.4845 || timer: 0.0920 sec.
iter 334180 || Loss: 0.9951 || timer: 0.0261 sec.
iter 334190 || Loss: 0.7076 || timer: 0.0858 sec.
iter 334200 || Loss: 0.6787 || timer: 0.0908 sec.
iter 334210 || Loss: 0.6910 || timer: 0.0856 sec.
iter 334220 || Loss: 0.5805 || timer: 0.0885 sec.
iter 334230 || Loss: 0.3521 || timer: 0.0898 sec.
iter 334240 || Loss: 0.7249 || timer: 0.1051 sec.
iter 334250 || Loss: 0.6133 || timer: 0.0826 sec.
iter 334260 || Loss: 0.4568 || timer: 0.1122 sec.
iter 334270 || Loss: 0.5180 || timer: 0.1054 sec.
iter 334280 || Loss: 0.5435 || timer: 0.0929 sec.
iter 334290 || Loss: 0.7157 || timer: 0.0909 sec.
iter 334300 || Loss: 0.9337 || timer: 0.0900 sec.
iter 334310 || Loss: 0.4492 || timer: 0.0840 sec.
iter 334320 || Loss: 0.8314 || timer: 0.1075 sec.
iter 334330 || Loss: 0.7840 || timer: 0.0808 sec.
iter 334340 || Loss: 0.6267 || timer: 0.0998 sec.
iter 334350 || Loss: 0.8342 || timer: 0.0876 sec.
iter 334360 || Loss: 0.8528 || timer: 0.0900 sec.
iter 334370 || Loss: 0.7304 || timer: 0.0900 sec.
iter 334380 || Loss: 0.7735 || timer: 0.0840 sec.
iter 334390 || Loss: 0.7936 || timer: 0.0831 sec.
iter 334400 || Loss: 0.8615 || timer: 0.0865 sec.
iter 334410 || Loss: 0.6558 || timer: 0.0832 sec.
iter 334420 || Loss: 0.5602 || timer: 0.1112 sec.
iter 334430 || Loss: 0.6749 || timer: 0.0850 sec.
iter 334440 || Loss: 0.6568 || timer: 0.1095 sec.
iter 334450 || Loss: 0.5841 || timer: 0.0996 sec.
iter 334460 || Loss: 0.7504 || timer: 0.1095 sec.
iter 334470 || Loss: 0.6643 || timer: 0.1090 sec.
iter 334480 || Loss: 0.5627 || timer: 0.0915 sec.
iter 334490 || Loss: 0.7730 || timer: 0.0935 sec.
iter 334500 || Loss: 0.4196 || timer: 0.0926 sec.
iter 334510 || Loss: 0.8392 || timer: 0.0210 sec.
iter 334520 || Loss: 0.5771 || timer: 0.0874 sec.
iter 334530 || Loss: 0.7686 || timer: 0.0836 sec.
iter 334540 || Loss: 0.7343 || timer: 0.0840 sec.
iter 334550 || Loss: 0.5778 || timer: 0.0896 sec.
iter 334560 || Loss: 0.6355 || timer: 0.0886 sec.
iter 334570 || Loss: 0.4991 || timer: 0.1170 sec.
iter 334580 || Loss: 0.7747 || timer: 0.0934 sec.
iter 334590 || Loss: 0.5170 || timer: 0.0921 sec.
iter 334600 || Loss: 0.5128 || timer: 0.0848 sec.
iter 334610 || Loss: 0.7350 || timer: 0.1132 sec.
iter 334620 || Loss: 0.6364 || timer: 0.1053 sec.
iter 334630 || Loss: 0.9003 || timer: 0.0986 sec.
iter 334640 || Loss: 0.3923 || timer: 0.0883 sec.
iter 334650 || Loss: 0.4558 || timer: 0.0826 sec.
iter 334660 || Loss: 0.6309 || timer: 0.0883 sec.
iter 334670 || Loss: 0.6063 || timer: 0.0930 sec.
iter 334680 || Loss: 0.6637 || timer: 0.1002 sec.
iter 334690 || Loss: 0.8424 || timer: 0.0837 sec.
iter 334700 || Loss: 0.6627 || timer: 0.0838 sec.
iter 334710 || Loss: 0.9728 || timer: 0.0874 sec.
iter 334720 || Loss: 0.6559 || timer: 0.1126 sec.
iter 334730 || Loss: 0.7241 || timer: 0.1060 sec.
iter 334740 || Loss: 0.7435 || timer: 0.0881 sec.
iter 334750 || Loss: 0.6276 || timer: 0.0893 sec.
iter 334760 || Loss: 0.6593 || timer: 0.1009 sec.
iter 334770 || Loss: 0.7377 || timer: 0.0815 sec.
iter 334780 || Loss: 0.6772 || timer: 0.0852 sec.
iter 334790 || Loss: 1.1082 || timer: 0.0926 sec.
iter 334800 || Loss: 0.6142 || timer: 0.0907 sec.
iter 334810 || Loss: 0.8012 || timer: 0.0837 sec.
iter 334820 || Loss: 0.6472 || timer: 0.0957 sec.
iter 334830 || Loss: 0.8527 || timer: 0.0873 sec.
iter 334840 || Loss: 0.6311 || timer: 0.0178 sec.
iter 334850 || Loss: 0.5445 || timer: 0.1070 sec.
iter 334860 || Loss: 0.6521 || timer: 0.0968 sec.
iter 334870 || Loss: 0.5522 || timer: 0.0914 sec.
iter 334880 || Loss: 0.8225 || timer: 0.1021 sec.
iter 334890 || Loss: 0.6112 || timer: 0.1111 sec.
iter 334900 || Loss: 1.0203 || timer: 0.0832 sec.
iter 334910 || Loss: 0.7277 || timer: 0.0909 sec.
iter 334920 || Loss: 0.5688 || timer: 0.1017 sec.
iter 334930 || Loss: 0.7049 || timer: 0.0897 sec.
iter 334940 || Loss: 0.7006 || timer: 0.0995 sec.
iter 334950 || Loss: 0.6285 || timer: 0.0901 sec.
iter 334960 || Loss: 0.7483 || timer: 0.1118 sec.
iter 334970 || Loss: 0.6714 || timer: 0.0914 sec.
iter 334980 || Loss: 0.9471 || timer: 0.0913 sec.
iter 334990 || Loss: 0.7317 || timer: 0.0918 sec.
iter 335000 || Loss: 0.9059 || Saving state, iter: 335000
timer: 0.0900 sec.
iter 335010 || Loss: 0.9384 || timer: 0.0851 sec.
iter 335020 || Loss: 0.8497 || timer: 0.0919 sec.
iter 335030 || Loss: 1.1752 || timer: 0.0944 sec.
iter 335040 || Loss: 0.8589 || timer: 0.0836 sec.
iter 335050 || Loss: 0.7396 || timer: 0.0928 sec.
iter 335060 || Loss: 0.6412 || timer: 0.0873 sec.
iter 335070 || Loss: 0.7935 || timer: 0.0836 sec.
iter 335080 || Loss: 0.7064 || timer: 0.0904 sec.
iter 335090 || Loss: 0.6262 || timer: 0.0903 sec.
iter 335100 || Loss: 0.8053 || timer: 0.0851 sec.
iter 335110 || Loss: 0.5583 || timer: 0.0881 sec.
iter 335120 || Loss: 0.9867 || timer: 0.1114 sec.
iter 335130 || Loss: 0.7491 || timer: 0.0902 sec.
iter 335140 || Loss: 0.6171 || timer: 0.1158 sec.
iter 335150 || Loss: 0.6109 || timer: 0.0824 sec.
iter 335160 || Loss: 0.7447 || timer: 0.0833 sec.
iter 335170 || Loss: 0.6920 || timer: 0.0244 sec.
iter 335180 || Loss: 0.5358 || timer: 0.0876 sec.
iter 335190 || Loss: 0.6441 || timer: 0.0904 sec.
iter 335200 || Loss: 1.3006 || timer: 0.0874 sec.
iter 335210 || Loss: 0.6238 || timer: 0.0912 sec.
iter 335220 || Loss: 0.6726 || timer: 0.0914 sec.
iter 335230 || Loss: 0.8863 || timer: 0.0838 sec.
iter 335240 || Loss: 0.5388 || timer: 0.0826 sec.
iter 335250 || Loss: 0.4175 || timer: 0.0834 sec.
iter 335260 || Loss: 0.7104 || timer: 0.0988 sec.
iter 335270 || Loss: 0.6106 || timer: 0.0997 sec.
iter 335280 || Loss: 0.7075 || timer: 0.0832 sec.
iter 335290 || Loss: 0.5966 || timer: 0.0835 sec.
iter 335300 || Loss: 0.7872 || timer: 0.0842 sec.
iter 335310 || Loss: 0.4877 || timer: 0.0896 sec.
iter 335320 || Loss: 0.7918 || timer: 0.0919 sec.
iter 335330 || Loss: 0.7571 || timer: 0.0840 sec.
iter 335340 || Loss: 0.8099 || timer: 0.0919 sec.
iter 335350 || Loss: 0.7713 || timer: 0.0838 sec.
iter 335360 || Loss: 0.6124 || timer: 0.0895 sec.
iter 335370 || Loss: 0.7541 || timer: 0.0848 sec.
iter 335380 || Loss: 0.7727 || timer: 0.0899 sec.
iter 335390 || Loss: 0.5822 || timer: 0.1049 sec.
iter 335400 || Loss: 0.5096 || timer: 0.0904 sec.
iter 335410 || Loss: 0.7295 || timer: 0.1265 sec.
iter 335420 || Loss: 0.6709 || timer: 0.1161 sec.
iter 335430 || Loss: 0.5705 || timer: 0.0930 sec.
iter 335440 || Loss: 0.6724 || timer: 0.0890 sec.
iter 335450 || Loss: 0.5038 || timer: 0.1010 sec.
iter 335460 || Loss: 0.7604 || timer: 0.0836 sec.
iter 335470 || Loss: 0.5509 || timer: 0.0841 sec.
iter 335480 || Loss: 0.9329 || timer: 0.0908 sec.
iter 335490 || Loss: 0.7071 || timer: 0.0907 sec.
iter 335500 || Loss: 0.5673 || timer: 0.0266 sec.
iter 335510 || Loss: 0.9298 || timer: 0.0884 sec.
iter 335520 || Loss: 0.4812 || timer: 0.0954 sec.
iter 335530 || Loss: 0.5770 || timer: 0.0919 sec.
iter 335540 || Loss: 0.7698 || timer: 0.1050 sec.
iter 335550 || Loss: 0.6815 || timer: 0.0877 sec.
iter 335560 || Loss: 0.9323 || timer: 0.0881 sec.
iter 335570 || Loss: 1.0120 || timer: 0.1041 sec.
iter 335580 || Loss: 0.9044 || timer: 0.1071 sec.
iter 335590 || Loss: 0.5767 || timer: 0.0894 sec.
iter 335600 || Loss: 0.9368 || timer: 0.1239 sec.
iter 335610 || Loss: 0.6655 || timer: 0.0961 sec.
iter 335620 || Loss: 0.6789 || timer: 0.0839 sec.
iter 335630 || Loss: 0.7165 || timer: 0.0879 sec.
iter 335640 || Loss: 0.6677 || timer: 0.0927 sec.
iter 335650 || Loss: 0.7257 || timer: 0.0927 sec.
iter 335660 || Loss: 0.7254 || timer: 0.0905 sec.
iter 335670 || Loss: 0.7867 || timer: 0.0920 sec.
iter 335680 || Loss: 0.5311 || timer: 0.0922 sec.
iter 335690 || Loss: 0.6834 || timer: 0.0889 sec.
iter 335700 || Loss: 0.5271 || timer: 0.1019 sec.
iter 335710 || Loss: 0.8576 || timer: 0.0824 sec.
iter 335720 || Loss: 0.6868 || timer: 0.0910 sec.
iter 335730 || Loss: 0.6549 || timer: 0.0900 sec.
iter 335740 || Loss: 0.7246 || timer: 0.0921 sec.
iter 335750 || Loss: 0.7837 || timer: 0.0985 sec.
iter 335760 || Loss: 0.6543 || timer: 0.0987 sec.
iter 335770 || Loss: 0.4143 || timer: 0.1232 sec.
iter 335780 || Loss: 0.7629 || timer: 0.0848 sec.
iter 335790 || Loss: 0.4409 || timer: 0.0898 sec.
iter 335800 || Loss: 0.8982 || timer: 0.0925 sec.
iter 335810 || Loss: 0.9496 || timer: 0.0892 sec.
iter 335820 || Loss: 0.5480 || timer: 0.0897 sec.
iter 335830 || Loss: 0.7404 || timer: 0.0243 sec.
iter 335840 || Loss: 0.6578 || timer: 0.0828 sec.
iter 335850 || Loss: 0.4995 || timer: 0.0893 sec.
iter 335860 || Loss: 0.6057 || timer: 0.0895 sec.
iter 335870 || Loss: 0.7447 || timer: 0.0908 sec.
iter 335880 || Loss: 0.7010 || timer: 0.0890 sec.
iter 335890 || Loss: 0.7738 || timer: 0.0842 sec.
iter 335900 || Loss: 0.6361 || timer: 0.0907 sec.
iter 335910 || Loss: 0.8688 || timer: 0.0835 sec.
iter 335920 || Loss: 0.5818 || timer: 0.1053 sec.
iter 335930 || Loss: 0.8171 || timer: 0.1032 sec.
iter 335940 || Loss: 0.7371 || timer: 0.0956 sec.
iter 335950 || Loss: 0.6428 || timer: 0.0847 sec.
iter 335960 || Loss: 0.5610 || timer: 0.0905 sec.
iter 335970 || Loss: 0.7281 || timer: 0.0863 sec.
iter 335980 || Loss: 0.6318 || timer: 0.1044 sec.
iter 335990 || Loss: 0.6125 || timer: 0.0772 sec.
iter 336000 || Loss: 0.6482 || timer: 0.0829 sec.
iter 336010 || Loss: 0.6067 || timer: 0.0899 sec.
iter 336020 || Loss: 0.7628 || timer: 0.0896 sec.
iter 336030 || Loss: 0.7515 || timer: 0.0836 sec.
iter 336040 || Loss: 0.7487 || timer: 0.1077 sec.
iter 336050 || Loss: 0.7588 || timer: 0.0837 sec.
iter 336060 || Loss: 0.5378 || timer: 0.0912 sec.
iter 336070 || Loss: 0.5816 || timer: 0.0933 sec.
iter 336080 || Loss: 0.9490 || timer: 0.0896 sec.
iter 336090 || Loss: 0.6901 || timer: 0.0894 sec.
iter 336100 || Loss: 0.9859 || timer: 0.0908 sec.
iter 336110 || Loss: 0.7880 || timer: 0.0920 sec.
iter 336120 || Loss: 0.6269 || timer: 0.0910 sec.
iter 336130 || Loss: 0.8479 || timer: 0.0898 sec.
iter 336140 || Loss: 0.6965 || timer: 0.0898 sec.
iter 336150 || Loss: 0.8318 || timer: 0.0838 sec.
iter 336160 || Loss: 0.9464 || timer: 0.0288 sec.
iter 336170 || Loss: 0.5345 || timer: 0.1023 sec.
iter 336180 || Loss: 0.6165 || timer: 0.1135 sec.
iter 336190 || Loss: 0.9085 || timer: 0.0919 sec.
iter 336200 || Loss: 0.7404 || timer: 0.0881 sec.
iter 336210 || Loss: 0.6082 || timer: 0.0855 sec.
iter 336220 || Loss: 0.7281 || timer: 0.1075 sec.
iter 336230 || Loss: 0.7366 || timer: 0.0909 sec.
iter 336240 || Loss: 0.9298 || timer: 0.0906 sec.
iter 336250 || Loss: 0.8036 || timer: 0.0985 sec.
iter 336260 || Loss: 0.7964 || timer: 0.1124 sec.
iter 336270 || Loss: 0.6706 || timer: 0.0910 sec.
iter 336280 || Loss: 0.5881 || timer: 0.0988 sec.
iter 336290 || Loss: 0.6542 || timer: 0.0841 sec.
iter 336300 || Loss: 0.7464 || timer: 0.0897 sec.
iter 336310 || Loss: 0.6953 || timer: 0.0894 sec.
iter 336320 || Loss: 0.7118 || timer: 0.0863 sec.
iter 336330 || Loss: 0.6303 || timer: 0.0877 sec.
iter 336340 || Loss: 0.7042 || timer: 0.0838 sec.
iter 336350 || Loss: 0.6124 || timer: 0.0890 sec.
iter 336360 || Loss: 0.5781 || timer: 0.0900 sec.
iter 336370 || Loss: 0.6747 || timer: 0.0861 sec.
iter 336380 || Loss: 0.6031 || timer: 0.0831 sec.
iter 336390 || Loss: 0.5209 || timer: 0.0842 sec.
iter 336400 || Loss: 0.6223 || timer: 0.0836 sec.
iter 336410 || Loss: 0.8582 || timer: 0.0874 sec.
iter 336420 || Loss: 0.9870 || timer: 0.0849 sec.
iter 336430 || Loss: 0.6219 || timer: 0.0886 sec.
iter 336440 || Loss: 0.8740 || timer: 0.0903 sec.
iter 336450 || Loss: 0.7236 || timer: 0.0843 sec.
iter 336460 || Loss: 1.0279 || timer: 0.0827 sec.
iter 336470 || Loss: 0.7008 || timer: 0.0966 sec.
iter 336480 || Loss: 0.5876 || timer: 0.1067 sec.
iter 336490 || Loss: 0.7491 || timer: 0.0170 sec.
iter 336500 || Loss: 0.5703 || timer: 0.0897 sec.
iter 336510 || Loss: 0.5790 || timer: 0.0821 sec.
iter 336520 || Loss: 0.7225 || timer: 0.0990 sec.
iter 336530 || Loss: 0.4540 || timer: 0.0830 sec.
iter 336540 || Loss: 0.5488 || timer: 0.0933 sec.
iter 336550 || Loss: 1.0672 || timer: 0.1014 sec.
iter 336560 || Loss: 0.5345 || timer: 0.0899 sec.
iter 336570 || Loss: 0.4968 || timer: 0.0893 sec.
iter 336580 || Loss: 0.9064 || timer: 0.0925 sec.
iter 336590 || Loss: 0.8118 || timer: 0.1151 sec.
iter 336600 || Loss: 0.7264 || timer: 0.0864 sec.
iter 336610 || Loss: 0.8039 || timer: 0.0955 sec.
iter 336620 || Loss: 0.6625 || timer: 0.1031 sec.
iter 336630 || Loss: 0.9281 || timer: 0.0916 sec.
iter 336640 || Loss: 0.8459 || timer: 0.1126 sec.
iter 336650 || Loss: 0.9075 || timer: 0.0880 sec.
iter 336660 || Loss: 0.6039 || timer: 0.0904 sec.
iter 336670 || Loss: 0.7935 || timer: 0.0968 sec.
iter 336680 || Loss: 0.8821 || timer: 0.0893 sec.
iter 336690 || Loss: 0.6503 || timer: 0.0834 sec.
iter 336700 || Loss: 0.6275 || timer: 0.0833 sec.
iter 336710 || Loss: 0.6204 || timer: 0.1117 sec.
iter 336720 || Loss: 0.7521 || timer: 0.0899 sec.
iter 336730 || Loss: 1.1158 || timer: 0.0904 sec.
iter 336740 || Loss: 0.8276 || timer: 0.0901 sec.
iter 336750 || Loss: 0.8411 || timer: 0.1180 sec.
iter 336760 || Loss: 0.6898 || timer: 0.0893 sec.
iter 336770 || Loss: 0.6853 || timer: 0.0930 sec.
iter 336780 || Loss: 0.6677 || timer: 0.0844 sec.
iter 336790 || Loss: 0.5879 || timer: 0.0913 sec.
iter 336800 || Loss: 0.7056 || timer: 0.0924 sec.
iter 336810 || Loss: 0.9349 || timer: 0.0921 sec.
iter 336820 || Loss: 1.1389 || timer: 0.0264 sec.
iter 336830 || Loss: 2.4523 || timer: 0.0905 sec.
iter 336840 || Loss: 0.6380 || timer: 0.0890 sec.
iter 336850 || Loss: 0.9552 || timer: 0.0949 sec.
iter 336860 || Loss: 0.5994 || timer: 0.0918 sec.
iter 336870 || Loss: 0.6503 || timer: 0.0924 sec.
iter 336880 || Loss: 0.7559 || timer: 0.0902 sec.
iter 336890 || Loss: 0.6579 || timer: 0.0828 sec.
iter 336900 || Loss: 0.6549 || timer: 0.0928 sec.
iter 336910 || Loss: 0.4720 || timer: 0.0931 sec.
iter 336920 || Loss: 0.5799 || timer: 0.1000 sec.
iter 336930 || Loss: 0.7756 || timer: 0.0929 sec.
iter 336940 || Loss: 0.4700 || timer: 0.0831 sec.
iter 336950 || Loss: 0.7747 || timer: 0.0907 sec.
iter 336960 || Loss: 0.8367 || timer: 0.0896 sec.
iter 336970 || Loss: 0.4685 || timer: 0.0897 sec.
iter 336980 || Loss: 0.6606 || timer: 0.1214 sec.
iter 336990 || Loss: 0.7883 || timer: 0.0902 sec.
iter 337000 || Loss: 0.9484 || timer: 0.0824 sec.
iter 337010 || Loss: 0.7170 || timer: 0.0842 sec.
iter 337020 || Loss: 0.8343 || timer: 0.0873 sec.
iter 337030 || Loss: 0.7545 || timer: 0.0903 sec.
iter 337040 || Loss: 1.0728 || timer: 0.0939 sec.
iter 337050 || Loss: 0.4697 || timer: 0.0918 sec.
iter 337060 || Loss: 0.6828 || timer: 0.0828 sec.
iter 337070 || Loss: 0.4491 || timer: 0.0838 sec.
iter 337080 || Loss: 0.5688 || timer: 0.0904 sec.
iter 337090 || Loss: 0.6233 || timer: 0.1172 sec.
iter 337100 || Loss: 0.8589 || timer: 0.0825 sec.
iter 337110 || Loss: 0.6802 || timer: 0.0893 sec.
iter 337120 || Loss: 0.6891 || timer: 0.0916 sec.
iter 337130 || Loss: 0.7063 || timer: 0.0894 sec.
iter 337140 || Loss: 0.6308 || timer: 0.0911 sec.
iter 337150 || Loss: 0.9529 || timer: 0.0177 sec.
iter 337160 || Loss: 4.4113 || timer: 0.1080 sec.
iter 337170 || Loss: 0.8188 || timer: 0.0912 sec.
iter 337180 || Loss: 0.7450 || timer: 0.0938 sec.
iter 337190 || Loss: 0.6371 || timer: 0.0915 sec.
iter 337200 || Loss: 0.9220 || timer: 0.0999 sec.
iter 337210 || Loss: 0.5619 || timer: 0.1055 sec.
iter 337220 || Loss: 0.6434 || timer: 0.0917 sec.
iter 337230 || Loss: 0.4663 || timer: 0.0821 sec.
iter 337240 || Loss: 0.6839 || timer: 0.0834 sec.
iter 337250 || Loss: 0.6524 || timer: 0.1202 sec.
iter 337260 || Loss: 0.9209 || timer: 0.0918 sec.
iter 337270 || Loss: 0.7853 || timer: 0.0905 sec.
iter 337280 || Loss: 0.7078 || timer: 0.0806 sec.
iter 337290 || Loss: 0.5606 || timer: 0.0830 sec.
iter 337300 || Loss: 0.4810 || timer: 0.0840 sec.
iter 337310 || Loss: 0.6145 || timer: 0.0835 sec.
iter 337320 || Loss: 0.7199 || timer: 0.0940 sec.
iter 337330 || Loss: 0.8064 || timer: 0.0888 sec.
iter 337340 || Loss: 0.7593 || timer: 0.0827 sec.
iter 337350 || Loss: 0.6352 || timer: 0.1130 sec.
iter 337360 || Loss: 0.7796 || timer: 0.0857 sec.
iter 337370 || Loss: 0.7714 || timer: 0.0897 sec.
iter 337380 || Loss: 0.9053 || timer: 0.0915 sec.
iter 337390 || Loss: 0.5179 || timer: 0.1022 sec.
iter 337400 || Loss: 0.6143 || timer: 0.0897 sec.
iter 337410 || Loss: 0.6387 || timer: 0.0896 sec.
iter 337420 || Loss: 0.4855 || timer: 0.0917 sec.
iter 337430 || Loss: 0.8630 || timer: 0.0874 sec.
iter 337440 || Loss: 0.5817 || timer: 0.0903 sec.
iter 337450 || Loss: 0.7304 || timer: 0.0837 sec.
iter 337460 || Loss: 0.5295 || timer: 0.0828 sec.
iter 337470 || Loss: 0.8801 || timer: 0.0924 sec.
iter 337480 || Loss: 0.6389 || timer: 0.0210 sec.
iter 337490 || Loss: 0.4472 || timer: 0.0920 sec.
iter 337500 || Loss: 0.6748 || timer: 0.0914 sec.
iter 337510 || Loss: 0.5147 || timer: 0.0937 sec.
iter 337520 || Loss: 0.9541 || timer: 0.0907 sec.
iter 337530 || Loss: 0.5424 || timer: 0.1089 sec.
iter 337540 || Loss: 0.7733 || timer: 0.0905 sec.
iter 337550 || Loss: 0.7452 || timer: 0.0912 sec.
iter 337560 || Loss: 0.6507 || timer: 0.0899 sec.
iter 337570 || Loss: 0.7835 || timer: 0.0838 sec.
iter 337580 || Loss: 0.7360 || timer: 0.0948 sec.
iter 337590 || Loss: 0.6376 || timer: 0.0833 sec.
iter 337600 || Loss: 0.6426 || timer: 0.0961 sec.
iter 337610 || Loss: 0.7979 || timer: 0.1124 sec.
iter 337620 || Loss: 0.8914 || timer: 0.0909 sec.
iter 337630 || Loss: 0.5573 || timer: 0.0832 sec.
iter 337640 || Loss: 0.7722 || timer: 0.0906 sec.
iter 337650 || Loss: 0.4618 || timer: 0.0894 sec.
iter 337660 || Loss: 0.8395 || timer: 0.0906 sec.
iter 337670 || Loss: 0.4651 || timer: 0.0831 sec.
iter 337680 || Loss: 0.5373 || timer: 0.0953 sec.
iter 337690 || Loss: 0.6240 || timer: 0.0907 sec.
iter 337700 || Loss: 0.6306 || timer: 0.0881 sec.
iter 337710 || Loss: 1.0296 || timer: 0.0905 sec.
iter 337720 || Loss: 0.8028 || timer: 0.0910 sec.
iter 337730 || Loss: 0.8183 || timer: 0.1086 sec.
iter 337740 || Loss: 0.6496 || timer: 0.0863 sec.
iter 337750 || Loss: 0.7214 || timer: 0.0986 sec.
iter 337760 || Loss: 0.6219 || timer: 0.0886 sec.
iter 337770 || Loss: 0.7869 || timer: 0.0915 sec.
iter 337780 || Loss: 0.5274 || timer: 0.0919 sec.
iter 337790 || Loss: 0.7685 || timer: 0.0894 sec.
iter 337800 || Loss: 0.9021 || timer: 0.0838 sec.
iter 337810 || Loss: 0.7562 || timer: 0.0173 sec.
iter 337820 || Loss: 0.9228 || timer: 0.0923 sec.
iter 337830 || Loss: 0.4924 || timer: 0.0887 sec.
iter 337840 || Loss: 0.6737 || timer: 0.0904 sec.
iter 337850 || Loss: 0.6683 || timer: 0.0842 sec.
iter 337860 || Loss: 0.6982 || timer: 0.0872 sec.
iter 337870 || Loss: 0.6854 || timer: 0.0828 sec.
iter 337880 || Loss: 0.6467 || timer: 0.0931 sec.
iter 337890 || Loss: 0.8258 || timer: 0.0899 sec.
iter 337900 || Loss: 0.5858 || timer: 0.0968 sec.
iter 337910 || Loss: 0.6367 || timer: 0.1206 sec.
iter 337920 || Loss: 0.7300 || timer: 0.0906 sec.
iter 337930 || Loss: 0.7816 || timer: 0.0841 sec.
iter 337940 || Loss: 0.9066 || timer: 0.0911 sec.
iter 337950 || Loss: 0.8478 || timer: 0.1047 sec.
iter 337960 || Loss: 0.5063 || timer: 0.0872 sec.
iter 337970 || Loss: 0.6355 || timer: 0.0983 sec.
iter 337980 || Loss: 0.5566 || timer: 0.0904 sec.
iter 337990 || Loss: 0.7425 || timer: 0.0909 sec.
iter 338000 || Loss: 0.5458 || timer: 0.0963 sec.
iter 338010 || Loss: 0.9978 || timer: 0.0913 sec.
iter 338020 || Loss: 0.8641 || timer: 0.0839 sec.
iter 338030 || Loss: 0.8426 || timer: 0.0932 sec.
iter 338040 || Loss: 0.6271 || timer: 0.0837 sec.
iter 338050 || Loss: 0.7395 || timer: 0.1047 sec.
iter 338060 || Loss: 0.5812 || timer: 0.0830 sec.
iter 338070 || Loss: 0.8058 || timer: 0.0906 sec.
iter 338080 || Loss: 0.5792 || timer: 0.0926 sec.
iter 338090 || Loss: 0.5806 || timer: 0.0909 sec.
iter 338100 || Loss: 0.6558 || timer: 0.1125 sec.
iter 338110 || Loss: 0.6936 || timer: 0.0856 sec.
iter 338120 || Loss: 0.8048 || timer: 0.0875 sec.
iter 338130 || Loss: 0.6725 || timer: 0.0915 sec.
iter 338140 || Loss: 0.6124 || timer: 0.0222 sec.
iter 338150 || Loss: 0.8642 || timer: 0.1060 sec.
iter 338160 || Loss: 0.5478 || timer: 0.0828 sec.
iter 338170 || Loss: 1.1010 || timer: 0.0897 sec.
iter 338180 || Loss: 0.7255 || timer: 0.0967 sec.
iter 338190 || Loss: 0.8527 || timer: 0.0872 sec.
iter 338200 || Loss: 0.9576 || timer: 0.0863 sec.
iter 338210 || Loss: 0.6151 || timer: 0.0870 sec.
iter 338220 || Loss: 0.8069 || timer: 0.0852 sec.
iter 338230 || Loss: 0.4264 || timer: 0.0840 sec.
iter 338240 || Loss: 1.0020 || timer: 0.0968 sec.
iter 338250 || Loss: 0.6099 || timer: 0.0865 sec.
iter 338260 || Loss: 0.6399 || timer: 0.0980 sec.
iter 338270 || Loss: 0.6904 || timer: 0.0949 sec.
iter 338280 || Loss: 0.8007 || timer: 0.0969 sec.
iter 338290 || Loss: 0.5521 || timer: 0.0855 sec.
iter 338300 || Loss: 0.5055 || timer: 0.0994 sec.
iter 338310 || Loss: 0.5138 || timer: 0.0937 sec.
iter 338320 || Loss: 0.7811 || timer: 0.0919 sec.
iter 338330 || Loss: 1.0315 || timer: 0.0907 sec.
iter 338340 || Loss: 0.5643 || timer: 0.0934 sec.
iter 338350 || Loss: 0.6058 || timer: 0.0891 sec.
iter 338360 || Loss: 0.4624 || timer: 0.0832 sec.
iter 338370 || Loss: 1.0652 || timer: 0.0829 sec.
iter 338380 || Loss: 0.6903 || timer: 0.0834 sec.
iter 338390 || Loss: 0.6539 || timer: 0.1090 sec.
iter 338400 || Loss: 0.5688 || timer: 0.0831 sec.
iter 338410 || Loss: 0.7459 || timer: 0.0922 sec.
iter 338420 || Loss: 0.5738 || timer: 0.0820 sec.
iter 338430 || Loss: 0.3926 || timer: 0.0907 sec.
iter 338440 || Loss: 1.0758 || timer: 0.0914 sec.
iter 338450 || Loss: 0.7115 || timer: 0.0829 sec.
iter 338460 || Loss: 0.5453 || timer: 0.0901 sec.
iter 338470 || Loss: 0.7857 || timer: 0.0225 sec.
iter 338480 || Loss: 0.7752 || timer: 0.0841 sec.
iter 338490 || Loss: 0.7113 || timer: 0.0833 sec.
iter 338500 || Loss: 0.7235 || timer: 0.0822 sec.
iter 338510 || Loss: 0.7279 || timer: 0.0829 sec.
iter 338520 || Loss: 0.7028 || timer: 0.0929 sec.
iter 338530 || Loss: 0.5725 || timer: 0.1064 sec.
iter 338540 || Loss: 0.6640 || timer: 0.0854 sec.
iter 338550 || Loss: 0.6568 || timer: 0.0977 sec.
iter 338560 || Loss: 0.9017 || timer: 0.0823 sec.
iter 338570 || Loss: 0.6665 || timer: 0.1019 sec.
iter 338580 || Loss: 0.4913 || timer: 0.0892 sec.
iter 338590 || Loss: 0.5051 || timer: 0.0935 sec.
iter 338600 || Loss: 0.5227 || timer: 0.0888 sec.
iter 338610 || Loss: 0.6126 || timer: 0.0920 sec.
iter 338620 || Loss: 0.7106 || timer: 0.0847 sec.
iter 338630 || Loss: 0.6047 || timer: 0.0856 sec.
iter 338640 || Loss: 0.5599 || timer: 0.0827 sec.
iter 338650 || Loss: 0.6689 || timer: 0.0838 sec.
iter 338660 || Loss: 0.5222 || timer: 0.1088 sec.
iter 338670 || Loss: 0.9245 || timer: 0.0836 sec.
iter 338680 || Loss: 0.6717 || timer: 0.0904 sec.
iter 338690 || Loss: 0.8293 || timer: 0.0889 sec.
iter 338700 || Loss: 0.6615 || timer: 0.0932 sec.
iter 338710 || Loss: 0.6604 || timer: 0.0934 sec.
iter 338720 || Loss: 0.7293 || timer: 0.0903 sec.
iter 338730 || Loss: 0.9614 || timer: 0.0901 sec.
iter 338740 || Loss: 0.6291 || timer: 0.0838 sec.
iter 338750 || Loss: 0.6432 || timer: 0.0922 sec.
iter 338760 || Loss: 0.4481 || timer: 0.0907 sec.
iter 338770 || Loss: 0.7352 || timer: 0.1158 sec.
iter 338780 || Loss: 0.9213 || timer: 0.1083 sec.
iter 338790 || Loss: 0.4798 || timer: 0.0831 sec.
iter 338800 || Loss: 0.5797 || timer: 0.0210 sec.
iter 338810 || Loss: 0.2019 || timer: 0.0831 sec.
iter 338820 || Loss: 0.7166 || timer: 0.0911 sec.
iter 338830 || Loss: 0.8529 || timer: 0.0927 sec.
iter 338840 || Loss: 0.8870 || timer: 0.0920 sec.
iter 338850 || Loss: 0.5971 || timer: 0.0950 sec.
iter 338860 || Loss: 0.7524 || timer: 0.0828 sec.
iter 338870 || Loss: 0.5564 || timer: 0.0822 sec.
iter 338880 || Loss: 0.5543 || timer: 0.0912 sec.
iter 338890 || Loss: 0.8080 || timer: 0.0919 sec.
iter 338900 || Loss: 0.8497 || timer: 0.1049 sec.
iter 338910 || Loss: 0.4468 || timer: 0.0856 sec.
iter 338920 || Loss: 0.6400 || timer: 0.0842 sec.
iter 338930 || Loss: 0.8132 || timer: 0.0919 sec.
iter 338940 || Loss: 0.6136 || timer: 0.0902 sec.
iter 338950 || Loss: 0.5129 || timer: 0.0878 sec.
iter 338960 || Loss: 0.5431 || timer: 0.0828 sec.
iter 338970 || Loss: 0.4765 || timer: 0.0998 sec.
iter 338980 || Loss: 0.5515 || timer: 0.0883 sec.
iter 338990 || Loss: 1.0680 || timer: 0.0881 sec.
iter 339000 || Loss: 0.6049 || timer: 0.0923 sec.
iter 339010 || Loss: 0.5946 || timer: 0.0878 sec.
iter 339020 || Loss: 1.0052 || timer: 0.0998 sec.
iter 339030 || Loss: 0.8630 || timer: 0.0907 sec.
iter 339040 || Loss: 0.6039 || timer: 0.1007 sec.
iter 339050 || Loss: 0.9411 || timer: 0.0882 sec.
iter 339060 || Loss: 0.8414 || timer: 0.0964 sec.
iter 339070 || Loss: 0.5995 || timer: 0.1132 sec.
iter 339080 || Loss: 0.7735 || timer: 0.0826 sec.
iter 339090 || Loss: 0.5726 || timer: 0.0940 sec.
iter 339100 || Loss: 0.5231 || timer: 0.0927 sec.
iter 339110 || Loss: 0.5856 || timer: 0.1024 sec.
iter 339120 || Loss: 0.6805 || timer: 0.1175 sec.
iter 339130 || Loss: 0.6947 || timer: 0.0217 sec.
iter 339140 || Loss: 2.6573 || timer: 0.0900 sec.
iter 339150 || Loss: 0.7479 || timer: 0.0908 sec.
iter 339160 || Loss: 0.6618 || timer: 0.0825 sec.
iter 339170 || Loss: 0.7959 || timer: 0.0839 sec.
iter 339180 || Loss: 0.5850 || timer: 0.0887 sec.
iter 339190 || Loss: 0.4677 || timer: 0.0909 sec.
iter 339200 || Loss: 0.8918 || timer: 0.0851 sec.
iter 339210 || Loss: 0.5531 || timer: 0.1205 sec.
iter 339220 || Loss: 0.8285 || timer: 0.0832 sec.
iter 339230 || Loss: 0.7954 || timer: 0.1002 sec.
iter 339240 || Loss: 0.4719 || timer: 0.0826 sec.
iter 339250 || Loss: 0.7585 || timer: 0.0830 sec.
iter 339260 || Loss: 0.5880 || timer: 0.0814 sec.
iter 339270 || Loss: 0.7428 || timer: 0.0918 sec.
iter 339280 || Loss: 0.6050 || timer: 0.1057 sec.
iter 339290 || Loss: 0.9996 || timer: 0.0931 sec.
iter 339300 || Loss: 0.6939 || timer: 0.0895 sec.
iter 339310 || Loss: 0.7189 || timer: 0.0986 sec.
iter 339320 || Loss: 0.9097 || timer: 0.0841 sec.
iter 339330 || Loss: 0.6824 || timer: 0.1075 sec.
iter 339340 || Loss: 0.5693 || timer: 0.0994 sec.
iter 339350 || Loss: 0.5881 || timer: 0.0843 sec.
iter 339360 || Loss: 0.5413 || timer: 0.0923 sec.
iter 339370 || Loss: 0.5190 || timer: 0.0917 sec.
iter 339380 || Loss: 0.9023 || timer: 0.0910 sec.
iter 339390 || Loss: 0.7678 || timer: 0.0823 sec.
iter 339400 || Loss: 0.5908 || timer: 0.0909 sec.
iter 339410 || Loss: 0.9590 || timer: 0.0943 sec.
iter 339420 || Loss: 0.4663 || timer: 0.0941 sec.
iter 339430 || Loss: 0.4065 || timer: 0.1048 sec.
iter 339440 || Loss: 0.6871 || timer: 0.1067 sec.
iter 339450 || Loss: 0.6192 || timer: 0.0820 sec.
iter 339460 || Loss: 1.0492 || timer: 0.0226 sec.
iter 339470 || Loss: 1.3542 || timer: 0.0872 sec.
iter 339480 || Loss: 0.4613 || timer: 0.0876 sec.
iter 339490 || Loss: 0.8524 || timer: 0.0907 sec.
iter 339500 || Loss: 0.7749 || timer: 0.0913 sec.
iter 339510 || Loss: 0.5699 || timer: 0.0904 sec.
iter 339520 || Loss: 0.6953 || timer: 0.0990 sec.
iter 339530 || Loss: 0.5680 || timer: 0.0957 sec.
iter 339540 || Loss: 0.6499 || timer: 0.0892 sec.
iter 339550 || Loss: 0.5537 || timer: 0.0912 sec.
iter 339560 || Loss: 0.8345 || timer: 0.1018 sec.
iter 339570 || Loss: 0.5925 || timer: 0.0900 sec.
iter 339580 || Loss: 0.5099 || timer: 0.0905 sec.
iter 339590 || Loss: 0.8298 || timer: 0.0925 sec.
iter 339600 || Loss: 0.6231 || timer: 0.0921 sec.
iter 339610 || Loss: 0.6896 || timer: 0.0835 sec.
iter 339620 || Loss: 0.8499 || timer: 0.0928 sec.
iter 339630 || Loss: 0.6615 || timer: 0.0903 sec.
iter 339640 || Loss: 1.0082 || timer: 0.0866 sec.
iter 339650 || Loss: 0.9117 || timer: 0.0906 sec.
iter 339660 || Loss: 0.7304 || timer: 0.1006 sec.
iter 339670 || Loss: 0.7875 || timer: 0.1006 sec.
iter 339680 || Loss: 0.6306 || timer: 0.0928 sec.
iter 339690 || Loss: 0.5276 || timer: 0.0907 sec.
iter 339700 || Loss: 0.7840 || timer: 0.0918 sec.
iter 339710 || Loss: 0.5983 || timer: 0.1029 sec.
iter 339720 || Loss: 0.9430 || timer: 0.0827 sec.
iter 339730 || Loss: 0.6492 || timer: 0.0904 sec.
iter 339740 || Loss: 0.8082 || timer: 0.1039 sec.
iter 339750 || Loss: 0.5442 || timer: 0.0983 sec.
iter 339760 || Loss: 0.6655 || timer: 0.0914 sec.
iter 339770 || Loss: 0.5381 || timer: 0.0892 sec.
iter 339780 || Loss: 0.7298 || timer: 0.0830 sec.
iter 339790 || Loss: 0.7163 || timer: 0.0229 sec.
iter 339800 || Loss: 0.1356 || timer: 0.0925 sec.
iter 339810 || Loss: 0.5844 || timer: 0.0907 sec.
iter 339820 || Loss: 0.6396 || timer: 0.0913 sec.
iter 339830 || Loss: 0.6352 || timer: 0.1143 sec.
iter 339840 || Loss: 0.5426 || timer: 0.0904 sec.
iter 339850 || Loss: 0.8042 || timer: 0.0840 sec.
iter 339860 || Loss: 1.0949 || timer: 0.0832 sec.
iter 339870 || Loss: 0.6916 || timer: 0.0828 sec.
iter 339880 || Loss: 0.7810 || timer: 0.0892 sec.
iter 339890 || Loss: 0.5815 || timer: 0.1000 sec.
iter 339900 || Loss: 0.6322 || timer: 0.0851 sec.
iter 339910 || Loss: 0.6950 || timer: 0.0838 sec.
iter 339920 || Loss: 0.6964 || timer: 0.0824 sec.
iter 339930 || Loss: 0.6915 || timer: 0.0835 sec.
iter 339940 || Loss: 0.8678 || timer: 0.1108 sec.
iter 339950 || Loss: 0.5072 || timer: 0.0905 sec.
iter 339960 || Loss: 0.7035 || timer: 0.0917 sec.
iter 339970 || Loss: 0.7437 || timer: 0.0881 sec.
iter 339980 || Loss: 0.5722 || timer: 0.0882 sec.
iter 339990 || Loss: 0.7105 || timer: 0.0988 sec.
iter 340000 || Loss: 0.6567 || Saving state, iter: 340000
timer: 0.1061 sec.
iter 340010 || Loss: 0.7220 || timer: 0.0843 sec.
iter 340020 || Loss: 0.6111 || timer: 0.0906 sec.
iter 340030 || Loss: 0.7600 || timer: 0.0885 sec.
iter 340040 || Loss: 0.9596 || timer: 0.0833 sec.
iter 340050 || Loss: 0.7755 || timer: 0.1087 sec.
iter 340060 || Loss: 0.5793 || timer: 0.0933 sec.
iter 340070 || Loss: 0.6734 || timer: 0.0936 sec.
iter 340080 || Loss: 0.7841 || timer: 0.0969 sec.
iter 340090 || Loss: 0.4530 || timer: 0.0826 sec.
iter 340100 || Loss: 0.7547 || timer: 0.0918 sec.
iter 340110 || Loss: 0.5822 || timer: 0.1215 sec.
iter 340120 || Loss: 0.8158 || timer: 0.0248 sec.
iter 340130 || Loss: 0.8098 || timer: 0.0833 sec.
iter 340140 || Loss: 0.6259 || timer: 0.0959 sec.
iter 340150 || Loss: 0.6265 || timer: 0.0914 sec.
iter 340160 || Loss: 0.8577 || timer: 0.0827 sec.
iter 340170 || Loss: 0.5475 || timer: 0.0830 sec.
iter 340180 || Loss: 0.6730 || timer: 0.0848 sec.
iter 340190 || Loss: 0.7610 || timer: 0.0917 sec.
iter 340200 || Loss: 0.7585 || timer: 0.0903 sec.
iter 340210 || Loss: 0.8595 || timer: 0.1100 sec.
iter 340220 || Loss: 0.6128 || timer: 0.1238 sec.
iter 340230 || Loss: 0.4511 || timer: 0.0829 sec.
iter 340240 || Loss: 0.6161 || timer: 0.0909 sec.
iter 340250 || Loss: 0.8774 || timer: 0.0832 sec.
iter 340260 || Loss: 0.4455 || timer: 0.0824 sec.
iter 340270 || Loss: 0.5246 || timer: 0.0875 sec.
iter 340280 || Loss: 0.8045 || timer: 0.1022 sec.
iter 340290 || Loss: 0.5550 || timer: 0.0903 sec.
iter 340300 || Loss: 0.9786 || timer: 0.0876 sec.
iter 340310 || Loss: 0.7366 || timer: 0.0871 sec.
iter 340320 || Loss: 0.8504 || timer: 0.0755 sec.
iter 340330 || Loss: 0.5012 || timer: 0.1067 sec.
iter 340340 || Loss: 0.9184 || timer: 0.0819 sec.
iter 340350 || Loss: 0.6284 || timer: 0.0933 sec.
iter 340360 || Loss: 1.0258 || timer: 0.0871 sec.
iter 340370 || Loss: 0.7988 || timer: 0.0823 sec.
iter 340380 || Loss: 0.7334 || timer: 0.0822 sec.
iter 340390 || Loss: 0.5452 || timer: 0.0909 sec.
iter 340400 || Loss: 0.7993 || timer: 0.0866 sec.
iter 340410 || Loss: 0.7039 || timer: 0.0883 sec.
iter 340420 || Loss: 0.5958 || timer: 0.0877 sec.
iter 340430 || Loss: 0.9107 || timer: 0.0903 sec.
iter 340440 || Loss: 0.6254 || timer: 0.1095 sec.
iter 340450 || Loss: 0.6064 || timer: 0.0314 sec.
iter 340460 || Loss: 0.5422 || timer: 0.1045 sec.
iter 340470 || Loss: 0.9139 || timer: 0.0837 sec.
iter 340480 || Loss: 0.9486 || timer: 0.0829 sec.
iter 340490 || Loss: 0.9506 || timer: 0.0926 sec.
iter 340500 || Loss: 0.6768 || timer: 0.0842 sec.
iter 340510 || Loss: 0.6527 || timer: 0.1092 sec.
iter 340520 || Loss: 0.5403 || timer: 0.0899 sec.
iter 340530 || Loss: 0.7831 || timer: 0.0932 sec.
iter 340540 || Loss: 0.8782 || timer: 0.0990 sec.
iter 340550 || Loss: 0.8368 || timer: 0.1019 sec.
iter 340560 || Loss: 0.7806 || timer: 0.0901 sec.
iter 340570 || Loss: 0.6556 || timer: 0.0948 sec.
iter 340580 || Loss: 0.7293 || timer: 0.0893 sec.
iter 340590 || Loss: 0.6725 || timer: 0.0867 sec.
iter 340600 || Loss: 0.5394 || timer: 0.0844 sec.
iter 340610 || Loss: 0.6119 || timer: 0.0909 sec.
iter 340620 || Loss: 0.5808 || timer: 0.0923 sec.
iter 340630 || Loss: 0.8793 || timer: 0.0829 sec.
iter 340640 || Loss: 0.6912 || timer: 0.0882 sec.
iter 340650 || Loss: 0.6370 || timer: 0.0916 sec.
iter 340660 || Loss: 0.5778 || timer: 0.0902 sec.
iter 340670 || Loss: 0.7906 || timer: 0.0910 sec.
iter 340680 || Loss: 0.8022 || timer: 0.0922 sec.
iter 340690 || Loss: 0.8301 || timer: 0.0895 sec.
iter 340700 || Loss: 0.4665 || timer: 0.0826 sec.
iter 340710 || Loss: 0.6866 || timer: 0.0849 sec.
iter 340720 || Loss: 0.9532 || timer: 0.1120 sec.
iter 340730 || Loss: 0.7274 || timer: 0.0827 sec.
iter 340740 || Loss: 0.6179 || timer: 0.1011 sec.
iter 340750 || Loss: 0.8771 || timer: 0.0919 sec.
iter 340760 || Loss: 0.8319 || timer: 0.0911 sec.
iter 340770 || Loss: 0.7956 || timer: 0.0912 sec.
iter 340780 || Loss: 0.8226 || timer: 0.0264 sec.
iter 340790 || Loss: 2.4203 || timer: 0.0845 sec.
iter 340800 || Loss: 0.9729 || timer: 0.0919 sec.
iter 340810 || Loss: 0.5906 || timer: 0.0859 sec.
iter 340820 || Loss: 0.7557 || timer: 0.0906 sec.
iter 340830 || Loss: 0.4437 || timer: 0.1136 sec.
iter 340840 || Loss: 1.0307 || timer: 0.0915 sec.
iter 340850 || Loss: 0.6953 || timer: 0.1081 sec.
iter 340860 || Loss: 0.9801 || timer: 0.0887 sec.
iter 340870 || Loss: 0.8194 || timer: 0.1205 sec.
iter 340880 || Loss: 0.6782 || timer: 0.1093 sec.
iter 340890 || Loss: 0.4769 || timer: 0.0836 sec.
iter 340900 || Loss: 0.8027 || timer: 0.1072 sec.
iter 340910 || Loss: 0.7371 || timer: 0.0850 sec.
iter 340920 || Loss: 0.6903 || timer: 0.0900 sec.
iter 340930 || Loss: 0.7332 || timer: 0.0948 sec.
iter 340940 || Loss: 0.9505 || timer: 0.1187 sec.
iter 340950 || Loss: 0.6174 || timer: 0.0903 sec.
iter 340960 || Loss: 0.7667 || timer: 0.0885 sec.
iter 340970 || Loss: 0.8606 || timer: 0.1043 sec.
iter 340980 || Loss: 0.6410 || timer: 0.1021 sec.
iter 340990 || Loss: 0.7353 || timer: 0.0881 sec.
iter 341000 || Loss: 0.6077 || timer: 0.1104 sec.
iter 341010 || Loss: 1.0950 || timer: 0.0924 sec.
iter 341020 || Loss: 0.9609 || timer: 0.0917 sec.
iter 341030 || Loss: 0.6477 || timer: 0.0904 sec.
iter 341040 || Loss: 0.6123 || timer: 0.0935 sec.
iter 341050 || Loss: 0.9342 || timer: 0.0822 sec.
iter 341060 || Loss: 0.8001 || timer: 0.0840 sec.
iter 341070 || Loss: 0.6609 || timer: 0.0877 sec.
iter 341080 || Loss: 0.3893 || timer: 0.0896 sec.
iter 341090 || Loss: 0.6263 || timer: 0.0918 sec.
iter 341100 || Loss: 0.8231 || timer: 0.0908 sec.
iter 341110 || Loss: 0.8823 || timer: 0.0183 sec.
iter 341120 || Loss: 1.2850 || timer: 0.0919 sec.
iter 341130 || Loss: 0.5472 || timer: 0.1004 sec.
iter 341140 || Loss: 0.7887 || timer: 0.0828 sec.
iter 341150 || Loss: 0.3872 || timer: 0.0925 sec.
iter 341160 || Loss: 0.7704 || timer: 0.0928 sec.
iter 341170 || Loss: 0.8818 || timer: 0.0953 sec.
iter 341180 || Loss: 0.7394 || timer: 0.0881 sec.
iter 341190 || Loss: 0.8520 || timer: 0.0830 sec.
iter 341200 || Loss: 0.4536 || timer: 0.0849 sec.
iter 341210 || Loss: 0.6761 || timer: 0.1147 sec.
iter 341220 || Loss: 0.5755 || timer: 0.0921 sec.
iter 341230 || Loss: 0.6384 || timer: 0.0816 sec.
iter 341240 || Loss: 0.5674 || timer: 0.0911 sec.
iter 341250 || Loss: 0.6368 || timer: 0.0910 sec.
iter 341260 || Loss: 0.9857 || timer: 0.0965 sec.
iter 341270 || Loss: 0.8650 || timer: 0.1187 sec.
iter 341280 || Loss: 0.4934 || timer: 0.0983 sec.
iter 341290 || Loss: 0.4159 || timer: 0.0825 sec.
iter 341300 || Loss: 0.5449 || timer: 0.1047 sec.
iter 341310 || Loss: 0.6422 || timer: 0.1032 sec.
iter 341320 || Loss: 0.9488 || timer: 0.0903 sec.
iter 341330 || Loss: 0.7137 || timer: 0.0830 sec.
iter 341340 || Loss: 0.4995 || timer: 0.0839 sec.
iter 341350 || Loss: 0.9280 || timer: 0.1106 sec.
iter 341360 || Loss: 0.6076 || timer: 0.0922 sec.
iter 341370 || Loss: 0.7882 || timer: 0.0899 sec.
iter 341380 || Loss: 0.6682 || timer: 0.0838 sec.
iter 341390 || Loss: 0.8334 || timer: 0.0840 sec.
iter 341400 || Loss: 0.6445 || timer: 0.0988 sec.
iter 341410 || Loss: 0.5937 || timer: 0.0837 sec.
iter 341420 || Loss: 0.7938 || timer: 0.0901 sec.
iter 341430 || Loss: 0.9494 || timer: 0.0924 sec.
iter 341440 || Loss: 0.6700 || timer: 0.0236 sec.
iter 341450 || Loss: 0.2775 || timer: 0.0901 sec.
iter 341460 || Loss: 1.1023 || timer: 0.1065 sec.
iter 341470 || Loss: 0.7965 || timer: 0.0834 sec.
iter 341480 || Loss: 0.6398 || timer: 0.0902 sec.
iter 341490 || Loss: 0.5963 || timer: 0.1134 sec.
iter 341500 || Loss: 0.6835 || timer: 0.0903 sec.
iter 341510 || Loss: 0.7205 || timer: 0.0909 sec.
iter 341520 || Loss: 0.7043 || timer: 0.0824 sec.
iter 341530 || Loss: 0.7197 || timer: 0.0909 sec.
iter 341540 || Loss: 0.8795 || timer: 0.1052 sec.
iter 341550 || Loss: 0.6222 || timer: 0.0941 sec.
iter 341560 || Loss: 0.6141 || timer: 0.1034 sec.
iter 341570 || Loss: 0.6929 || timer: 0.0850 sec.
iter 341580 || Loss: 0.5645 || timer: 0.0906 sec.
iter 341590 || Loss: 0.8901 || timer: 0.0986 sec.
iter 341600 || Loss: 0.5574 || timer: 0.0900 sec.
iter 341610 || Loss: 0.5973 || timer: 0.0911 sec.
iter 341620 || Loss: 0.8080 || timer: 0.1060 sec.
iter 341630 || Loss: 0.5340 || timer: 0.0837 sec.
iter 341640 || Loss: 0.7659 || timer: 0.0940 sec.
iter 341650 || Loss: 0.7979 || timer: 0.0875 sec.
iter 341660 || Loss: 0.4169 || timer: 0.0871 sec.
iter 341670 || Loss: 0.6532 || timer: 0.0830 sec.
iter 341680 || Loss: 0.7135 || timer: 0.0896 sec.
iter 341690 || Loss: 0.8311 || timer: 0.0928 sec.
iter 341700 || Loss: 0.9168 || timer: 0.0900 sec.
iter 341710 || Loss: 0.5863 || timer: 0.0828 sec.
iter 341720 || Loss: 0.7122 || timer: 0.1175 sec.
iter 341730 || Loss: 0.8758 || timer: 0.0905 sec.
iter 341740 || Loss: 0.8010 || timer: 0.0813 sec.
iter 341750 || Loss: 0.8245 || timer: 0.0913 sec.
iter 341760 || Loss: 0.7863 || timer: 0.0838 sec.
iter 341770 || Loss: 0.5186 || timer: 0.0170 sec.
iter 341780 || Loss: 0.9678 || timer: 0.0930 sec.
iter 341790 || Loss: 0.5584 || timer: 0.0895 sec.
iter 341800 || Loss: 0.6707 || timer: 0.0858 sec.
iter 341810 || Loss: 1.0620 || timer: 0.0999 sec.
iter 341820 || Loss: 0.4862 || timer: 0.0898 sec.
iter 341830 || Loss: 0.8361 || timer: 0.1098 sec.
iter 341840 || Loss: 0.6715 || timer: 0.0836 sec.
iter 341850 || Loss: 0.7194 || timer: 0.1090 sec.
iter 341860 || Loss: 1.0576 || timer: 0.0842 sec.
iter 341870 || Loss: 0.8102 || timer: 0.1296 sec.
iter 341880 || Loss: 0.9478 || timer: 0.0920 sec.
iter 341890 || Loss: 0.9347 || timer: 0.0899 sec.
iter 341900 || Loss: 0.7651 || timer: 0.0845 sec.
iter 341910 || Loss: 0.8514 || timer: 0.0906 sec.
iter 341920 || Loss: 0.6451 || timer: 0.0835 sec.
iter 341930 || Loss: 0.8043 || timer: 0.0856 sec.
iter 341940 || Loss: 0.6715 || timer: 0.0901 sec.
iter 341950 || Loss: 0.6916 || timer: 0.0816 sec.
iter 341960 || Loss: 0.6037 || timer: 0.0901 sec.
iter 341970 || Loss: 0.6845 || timer: 0.1051 sec.
iter 341980 || Loss: 0.7444 || timer: 0.0867 sec.
iter 341990 || Loss: 0.7279 || timer: 0.0900 sec.
iter 342000 || Loss: 0.6085 || timer: 0.0856 sec.
iter 342010 || Loss: 0.7625 || timer: 0.0930 sec.
iter 342020 || Loss: 0.8328 || timer: 0.0991 sec.
iter 342030 || Loss: 0.9018 || timer: 0.0835 sec.
iter 342040 || Loss: 0.7636 || timer: 0.0838 sec.
iter 342050 || Loss: 0.5437 || timer: 0.0929 sec.
iter 342060 || Loss: 0.5619 || timer: 0.1053 sec.
iter 342070 || Loss: 0.7347 || timer: 0.0898 sec.
iter 342080 || Loss: 0.6161 || timer: 0.1087 sec.
iter 342090 || Loss: 0.7329 || timer: 0.0877 sec.
iter 342100 || Loss: 0.7742 || timer: 0.0173 sec.
iter 342110 || Loss: 1.7856 || timer: 0.0897 sec.
iter 342120 || Loss: 0.9960 || timer: 0.0888 sec.
iter 342130 || Loss: 0.5934 || timer: 0.0947 sec.
iter 342140 || Loss: 0.8004 || timer: 0.0842 sec.
iter 342150 || Loss: 0.9355 || timer: 0.0910 sec.
iter 342160 || Loss: 0.6486 || timer: 0.0903 sec.
iter 342170 || Loss: 0.7285 || timer: 0.0875 sec.
iter 342180 || Loss: 0.5905 || timer: 0.0906 sec.
iter 342190 || Loss: 0.5881 || timer: 0.0904 sec.
iter 342200 || Loss: 0.4765 || timer: 0.1067 sec.
iter 342210 || Loss: 0.6523 || timer: 0.0889 sec.
iter 342220 || Loss: 0.7698 || timer: 0.0906 sec.
iter 342230 || Loss: 0.5682 || timer: 0.0895 sec.
iter 342240 || Loss: 0.5748 || timer: 0.0919 sec.
iter 342250 || Loss: 0.8042 || timer: 0.0842 sec.
iter 342260 || Loss: 0.5841 || timer: 0.0894 sec.
iter 342270 || Loss: 0.7995 || timer: 0.0961 sec.
iter 342280 || Loss: 0.6091 || timer: 0.0890 sec.
iter 342290 || Loss: 1.3406 || timer: 0.1078 sec.
iter 342300 || Loss: 0.6705 || timer: 0.0931 sec.
iter 342310 || Loss: 0.9051 || timer: 0.0829 sec.
iter 342320 || Loss: 0.8377 || timer: 0.1035 sec.
iter 342330 || Loss: 0.4882 || timer: 0.0835 sec.
iter 342340 || Loss: 0.5334 || timer: 0.1175 sec.
iter 342350 || Loss: 0.6703 || timer: 0.0836 sec.
iter 342360 || Loss: 0.9646 || timer: 0.0906 sec.
iter 342370 || Loss: 0.5689 || timer: 0.0881 sec.
iter 342380 || Loss: 0.4564 || timer: 0.0842 sec.
iter 342390 || Loss: 0.6891 || timer: 0.0915 sec.
iter 342400 || Loss: 0.5219 || timer: 0.0836 sec.
iter 342410 || Loss: 0.5897 || timer: 0.1029 sec.
iter 342420 || Loss: 0.6081 || timer: 0.0823 sec.
iter 342430 || Loss: 0.5743 || timer: 0.0158 sec.
iter 342440 || Loss: 1.5222 || timer: 0.0828 sec.
iter 342450 || Loss: 0.6743 || timer: 0.0891 sec.
iter 342460 || Loss: 0.7726 || timer: 0.1054 sec.
iter 342470 || Loss: 0.7087 || timer: 0.1199 sec.
iter 342480 || Loss: 0.7679 || timer: 0.0846 sec.
iter 342490 || Loss: 0.8666 || timer: 0.0841 sec.
iter 342500 || Loss: 0.9076 || timer: 0.0756 sec.
iter 342510 || Loss: 0.5315 || timer: 0.0839 sec.
iter 342520 || Loss: 0.8498 || timer: 0.1029 sec.
iter 342530 || Loss: 0.6616 || timer: 0.1181 sec.
iter 342540 || Loss: 0.5784 || timer: 0.0829 sec.
iter 342550 || Loss: 0.8953 || timer: 0.0914 sec.
iter 342560 || Loss: 0.5756 || timer: 0.0905 sec.
iter 342570 || Loss: 0.6962 || timer: 0.1084 sec.
iter 342580 || Loss: 0.6215 || timer: 0.0893 sec.
iter 342590 || Loss: 0.7409 || timer: 0.0897 sec.
iter 342600 || Loss: 0.6039 || timer: 0.0913 sec.
iter 342610 || Loss: 0.8138 || timer: 0.0826 sec.
iter 342620 || Loss: 0.7177 || timer: 0.0825 sec.
iter 342630 || Loss: 0.4611 || timer: 0.1056 sec.
iter 342640 || Loss: 0.6026 || timer: 0.0898 sec.
iter 342650 || Loss: 0.7310 || timer: 0.0877 sec.
iter 342660 || Loss: 0.5751 || timer: 0.0904 sec.
iter 342670 || Loss: 0.5735 || timer: 0.1057 sec.
iter 342680 || Loss: 0.7438 || timer: 0.0867 sec.
iter 342690 || Loss: 0.8019 || timer: 0.0827 sec.
iter 342700 || Loss: 0.8211 || timer: 0.1084 sec.
iter 342710 || Loss: 1.0449 || timer: 0.0906 sec.
iter 342720 || Loss: 0.7181 || timer: 0.0908 sec.
iter 342730 || Loss: 0.4438 || timer: 0.0907 sec.
iter 342740 || Loss: 0.6927 || timer: 0.0891 sec.
iter 342750 || Loss: 0.7920 || timer: 0.0893 sec.
iter 342760 || Loss: 0.4635 || timer: 0.0222 sec.
iter 342770 || Loss: 0.3433 || timer: 0.0875 sec.
iter 342780 || Loss: 0.9935 || timer: 0.0868 sec.
iter 342790 || Loss: 0.6885 || timer: 0.0830 sec.
iter 342800 || Loss: 0.6899 || timer: 0.0914 sec.
iter 342810 || Loss: 0.8000 || timer: 0.0824 sec.
iter 342820 || Loss: 0.5290 || timer: 0.0868 sec.
iter 342830 || Loss: 0.7444 || timer: 0.0927 sec.
iter 342840 || Loss: 0.7524 || timer: 0.0885 sec.
iter 342850 || Loss: 0.6313 || timer: 0.0891 sec.
iter 342860 || Loss: 0.6558 || timer: 0.0933 sec.
iter 342870 || Loss: 0.7721 || timer: 0.0879 sec.
iter 342880 || Loss: 0.8244 || timer: 0.0824 sec.
iter 342890 || Loss: 0.7187 || timer: 0.0864 sec.
iter 342900 || Loss: 0.7311 || timer: 0.1049 sec.
iter 342910 || Loss: 0.7807 || timer: 0.0909 sec.
iter 342920 || Loss: 0.8028 || timer: 0.1077 sec.
iter 342930 || Loss: 0.6413 || timer: 0.1148 sec.
iter 342940 || Loss: 0.6006 || timer: 0.0883 sec.
iter 342950 || Loss: 0.6860 || timer: 0.0889 sec.
iter 342960 || Loss: 0.7677 || timer: 0.0861 sec.
iter 342970 || Loss: 0.6143 || timer: 0.0838 sec.
iter 342980 || Loss: 0.8137 || timer: 0.0853 sec.
iter 342990 || Loss: 0.6600 || timer: 0.0912 sec.
iter 343000 || Loss: 0.8077 || timer: 0.0889 sec.
iter 343010 || Loss: 0.7834 || timer: 0.1180 sec.
iter 343020 || Loss: 0.7482 || timer: 0.0889 sec.
iter 343030 || Loss: 0.6272 || timer: 0.0878 sec.
iter 343040 || Loss: 0.6614 || timer: 0.0824 sec.
iter 343050 || Loss: 0.5382 || timer: 0.0956 sec.
iter 343060 || Loss: 0.5133 || timer: 0.0830 sec.
iter 343070 || Loss: 0.5323 || timer: 0.1012 sec.
iter 343080 || Loss: 0.6167 || timer: 0.1020 sec.
iter 343090 || Loss: 0.7858 || timer: 0.0256 sec.
iter 343100 || Loss: 0.6690 || timer: 0.0837 sec.
iter 343110 || Loss: 0.7341 || timer: 0.0823 sec.
iter 343120 || Loss: 1.1073 || timer: 0.0886 sec.
iter 343130 || Loss: 0.8746 || timer: 0.1040 sec.
iter 343140 || Loss: 0.7119 || timer: 0.0838 sec.
iter 343150 || Loss: 0.6799 || timer: 0.0843 sec.
iter 343160 || Loss: 0.8957 || timer: 0.0903 sec.
iter 343170 || Loss: 0.4917 || timer: 0.0990 sec.
iter 343180 || Loss: 0.6735 || timer: 0.0877 sec.
iter 343190 || Loss: 0.5206 || timer: 0.1104 sec.
iter 343200 || Loss: 0.8407 || timer: 0.0898 sec.
iter 343210 || Loss: 0.7172 || timer: 0.1010 sec.
iter 343220 || Loss: 0.6423 || timer: 0.0909 sec.
iter 343230 || Loss: 0.7344 || timer: 0.0916 sec.
iter 343240 || Loss: 0.6282 || timer: 0.0904 sec.
iter 343250 || Loss: 0.8682 || timer: 0.0863 sec.
iter 343260 || Loss: 0.5579 || timer: 0.0914 sec.
iter 343270 || Loss: 0.6200 || timer: 0.1094 sec.
iter 343280 || Loss: 0.6226 || timer: 0.0909 sec.
iter 343290 || Loss: 0.6139 || timer: 0.0832 sec.
iter 343300 || Loss: 0.6326 || timer: 0.0885 sec.
iter 343310 || Loss: 0.8433 || timer: 0.0901 sec.
iter 343320 || Loss: 0.8117 || timer: 0.0836 sec.
iter 343330 || Loss: 0.6051 || timer: 0.0822 sec.
iter 343340 || Loss: 0.5835 || timer: 0.0880 sec.
iter 343350 || Loss: 0.4596 || timer: 0.0911 sec.
iter 343360 || Loss: 0.5811 || timer: 0.0825 sec.
iter 343370 || Loss: 0.9195 || timer: 0.1043 sec.
iter 343380 || Loss: 0.6440 || timer: 0.0826 sec.
iter 343390 || Loss: 0.6255 || timer: 0.0899 sec.
iter 343400 || Loss: 0.9766 || timer: 0.0829 sec.
iter 343410 || Loss: 0.8805 || timer: 0.0999 sec.
iter 343420 || Loss: 0.6011 || timer: 0.0216 sec.
iter 343430 || Loss: 1.1389 || timer: 0.0948 sec.
iter 343440 || Loss: 0.6044 || timer: 0.0833 sec.
iter 343450 || Loss: 0.9084 || timer: 0.0853 sec.
iter 343460 || Loss: 0.6675 || timer: 0.0826 sec.
iter 343470 || Loss: 0.5051 || timer: 0.1087 sec.
iter 343480 || Loss: 0.5591 || timer: 0.0893 sec.
iter 343490 || Loss: 0.7469 || timer: 0.1080 sec.
iter 343500 || Loss: 0.6266 || timer: 0.0912 sec.
iter 343510 || Loss: 0.6024 || timer: 0.0916 sec.
iter 343520 || Loss: 0.6636 || timer: 0.1023 sec.
iter 343530 || Loss: 0.6555 || timer: 0.0897 sec.
iter 343540 || Loss: 0.8099 || timer: 0.1100 sec.
iter 343550 || Loss: 0.6850 || timer: 0.0907 sec.
iter 343560 || Loss: 0.5736 || timer: 0.0822 sec.
iter 343570 || Loss: 0.6677 || timer: 0.0915 sec.
iter 343580 || Loss: 0.7313 || timer: 0.1090 sec.
iter 343590 || Loss: 0.9166 || timer: 0.0999 sec.
iter 343600 || Loss: 0.6492 || timer: 0.0882 sec.
iter 343610 || Loss: 0.6810 || timer: 0.0929 sec.
iter 343620 || Loss: 0.8466 || timer: 0.0901 sec.
iter 343630 || Loss: 0.6889 || timer: 0.0898 sec.
iter 343640 || Loss: 0.9617 || timer: 0.0904 sec.
iter 343650 || Loss: 0.6954 || timer: 0.0924 sec.
iter 343660 || Loss: 0.4981 || timer: 0.0904 sec.
iter 343670 || Loss: 0.4206 || timer: 0.0869 sec.
iter 343680 || Loss: 0.6770 || timer: 0.0841 sec.
iter 343690 || Loss: 0.5329 || timer: 0.0826 sec.
iter 343700 || Loss: 0.7127 || timer: 0.0842 sec.
iter 343710 || Loss: 1.0723 || timer: 0.0901 sec.
iter 343720 || Loss: 0.7364 || timer: 0.0832 sec.
iter 343730 || Loss: 0.6665 || timer: 0.0967 sec.
iter 343740 || Loss: 0.5931 || timer: 0.1068 sec.
iter 343750 || Loss: 0.6774 || timer: 0.0251 sec.
iter 343760 || Loss: 0.4494 || timer: 0.0826 sec.
iter 343770 || Loss: 0.7565 || timer: 0.0908 sec.
iter 343780 || Loss: 0.9069 || timer: 0.1128 sec.
iter 343790 || Loss: 0.6743 || timer: 0.0871 sec.
iter 343800 || Loss: 0.7553 || timer: 0.0903 sec.
iter 343810 || Loss: 0.6397 || timer: 0.1012 sec.
iter 343820 || Loss: 0.8180 || timer: 0.0908 sec.
iter 343830 || Loss: 0.7701 || timer: 0.1143 sec.
iter 343840 || Loss: 0.4779 || timer: 0.0838 sec.
iter 343850 || Loss: 0.7505 || timer: 0.0992 sec.
iter 343860 || Loss: 1.0888 || timer: 0.0934 sec.
iter 343870 || Loss: 0.7795 || timer: 0.0926 sec.
iter 343880 || Loss: 0.6320 || timer: 0.0954 sec.
iter 343890 || Loss: 0.5585 || timer: 0.0919 sec.
iter 343900 || Loss: 0.6613 || timer: 0.0814 sec.
iter 343910 || Loss: 1.1102 || timer: 0.1086 sec.
iter 343920 || Loss: 0.7452 || timer: 0.0890 sec.
iter 343930 || Loss: 0.7031 || timer: 0.0967 sec.
iter 343940 || Loss: 0.9142 || timer: 0.0893 sec.
iter 343950 || Loss: 0.7898 || timer: 0.0934 sec.
iter 343960 || Loss: 0.7007 || timer: 0.0885 sec.
iter 343970 || Loss: 0.6592 || timer: 0.0916 sec.
iter 343980 || Loss: 0.5883 || timer: 0.0867 sec.
iter 343990 || Loss: 0.8459 || timer: 0.0966 sec.
iter 344000 || Loss: 0.7494 || timer: 0.0858 sec.
iter 344010 || Loss: 0.6752 || timer: 0.0907 sec.
iter 344020 || Loss: 0.9585 || timer: 0.0854 sec.
iter 344030 || Loss: 0.6072 || timer: 0.0840 sec.
iter 344040 || Loss: 0.9718 || timer: 0.1233 sec.
iter 344050 || Loss: 0.6555 || timer: 0.0917 sec.
iter 344060 || Loss: 0.7037 || timer: 0.0909 sec.
iter 344070 || Loss: 0.9348 || timer: 0.0839 sec.
iter 344080 || Loss: 0.5040 || timer: 0.0259 sec.
iter 344090 || Loss: 1.5055 || timer: 0.0949 sec.
iter 344100 || Loss: 0.5888 || timer: 0.1124 sec.
iter 344110 || Loss: 0.6002 || timer: 0.0906 sec.
iter 344120 || Loss: 0.6755 || timer: 0.1033 sec.
iter 344130 || Loss: 0.5254 || timer: 0.0920 sec.
iter 344140 || Loss: 0.8980 || timer: 0.0909 sec.
iter 344150 || Loss: 0.5868 || timer: 0.0909 sec.
iter 344160 || Loss: 0.8369 || timer: 0.1138 sec.
iter 344170 || Loss: 0.7611 || timer: 0.0896 sec.
iter 344180 || Loss: 0.6524 || timer: 0.0972 sec.
iter 344190 || Loss: 0.8256 || timer: 0.0881 sec.
iter 344200 || Loss: 0.8220 || timer: 0.0846 sec.
iter 344210 || Loss: 0.4658 || timer: 0.0915 sec.
iter 344220 || Loss: 0.8451 || timer: 0.0919 sec.
iter 344230 || Loss: 0.4790 || timer: 0.0896 sec.
iter 344240 || Loss: 0.9075 || timer: 0.0829 sec.
iter 344250 || Loss: 0.7064 || timer: 0.0915 sec.
iter 344260 || Loss: 0.4586 || timer: 0.0906 sec.
iter 344270 || Loss: 0.5756 || timer: 0.0887 sec.
iter 344280 || Loss: 1.1953 || timer: 0.0962 sec.
iter 344290 || Loss: 0.5572 || timer: 0.0897 sec.
iter 344300 || Loss: 0.7221 || timer: 0.0892 sec.
iter 344310 || Loss: 0.5786 || timer: 0.0892 sec.
iter 344320 || Loss: 0.9146 || timer: 0.0943 sec.
iter 344330 || Loss: 0.7142 || timer: 0.1107 sec.
iter 344340 || Loss: 0.9320 || timer: 0.0900 sec.
iter 344350 || Loss: 0.9758 || timer: 0.1030 sec.
iter 344360 || Loss: 0.8011 || timer: 0.0830 sec.
iter 344370 || Loss: 0.6047 || timer: 0.0818 sec.
iter 344380 || Loss: 0.6557 || timer: 0.0914 sec.
iter 344390 || Loss: 0.6494 || timer: 0.0892 sec.
iter 344400 || Loss: 0.6088 || timer: 0.0938 sec.
iter 344410 || Loss: 0.8450 || timer: 0.0277 sec.
iter 344420 || Loss: 1.4018 || timer: 0.0883 sec.
iter 344430 || Loss: 0.5686 || timer: 0.1296 sec.
iter 344440 || Loss: 0.5713 || timer: 0.0850 sec.
iter 344450 || Loss: 0.6474 || timer: 0.0830 sec.
iter 344460 || Loss: 0.6439 || timer: 0.1014 sec.
iter 344470 || Loss: 0.4673 || timer: 0.0822 sec.
iter 344480 || Loss: 0.7247 || timer: 0.0836 sec.
iter 344490 || Loss: 0.5486 || timer: 0.0828 sec.
iter 344500 || Loss: 0.7474 || timer: 0.1019 sec.
iter 344510 || Loss: 0.5017 || timer: 0.1203 sec.
iter 344520 || Loss: 0.7118 || timer: 0.1013 sec.
iter 344530 || Loss: 0.6844 || timer: 0.0918 sec.
iter 344540 || Loss: 0.5186 || timer: 0.0846 sec.
iter 344550 || Loss: 0.4695 || timer: 0.0834 sec.
iter 344560 || Loss: 0.5521 || timer: 0.0836 sec.
iter 344570 || Loss: 0.7774 || timer: 0.1063 sec.
iter 344580 || Loss: 0.7500 || timer: 0.0906 sec.
iter 344590 || Loss: 0.6975 || timer: 0.0892 sec.
iter 344600 || Loss: 0.9512 || timer: 0.0867 sec.
iter 344610 || Loss: 0.6941 || timer: 0.0924 sec.
iter 344620 || Loss: 0.7134 || timer: 0.1182 sec.
iter 344630 || Loss: 0.5473 || timer: 0.0841 sec.
iter 344640 || Loss: 0.3866 || timer: 0.0885 sec.
iter 344650 || Loss: 0.6631 || timer: 0.0915 sec.
iter 344660 || Loss: 0.6630 || timer: 0.0920 sec.
iter 344670 || Loss: 0.8875 || timer: 0.0987 sec.
iter 344680 || Loss: 0.4864 || timer: 0.0817 sec.
iter 344690 || Loss: 0.8478 || timer: 0.0900 sec.
iter 344700 || Loss: 0.4633 || timer: 0.0840 sec.
iter 344710 || Loss: 0.5902 || timer: 0.0888 sec.
iter 344720 || Loss: 0.4879 || timer: 0.0839 sec.
iter 344730 || Loss: 0.7368 || timer: 0.0882 sec.
iter 344740 || Loss: 0.6708 || timer: 0.0276 sec.
iter 344750 || Loss: 1.5494 || timer: 0.0833 sec.
iter 344760 || Loss: 0.7043 || timer: 0.0908 sec.
iter 344770 || Loss: 0.5111 || timer: 0.0830 sec.
iter 344780 || Loss: 0.8614 || timer: 0.0928 sec.
iter 344790 || Loss: 0.6891 || timer: 0.0804 sec.
iter 344800 || Loss: 0.6520 || timer: 0.0904 sec.
iter 344810 || Loss: 0.6975 || timer: 0.0843 sec.
iter 344820 || Loss: 0.6413 || timer: 0.1234 sec.
iter 344830 || Loss: 0.8200 || timer: 0.0997 sec.
iter 344840 || Loss: 0.7747 || timer: 0.1027 sec.
iter 344850 || Loss: 0.6641 || timer: 0.0907 sec.
iter 344860 || Loss: 0.8824 || timer: 0.0826 sec.
iter 344870 || Loss: 0.6034 || timer: 0.0888 sec.
iter 344880 || Loss: 0.6958 || timer: 0.0841 sec.
iter 344890 || Loss: 0.6331 || timer: 0.0913 sec.
iter 344900 || Loss: 0.7683 || timer: 0.0910 sec.
iter 344910 || Loss: 0.6273 || timer: 0.0896 sec.
iter 344920 || Loss: 0.5300 || timer: 0.0923 sec.
iter 344930 || Loss: 0.5676 || timer: 0.0904 sec.
iter 344940 || Loss: 0.6679 || timer: 0.0843 sec.
iter 344950 || Loss: 0.6469 || timer: 0.0912 sec.
iter 344960 || Loss: 0.5871 || timer: 0.0910 sec.
iter 344970 || Loss: 0.7836 || timer: 0.0899 sec.
iter 344980 || Loss: 0.7051 || timer: 0.0909 sec.
iter 344990 || Loss: 0.6223 || timer: 0.1091 sec.
iter 345000 || Loss: 0.6625 || Saving state, iter: 345000
timer: 0.0912 sec.
iter 345010 || Loss: 1.0686 || timer: 0.0824 sec.
iter 345020 || Loss: 0.3817 || timer: 0.0913 sec.
iter 345030 || Loss: 0.8734 || timer: 0.0921 sec.
iter 345040 || Loss: 0.7511 || timer: 0.0918 sec.
iter 345050 || Loss: 0.6268 || timer: 0.0887 sec.
iter 345060 || Loss: 0.7002 || timer: 0.0915 sec.
iter 345070 || Loss: 0.9862 || timer: 0.0304 sec.
iter 345080 || Loss: 0.7056 || timer: 0.0852 sec.
iter 345090 || Loss: 1.0154 || timer: 0.0910 sec.
iter 345100 || Loss: 0.9416 || timer: 0.0905 sec.
iter 345110 || Loss: 0.6268 || timer: 0.0845 sec.
iter 345120 || Loss: 0.8287 || timer: 0.0915 sec.
iter 345130 || Loss: 0.5391 || timer: 0.0896 sec.
iter 345140 || Loss: 0.6893 || timer: 0.0844 sec.
iter 345150 || Loss: 0.7365 || timer: 0.0845 sec.
iter 345160 || Loss: 0.8711 || timer: 0.0901 sec.
iter 345170 || Loss: 0.5753 || timer: 0.0991 sec.
iter 345180 || Loss: 0.7323 || timer: 0.0834 sec.
iter 345190 || Loss: 0.7136 || timer: 0.0892 sec.
iter 345200 || Loss: 0.4280 || timer: 0.0831 sec.
iter 345210 || Loss: 0.7307 || timer: 0.0904 sec.
iter 345220 || Loss: 0.6669 || timer: 0.0910 sec.
iter 345230 || Loss: 0.6108 || timer: 0.0902 sec.
iter 345240 || Loss: 0.8164 || timer: 0.0825 sec.
iter 345250 || Loss: 0.7220 || timer: 0.0887 sec.
iter 345260 || Loss: 0.6614 || timer: 0.0927 sec.
iter 345270 || Loss: 0.6201 || timer: 0.0828 sec.
iter 345280 || Loss: 0.6410 || timer: 0.0913 sec.
iter 345290 || Loss: 0.6217 || timer: 0.0877 sec.
iter 345300 || Loss: 0.7200 || timer: 0.0883 sec.
iter 345310 || Loss: 0.4929 || timer: 0.0848 sec.
iter 345320 || Loss: 0.6833 || timer: 0.0875 sec.
iter 345330 || Loss: 0.8536 || timer: 0.0872 sec.
iter 345340 || Loss: 0.7501 || timer: 0.0924 sec.
iter 345350 || Loss: 0.6091 || timer: 0.0884 sec.
iter 345360 || Loss: 0.7797 || timer: 0.0845 sec.
iter 345370 || Loss: 0.7796 || timer: 0.0828 sec.
iter 345380 || Loss: 0.8217 || timer: 0.0836 sec.
iter 345390 || Loss: 0.8318 || timer: 0.0879 sec.
iter 345400 || Loss: 0.8469 || timer: 0.0196 sec.
iter 345410 || Loss: 0.6522 || timer: 0.0932 sec.
iter 345420 || Loss: 0.7835 || timer: 0.1009 sec.
iter 345430 || Loss: 1.0936 || timer: 0.0924 sec.
iter 345440 || Loss: 0.8468 || timer: 0.1151 sec.
iter 345450 || Loss: 0.5398 || timer: 0.0919 sec.
iter 345460 || Loss: 0.7528 || timer: 0.0920 sec.
iter 345470 || Loss: 0.5526 || timer: 0.0894 sec.
iter 345480 || Loss: 0.7669 || timer: 0.0827 sec.
iter 345490 || Loss: 0.6042 || timer: 0.0832 sec.
iter 345500 || Loss: 0.6252 || timer: 0.1269 sec.
iter 345510 || Loss: 0.3980 || timer: 0.1092 sec.
iter 345520 || Loss: 0.6299 || timer: 0.1003 sec.
iter 345530 || Loss: 0.7805 || timer: 0.0866 sec.
iter 345540 || Loss: 0.6833 || timer: 0.0887 sec.
iter 345550 || Loss: 0.4195 || timer: 0.0862 sec.
iter 345560 || Loss: 0.8311 || timer: 0.0820 sec.
iter 345570 || Loss: 0.6595 || timer: 0.0912 sec.
iter 345580 || Loss: 0.6508 || timer: 0.0830 sec.
iter 345590 || Loss: 0.6525 || timer: 0.0908 sec.
iter 345600 || Loss: 0.7178 || timer: 0.0885 sec.
iter 345610 || Loss: 0.5792 || timer: 0.0835 sec.
iter 345620 || Loss: 0.5380 || timer: 0.0757 sec.
iter 345630 || Loss: 0.9755 || timer: 0.0878 sec.
iter 345640 || Loss: 0.7550 || timer: 0.0930 sec.
iter 345650 || Loss: 0.5169 || timer: 0.0849 sec.
iter 345660 || Loss: 0.9060 || timer: 0.1148 sec.
iter 345670 || Loss: 0.9838 || timer: 0.0898 sec.
iter 345680 || Loss: 0.6536 || timer: 0.1336 sec.
iter 345690 || Loss: 0.5743 || timer: 0.0911 sec.
iter 345700 || Loss: 0.7774 || timer: 0.0969 sec.
iter 345710 || Loss: 0.7148 || timer: 0.0919 sec.
iter 345720 || Loss: 0.5717 || timer: 0.0905 sec.
iter 345730 || Loss: 0.9619 || timer: 0.0304 sec.
iter 345740 || Loss: 0.5479 || timer: 0.1052 sec.
iter 345750 || Loss: 0.6572 || timer: 0.0832 sec.
iter 345760 || Loss: 0.7143 || timer: 0.0775 sec.
iter 345770 || Loss: 0.8559 || timer: 0.0818 sec.
iter 345780 || Loss: 0.4582 || timer: 0.0832 sec.
iter 345790 || Loss: 0.4887 || timer: 0.0904 sec.
iter 345800 || Loss: 0.6539 || timer: 0.0868 sec.
iter 345810 || Loss: 0.8948 || timer: 0.0905 sec.
iter 345820 || Loss: 0.6473 || timer: 0.0842 sec.
iter 345830 || Loss: 0.7546 || timer: 0.0958 sec.
iter 345840 || Loss: 0.8313 || timer: 0.1056 sec.
iter 345850 || Loss: 0.7415 || timer: 0.0825 sec.
iter 345860 || Loss: 0.5672 || timer: 0.0832 sec.
iter 345870 || Loss: 0.9332 || timer: 0.0833 sec.
iter 345880 || Loss: 0.6922 || timer: 0.0925 sec.
iter 345890 || Loss: 0.8049 || timer: 0.0920 sec.
iter 345900 || Loss: 0.5416 || timer: 0.1186 sec.
iter 345910 || Loss: 1.1856 || timer: 0.0892 sec.
iter 345920 || Loss: 0.5432 || timer: 0.0840 sec.
iter 345930 || Loss: 0.8414 || timer: 0.0889 sec.
iter 345940 || Loss: 0.6619 || timer: 0.0903 sec.
iter 345950 || Loss: 0.4812 || timer: 0.0940 sec.
iter 345960 || Loss: 0.4585 || timer: 0.0909 sec.
iter 345970 || Loss: 0.5362 || timer: 0.0958 sec.
iter 345980 || Loss: 0.6289 || timer: 0.0923 sec.
iter 345990 || Loss: 0.6175 || timer: 0.0909 sec.
iter 346000 || Loss: 0.6331 || timer: 0.0909 sec.
iter 346010 || Loss: 0.8009 || timer: 0.0845 sec.
iter 346020 || Loss: 0.7642 || timer: 0.0847 sec.
iter 346030 || Loss: 0.7011 || timer: 0.0868 sec.
iter 346040 || Loss: 0.9124 || timer: 0.0950 sec.
iter 346050 || Loss: 0.5531 || timer: 0.0906 sec.
iter 346060 || Loss: 0.7430 || timer: 0.0208 sec.
iter 346070 || Loss: 1.2118 || timer: 0.1057 sec.
iter 346080 || Loss: 0.8688 || timer: 0.0925 sec.
iter 346090 || Loss: 0.8302 || timer: 0.0902 sec.
iter 346100 || Loss: 0.8578 || timer: 0.0913 sec.
iter 346110 || Loss: 0.6067 || timer: 0.0842 sec.
iter 346120 || Loss: 0.7883 || timer: 0.1137 sec.
iter 346130 || Loss: 0.7700 || timer: 0.1008 sec.
iter 346140 || Loss: 0.9838 || timer: 0.0907 sec.
iter 346150 || Loss: 0.6543 || timer: 0.1031 sec.
iter 346160 || Loss: 0.7150 || timer: 0.1149 sec.
iter 346170 || Loss: 0.7092 || timer: 0.1062 sec.
iter 346180 || Loss: 0.7780 || timer: 0.0898 sec.
iter 346190 || Loss: 0.6351 || timer: 0.0895 sec.
iter 346200 || Loss: 0.5999 || timer: 0.0905 sec.
iter 346210 || Loss: 0.9029 || timer: 0.0918 sec.
iter 346220 || Loss: 0.7866 || timer: 0.0837 sec.
iter 346230 || Loss: 0.6397 || timer: 0.0917 sec.
iter 346240 || Loss: 0.5539 || timer: 0.0913 sec.
iter 346250 || Loss: 0.7025 || timer: 0.0835 sec.
iter 346260 || Loss: 0.6615 || timer: 0.0902 sec.
iter 346270 || Loss: 0.9349 || timer: 0.1043 sec.
iter 346280 || Loss: 0.5764 || timer: 0.0882 sec.
iter 346290 || Loss: 0.5935 || timer: 0.0801 sec.
iter 346300 || Loss: 0.6906 || timer: 0.1266 sec.
iter 346310 || Loss: 0.8169 || timer: 0.0825 sec.
iter 346320 || Loss: 0.6315 || timer: 0.0921 sec.
iter 346330 || Loss: 0.6374 || timer: 0.0909 sec.
iter 346340 || Loss: 0.7998 || timer: 0.1128 sec.
iter 346350 || Loss: 0.7678 || timer: 0.0909 sec.
iter 346360 || Loss: 0.7122 || timer: 0.0920 sec.
iter 346370 || Loss: 0.6797 || timer: 0.0920 sec.
iter 346380 || Loss: 0.6924 || timer: 0.0840 sec.
iter 346390 || Loss: 0.5592 || timer: 0.0267 sec.
iter 346400 || Loss: 0.2722 || timer: 0.0900 sec.
iter 346410 || Loss: 0.7500 || timer: 0.0926 sec.
iter 346420 || Loss: 0.7108 || timer: 0.0922 sec.
iter 346430 || Loss: 0.6451 || timer: 0.0867 sec.
iter 346440 || Loss: 0.7066 || timer: 0.1128 sec.
iter 346450 || Loss: 0.7195 || timer: 0.0875 sec.
iter 346460 || Loss: 0.5360 || timer: 0.0914 sec.
iter 346470 || Loss: 0.9328 || timer: 0.0903 sec.
iter 346480 || Loss: 0.7505 || timer: 0.0840 sec.
iter 346490 || Loss: 0.5971 || timer: 0.1162 sec.
iter 346500 || Loss: 0.5862 || timer: 0.0888 sec.
iter 346510 || Loss: 0.6727 || timer: 0.0852 sec.
iter 346520 || Loss: 0.4360 || timer: 0.0907 sec.
iter 346530 || Loss: 0.6856 || timer: 0.0923 sec.
iter 346540 || Loss: 0.6491 || timer: 0.0860 sec.
iter 346550 || Loss: 0.8352 || timer: 0.1064 sec.
iter 346560 || Loss: 0.7818 || timer: 0.0909 sec.
iter 346570 || Loss: 0.8221 || timer: 0.0903 sec.
iter 346580 || Loss: 0.5656 || timer: 0.0904 sec.
iter 346590 || Loss: 0.5022 || timer: 0.0848 sec.
iter 346600 || Loss: 0.8517 || timer: 0.0839 sec.
iter 346610 || Loss: 0.8463 || timer: 0.1041 sec.
iter 346620 || Loss: 0.6463 || timer: 0.0925 sec.
iter 346630 || Loss: 0.5716 || timer: 0.0830 sec.
iter 346640 || Loss: 0.8086 || timer: 0.0826 sec.
iter 346650 || Loss: 0.5865 || timer: 0.0839 sec.
iter 346660 || Loss: 0.7246 || timer: 0.0830 sec.
iter 346670 || Loss: 0.8755 || timer: 0.1049 sec.
iter 346680 || Loss: 0.6689 || timer: 0.0926 sec.
iter 346690 || Loss: 0.7327 || timer: 0.0880 sec.
iter 346700 || Loss: 0.7942 || timer: 0.0915 sec.
iter 346710 || Loss: 0.8300 || timer: 0.1085 sec.
iter 346720 || Loss: 0.5895 || timer: 0.0220 sec.
iter 346730 || Loss: 0.2476 || timer: 0.0869 sec.
iter 346740 || Loss: 0.8581 || timer: 0.1103 sec.
iter 346750 || Loss: 0.6690 || timer: 0.0832 sec.
iter 346760 || Loss: 1.0127 || timer: 0.0923 sec.
iter 346770 || Loss: 0.7287 || timer: 0.1047 sec.
iter 346780 || Loss: 0.5257 || timer: 0.0893 sec.
iter 346790 || Loss: 0.6679 || timer: 0.0895 sec.
iter 346800 || Loss: 0.5947 || timer: 0.0985 sec.
iter 346810 || Loss: 0.6620 || timer: 0.0903 sec.
iter 346820 || Loss: 0.7455 || timer: 0.0893 sec.
iter 346830 || Loss: 0.8618 || timer: 0.0923 sec.
iter 346840 || Loss: 0.7341 || timer: 0.1181 sec.
iter 346850 || Loss: 1.0924 || timer: 0.0904 sec.
iter 346860 || Loss: 0.6556 || timer: 0.0880 sec.
iter 346870 || Loss: 0.8073 || timer: 0.0905 sec.
iter 346880 || Loss: 0.7135 || timer: 0.0894 sec.
iter 346890 || Loss: 0.6647 || timer: 0.1033 sec.
iter 346900 || Loss: 0.6079 || timer: 0.0934 sec.
iter 346910 || Loss: 0.5155 || timer: 0.0896 sec.
iter 346920 || Loss: 0.7289 || timer: 0.0910 sec.
iter 346930 || Loss: 0.7462 || timer: 0.0958 sec.
iter 346940 || Loss: 0.7127 || timer: 0.0904 sec.
iter 346950 || Loss: 0.7118 || timer: 0.0841 sec.
iter 346960 || Loss: 0.7617 || timer: 0.0856 sec.
iter 346970 || Loss: 0.7234 || timer: 0.0844 sec.
iter 346980 || Loss: 0.5346 || timer: 0.0940 sec.
iter 346990 || Loss: 0.4280 || timer: 0.0868 sec.
iter 347000 || Loss: 0.8561 || timer: 0.1146 sec.
iter 347010 || Loss: 0.6811 || timer: 0.1076 sec.
iter 347020 || Loss: 0.8186 || timer: 0.0837 sec.
iter 347030 || Loss: 0.7084 || timer: 0.0923 sec.
iter 347040 || Loss: 0.6189 || timer: 0.0824 sec.
iter 347050 || Loss: 0.8865 || timer: 0.0251 sec.
iter 347060 || Loss: 1.5212 || timer: 0.1325 sec.
iter 347070 || Loss: 0.6675 || timer: 0.0880 sec.
iter 347080 || Loss: 0.7787 || timer: 0.0807 sec.
iter 347090 || Loss: 0.6645 || timer: 0.0906 sec.
iter 347100 || Loss: 0.7101 || timer: 0.1038 sec.
iter 347110 || Loss: 0.6374 || timer: 0.0911 sec.
iter 347120 || Loss: 0.7382 || timer: 0.0932 sec.
iter 347130 || Loss: 1.0562 || timer: 0.1019 sec.
iter 347140 || Loss: 0.7282 || timer: 0.0765 sec.
iter 347150 || Loss: 0.6142 || timer: 0.1007 sec.
iter 347160 || Loss: 0.7708 || timer: 0.0822 sec.
iter 347170 || Loss: 0.5371 || timer: 0.0851 sec.
iter 347180 || Loss: 0.8184 || timer: 0.0920 sec.
iter 347190 || Loss: 0.7314 || timer: 0.0825 sec.
iter 347200 || Loss: 0.5864 || timer: 0.0904 sec.
iter 347210 || Loss: 0.7438 || timer: 0.0924 sec.
iter 347220 || Loss: 0.7649 || timer: 0.0819 sec.
iter 347230 || Loss: 0.5602 || timer: 0.0907 sec.
iter 347240 || Loss: 0.7516 || timer: 0.1103 sec.
iter 347250 || Loss: 0.6224 || timer: 0.1101 sec.
iter 347260 || Loss: 0.7050 || timer: 0.0829 sec.
iter 347270 || Loss: 0.4449 || timer: 0.0909 sec.
iter 347280 || Loss: 0.5871 || timer: 0.0931 sec.
iter 347290 || Loss: 0.6927 || timer: 0.1001 sec.
iter 347300 || Loss: 0.4345 || timer: 0.0895 sec.
iter 347310 || Loss: 1.0294 || timer: 0.1065 sec.
iter 347320 || Loss: 0.9121 || timer: 0.1081 sec.
iter 347330 || Loss: 0.6478 || timer: 0.0889 sec.
iter 347340 || Loss: 0.5956 || timer: 0.0819 sec.
iter 347350 || Loss: 0.6912 || timer: 0.0842 sec.
iter 347360 || Loss: 0.6753 || timer: 0.0902 sec.
iter 347370 || Loss: 0.5273 || timer: 0.1087 sec.
iter 347380 || Loss: 0.7853 || timer: 0.0275 sec.
iter 347390 || Loss: 0.7713 || timer: 0.0914 sec.
iter 347400 || Loss: 0.5754 || timer: 0.0837 sec.
iter 347410 || Loss: 0.6763 || timer: 0.0846 sec.
iter 347420 || Loss: 0.7105 || timer: 0.1115 sec.
iter 347430 || Loss: 0.7404 || timer: 0.1227 sec.
iter 347440 || Loss: 0.6778 || timer: 0.1097 sec.
iter 347450 || Loss: 0.7236 || timer: 0.0885 sec.
iter 347460 || Loss: 0.6933 || timer: 0.0969 sec.
iter 347470 || Loss: 0.8885 || timer: 0.0944 sec.
iter 347480 || Loss: 0.7674 || timer: 0.1143 sec.
iter 347490 || Loss: 0.6870 || timer: 0.1111 sec.
iter 347500 || Loss: 0.7772 || timer: 0.0918 sec.
iter 347510 || Loss: 0.6648 || timer: 0.0821 sec.
iter 347520 || Loss: 0.9111 || timer: 0.0913 sec.
iter 347530 || Loss: 0.5766 || timer: 0.0929 sec.
iter 347540 || Loss: 0.7724 || timer: 0.1052 sec.
iter 347550 || Loss: 0.7374 || timer: 0.1111 sec.
iter 347560 || Loss: 0.4357 || timer: 0.0828 sec.
iter 347570 || Loss: 0.6433 || timer: 0.0881 sec.
iter 347580 || Loss: 0.7894 || timer: 0.0931 sec.
iter 347590 || Loss: 0.8718 || timer: 0.0827 sec.
iter 347600 || Loss: 0.7616 || timer: 0.0835 sec.
iter 347610 || Loss: 0.7183 || timer: 0.0914 sec.
iter 347620 || Loss: 0.5604 || timer: 0.1027 sec.
iter 347630 || Loss: 0.6367 || timer: 0.0910 sec.
iter 347640 || Loss: 0.6869 || timer: 0.0950 sec.
iter 347650 || Loss: 0.9630 || timer: 0.0957 sec.
iter 347660 || Loss: 0.5118 || timer: 0.0838 sec.
iter 347670 || Loss: 0.8477 || timer: 0.0893 sec.
iter 347680 || Loss: 0.8177 || timer: 0.0901 sec.
iter 347690 || Loss: 0.4890 || timer: 0.0887 sec.
iter 347700 || Loss: 0.7193 || timer: 0.0898 sec.
iter 347710 || Loss: 0.6534 || timer: 0.0263 sec.
iter 347720 || Loss: 0.5555 || timer: 0.0844 sec.
iter 347730 || Loss: 0.8469 || timer: 0.0903 sec.
iter 347740 || Loss: 1.0101 || timer: 0.0842 sec.
iter 347750 || Loss: 0.7674 || timer: 0.0891 sec.
iter 347760 || Loss: 0.6500 || timer: 0.0821 sec.
iter 347770 || Loss: 0.5144 || timer: 0.0946 sec.
iter 347780 || Loss: 0.5975 || timer: 0.0916 sec.
iter 347790 || Loss: 0.6059 || timer: 0.0852 sec.
iter 347800 || Loss: 0.9122 || timer: 0.0940 sec.
iter 347810 || Loss: 0.5523 || timer: 0.1003 sec.
iter 347820 || Loss: 0.6620 || timer: 0.0871 sec.
iter 347830 || Loss: 0.7871 || timer: 0.0904 sec.
iter 347840 || Loss: 0.8242 || timer: 0.0828 sec.
iter 347850 || Loss: 0.8078 || timer: 0.0893 sec.
iter 347860 || Loss: 0.9829 || timer: 0.0863 sec.
iter 347870 || Loss: 0.5386 || timer: 0.0846 sec.
iter 347880 || Loss: 0.4139 || timer: 0.0858 sec.
iter 347890 || Loss: 1.0388 || timer: 0.1047 sec.
iter 347900 || Loss: 0.6426 || timer: 0.1044 sec.
iter 347910 || Loss: 0.6145 || timer: 0.0902 sec.
iter 347920 || Loss: 0.6483 || timer: 0.0918 sec.
iter 347930 || Loss: 0.7193 || timer: 0.1129 sec.
iter 347940 || Loss: 0.6876 || timer: 0.0921 sec.
iter 347950 || Loss: 1.1413 || timer: 0.0933 sec.
iter 347960 || Loss: 0.5804 || timer: 0.0867 sec.
iter 347970 || Loss: 0.4867 || timer: 0.0874 sec.
iter 347980 || Loss: 0.5857 || timer: 0.0839 sec.
iter 347990 || Loss: 0.7741 || timer: 0.0896 sec.
iter 348000 || Loss: 1.0228 || timer: 0.1007 sec.
iter 348010 || Loss: 0.8234 || timer: 0.0840 sec.
iter 348020 || Loss: 0.8875 || timer: 0.0926 sec.
iter 348030 || Loss: 0.6007 || timer: 0.0893 sec.
iter 348040 || Loss: 0.6738 || timer: 0.0201 sec.
iter 348050 || Loss: 0.0737 || timer: 0.0826 sec.
iter 348060 || Loss: 0.6450 || timer: 0.0833 sec.
iter 348070 || Loss: 0.7770 || timer: 0.0830 sec.
iter 348080 || Loss: 0.7712 || timer: 0.1024 sec.
iter 348090 || Loss: 0.7551 || timer: 0.0925 sec.
iter 348100 || Loss: 0.6279 || timer: 0.0851 sec.
iter 348110 || Loss: 0.6061 || timer: 0.0842 sec.
iter 348120 || Loss: 0.6801 || timer: 0.0918 sec.
iter 348130 || Loss: 0.9953 || timer: 0.0899 sec.
iter 348140 || Loss: 0.6834 || timer: 0.0982 sec.
iter 348150 || Loss: 1.2006 || timer: 0.0827 sec.
iter 348160 || Loss: 0.7290 || timer: 0.0898 sec.
iter 348170 || Loss: 0.3807 || timer: 0.0925 sec.
iter 348180 || Loss: 0.7309 || timer: 0.0897 sec.
iter 348190 || Loss: 1.0244 || timer: 0.0815 sec.
iter 348200 || Loss: 0.5972 || timer: 0.0914 sec.
iter 348210 || Loss: 0.6425 || timer: 0.0894 sec.
iter 348220 || Loss: 0.7445 || timer: 0.0965 sec.
iter 348230 || Loss: 0.8400 || timer: 0.0891 sec.
iter 348240 || Loss: 0.5473 || timer: 0.0829 sec.
iter 348250 || Loss: 0.8124 || timer: 0.0831 sec.
iter 348260 || Loss: 0.8966 || timer: 0.0906 sec.
iter 348270 || Loss: 0.8945 || timer: 0.0849 sec.
iter 348280 || Loss: 1.0169 || timer: 0.0900 sec.
iter 348290 || Loss: 0.7618 || timer: 0.1030 sec.
iter 348300 || Loss: 0.6139 || timer: 0.1125 sec.
iter 348310 || Loss: 0.6381 || timer: 0.0838 sec.
iter 348320 || Loss: 0.9081 || timer: 0.0989 sec.
iter 348330 || Loss: 0.9682 || timer: 0.0927 sec.
iter 348340 || Loss: 0.6459 || timer: 0.0891 sec.
iter 348350 || Loss: 0.7099 || timer: 0.1063 sec.
iter 348360 || Loss: 0.6682 || timer: 0.1093 sec.
iter 348370 || Loss: 0.5919 || timer: 0.0183 sec.
iter 348380 || Loss: 0.4880 || timer: 0.0826 sec.
iter 348390 || Loss: 1.0431 || timer: 0.0852 sec.
iter 348400 || Loss: 1.1169 || timer: 0.0902 sec.
iter 348410 || Loss: 0.5637 || timer: 0.0965 sec.
iter 348420 || Loss: 0.8702 || timer: 0.1239 sec.
iter 348430 || Loss: 0.6611 || timer: 0.0849 sec.
iter 348440 || Loss: 0.7700 || timer: 0.1162 sec.
iter 348450 || Loss: 0.7867 || timer: 0.0898 sec.
iter 348460 || Loss: 0.6683 || timer: 0.1033 sec.
iter 348470 || Loss: 0.8279 || timer: 0.0981 sec.
iter 348480 || Loss: 0.5091 || timer: 0.1026 sec.
iter 348490 || Loss: 0.5391 || timer: 0.0898 sec.
iter 348500 || Loss: 0.7879 || timer: 0.0901 sec.
iter 348510 || Loss: 0.5340 || timer: 0.1184 sec.
iter 348520 || Loss: 0.5037 || timer: 0.0902 sec.
iter 348530 || Loss: 0.7635 || timer: 0.0824 sec.
iter 348540 || Loss: 0.7186 || timer: 0.0913 sec.
iter 348550 || Loss: 0.6918 || timer: 0.0919 sec.
iter 348560 || Loss: 0.7833 || timer: 0.0904 sec.
iter 348570 || Loss: 1.4017 || timer: 0.0922 sec.
iter 348580 || Loss: 0.5216 || timer: 0.0923 sec.
iter 348590 || Loss: 0.7570 || timer: 0.1045 sec.
iter 348600 || Loss: 0.6489 || timer: 0.0839 sec.
iter 348610 || Loss: 0.7678 || timer: 0.0898 sec.
iter 348620 || Loss: 0.4674 || timer: 0.0905 sec.
iter 348630 || Loss: 0.6168 || timer: 0.0928 sec.
iter 348640 || Loss: 0.7869 || timer: 0.0839 sec.
iter 348650 || Loss: 0.6062 || timer: 0.0869 sec.
iter 348660 || Loss: 0.7378 || timer: 0.0906 sec.
iter 348670 || Loss: 0.6649 || timer: 0.0890 sec.
iter 348680 || Loss: 0.7870 || timer: 0.0895 sec.
iter 348690 || Loss: 0.6561 || timer: 0.0883 sec.
iter 348700 || Loss: 0.7590 || timer: 0.0254 sec.
iter 348710 || Loss: 0.1132 || timer: 0.0904 sec.
iter 348720 || Loss: 0.5743 || timer: 0.0897 sec.
iter 348730 || Loss: 0.8029 || timer: 0.0821 sec.
iter 348740 || Loss: 0.5195 || timer: 0.1043 sec.
iter 348750 || Loss: 0.7361 || timer: 0.1133 sec.
iter 348760 || Loss: 0.6438 || timer: 0.1019 sec.
iter 348770 || Loss: 0.6628 || timer: 0.0896 sec.
iter 348780 || Loss: 0.4691 || timer: 0.1177 sec.
iter 348790 || Loss: 0.5515 || timer: 0.0906 sec.
iter 348800 || Loss: 0.8329 || timer: 0.1271 sec.
iter 348810 || Loss: 0.6501 || timer: 0.0828 sec.
iter 348820 || Loss: 0.6300 || timer: 0.0936 sec.
iter 348830 || Loss: 0.6639 || timer: 0.0909 sec.
iter 348840 || Loss: 0.8029 || timer: 0.0941 sec.
iter 348850 || Loss: 0.7107 || timer: 0.0905 sec.
iter 348860 || Loss: 0.6536 || timer: 0.1161 sec.
iter 348870 || Loss: 0.9705 || timer: 0.0884 sec.
iter 348880 || Loss: 0.4883 || timer: 0.0824 sec.
iter 348890 || Loss: 0.4643 || timer: 0.0840 sec.
iter 348900 || Loss: 0.8255 || timer: 0.0838 sec.
iter 348910 || Loss: 0.5740 || timer: 0.0825 sec.
iter 348920 || Loss: 0.8091 || timer: 0.0908 sec.
iter 348930 || Loss: 0.9528 || timer: 0.0931 sec.
iter 348940 || Loss: 0.8445 || timer: 0.0970 sec.
iter 348950 || Loss: 0.7337 || timer: 0.0896 sec.
iter 348960 || Loss: 0.6901 || timer: 0.0907 sec.
iter 348970 || Loss: 0.8212 || timer: 0.0838 sec.
iter 348980 || Loss: 0.5136 || timer: 0.0892 sec.
iter 348990 || Loss: 0.7907 || timer: 0.0824 sec.
iter 349000 || Loss: 0.7712 || timer: 0.0994 sec.
iter 349010 || Loss: 0.6229 || timer: 0.1016 sec.
iter 349020 || Loss: 0.9239 || timer: 0.0911 sec.
iter 349030 || Loss: 0.8218 || timer: 0.0236 sec.
iter 349040 || Loss: 0.8384 || timer: 0.0900 sec.
iter 349050 || Loss: 0.4963 || timer: 0.0924 sec.
iter 349060 || Loss: 0.6138 || timer: 0.0837 sec.
iter 349070 || Loss: 0.5516 || timer: 0.0825 sec.
iter 349080 || Loss: 0.7186 || timer: 0.0898 sec.
iter 349090 || Loss: 0.6704 || timer: 0.0834 sec.
iter 349100 || Loss: 0.7725 || timer: 0.0937 sec.
iter 349110 || Loss: 0.7755 || timer: 0.1036 sec.
iter 349120 || Loss: 0.6660 || timer: 0.1096 sec.
iter 349130 || Loss: 0.6983 || timer: 0.1078 sec.
iter 349140 || Loss: 0.6077 || timer: 0.0927 sec.
iter 349150 || Loss: 0.6184 || timer: 0.0912 sec.
iter 349160 || Loss: 0.6180 || timer: 0.0834 sec.
iter 349170 || Loss: 0.8028 || timer: 0.1131 sec.
iter 349180 || Loss: 0.6096 || timer: 0.1176 sec.
iter 349190 || Loss: 0.6516 || timer: 0.0901 sec.
iter 349200 || Loss: 0.7989 || timer: 0.0902 sec.
iter 349210 || Loss: 0.7549 || timer: 0.0895 sec.
iter 349220 || Loss: 0.6774 || timer: 0.0817 sec.
iter 349230 || Loss: 0.7875 || timer: 0.0831 sec.
iter 349240 || Loss: 0.9535 || timer: 0.0905 sec.
iter 349250 || Loss: 0.7078 || timer: 0.0896 sec.
iter 349260 || Loss: 0.6855 || timer: 0.0842 sec.
iter 349270 || Loss: 0.9126 || timer: 0.0828 sec.
iter 349280 || Loss: 0.5165 || timer: 0.0910 sec.
iter 349290 || Loss: 0.7545 || timer: 0.0841 sec.
iter 349300 || Loss: 0.4996 || timer: 0.0818 sec.
iter 349310 || Loss: 1.0276 || timer: 0.0997 sec.
iter 349320 || Loss: 0.8912 || timer: 0.0896 sec.
iter 349330 || Loss: 0.3718 || timer: 0.0856 sec.
iter 349340 || Loss: 0.7885 || timer: 0.0934 sec.
iter 349350 || Loss: 0.8051 || timer: 0.0905 sec.
iter 349360 || Loss: 0.7636 || timer: 0.0230 sec.
iter 349370 || Loss: 0.9424 || timer: 0.0823 sec.
iter 349380 || Loss: 0.4158 || timer: 0.0911 sec.
iter 349390 || Loss: 0.8841 || timer: 0.0912 sec.
iter 349400 || Loss: 0.5974 || timer: 0.0907 sec.
iter 349410 || Loss: 0.6706 || timer: 0.0895 sec.
iter 349420 || Loss: 0.6248 || timer: 0.0940 sec.
iter 349430 || Loss: 0.6519 || timer: 0.0834 sec.
iter 349440 || Loss: 0.9194 || timer: 0.0924 sec.
iter 349450 || Loss: 0.5500 || timer: 0.1062 sec.
iter 349460 || Loss: 0.7709 || timer: 0.1239 sec.
iter 349470 || Loss: 0.6381 || timer: 0.0825 sec.
iter 349480 || Loss: 0.6308 || timer: 0.0881 sec.
iter 349490 || Loss: 0.7232 || timer: 0.0843 sec.
iter 349500 || Loss: 0.7464 || timer: 0.0885 sec.
iter 349510 || Loss: 0.7626 || timer: 0.0889 sec.
iter 349520 || Loss: 0.6054 || timer: 0.1267 sec.
iter 349530 || Loss: 0.6203 || timer: 0.0921 sec.
iter 349540 || Loss: 1.0587 || timer: 0.0896 sec.
iter 349550 || Loss: 0.6091 || timer: 0.0909 sec.
iter 349560 || Loss: 0.5695 || timer: 0.0949 sec.
iter 349570 || Loss: 0.5867 || timer: 0.0893 sec.
iter 349580 || Loss: 0.7698 || timer: 0.0910 sec.
iter 349590 || Loss: 0.7268 || timer: 0.0895 sec.
iter 349600 || Loss: 0.4660 || timer: 0.0894 sec.
iter 349610 || Loss: 0.5827 || timer: 0.0831 sec.
iter 349620 || Loss: 0.7541 || timer: 0.0909 sec.
iter 349630 || Loss: 0.7927 || timer: 0.0932 sec.
iter 349640 || Loss: 0.6706 || timer: 0.0884 sec.
iter 349650 || Loss: 0.5146 || timer: 0.0893 sec.
iter 349660 || Loss: 0.6291 || timer: 0.0826 sec.
iter 349670 || Loss: 0.5440 || timer: 0.0951 sec.
iter 349680 || Loss: 0.6660 || timer: 0.0871 sec.
iter 349690 || Loss: 0.6920 || timer: 0.0216 sec.
iter 349700 || Loss: 0.4030 || timer: 0.0897 sec.
iter 349710 || Loss: 0.5409 || timer: 0.1256 sec.
iter 349720 || Loss: 0.7836 || timer: 0.0886 sec.
iter 349730 || Loss: 0.6800 || timer: 0.0879 sec.
iter 349740 || Loss: 0.6785 || timer: 0.0829 sec.
iter 349750 || Loss: 0.8729 || timer: 0.0896 sec.
iter 349760 || Loss: 0.5233 || timer: 0.1050 sec.
iter 349770 || Loss: 0.8259 || timer: 0.0976 sec.
iter 349780 || Loss: 0.6035 || timer: 0.0903 sec.
iter 349790 || Loss: 0.5628 || timer: 0.0996 sec.
iter 349800 || Loss: 0.8428 || timer: 0.0922 sec.
iter 349810 || Loss: 0.6587 || timer: 0.0820 sec.
iter 349820 || Loss: 0.6862 || timer: 0.0820 sec.
iter 349830 || Loss: 0.4938 || timer: 0.0839 sec.
iter 349840 || Loss: 0.7513 || timer: 0.0904 sec.
iter 349850 || Loss: 0.6376 || timer: 0.0863 sec.
iter 349860 || Loss: 0.8005 || timer: 0.0908 sec.
iter 349870 || Loss: 0.8550 || timer: 0.1044 sec.
iter 349880 || Loss: 0.4868 || timer: 0.0892 sec.
iter 349890 || Loss: 0.8079 || timer: 0.0898 sec.
iter 349900 || Loss: 0.7927 || timer: 0.0953 sec.
iter 349910 || Loss: 0.6343 || timer: 0.0825 sec.
iter 349920 || Loss: 0.7055 || timer: 0.0870 sec.
iter 349930 || Loss: 0.7352 || timer: 0.1136 sec.
iter 349940 || Loss: 0.5616 || timer: 0.0916 sec.
iter 349950 || Loss: 0.5691 || timer: 0.0952 sec.
iter 349960 || Loss: 0.7225 || timer: 0.0838 sec.
iter 349970 || Loss: 0.8826 || timer: 0.0883 sec.
iter 349980 || Loss: 0.7534 || timer: 0.0918 sec.
iter 349990 || Loss: 0.6545 || timer: 0.0907 sec.
iter 350000 || Loss: 0.9800 || Saving state, iter: 350000
timer: 0.0831 sec.
iter 350010 || Loss: 0.6028 || timer: 0.0933 sec.
iter 350020 || Loss: 0.4704 || timer: 0.0304 sec.
iter 350030 || Loss: 0.1410 || timer: 0.0948 sec.
iter 350040 || Loss: 0.6642 || timer: 0.1087 sec.
iter 350050 || Loss: 0.5614 || timer: 0.0831 sec.
iter 350060 || Loss: 0.7153 || timer: 0.1222 sec.
iter 350070 || Loss: 0.4361 || timer: 0.0896 sec.
iter 350080 || Loss: 0.7146 || timer: 0.0898 sec.
iter 350090 || Loss: 0.8650 || timer: 0.0940 sec.
iter 350100 || Loss: 0.6604 || timer: 0.0915 sec.
iter 350110 || Loss: 0.5973 || timer: 0.1050 sec.
iter 350120 || Loss: 0.9145 || timer: 0.1105 sec.
iter 350130 || Loss: 0.5312 || timer: 0.0910 sec.
iter 350140 || Loss: 0.4742 || timer: 0.1033 sec.
iter 350150 || Loss: 0.7686 || timer: 0.0842 sec.
iter 350160 || Loss: 0.8116 || timer: 0.0905 sec.
iter 350170 || Loss: 0.7253 || timer: 0.0999 sec.
iter 350180 || Loss: 0.7896 || timer: 0.0906 sec.
iter 350190 || Loss: 0.6764 || timer: 0.0896 sec.
iter 350200 || Loss: 0.5692 || timer: 0.0923 sec.
iter 350210 || Loss: 1.0290 || timer: 0.0845 sec.
iter 350220 || Loss: 0.6970 || timer: 0.0919 sec.
iter 350230 || Loss: 0.6910 || timer: 0.0909 sec.
iter 350240 || Loss: 0.8670 || timer: 0.0876 sec.
iter 350250 || Loss: 0.6287 || timer: 0.0834 sec.
iter 350260 || Loss: 0.8630 || timer: 0.1100 sec.
iter 350270 || Loss: 0.7623 || timer: 0.0926 sec.
iter 350280 || Loss: 0.6234 || timer: 0.0899 sec.
iter 350290 || Loss: 0.7709 || timer: 0.0836 sec.
iter 350300 || Loss: 0.7776 || timer: 0.1040 sec.
iter 350310 || Loss: 0.7868 || timer: 0.1182 sec.
iter 350320 || Loss: 0.6510 || timer: 0.0928 sec.
iter 350330 || Loss: 0.7416 || timer: 0.0904 sec.
iter 350340 || Loss: 0.5820 || timer: 0.0894 sec.
iter 350350 || Loss: 0.6839 || timer: 0.0233 sec.
iter 350360 || Loss: 0.8004 || timer: 0.0888 sec.
iter 350370 || Loss: 0.8369 || timer: 0.0927 sec.
iter 350380 || Loss: 0.9383 || timer: 0.0916 sec.
iter 350390 || Loss: 1.0195 || timer: 0.0900 sec.
iter 350400 || Loss: 0.5906 || timer: 0.1044 sec.
iter 350410 || Loss: 0.7861 || timer: 0.1016 sec.
iter 350420 || Loss: 0.7426 || timer: 0.0821 sec.
iter 350430 || Loss: 0.5830 || timer: 0.0827 sec.
iter 350440 || Loss: 0.7272 || timer: 0.0838 sec.
iter 350450 || Loss: 0.7901 || timer: 0.0965 sec.
iter 350460 || Loss: 0.6191 || timer: 0.0961 sec.
iter 350470 || Loss: 0.6857 || timer: 0.0886 sec.
iter 350480 || Loss: 0.7191 || timer: 0.0911 sec.
iter 350490 || Loss: 0.8228 || timer: 0.1036 sec.
iter 350500 || Loss: 0.7721 || timer: 0.0819 sec.
iter 350510 || Loss: 0.7098 || timer: 0.0940 sec.
iter 350520 || Loss: 0.9428 || timer: 0.0972 sec.
iter 350530 || Loss: 0.7755 || timer: 0.0838 sec.
iter 350540 || Loss: 0.7050 || timer: 0.1145 sec.
iter 350550 || Loss: 0.6629 || timer: 0.1073 sec.
iter 350560 || Loss: 0.5261 || timer: 0.1196 sec.
iter 350570 || Loss: 0.6830 || timer: 0.0824 sec.
iter 350580 || Loss: 0.5772 || timer: 0.0922 sec.
iter 350590 || Loss: 0.7118 || timer: 0.1004 sec.
iter 350600 || Loss: 0.5885 || timer: 0.0904 sec.
iter 350610 || Loss: 0.6764 || timer: 0.0895 sec.
iter 350620 || Loss: 0.6509 || timer: 0.0823 sec.
iter 350630 || Loss: 0.7526 || timer: 0.1200 sec.
iter 350640 || Loss: 0.4740 || timer: 0.0928 sec.
iter 350650 || Loss: 0.7858 || timer: 0.0897 sec.
iter 350660 || Loss: 0.5990 || timer: 0.0865 sec.
iter 350670 || Loss: 0.5959 || timer: 0.0882 sec.
iter 350680 || Loss: 0.7347 || timer: 0.0203 sec.
iter 350690 || Loss: 0.5155 || timer: 0.1020 sec.
iter 350700 || Loss: 0.6300 || timer: 0.0829 sec.
iter 350710 || Loss: 0.6996 || timer: 0.0898 sec.
iter 350720 || Loss: 0.7996 || timer: 0.1004 sec.
iter 350730 || Loss: 0.7759 || timer: 0.0910 sec.
iter 350740 || Loss: 0.6668 || timer: 0.0821 sec.
iter 350750 || Loss: 0.5989 || timer: 0.0896 sec.
iter 350760 || Loss: 0.8739 || timer: 0.0830 sec.
iter 350770 || Loss: 0.7352 || timer: 0.0908 sec.
iter 350780 || Loss: 0.7046 || timer: 0.1001 sec.
iter 350790 || Loss: 0.7333 || timer: 0.0829 sec.
iter 350800 || Loss: 0.5445 || timer: 0.0921 sec.
iter 350810 || Loss: 0.7122 || timer: 0.0912 sec.
iter 350820 || Loss: 0.5824 || timer: 0.0830 sec.
iter 350830 || Loss: 0.8796 || timer: 0.0901 sec.
iter 350840 || Loss: 0.6831 || timer: 0.0920 sec.
iter 350850 || Loss: 0.5936 || timer: 0.0827 sec.
iter 350860 || Loss: 0.4836 || timer: 0.0897 sec.
iter 350870 || Loss: 0.6087 || timer: 0.0974 sec.
iter 350880 || Loss: 0.5870 || timer: 0.1036 sec.
iter 350890 || Loss: 0.5204 || timer: 0.0910 sec.
iter 350900 || Loss: 0.7203 || timer: 0.1035 sec.
iter 350910 || Loss: 0.7788 || timer: 0.0831 sec.
iter 350920 || Loss: 0.5084 || timer: 0.0923 sec.
iter 350930 || Loss: 0.6882 || timer: 0.0871 sec.
iter 350940 || Loss: 0.6984 || timer: 0.1076 sec.
iter 350950 || Loss: 0.5593 || timer: 0.1107 sec.
iter 350960 || Loss: 0.8727 || timer: 0.1053 sec.
iter 350970 || Loss: 0.8053 || timer: 0.0836 sec.
iter 350980 || Loss: 0.7627 || timer: 0.0890 sec.
iter 350990 || Loss: 1.1566 || timer: 0.0927 sec.
iter 351000 || Loss: 0.8319 || timer: 0.0905 sec.
iter 351010 || Loss: 0.8042 || timer: 0.0261 sec.
iter 351020 || Loss: 0.8385 || timer: 0.0834 sec.
iter 351030 || Loss: 0.5279 || timer: 0.0980 sec.
iter 351040 || Loss: 0.7794 || timer: 0.0907 sec.
iter 351050 || Loss: 0.6344 || timer: 0.0906 sec.
iter 351060 || Loss: 1.0116 || timer: 0.0974 sec.
iter 351070 || Loss: 0.4421 || timer: 0.1106 sec.
iter 351080 || Loss: 0.5864 || timer: 0.0928 sec.
iter 351090 || Loss: 0.5937 || timer: 0.0883 sec.
iter 351100 || Loss: 0.6379 || timer: 0.0833 sec.
iter 351110 || Loss: 0.5128 || timer: 0.1041 sec.
iter 351120 || Loss: 0.5187 || timer: 0.1085 sec.
iter 351130 || Loss: 0.7125 || timer: 0.0894 sec.
iter 351140 || Loss: 0.8111 || timer: 0.0932 sec.
iter 351150 || Loss: 0.6812 || timer: 0.0933 sec.
iter 351160 || Loss: 0.7332 || timer: 0.0830 sec.
iter 351170 || Loss: 0.5734 || timer: 0.0884 sec.
iter 351180 || Loss: 0.8398 || timer: 0.0839 sec.
iter 351190 || Loss: 0.7009 || timer: 0.1138 sec.
iter 351200 || Loss: 0.7043 || timer: 0.0927 sec.
iter 351210 || Loss: 0.8491 || timer: 0.0910 sec.
iter 351220 || Loss: 0.6467 || timer: 0.0903 sec.
iter 351230 || Loss: 0.5310 || timer: 0.0897 sec.
iter 351240 || Loss: 0.9700 || timer: 0.0913 sec.
iter 351250 || Loss: 0.8285 || timer: 0.0881 sec.
iter 351260 || Loss: 0.7806 || timer: 0.0901 sec.
iter 351270 || Loss: 1.0346 || timer: 0.0868 sec.
iter 351280 || Loss: 0.8676 || timer: 0.0834 sec.
iter 351290 || Loss: 0.7341 || timer: 0.0920 sec.
iter 351300 || Loss: 0.6718 || timer: 0.0929 sec.
iter 351310 || Loss: 0.9131 || timer: 0.0884 sec.
iter 351320 || Loss: 0.4708 || timer: 0.0852 sec.
iter 351330 || Loss: 0.6647 || timer: 0.0846 sec.
iter 351340 || Loss: 0.7468 || timer: 0.0169 sec.
iter 351350 || Loss: 1.0179 || timer: 0.0894 sec.
iter 351360 || Loss: 0.7074 || timer: 0.0911 sec.
iter 351370 || Loss: 0.8260 || timer: 0.0948 sec.
iter 351380 || Loss: 0.7353 || timer: 0.0814 sec.
iter 351390 || Loss: 0.6290 || timer: 0.0995 sec.
iter 351400 || Loss: 0.7453 || timer: 0.0829 sec.
iter 351410 || Loss: 0.8768 || timer: 0.0927 sec.
iter 351420 || Loss: 0.5632 || timer: 0.1116 sec.
iter 351430 || Loss: 0.7172 || timer: 0.0881 sec.
iter 351440 || Loss: 0.4665 || timer: 0.1242 sec.
iter 351450 || Loss: 0.6751 || timer: 0.0822 sec.
iter 351460 || Loss: 0.5259 || timer: 0.0914 sec.
iter 351470 || Loss: 0.7715 || timer: 0.0925 sec.
iter 351480 || Loss: 0.5540 || timer: 0.0990 sec.
iter 351490 || Loss: 0.9806 || timer: 0.0826 sec.
iter 351500 || Loss: 0.5593 || timer: 0.0909 sec.
iter 351510 || Loss: 0.6726 || timer: 0.1024 sec.
iter 351520 || Loss: 0.5321 || timer: 0.0902 sec.
iter 351530 || Loss: 0.9920 || timer: 0.0888 sec.
iter 351540 || Loss: 0.5078 || timer: 0.0950 sec.
iter 351550 || Loss: 0.6373 || timer: 0.0913 sec.
iter 351560 || Loss: 0.7992 || timer: 0.0837 sec.
iter 351570 || Loss: 0.9276 || timer: 0.0853 sec.
iter 351580 || Loss: 0.5734 || timer: 0.0906 sec.
iter 351590 || Loss: 0.5289 || timer: 0.1006 sec.
iter 351600 || Loss: 0.6409 || timer: 0.1058 sec.
iter 351610 || Loss: 0.5475 || timer: 0.0899 sec.
iter 351620 || Loss: 0.5973 || timer: 0.0899 sec.
iter 351630 || Loss: 0.8711 || timer: 0.0878 sec.
iter 351640 || Loss: 0.5883 || timer: 0.1079 sec.
iter 351650 || Loss: 0.8170 || timer: 0.0957 sec.
iter 351660 || Loss: 0.6552 || timer: 0.0897 sec.
iter 351670 || Loss: 0.6032 || timer: 0.0178 sec.
iter 351680 || Loss: 0.1915 || timer: 0.0909 sec.
iter 351690 || Loss: 0.6116 || timer: 0.0863 sec.
iter 351700 || Loss: 0.6385 || timer: 0.0844 sec.
iter 351710 || Loss: 0.5349 || timer: 0.0827 sec.
iter 351720 || Loss: 0.5977 || timer: 0.0883 sec.
iter 351730 || Loss: 0.5297 || timer: 0.0898 sec.
iter 351740 || Loss: 0.6248 || timer: 0.0825 sec.
iter 351750 || Loss: 0.6320 || timer: 0.0843 sec.
iter 351760 || Loss: 1.0107 || timer: 0.0891 sec.
iter 351770 || Loss: 0.7806 || timer: 0.1063 sec.
iter 351780 || Loss: 0.8213 || timer: 0.0919 sec.
iter 351790 || Loss: 0.6066 || timer: 0.1137 sec.
iter 351800 || Loss: 0.6803 || timer: 0.0830 sec.
iter 351810 || Loss: 0.7758 || timer: 0.0907 sec.
iter 351820 || Loss: 0.7249 || timer: 0.0927 sec.
iter 351830 || Loss: 0.5572 || timer: 0.0835 sec.
iter 351840 || Loss: 0.8788 || timer: 0.1041 sec.
iter 351850 || Loss: 0.7974 || timer: 0.0925 sec.
iter 351860 || Loss: 0.5963 || timer: 0.0926 sec.
iter 351870 || Loss: 0.6543 || timer: 0.0903 sec.
iter 351880 || Loss: 0.6571 || timer: 0.0829 sec.
iter 351890 || Loss: 0.7389 || timer: 0.0825 sec.
iter 351900 || Loss: 0.8078 || timer: 0.0963 sec.
iter 351910 || Loss: 0.8168 || timer: 0.1142 sec.
iter 351920 || Loss: 0.7141 || timer: 0.0903 sec.
iter 351930 || Loss: 0.7376 || timer: 0.0855 sec.
iter 351940 || Loss: 0.5736 || timer: 0.0920 sec.
iter 351950 || Loss: 0.5393 || timer: 0.0837 sec.
iter 351960 || Loss: 0.5197 || timer: 0.0824 sec.
iter 351970 || Loss: 0.8439 || timer: 0.1120 sec.
iter 351980 || Loss: 0.5845 || timer: 0.0897 sec.
iter 351990 || Loss: 0.7975 || timer: 0.0981 sec.
iter 352000 || Loss: 1.0110 || timer: 0.0184 sec.
iter 352010 || Loss: 0.1242 || timer: 0.0913 sec.
iter 352020 || Loss: 0.6733 || timer: 0.0916 sec.
iter 352030 || Loss: 0.6400 || timer: 0.0829 sec.
iter 352040 || Loss: 0.4632 || timer: 0.1083 sec.
iter 352050 || Loss: 1.0154 || timer: 0.0942 sec.
iter 352060 || Loss: 0.5478 || timer: 0.0900 sec.
iter 352070 || Loss: 0.6278 || timer: 0.0896 sec.
iter 352080 || Loss: 0.5957 || timer: 0.0907 sec.
iter 352090 || Loss: 0.8862 || timer: 0.0860 sec.
iter 352100 || Loss: 0.4822 || timer: 0.1179 sec.
iter 352110 || Loss: 0.7801 || timer: 0.0875 sec.
iter 352120 || Loss: 0.7013 || timer: 0.0910 sec.
iter 352130 || Loss: 0.5888 || timer: 0.0867 sec.
iter 352140 || Loss: 0.5810 || timer: 0.0898 sec.
iter 352150 || Loss: 0.6068 || timer: 0.0827 sec.
iter 352160 || Loss: 0.7624 || timer: 0.1034 sec.
iter 352170 || Loss: 0.9584 || timer: 0.0889 sec.
iter 352180 || Loss: 0.6955 || timer: 0.1084 sec.
iter 352190 || Loss: 0.3599 || timer: 0.0912 sec.
iter 352200 || Loss: 1.0247 || timer: 0.0909 sec.
iter 352210 || Loss: 0.8702 || timer: 0.1108 sec.
iter 352220 || Loss: 0.7185 || timer: 0.0838 sec.
iter 352230 || Loss: 0.7002 || timer: 0.0918 sec.
iter 352240 || Loss: 0.5997 || timer: 0.0853 sec.
iter 352250 || Loss: 0.4779 || timer: 0.0900 sec.
iter 352260 || Loss: 0.7005 || timer: 0.0932 sec.
iter 352270 || Loss: 0.6847 || timer: 0.0940 sec.
iter 352280 || Loss: 0.7437 || timer: 0.0830 sec.
iter 352290 || Loss: 0.6221 || timer: 0.0824 sec.
iter 352300 || Loss: 0.5812 || timer: 0.1083 sec.
iter 352310 || Loss: 0.7749 || timer: 0.0904 sec.
iter 352320 || Loss: 0.8364 || timer: 0.0836 sec.
iter 352330 || Loss: 0.5144 || timer: 0.0232 sec.
iter 352340 || Loss: 0.2944 || timer: 0.0931 sec.
iter 352350 || Loss: 0.7176 || timer: 0.1008 sec.
iter 352360 || Loss: 0.6534 || timer: 0.0920 sec.
iter 352370 || Loss: 1.0634 || timer: 0.0822 sec.
iter 352380 || Loss: 0.5557 || timer: 0.0901 sec.
iter 352390 || Loss: 0.6380 || timer: 0.0910 sec.
iter 352400 || Loss: 0.6191 || timer: 0.0857 sec.
iter 352410 || Loss: 0.6150 || timer: 0.0939 sec.
iter 352420 || Loss: 0.6461 || timer: 0.1141 sec.
iter 352430 || Loss: 0.6229 || timer: 0.0981 sec.
iter 352440 || Loss: 0.4970 || timer: 0.0900 sec.
iter 352450 || Loss: 0.6708 || timer: 0.0888 sec.
iter 352460 || Loss: 0.6953 || timer: 0.0843 sec.
iter 352470 || Loss: 0.9454 || timer: 0.0888 sec.
iter 352480 || Loss: 0.6814 || timer: 0.0906 sec.
iter 352490 || Loss: 0.5658 || timer: 0.0911 sec.
iter 352500 || Loss: 0.6980 || timer: 0.0882 sec.
iter 352510 || Loss: 0.8077 || timer: 0.0900 sec.
iter 352520 || Loss: 0.7837 || timer: 0.0907 sec.
iter 352530 || Loss: 0.8202 || timer: 0.1094 sec.
iter 352540 || Loss: 0.7400 || timer: 0.0912 sec.
iter 352550 || Loss: 0.8130 || timer: 0.0966 sec.
iter 352560 || Loss: 0.8293 || timer: 0.0838 sec.
iter 352570 || Loss: 0.6218 || timer: 0.0988 sec.
iter 352580 || Loss: 0.7180 || timer: 0.0890 sec.
iter 352590 || Loss: 0.8953 || timer: 0.0884 sec.
iter 352600 || Loss: 0.7356 || timer: 0.0883 sec.
iter 352610 || Loss: 0.8628 || timer: 0.1024 sec.
iter 352620 || Loss: 0.4654 || timer: 0.1324 sec.
iter 352630 || Loss: 0.5922 || timer: 0.0958 sec.
iter 352640 || Loss: 0.7092 || timer: 0.0909 sec.
iter 352650 || Loss: 0.6660 || timer: 0.0931 sec.
iter 352660 || Loss: 0.7520 || timer: 0.0259 sec.
iter 352670 || Loss: 3.2824 || timer: 0.0880 sec.
iter 352680 || Loss: 0.5406 || timer: 0.0911 sec.
iter 352690 || Loss: 0.7207 || timer: 0.0831 sec.
iter 352700 || Loss: 0.7005 || timer: 0.0912 sec.
iter 352710 || Loss: 0.6872 || timer: 0.0906 sec.
iter 352720 || Loss: 0.7279 || timer: 0.0843 sec.
iter 352730 || Loss: 0.6124 || timer: 0.0907 sec.
iter 352740 || Loss: 0.3889 || timer: 0.0928 sec.
iter 352750 || Loss: 0.6763 || timer: 0.0837 sec.
iter 352760 || Loss: 0.5892 || timer: 0.0970 sec.
iter 352770 || Loss: 0.6820 || timer: 0.1047 sec.
iter 352780 || Loss: 0.7329 || timer: 0.0887 sec.
iter 352790 || Loss: 0.8174 || timer: 0.0908 sec.
iter 352800 || Loss: 0.3955 || timer: 0.1044 sec.
iter 352810 || Loss: 0.9002 || timer: 0.0912 sec.
iter 352820 || Loss: 0.7069 || timer: 0.0917 sec.
iter 352830 || Loss: 0.6780 || timer: 0.0905 sec.
iter 352840 || Loss: 0.6948 || timer: 0.1077 sec.
iter 352850 || Loss: 0.9052 || timer: 0.0905 sec.
iter 352860 || Loss: 0.8091 || timer: 0.1175 sec.
iter 352870 || Loss: 0.5091 || timer: 0.0881 sec.
iter 352880 || Loss: 0.7218 || timer: 0.0895 sec.
iter 352890 || Loss: 0.8048 || timer: 0.0900 sec.
iter 352900 || Loss: 0.4579 || timer: 0.0938 sec.
iter 352910 || Loss: 0.4809 || timer: 0.0898 sec.
iter 352920 || Loss: 0.6197 || timer: 0.0892 sec.
iter 352930 || Loss: 0.6650 || timer: 0.1006 sec.
iter 352940 || Loss: 1.0640 || timer: 0.1201 sec.
iter 352950 || Loss: 0.6535 || timer: 0.0831 sec.
iter 352960 || Loss: 0.8047 || timer: 0.0913 sec.
iter 352970 || Loss: 0.7556 || timer: 0.0925 sec.
iter 352980 || Loss: 0.7967 || timer: 0.1065 sec.
iter 352990 || Loss: 0.7451 || timer: 0.0177 sec.
iter 353000 || Loss: 0.1228 || timer: 0.0904 sec.
iter 353010 || Loss: 0.6991 || timer: 0.1030 sec.
iter 353020 || Loss: 0.6670 || timer: 0.0839 sec.
iter 353030 || Loss: 0.3723 || timer: 0.1044 sec.
iter 353040 || Loss: 0.7741 || timer: 0.0914 sec.
iter 353050 || Loss: 0.8482 || timer: 0.0845 sec.
iter 353060 || Loss: 0.6256 || timer: 0.0829 sec.
iter 353070 || Loss: 0.9194 || timer: 0.0824 sec.
iter 353080 || Loss: 0.8895 || timer: 0.0889 sec.
iter 353090 || Loss: 0.5245 || timer: 0.0985 sec.
iter 353100 || Loss: 0.7079 || timer: 0.0984 sec.
iter 353110 || Loss: 0.5100 || timer: 0.1102 sec.
iter 353120 || Loss: 0.5815 || timer: 0.0914 sec.
iter 353130 || Loss: 0.6569 || timer: 0.0842 sec.
iter 353140 || Loss: 0.4526 || timer: 0.0915 sec.
iter 353150 || Loss: 0.9844 || timer: 0.0919 sec.
iter 353160 || Loss: 0.8709 || timer: 0.0817 sec.
iter 353170 || Loss: 0.6976 || timer: 0.0906 sec.
iter 353180 || Loss: 0.6135 || timer: 0.0913 sec.
iter 353190 || Loss: 0.6144 || timer: 0.0879 sec.
iter 353200 || Loss: 0.5226 || timer: 0.0881 sec.
iter 353210 || Loss: 0.8109 || timer: 0.1211 sec.
iter 353220 || Loss: 0.7666 || timer: 0.0837 sec.
iter 353230 || Loss: 0.5299 || timer: 0.1037 sec.
iter 353240 || Loss: 0.6104 || timer: 0.0887 sec.
iter 353250 || Loss: 0.5115 || timer: 0.0893 sec.
iter 353260 || Loss: 0.7650 || timer: 0.0904 sec.
iter 353270 || Loss: 0.5186 || timer: 0.0838 sec.
iter 353280 || Loss: 0.7107 || timer: 0.0842 sec.
iter 353290 || Loss: 0.5900 || timer: 0.0826 sec.
iter 353300 || Loss: 0.7432 || timer: 0.0934 sec.
iter 353310 || Loss: 0.7741 || timer: 0.0921 sec.
iter 353320 || Loss: 0.7823 || timer: 0.0267 sec.
iter 353330 || Loss: 1.9414 || timer: 0.0907 sec.
iter 353340 || Loss: 1.0014 || timer: 0.0906 sec.
iter 353350 || Loss: 0.7373 || timer: 0.0898 sec.
iter 353360 || Loss: 0.6316 || timer: 0.0884 sec.
iter 353370 || Loss: 0.5721 || timer: 0.0812 sec.
iter 353380 || Loss: 0.6987 || timer: 0.0836 sec.
iter 353390 || Loss: 0.7678 || timer: 0.0875 sec.
iter 353400 || Loss: 0.6239 || timer: 0.0903 sec.
iter 353410 || Loss: 0.5439 || timer: 0.1159 sec.
iter 353420 || Loss: 0.6132 || timer: 0.1171 sec.
iter 353430 || Loss: 0.6090 || timer: 0.0904 sec.
iter 353440 || Loss: 0.6627 || timer: 0.0827 sec.
iter 353450 || Loss: 0.7593 || timer: 0.1060 sec.
iter 353460 || Loss: 0.8617 || timer: 0.0915 sec.
iter 353470 || Loss: 0.7090 || timer: 0.0913 sec.
iter 353480 || Loss: 0.8155 || timer: 0.0925 sec.
iter 353490 || Loss: 0.5763 || timer: 0.0971 sec.
iter 353500 || Loss: 0.8087 || timer: 0.0886 sec.
iter 353510 || Loss: 0.6076 || timer: 0.0841 sec.
iter 353520 || Loss: 0.9255 || timer: 0.0900 sec.
iter 353530 || Loss: 0.4382 || timer: 0.0900 sec.
iter 353540 || Loss: 0.6366 || timer: 0.0912 sec.
iter 353550 || Loss: 0.7701 || timer: 0.0837 sec.
iter 353560 || Loss: 0.6141 || timer: 0.0827 sec.
iter 353570 || Loss: 0.5711 || timer: 0.0846 sec.
iter 353580 || Loss: 0.7964 || timer: 0.1107 sec.
iter 353590 || Loss: 0.5496 || timer: 0.0903 sec.
iter 353600 || Loss: 0.8495 || timer: 0.0908 sec.
iter 353610 || Loss: 0.6593 || timer: 0.0895 sec.
iter 353620 || Loss: 0.5981 || timer: 0.0822 sec.
iter 353630 || Loss: 0.6718 || timer: 0.0875 sec.
iter 353640 || Loss: 0.6681 || timer: 0.0829 sec.
iter 353650 || Loss: 0.7575 || timer: 0.0194 sec.
iter 353660 || Loss: 2.4036 || timer: 0.0898 sec.
iter 353670 || Loss: 0.4605 || timer: 0.0992 sec.
iter 353680 || Loss: 0.7657 || timer: 0.0831 sec.
iter 353690 || Loss: 0.6482 || timer: 0.0829 sec.
iter 353700 || Loss: 0.6383 || timer: 0.0826 sec.
iter 353710 || Loss: 0.5043 || timer: 0.0914 sec.
iter 353720 || Loss: 0.6482 || timer: 0.0889 sec.
iter 353730 || Loss: 0.7760 || timer: 0.0953 sec.
iter 353740 || Loss: 0.7391 || timer: 0.0826 sec.
iter 353750 || Loss: 0.7109 || timer: 0.1006 sec.
iter 353760 || Loss: 0.6826 || timer: 0.0876 sec.
iter 353770 || Loss: 0.7153 || timer: 0.0903 sec.
iter 353780 || Loss: 0.5860 || timer: 0.0836 sec.
iter 353790 || Loss: 0.7514 || timer: 0.0847 sec.
iter 353800 || Loss: 0.6241 || timer: 0.0965 sec.
iter 353810 || Loss: 0.6888 || timer: 0.0908 sec.
iter 353820 || Loss: 0.5710 || timer: 0.0954 sec.
iter 353830 || Loss: 0.7903 || timer: 0.0918 sec.
iter 353840 || Loss: 0.5778 || timer: 0.0907 sec.
iter 353850 || Loss: 0.8538 || timer: 0.0849 sec.
iter 353860 || Loss: 0.6122 || timer: 0.0924 sec.
iter 353870 || Loss: 0.6748 || timer: 0.0839 sec.
iter 353880 || Loss: 0.6054 || timer: 0.0902 sec.
iter 353890 || Loss: 0.7777 || timer: 0.0888 sec.
iter 353900 || Loss: 0.8121 || timer: 0.0836 sec.
iter 353910 || Loss: 0.5451 || timer: 0.0876 sec.
iter 353920 || Loss: 0.7121 || timer: 0.0832 sec.
iter 353930 || Loss: 0.5121 || timer: 0.0924 sec.
iter 353940 || Loss: 0.8379 || timer: 0.0826 sec.
iter 353950 || Loss: 0.6550 || timer: 0.0904 sec.
iter 353960 || Loss: 0.7150 || timer: 0.0894 sec.
iter 353970 || Loss: 0.6210 || timer: 0.0983 sec.
iter 353980 || Loss: 0.7472 || timer: 0.0216 sec.
iter 353990 || Loss: 0.4886 || timer: 0.0892 sec.
iter 354000 || Loss: 0.6261 || timer: 0.0896 sec.
iter 354010 || Loss: 0.8996 || timer: 0.0996 sec.
iter 354020 || Loss: 0.6408 || timer: 0.0832 sec.
iter 354030 || Loss: 0.5789 || timer: 0.0915 sec.
iter 354040 || Loss: 0.7914 || timer: 0.0916 sec.
iter 354050 || Loss: 0.5490 || timer: 0.0836 sec.
iter 354060 || Loss: 0.6472 || timer: 0.0817 sec.
iter 354070 || Loss: 0.5482 || timer: 0.0944 sec.
iter 354080 || Loss: 0.7801 || timer: 0.1220 sec.
iter 354090 || Loss: 0.7098 || timer: 0.0894 sec.
iter 354100 || Loss: 0.9744 || timer: 0.0839 sec.
iter 354110 || Loss: 0.7903 || timer: 0.0903 sec.
iter 354120 || Loss: 0.6591 || timer: 0.0828 sec.
iter 354130 || Loss: 0.7132 || timer: 0.0923 sec.
iter 354140 || Loss: 0.6514 || timer: 0.1083 sec.
iter 354150 || Loss: 0.3521 || timer: 0.0894 sec.
iter 354160 || Loss: 0.7923 || timer: 0.0899 sec.
iter 354170 || Loss: 0.7281 || timer: 0.0887 sec.
iter 354180 || Loss: 0.6455 || timer: 0.1085 sec.
iter 354190 || Loss: 0.7138 || timer: 0.0824 sec.
iter 354200 || Loss: 0.7255 || timer: 0.0922 sec.
iter 354210 || Loss: 0.4782 || timer: 0.0995 sec.
iter 354220 || Loss: 0.5914 || timer: 0.0905 sec.
iter 354230 || Loss: 0.6828 || timer: 0.0954 sec.
iter 354240 || Loss: 0.5955 || timer: 0.0838 sec.
iter 354250 || Loss: 0.5807 || timer: 0.0914 sec.
iter 354260 || Loss: 0.9111 || timer: 0.1066 sec.
iter 354270 || Loss: 0.6845 || timer: 0.0871 sec.
iter 354280 || Loss: 0.7781 || timer: 0.0896 sec.
iter 354290 || Loss: 0.6158 || timer: 0.0888 sec.
iter 354300 || Loss: 0.4545 || timer: 0.0898 sec.
iter 354310 || Loss: 1.1359 || timer: 0.0197 sec.
iter 354320 || Loss: 0.1946 || timer: 0.0920 sec.
iter 354330 || Loss: 0.7937 || timer: 0.0908 sec.
iter 354340 || Loss: 0.9683 || timer: 0.0925 sec.
iter 354350 || Loss: 0.7554 || timer: 0.0832 sec.
iter 354360 || Loss: 0.8213 || timer: 0.0825 sec.
iter 354370 || Loss: 0.7288 || timer: 0.0857 sec.
iter 354380 || Loss: 0.8258 || timer: 0.0912 sec.
iter 354390 || Loss: 0.5827 || timer: 0.0842 sec.
iter 354400 || Loss: 0.7233 || timer: 0.0874 sec.
iter 354410 || Loss: 0.5765 || timer: 0.0916 sec.
iter 354420 || Loss: 0.5663 || timer: 0.0882 sec.
iter 354430 || Loss: 0.7643 || timer: 0.0967 sec.
iter 354440 || Loss: 0.6875 || timer: 0.0899 sec.
iter 354450 || Loss: 0.4914 || timer: 0.0900 sec.
iter 354460 || Loss: 0.7079 || timer: 0.1109 sec.
iter 354470 || Loss: 0.9370 || timer: 0.0896 sec.
iter 354480 || Loss: 0.6430 || timer: 0.1110 sec.
iter 354490 || Loss: 0.6396 || timer: 0.0828 sec.
iter 354500 || Loss: 0.6429 || timer: 0.0821 sec.
iter 354510 || Loss: 0.6601 || timer: 0.0826 sec.
iter 354520 || Loss: 0.5796 || timer: 0.0904 sec.
iter 354530 || Loss: 0.8325 || timer: 0.0891 sec.
iter 354540 || Loss: 0.7048 || timer: 0.0911 sec.
iter 354550 || Loss: 0.6176 || timer: 0.1101 sec.
iter 354560 || Loss: 0.6213 || timer: 0.0882 sec.
iter 354570 || Loss: 0.4805 || timer: 0.1052 sec.
iter 354580 || Loss: 0.9077 || timer: 0.1007 sec.
iter 354590 || Loss: 1.2124 || timer: 0.1049 sec.
iter 354600 || Loss: 0.8969 || timer: 0.0828 sec.
iter 354610 || Loss: 0.7346 || timer: 0.0933 sec.
iter 354620 || Loss: 0.7277 || timer: 0.1038 sec.
iter 354630 || Loss: 0.6813 || timer: 0.0914 sec.
iter 354640 || Loss: 0.5437 || timer: 0.0183 sec.
iter 354650 || Loss: 2.4579 || timer: 0.0888 sec.
iter 354660 || Loss: 0.8017 || timer: 0.0882 sec.
iter 354670 || Loss: 0.8224 || timer: 0.0876 sec.
iter 354680 || Loss: 0.7441 || timer: 0.0909 sec.
iter 354690 || Loss: 0.6669 || timer: 0.0906 sec.
iter 354700 || Loss: 0.5288 || timer: 0.0841 sec.
iter 354710 || Loss: 0.5039 || timer: 0.0913 sec.
iter 354720 || Loss: 0.7147 || timer: 0.0897 sec.
iter 354730 || Loss: 0.6669 || timer: 0.0972 sec.
iter 354740 || Loss: 0.7052 || timer: 0.1542 sec.
iter 354750 || Loss: 0.8216 || timer: 0.0904 sec.
iter 354760 || Loss: 0.5681 || timer: 0.0900 sec.
iter 354770 || Loss: 0.7367 || timer: 0.0835 sec.
iter 354780 || Loss: 0.7253 || timer: 0.0916 sec.
iter 354790 || Loss: 0.6738 || timer: 0.1152 sec.
iter 354800 || Loss: 0.5667 || timer: 0.0910 sec.
iter 354810 || Loss: 0.9500 || timer: 0.0905 sec.
iter 354820 || Loss: 0.5987 || timer: 0.0892 sec.
iter 354830 || Loss: 0.5063 || timer: 0.0967 sec.
iter 354840 || Loss: 0.6631 || timer: 0.0871 sec.
iter 354850 || Loss: 0.4931 || timer: 0.0951 sec.
iter 354860 || Loss: 0.7256 || timer: 0.0827 sec.
iter 354870 || Loss: 0.6592 || timer: 0.0899 sec.
iter 354880 || Loss: 0.7094 || timer: 0.0928 sec.
iter 354890 || Loss: 0.7512 || timer: 0.0914 sec.
iter 354900 || Loss: 0.7096 || timer: 0.0912 sec.
iter 354910 || Loss: 0.6900 || timer: 0.1034 sec.
iter 354920 || Loss: 0.7810 || timer: 0.0931 sec.
iter 354930 || Loss: 0.6140 || timer: 0.0916 sec.
iter 354940 || Loss: 0.5855 || timer: 0.0904 sec.
iter 354950 || Loss: 0.7642 || timer: 0.0906 sec.
iter 354960 || Loss: 0.7643 || timer: 0.0922 sec.
iter 354970 || Loss: 0.6000 || timer: 0.0211 sec.
iter 354980 || Loss: 0.9395 || timer: 0.0908 sec.
iter 354990 || Loss: 0.6986 || timer: 0.1077 sec.
iter 355000 || Loss: 0.8021 || Saving state, iter: 355000
timer: 0.0900 sec.
iter 355010 || Loss: 0.6561 || timer: 0.0913 sec.
iter 355020 || Loss: 0.7516 || timer: 0.0927 sec.
iter 355030 || Loss: 0.6028 || timer: 0.0902 sec.
iter 355040 || Loss: 0.7749 || timer: 0.0836 sec.
iter 355050 || Loss: 0.9379 || timer: 0.0830 sec.
iter 355060 || Loss: 0.6385 || timer: 0.1067 sec.
iter 355070 || Loss: 0.6780 || timer: 0.0978 sec.
iter 355080 || Loss: 0.6612 || timer: 0.0850 sec.
iter 355090 || Loss: 0.8195 || timer: 0.0820 sec.
iter 355100 || Loss: 0.8879 || timer: 0.0831 sec.
iter 355110 || Loss: 0.5882 || timer: 0.0833 sec.
iter 355120 || Loss: 0.8032 || timer: 0.0901 sec.
iter 355130 || Loss: 0.7108 || timer: 0.0886 sec.
iter 355140 || Loss: 0.6970 || timer: 0.1061 sec.
iter 355150 || Loss: 0.5880 || timer: 0.0933 sec.
iter 355160 || Loss: 1.1305 || timer: 0.1136 sec.
iter 355170 || Loss: 0.6749 || timer: 0.1170 sec.
iter 355180 || Loss: 0.6977 || timer: 0.0912 sec.
iter 355190 || Loss: 0.4482 || timer: 0.0904 sec.
iter 355200 || Loss: 0.5135 || timer: 0.0842 sec.
iter 355210 || Loss: 0.7355 || timer: 0.0900 sec.
iter 355220 || Loss: 0.5029 || timer: 0.1030 sec.
iter 355230 || Loss: 0.6690 || timer: 0.0984 sec.
iter 355240 || Loss: 0.6246 || timer: 0.0904 sec.
iter 355250 || Loss: 1.1524 || timer: 0.0915 sec.
iter 355260 || Loss: 0.8031 || timer: 0.1220 sec.
iter 355270 || Loss: 0.8290 || timer: 0.0833 sec.
iter 355280 || Loss: 0.7018 || timer: 0.0923 sec.
iter 355290 || Loss: 0.8673 || timer: 0.0919 sec.
iter 355300 || Loss: 0.4636 || timer: 0.0191 sec.
iter 355310 || Loss: 0.4268 || timer: 0.1252 sec.
iter 355320 || Loss: 0.9528 || timer: 0.1011 sec.
iter 355330 || Loss: 0.8554 || timer: 0.0910 sec.
iter 355340 || Loss: 0.6887 || timer: 0.0835 sec.
iter 355350 || Loss: 0.6758 || timer: 0.0916 sec.
iter 355360 || Loss: 0.5016 || timer: 0.0830 sec.
iter 355370 || Loss: 0.5532 || timer: 0.0921 sec.
iter 355380 || Loss: 1.0209 || timer: 0.0916 sec.
iter 355390 || Loss: 0.8277 || timer: 0.0925 sec.
iter 355400 || Loss: 0.6807 || timer: 0.1193 sec.
iter 355410 || Loss: 0.7476 || timer: 0.0900 sec.
iter 355420 || Loss: 0.5135 || timer: 0.1006 sec.
iter 355430 || Loss: 0.6464 || timer: 0.0847 sec.
iter 355440 || Loss: 0.8935 || timer: 0.0881 sec.
iter 355450 || Loss: 0.8670 || timer: 0.0930 sec.
iter 355460 || Loss: 0.9767 || timer: 0.0865 sec.
iter 355470 || Loss: 0.6209 || timer: 0.1028 sec.
iter 355480 || Loss: 0.6985 || timer: 0.0836 sec.
iter 355490 || Loss: 0.8290 || timer: 0.0879 sec.
iter 355500 || Loss: 0.5991 || timer: 0.1065 sec.
iter 355510 || Loss: 0.5397 || timer: 0.0947 sec.
iter 355520 || Loss: 0.6220 || timer: 0.0895 sec.
iter 355530 || Loss: 0.6422 || timer: 0.0830 sec.
iter 355540 || Loss: 0.7038 || timer: 0.0865 sec.
iter 355550 || Loss: 0.6977 || timer: 0.0826 sec.
iter 355560 || Loss: 0.5609 || timer: 0.0963 sec.
iter 355570 || Loss: 0.6387 || timer: 0.0899 sec.
iter 355580 || Loss: 0.6441 || timer: 0.0886 sec.
iter 355590 || Loss: 0.6140 || timer: 0.0924 sec.
iter 355600 || Loss: 0.4973 || timer: 0.0830 sec.
iter 355610 || Loss: 0.8622 || timer: 0.0826 sec.
iter 355620 || Loss: 0.8673 || timer: 0.0900 sec.
iter 355630 || Loss: 0.4806 || timer: 0.0201 sec.
iter 355640 || Loss: 0.2619 || timer: 0.0915 sec.
iter 355650 || Loss: 0.6368 || timer: 0.0906 sec.
iter 355660 || Loss: 0.7588 || timer: 0.0918 sec.
iter 355670 || Loss: 0.6751 || timer: 0.0898 sec.
iter 355680 || Loss: 0.7551 || timer: 0.0894 sec.
iter 355690 || Loss: 0.8352 || timer: 0.0880 sec.
iter 355700 || Loss: 0.4055 || timer: 0.1145 sec.
iter 355710 || Loss: 0.7857 || timer: 0.0830 sec.
iter 355720 || Loss: 0.5662 || timer: 0.0889 sec.
iter 355730 || Loss: 0.7768 || timer: 0.1101 sec.
iter 355740 || Loss: 0.5455 || timer: 0.0898 sec.
iter 355750 || Loss: 0.7088 || timer: 0.1025 sec.
iter 355760 || Loss: 0.5891 || timer: 0.0893 sec.
iter 355770 || Loss: 0.5345 || timer: 0.0903 sec.
iter 355780 || Loss: 0.7811 || timer: 0.0831 sec.
iter 355790 || Loss: 0.6996 || timer: 0.0927 sec.
iter 355800 || Loss: 0.4962 || timer: 0.0946 sec.
iter 355810 || Loss: 0.7331 || timer: 0.0834 sec.
iter 355820 || Loss: 0.5765 || timer: 0.0907 sec.
iter 355830 || Loss: 0.6767 || timer: 0.0853 sec.
iter 355840 || Loss: 0.6683 || timer: 0.0924 sec.
iter 355850 || Loss: 0.6190 || timer: 0.0970 sec.
iter 355860 || Loss: 0.7932 || timer: 0.0915 sec.
iter 355870 || Loss: 0.5179 || timer: 0.0908 sec.
iter 355880 || Loss: 0.7438 || timer: 0.0836 sec.
iter 355890 || Loss: 0.7790 || timer: 0.0925 sec.
iter 355900 || Loss: 0.7468 || timer: 0.0858 sec.
iter 355910 || Loss: 0.5139 || timer: 0.0916 sec.
iter 355920 || Loss: 0.7306 || timer: 0.0841 sec.
iter 355930 || Loss: 0.6676 || timer: 0.0944 sec.
iter 355940 || Loss: 0.5663 || timer: 0.0935 sec.
iter 355950 || Loss: 0.5569 || timer: 0.0925 sec.
iter 355960 || Loss: 0.7607 || timer: 0.0178 sec.
iter 355970 || Loss: 0.3085 || timer: 0.0898 sec.
iter 355980 || Loss: 0.5978 || timer: 0.0896 sec.
iter 355990 || Loss: 0.5633 || timer: 0.1034 sec.
iter 356000 || Loss: 0.4623 || timer: 0.0926 sec.
iter 356010 || Loss: 0.6292 || timer: 0.0981 sec.
iter 356020 || Loss: 0.7180 || timer: 0.0936 sec.
iter 356030 || Loss: 0.4832 || timer: 0.0865 sec.
iter 356040 || Loss: 0.7489 || timer: 0.0902 sec.
iter 356050 || Loss: 0.8110 || timer: 0.1075 sec.
iter 356060 || Loss: 0.7819 || timer: 0.1182 sec.
iter 356070 || Loss: 0.8618 || timer: 0.0909 sec.
iter 356080 || Loss: 0.5525 || timer: 0.0842 sec.
iter 356090 || Loss: 0.6340 || timer: 0.0890 sec.
iter 356100 || Loss: 0.9763 || timer: 0.0864 sec.
iter 356110 || Loss: 0.6432 || timer: 0.0978 sec.
iter 356120 || Loss: 0.7289 || timer: 0.0959 sec.
iter 356130 || Loss: 0.5537 || timer: 0.1018 sec.
iter 356140 || Loss: 0.7237 || timer: 0.1033 sec.
iter 356150 || Loss: 0.8052 || timer: 0.0837 sec.
iter 356160 || Loss: 0.7411 || timer: 0.0861 sec.
iter 356170 || Loss: 0.5887 || timer: 0.0948 sec.
iter 356180 || Loss: 0.7800 || timer: 0.0839 sec.
iter 356190 || Loss: 0.8898 || timer: 0.0904 sec.
iter 356200 || Loss: 0.9405 || timer: 0.0825 sec.
iter 356210 || Loss: 0.6436 || timer: 0.1037 sec.
iter 356220 || Loss: 0.6900 || timer: 0.0820 sec.
iter 356230 || Loss: 0.6529 || timer: 0.0818 sec.
iter 356240 || Loss: 0.5585 || timer: 0.0997 sec.
iter 356250 || Loss: 0.4270 || timer: 0.0949 sec.
iter 356260 || Loss: 0.4493 || timer: 0.0896 sec.
iter 356270 || Loss: 0.8559 || timer: 0.0835 sec.
iter 356280 || Loss: 0.7036 || timer: 0.0879 sec.
iter 356290 || Loss: 0.7335 || timer: 0.0184 sec.
iter 356300 || Loss: 2.2019 || timer: 0.0902 sec.
iter 356310 || Loss: 0.6277 || timer: 0.0875 sec.
iter 356320 || Loss: 0.7565 || timer: 0.0892 sec.
iter 356330 || Loss: 0.5590 || timer: 0.0841 sec.
iter 356340 || Loss: 0.6068 || timer: 0.1068 sec.
iter 356350 || Loss: 0.4770 || timer: 0.0827 sec.
iter 356360 || Loss: 0.6379 || timer: 0.0899 sec.
iter 356370 || Loss: 0.7667 || timer: 0.0908 sec.
iter 356380 || Loss: 0.5314 || timer: 0.0945 sec.
iter 356390 || Loss: 0.6235 || timer: 0.0941 sec.
iter 356400 || Loss: 0.7855 || timer: 0.0890 sec.
iter 356410 || Loss: 0.6533 || timer: 0.0922 sec.
iter 356420 || Loss: 1.0402 || timer: 0.0908 sec.
iter 356430 || Loss: 1.0859 || timer: 0.0868 sec.
iter 356440 || Loss: 0.7767 || timer: 0.0903 sec.
iter 356450 || Loss: 0.6566 || timer: 0.0848 sec.
iter 356460 || Loss: 0.5314 || timer: 0.0897 sec.
iter 356470 || Loss: 0.7920 || timer: 0.0915 sec.
iter 356480 || Loss: 0.6786 || timer: 0.0920 sec.
iter 356490 || Loss: 0.6873 || timer: 0.0887 sec.
iter 356500 || Loss: 1.1917 || timer: 0.0907 sec.
iter 356510 || Loss: 0.9869 || timer: 0.0885 sec.
iter 356520 || Loss: 0.6434 || timer: 0.0959 sec.
iter 356530 || Loss: 0.7642 || timer: 0.0830 sec.
iter 356540 || Loss: 0.8565 || timer: 0.0871 sec.
iter 356550 || Loss: 0.5362 || timer: 0.1025 sec.
iter 356560 || Loss: 0.5621 || timer: 0.0877 sec.
iter 356570 || Loss: 0.6400 || timer: 0.0897 sec.
iter 356580 || Loss: 0.6268 || timer: 0.0843 sec.
iter 356590 || Loss: 0.9572 || timer: 0.0906 sec.
iter 356600 || Loss: 0.5371 || timer: 0.0998 sec.
iter 356610 || Loss: 0.8238 || timer: 0.0920 sec.
iter 356620 || Loss: 0.6263 || timer: 0.0172 sec.
iter 356630 || Loss: 3.4959 || timer: 0.0895 sec.
iter 356640 || Loss: 0.5584 || timer: 0.1086 sec.
iter 356650 || Loss: 0.4532 || timer: 0.0919 sec.
iter 356660 || Loss: 1.1414 || timer: 0.0826 sec.
iter 356670 || Loss: 0.7033 || timer: 0.0936 sec.
iter 356680 || Loss: 0.6961 || timer: 0.0861 sec.
iter 356690 || Loss: 0.6909 || timer: 0.0913 sec.
iter 356700 || Loss: 0.5236 || timer: 0.0827 sec.
iter 356710 || Loss: 0.5410 || timer: 0.0903 sec.
iter 356720 || Loss: 0.5623 || timer: 0.1108 sec.
iter 356730 || Loss: 0.5832 || timer: 0.0899 sec.
iter 356740 || Loss: 0.6391 || timer: 0.0994 sec.
iter 356750 || Loss: 0.9276 || timer: 0.0838 sec.
iter 356760 || Loss: 0.6469 || timer: 0.0827 sec.
iter 356770 || Loss: 0.6011 || timer: 0.1117 sec.
iter 356780 || Loss: 0.5641 || timer: 0.0934 sec.
iter 356790 || Loss: 0.6585 || timer: 0.0885 sec.
iter 356800 || Loss: 0.6885 || timer: 0.0908 sec.
iter 356810 || Loss: 0.7712 || timer: 0.0918 sec.
iter 356820 || Loss: 0.6867 || timer: 0.0830 sec.
iter 356830 || Loss: 0.6508 || timer: 0.1063 sec.
iter 356840 || Loss: 0.6095 || timer: 0.1069 sec.
iter 356850 || Loss: 0.3845 || timer: 0.0878 sec.
iter 356860 || Loss: 0.9208 || timer: 0.1083 sec.
iter 356870 || Loss: 0.7090 || timer: 0.1028 sec.
iter 356880 || Loss: 0.8297 || timer: 0.0847 sec.
iter 356890 || Loss: 0.8315 || timer: 0.0835 sec.
iter 356900 || Loss: 0.4946 || timer: 0.0873 sec.
iter 356910 || Loss: 0.8785 || timer: 0.0905 sec.
iter 356920 || Loss: 0.5459 || timer: 0.0829 sec.
iter 356930 || Loss: 0.8108 || timer: 0.0890 sec.
iter 356940 || Loss: 0.5312 || timer: 0.1135 sec.
iter 356950 || Loss: 0.4269 || timer: 0.0236 sec.
iter 356960 || Loss: 0.7012 || timer: 0.1140 sec.
iter 356970 || Loss: 0.6502 || timer: 0.0834 sec.
iter 356980 || Loss: 0.6393 || timer: 0.0987 sec.
iter 356990 || Loss: 0.9138 || timer: 0.0906 sec.
iter 357000 || Loss: 0.6161 || timer: 0.0844 sec.
iter 357010 || Loss: 0.5254 || timer: 0.0849 sec.
iter 357020 || Loss: 0.9648 || timer: 0.0836 sec.
iter 357030 || Loss: 0.4920 || timer: 0.1021 sec.
iter 357040 || Loss: 0.5888 || timer: 0.0964 sec.
iter 357050 || Loss: 0.6273 || timer: 0.1142 sec.
iter 357060 || Loss: 0.4423 || timer: 0.0946 sec.
iter 357070 || Loss: 1.2949 || timer: 0.0902 sec.
iter 357080 || Loss: 0.9428 || timer: 0.0923 sec.
iter 357090 || Loss: 0.6866 || timer: 0.0918 sec.
iter 357100 || Loss: 0.4702 || timer: 0.0919 sec.
iter 357110 || Loss: 0.8479 || timer: 0.0912 sec.
iter 357120 || Loss: 0.6395 || timer: 0.0913 sec.
iter 357130 || Loss: 0.7055 || timer: 0.0914 sec.
iter 357140 || Loss: 0.6094 || timer: 0.1023 sec.
iter 357150 || Loss: 0.7491 || timer: 0.1202 sec.
iter 357160 || Loss: 0.5794 || timer: 0.0872 sec.
iter 357170 || Loss: 0.7722 || timer: 0.0875 sec.
iter 357180 || Loss: 0.6923 || timer: 0.0898 sec.
iter 357190 || Loss: 0.8194 || timer: 0.0890 sec.
iter 357200 || Loss: 0.7339 || timer: 0.0915 sec.
iter 357210 || Loss: 0.6940 || timer: 0.0892 sec.
iter 357220 || Loss: 1.0000 || timer: 0.0957 sec.
iter 357230 || Loss: 0.8444 || timer: 0.0886 sec.
iter 357240 || Loss: 0.8824 || timer: 0.0911 sec.
iter 357250 || Loss: 0.5422 || timer: 0.0860 sec.
iter 357260 || Loss: 0.6363 || timer: 0.1042 sec.
iter 357270 || Loss: 0.6107 || timer: 0.0935 sec.
iter 357280 || Loss: 0.6605 || timer: 0.0246 sec.
iter 357290 || Loss: 0.7953 || timer: 0.0919 sec.
iter 357300 || Loss: 1.0630 || timer: 0.1101 sec.
iter 357310 || Loss: 0.6274 || timer: 0.0923 sec.
iter 357320 || Loss: 0.6301 || timer: 0.0904 sec.
iter 357330 || Loss: 0.4167 || timer: 0.1070 sec.
iter 357340 || Loss: 0.3954 || timer: 0.1117 sec.
iter 357350 || Loss: 0.6624 || timer: 0.1057 sec.
iter 357360 || Loss: 0.7721 || timer: 0.1074 sec.
iter 357370 || Loss: 0.8660 || timer: 0.0937 sec.
iter 357380 || Loss: 0.7658 || timer: 0.0990 sec.
iter 357390 || Loss: 0.7206 || timer: 0.0915 sec.
iter 357400 || Loss: 0.6335 || timer: 0.0941 sec.
iter 357410 || Loss: 0.6968 || timer: 0.0905 sec.
iter 357420 || Loss: 0.7253 || timer: 0.0877 sec.
iter 357430 || Loss: 0.9388 || timer: 0.0882 sec.
iter 357440 || Loss: 0.5556 || timer: 0.0964 sec.
iter 357450 || Loss: 0.7370 || timer: 0.0917 sec.
iter 357460 || Loss: 0.7362 || timer: 0.0956 sec.
iter 357470 || Loss: 0.6676 || timer: 0.1035 sec.
iter 357480 || Loss: 0.6619 || timer: 0.0913 sec.
iter 357490 || Loss: 0.7267 || timer: 0.0849 sec.
iter 357500 || Loss: 0.6112 || timer: 0.0889 sec.
iter 357510 || Loss: 0.9604 || timer: 0.0924 sec.
iter 357520 || Loss: 0.6816 || timer: 0.1185 sec.
iter 357530 || Loss: 0.5590 || timer: 0.1114 sec.
iter 357540 || Loss: 0.5018 || timer: 0.0899 sec.
iter 357550 || Loss: 0.6323 || timer: 0.0926 sec.
iter 357560 || Loss: 0.9866 || timer: 0.0836 sec.
iter 357570 || Loss: 0.6930 || timer: 0.0918 sec.
iter 357580 || Loss: 0.7035 || timer: 0.0885 sec.
iter 357590 || Loss: 0.6881 || timer: 0.0828 sec.
iter 357600 || Loss: 0.6433 || timer: 0.1086 sec.
iter 357610 || Loss: 0.6000 || timer: 0.0272 sec.
iter 357620 || Loss: 0.5614 || timer: 0.1272 sec.
iter 357630 || Loss: 0.7525 || timer: 0.0949 sec.
iter 357640 || Loss: 0.6349 || timer: 0.0839 sec.
iter 357650 || Loss: 0.6576 || timer: 0.0919 sec.
iter 357660 || Loss: 0.6957 || timer: 0.0915 sec.
iter 357670 || Loss: 0.7521 || timer: 0.1048 sec.
iter 357680 || Loss: 0.5420 || timer: 0.0829 sec.
iter 357690 || Loss: 1.3181 || timer: 0.0895 sec.
iter 357700 || Loss: 0.6440 || timer: 0.0910 sec.
iter 357710 || Loss: 0.4679 || timer: 0.0995 sec.
iter 357720 || Loss: 0.6000 || timer: 0.0902 sec.
iter 357730 || Loss: 0.6073 || timer: 0.1024 sec.
iter 357740 || Loss: 0.4501 || timer: 0.0888 sec.
iter 357750 || Loss: 0.5781 || timer: 0.0820 sec.
iter 357760 || Loss: 0.6130 || timer: 0.0832 sec.
iter 357770 || Loss: 0.5427 || timer: 0.1084 sec.
iter 357780 || Loss: 0.8330 || timer: 0.0865 sec.
iter 357790 || Loss: 0.7535 || timer: 0.0893 sec.
iter 357800 || Loss: 0.4911 || timer: 0.0925 sec.
iter 357810 || Loss: 0.8656 || timer: 0.1036 sec.
iter 357820 || Loss: 0.8874 || timer: 0.0821 sec.
iter 357830 || Loss: 0.5361 || timer: 0.0905 sec.
iter 357840 || Loss: 0.5091 || timer: 0.0852 sec.
iter 357850 || Loss: 0.8635 || timer: 0.0832 sec.
iter 357860 || Loss: 0.6598 || timer: 0.1029 sec.
iter 357870 || Loss: 0.6311 || timer: 0.0829 sec.
iter 357880 || Loss: 0.7339 || timer: 0.0829 sec.
iter 357890 || Loss: 0.5921 || timer: 0.0894 sec.
iter 357900 || Loss: 0.7704 || timer: 0.0894 sec.
iter 357910 || Loss: 0.5320 || timer: 0.0898 sec.
iter 357920 || Loss: 0.5493 || timer: 0.0905 sec.
iter 357930 || Loss: 0.5395 || timer: 0.0833 sec.
iter 357940 || Loss: 0.6393 || timer: 0.0188 sec.
iter 357950 || Loss: 0.8048 || timer: 0.0830 sec.
iter 357960 || Loss: 0.8249 || timer: 0.0828 sec.
iter 357970 || Loss: 0.6322 || timer: 0.0827 sec.
iter 357980 || Loss: 0.7741 || timer: 0.0863 sec.
iter 357990 || Loss: 0.6588 || timer: 0.0920 sec.
iter 358000 || Loss: 0.7358 || timer: 0.1085 sec.
iter 358010 || Loss: 0.8434 || timer: 0.0908 sec.
iter 358020 || Loss: 0.5684 || timer: 0.0873 sec.
iter 358030 || Loss: 0.7561 || timer: 0.0897 sec.
iter 358040 || Loss: 0.4822 || timer: 0.0969 sec.
iter 358050 || Loss: 0.5909 || timer: 0.0956 sec.
iter 358060 || Loss: 0.8035 || timer: 0.0838 sec.
iter 358070 || Loss: 0.8099 || timer: 0.0880 sec.
iter 358080 || Loss: 0.7408 || timer: 0.0937 sec.
iter 358090 || Loss: 0.7538 || timer: 0.0861 sec.
iter 358100 || Loss: 0.7524 || timer: 0.0920 sec.
iter 358110 || Loss: 0.6189 || timer: 0.0918 sec.
iter 358120 || Loss: 0.4531 || timer: 0.0827 sec.
iter 358130 || Loss: 0.6724 || timer: 0.0989 sec.
iter 358140 || Loss: 0.7646 || timer: 0.0944 sec.
iter 358150 || Loss: 0.6758 || timer: 0.0835 sec.
iter 358160 || Loss: 0.6202 || timer: 0.0858 sec.
iter 358170 || Loss: 0.6966 || timer: 0.0911 sec.
iter 358180 || Loss: 0.6139 || timer: 0.0837 sec.
iter 358190 || Loss: 0.8131 || timer: 0.0903 sec.
iter 358200 || Loss: 0.7420 || timer: 0.0838 sec.
iter 358210 || Loss: 0.8815 || timer: 0.0921 sec.
iter 358220 || Loss: 0.6362 || timer: 0.0895 sec.
iter 358230 || Loss: 0.7478 || timer: 0.0914 sec.
iter 358240 || Loss: 0.7136 || timer: 0.0897 sec.
iter 358250 || Loss: 0.6033 || timer: 0.1200 sec.
iter 358260 || Loss: 0.6507 || timer: 0.0873 sec.
iter 358270 || Loss: 0.8129 || timer: 0.0244 sec.
iter 358280 || Loss: 0.7911 || timer: 0.1027 sec.
iter 358290 || Loss: 0.5743 || timer: 0.0896 sec.
iter 358300 || Loss: 0.8399 || timer: 0.0910 sec.
iter 358310 || Loss: 0.5488 || timer: 0.0828 sec.
iter 358320 || Loss: 0.6010 || timer: 0.1100 sec.
iter 358330 || Loss: 0.6559 || timer: 0.0891 sec.
iter 358340 || Loss: 0.6931 || timer: 0.0955 sec.
iter 358350 || Loss: 0.7109 || timer: 0.0916 sec.
iter 358360 || Loss: 0.6034 || timer: 0.0858 sec.
iter 358370 || Loss: 0.8144 || timer: 0.1104 sec.
iter 358380 || Loss: 0.7948 || timer: 0.0909 sec.
iter 358390 || Loss: 0.7636 || timer: 0.0827 sec.
iter 358400 || Loss: 0.7689 || timer: 0.0895 sec.
iter 358410 || Loss: 0.7885 || timer: 0.0979 sec.
iter 358420 || Loss: 0.5857 || timer: 0.0907 sec.
iter 358430 || Loss: 0.9024 || timer: 0.0916 sec.
iter 358440 || Loss: 0.7873 || timer: 0.1083 sec.
iter 358450 || Loss: 0.6826 || timer: 0.1077 sec.
iter 358460 || Loss: 0.7926 || timer: 0.0871 sec.
iter 358470 || Loss: 0.6932 || timer: 0.0884 sec.
iter 358480 || Loss: 0.4679 || timer: 0.0874 sec.
iter 358490 || Loss: 0.7024 || timer: 0.1126 sec.
iter 358500 || Loss: 0.7846 || timer: 0.0909 sec.
iter 358510 || Loss: 0.5714 || timer: 0.0916 sec.
iter 358520 || Loss: 0.6863 || timer: 0.0880 sec.
iter 358530 || Loss: 0.6810 || timer: 0.1174 sec.
iter 358540 || Loss: 0.7987 || timer: 0.0967 sec.
iter 358550 || Loss: 0.9697 || timer: 0.0868 sec.
iter 358560 || Loss: 0.6756 || timer: 0.0860 sec.
iter 358570 || Loss: 0.6242 || timer: 0.0822 sec.
iter 358580 || Loss: 0.8443 || timer: 0.1029 sec.
iter 358590 || Loss: 0.6980 || timer: 0.0848 sec.
iter 358600 || Loss: 0.5940 || timer: 0.0212 sec.
iter 358610 || Loss: 0.1914 || timer: 0.0852 sec.
iter 358620 || Loss: 0.7347 || timer: 0.0914 sec.
iter 358630 || Loss: 0.6423 || timer: 0.0912 sec.
iter 358640 || Loss: 0.7080 || timer: 0.0881 sec.
iter 358650 || Loss: 0.5413 || timer: 0.1156 sec.
iter 358660 || Loss: 0.7106 || timer: 0.0902 sec.
iter 358670 || Loss: 0.5585 || timer: 0.0924 sec.
iter 358680 || Loss: 1.0908 || timer: 0.0904 sec.
iter 358690 || Loss: 0.5891 || timer: 0.1037 sec.
iter 358700 || Loss: 0.6718 || timer: 0.1042 sec.
iter 358710 || Loss: 0.4707 || timer: 0.0877 sec.
iter 358720 || Loss: 0.6505 || timer: 0.1078 sec.
iter 358730 || Loss: 0.8011 || timer: 0.0906 sec.
iter 358740 || Loss: 0.7345 || timer: 0.0907 sec.
iter 358750 || Loss: 0.7026 || timer: 0.0856 sec.
iter 358760 || Loss: 0.8628 || timer: 0.1192 sec.
iter 358770 || Loss: 0.8156 || timer: 0.1134 sec.
iter 358780 || Loss: 0.6356 || timer: 0.1160 sec.
iter 358790 || Loss: 0.4594 || timer: 0.0878 sec.
iter 358800 || Loss: 0.8432 || timer: 0.0932 sec.
iter 358810 || Loss: 0.9367 || timer: 0.0935 sec.
iter 358820 || Loss: 0.6601 || timer: 0.0908 sec.
iter 358830 || Loss: 0.4908 || timer: 0.0916 sec.
iter 358840 || Loss: 0.4267 || timer: 0.0840 sec.
iter 358850 || Loss: 0.7298 || timer: 0.0883 sec.
iter 358860 || Loss: 0.5485 || timer: 0.0831 sec.
iter 358870 || Loss: 0.8471 || timer: 0.0894 sec.
iter 358880 || Loss: 0.8372 || timer: 0.1106 sec.
iter 358890 || Loss: 0.7761 || timer: 0.1008 sec.
iter 358900 || Loss: 0.6454 || timer: 0.0929 sec.
iter 358910 || Loss: 0.7857 || timer: 0.0932 sec.
iter 358920 || Loss: 0.8404 || timer: 0.0843 sec.
iter 358930 || Loss: 0.9307 || timer: 0.0245 sec.
iter 358940 || Loss: 0.6092 || timer: 0.0864 sec.
iter 358950 || Loss: 0.6962 || timer: 0.0998 sec.
iter 358960 || Loss: 0.6425 || timer: 0.1062 sec.
iter 358970 || Loss: 0.4063 || timer: 0.0949 sec.
iter 358980 || Loss: 0.7772 || timer: 0.0916 sec.
iter 358990 || Loss: 0.6816 || timer: 0.0933 sec.
iter 359000 || Loss: 0.4934 || timer: 0.0842 sec.
iter 359010 || Loss: 0.6330 || timer: 0.0896 sec.
iter 359020 || Loss: 0.7572 || timer: 0.0894 sec.
iter 359030 || Loss: 0.6822 || timer: 0.0959 sec.
iter 359040 || Loss: 0.7137 || timer: 0.0929 sec.
iter 359050 || Loss: 0.4805 || timer: 0.1004 sec.
iter 359060 || Loss: 0.7266 || timer: 0.0923 sec.
iter 359070 || Loss: 0.7370 || timer: 0.0928 sec.
iter 359080 || Loss: 0.8683 || timer: 0.0845 sec.
iter 359090 || Loss: 0.7223 || timer: 0.0895 sec.
iter 359100 || Loss: 0.5153 || timer: 0.0916 sec.
iter 359110 || Loss: 0.6940 || timer: 0.1012 sec.
iter 359120 || Loss: 0.8299 || timer: 0.0882 sec.
iter 359130 || Loss: 0.6334 || timer: 0.1049 sec.
iter 359140 || Loss: 0.7012 || timer: 0.1053 sec.
iter 359150 || Loss: 0.5611 || timer: 0.0902 sec.
iter 359160 || Loss: 0.3997 || timer: 0.0895 sec.
iter 359170 || Loss: 0.6213 || timer: 0.1110 sec.
iter 359180 || Loss: 0.6908 || timer: 0.0843 sec.
iter 359190 || Loss: 0.7627 || timer: 0.0905 sec.
iter 359200 || Loss: 0.6929 || timer: 0.0908 sec.
iter 359210 || Loss: 0.6363 || timer: 0.0888 sec.
iter 359220 || Loss: 0.6595 || timer: 0.0894 sec.
iter 359230 || Loss: 0.6871 || timer: 0.0830 sec.
iter 359240 || Loss: 0.7730 || timer: 0.0911 sec.
iter 359250 || Loss: 0.4979 || timer: 0.1071 sec.
iter 359260 || Loss: 0.8046 || timer: 0.0170 sec.
iter 359270 || Loss: 0.7579 || timer: 0.0911 sec.
iter 359280 || Loss: 0.7084 || timer: 0.0901 sec.
iter 359290 || Loss: 0.6224 || timer: 0.0857 sec.
iter 359300 || Loss: 0.8703 || timer: 0.0833 sec.
iter 359310 || Loss: 0.4910 || timer: 0.1064 sec.
iter 359320 || Loss: 0.3354 || timer: 0.1012 sec.
iter 359330 || Loss: 0.8659 || timer: 0.0951 sec.
iter 359340 || Loss: 0.6448 || timer: 0.1069 sec.
iter 359350 || Loss: 0.5254 || timer: 0.0893 sec.
iter 359360 || Loss: 1.1279 || timer: 0.1095 sec.
iter 359370 || Loss: 0.5400 || timer: 0.0917 sec.
iter 359380 || Loss: 0.8875 || timer: 0.0893 sec.
iter 359390 || Loss: 0.5659 || timer: 0.0898 sec.
iter 359400 || Loss: 0.5096 || timer: 0.0895 sec.
iter 359410 || Loss: 0.6117 || timer: 0.0899 sec.
iter 359420 || Loss: 0.6321 || timer: 0.0908 sec.
iter 359430 || Loss: 0.7158 || timer: 0.1152 sec.
iter 359440 || Loss: 0.5492 || timer: 0.0869 sec.
iter 359450 || Loss: 0.6190 || timer: 0.0834 sec.
iter 359460 || Loss: 0.6811 || timer: 0.0953 sec.
iter 359470 || Loss: 0.5953 || timer: 0.0836 sec.
iter 359480 || Loss: 0.7692 || timer: 0.0869 sec.
iter 359490 || Loss: 0.5865 || timer: 0.0922 sec.
iter 359500 || Loss: 0.7224 || timer: 0.0881 sec.
iter 359510 || Loss: 0.5918 || timer: 0.0882 sec.
iter 359520 || Loss: 0.7665 || timer: 0.0828 sec.
iter 359530 || Loss: 0.6657 || timer: 0.0993 sec.
iter 359540 || Loss: 0.6904 || timer: 0.0929 sec.
iter 359550 || Loss: 0.6184 || timer: 0.0926 sec.
iter 359560 || Loss: 0.9694 || timer: 0.0825 sec.
iter 359570 || Loss: 0.6818 || timer: 0.0967 sec.
iter 359580 || Loss: 0.7253 || timer: 0.0902 sec.
iter 359590 || Loss: 0.6446 || timer: 0.0246 sec.
iter 359600 || Loss: 0.9815 || timer: 0.1427 sec.
iter 359610 || Loss: 0.5626 || timer: 0.0823 sec.
iter 359620 || Loss: 0.6009 || timer: 0.1094 sec.
iter 359630 || Loss: 0.5925 || timer: 0.0913 sec.
iter 359640 || Loss: 0.5691 || timer: 0.0834 sec.
iter 359650 || Loss: 0.8642 || timer: 0.0856 sec.
iter 359660 || Loss: 0.4835 || timer: 0.0829 sec.
iter 359670 || Loss: 0.7586 || timer: 0.0951 sec.
iter 359680 || Loss: 0.7092 || timer: 0.0925 sec.
iter 359690 || Loss: 0.5846 || timer: 0.0990 sec.
iter 359700 || Loss: 0.5842 || timer: 0.0885 sec.
iter 359710 || Loss: 0.6211 || timer: 0.0835 sec.
iter 359720 || Loss: 0.9424 || timer: 0.0838 sec.
iter 359730 || Loss: 0.8520 || timer: 0.0824 sec.
iter 359740 || Loss: 0.6424 || timer: 0.0924 sec.
iter 359750 || Loss: 0.8435 || timer: 0.0840 sec.
iter 359760 || Loss: 0.7030 || timer: 0.0832 sec.
iter 359770 || Loss: 0.7062 || timer: 0.0950 sec.
iter 359780 || Loss: 0.6428 || timer: 0.0830 sec.
iter 359790 || Loss: 0.7230 || timer: 0.0838 sec.
iter 359800 || Loss: 0.9481 || timer: 0.0923 sec.
iter 359810 || Loss: 0.8375 || timer: 0.0824 sec.
iter 359820 || Loss: 0.7993 || timer: 0.0893 sec.
iter 359830 || Loss: 0.6058 || timer: 0.0897 sec.
iter 359840 || Loss: 0.8290 || timer: 0.0760 sec.
iter 359850 || Loss: 0.7611 || timer: 0.0900 sec.
iter 359860 || Loss: 0.5819 || timer: 0.0761 sec.
iter 359870 || Loss: 0.7158 || timer: 0.0899 sec.
iter 359880 || Loss: 0.4346 || timer: 0.0877 sec.
iter 359890 || Loss: 0.6697 || timer: 0.0830 sec.
iter 359900 || Loss: 0.5116 || timer: 0.1046 sec.
iter 359910 || Loss: 0.9691 || timer: 0.0824 sec.
iter 359920 || Loss: 0.4784 || timer: 0.0268 sec.
iter 359930 || Loss: 0.2181 || timer: 0.0786 sec.
iter 359940 || Loss: 0.6379 || timer: 0.0901 sec.
iter 359950 || Loss: 0.9268 || timer: 0.0969 sec.
iter 359960 || Loss: 0.6163 || timer: 0.0871 sec.
iter 359970 || Loss: 0.5702 || timer: 0.0817 sec.
iter 359980 || Loss: 0.7747 || timer: 0.0922 sec.
iter 359990 || Loss: 0.6360 || timer: 0.0910 sec.
iter 360000 || Loss: 0.7173 || Saving state, iter: 360000
timer: 0.0756 sec.
iter 360010 || Loss: 0.8281 || timer: 0.0907 sec.
iter 360020 || Loss: 0.7962 || timer: 0.0969 sec.
iter 360030 || Loss: 0.6395 || timer: 0.0972 sec.
iter 360040 || Loss: 0.9067 || timer: 0.0884 sec.
iter 360050 || Loss: 0.7848 || timer: 0.0822 sec.
iter 360060 || Loss: 0.7980 || timer: 0.0827 sec.
iter 360070 || Loss: 0.7278 || timer: 0.1047 sec.
iter 360080 || Loss: 0.5767 || timer: 0.0833 sec.
iter 360090 || Loss: 0.8283 || timer: 0.0908 sec.
iter 360100 || Loss: 0.7320 || timer: 0.0830 sec.
iter 360110 || Loss: 0.8334 || timer: 0.0904 sec.
iter 360120 || Loss: 0.7385 || timer: 0.0785 sec.
iter 360130 || Loss: 0.8461 || timer: 0.0760 sec.
iter 360140 || Loss: 0.9288 || timer: 0.0984 sec.
iter 360150 || Loss: 0.7543 || timer: 0.0850 sec.
iter 360160 || Loss: 1.0058 || timer: 0.1271 sec.
iter 360170 || Loss: 0.8034 || timer: 0.0874 sec.
iter 360180 || Loss: 0.4818 || timer: 0.0895 sec.
iter 360190 || Loss: 0.7260 || timer: 0.0931 sec.
iter 360200 || Loss: 0.4163 || timer: 0.0897 sec.
iter 360210 || Loss: 0.6586 || timer: 0.0902 sec.
iter 360220 || Loss: 0.5954 || timer: 0.0908 sec.
iter 360230 || Loss: 0.5593 || timer: 0.1368 sec.
iter 360240 || Loss: 0.7495 || timer: 0.1056 sec.
iter 360250 || Loss: 0.5226 || timer: 0.0265 sec.
iter 360260 || Loss: 0.4591 || timer: 0.0913 sec.
iter 360270 || Loss: 0.7690 || timer: 0.1118 sec.
iter 360280 || Loss: 0.7317 || timer: 0.0882 sec.
iter 360290 || Loss: 0.5160 || timer: 0.0837 sec.
iter 360300 || Loss: 0.7818 || timer: 0.0948 sec.
iter 360310 || Loss: 0.5979 || timer: 0.0849 sec.
iter 360320 || Loss: 0.6012 || timer: 0.0958 sec.
iter 360330 || Loss: 0.5511 || timer: 0.0888 sec.
iter 360340 || Loss: 0.9270 || timer: 0.0967 sec.
iter 360350 || Loss: 0.6474 || timer: 0.0993 sec.
iter 360360 || Loss: 0.6931 || timer: 0.0819 sec.
iter 360370 || Loss: 0.7410 || timer: 0.1001 sec.
iter 360380 || Loss: 0.6057 || timer: 0.0792 sec.
iter 360390 || Loss: 0.7842 || timer: 0.0814 sec.
iter 360400 || Loss: 0.6341 || timer: 0.0897 sec.
iter 360410 || Loss: 0.6280 || timer: 0.0999 sec.
iter 360420 || Loss: 1.0478 || timer: 0.1024 sec.
iter 360430 || Loss: 1.0221 || timer: 0.0900 sec.
iter 360440 || Loss: 0.6980 || timer: 0.0857 sec.
iter 360450 || Loss: 0.8224 || timer: 0.0762 sec.
iter 360460 || Loss: 0.6134 || timer: 0.0840 sec.
iter 360470 || Loss: 0.5927 || timer: 0.0826 sec.
iter 360480 || Loss: 0.8248 || timer: 0.0846 sec.
iter 360490 || Loss: 0.6482 || timer: 0.0888 sec.
iter 360500 || Loss: 0.6541 || timer: 0.0822 sec.
iter 360510 || Loss: 0.7944 || timer: 0.0837 sec.
iter 360520 || Loss: 0.6428 || timer: 0.0744 sec.
iter 360530 || Loss: 0.6971 || timer: 0.0820 sec.
iter 360540 || Loss: 0.6314 || timer: 0.0823 sec.
iter 360550 || Loss: 0.5290 || timer: 0.0921 sec.
iter 360560 || Loss: 0.6174 || timer: 0.1044 sec.
iter 360570 || Loss: 0.8468 || timer: 0.1068 sec.
iter 360580 || Loss: 0.6840 || timer: 0.0282 sec.
iter 360590 || Loss: 0.1524 || timer: 0.0979 sec.
iter 360600 || Loss: 0.6150 || timer: 0.0759 sec.
iter 360610 || Loss: 0.7103 || timer: 0.0811 sec.
iter 360620 || Loss: 0.3941 || timer: 0.0772 sec.
iter 360630 || Loss: 0.5897 || timer: 0.1023 sec.
iter 360640 || Loss: 0.5777 || timer: 0.0906 sec.
iter 360650 || Loss: 0.5581 || timer: 0.0832 sec.
iter 360660 || Loss: 0.7264 || timer: 0.0907 sec.
iter 360670 || Loss: 0.5503 || timer: 0.0919 sec.
iter 360680 || Loss: 0.5456 || timer: 0.1272 sec.
iter 360690 || Loss: 0.6371 || timer: 0.0844 sec.
iter 360700 || Loss: 0.7345 || timer: 0.0905 sec.
iter 360710 || Loss: 1.0868 || timer: 0.0738 sec.
iter 360720 || Loss: 0.9094 || timer: 0.0835 sec.
iter 360730 || Loss: 0.9604 || timer: 0.1079 sec.
iter 360740 || Loss: 0.3657 || timer: 0.0816 sec.
iter 360750 || Loss: 0.8114 || timer: 0.0908 sec.
iter 360760 || Loss: 0.7374 || timer: 0.0992 sec.
iter 360770 || Loss: 0.6808 || timer: 0.0884 sec.
iter 360780 || Loss: 0.7256 || timer: 0.0815 sec.
iter 360790 || Loss: 0.7013 || timer: 0.0912 sec.
iter 360800 || Loss: 1.0369 || timer: 0.0829 sec.
iter 360810 || Loss: 0.6539 || timer: 0.0910 sec.
iter 360820 || Loss: 0.7054 || timer: 0.0825 sec.
iter 360830 || Loss: 0.6624 || timer: 0.0894 sec.
iter 360840 || Loss: 0.4263 || timer: 0.0866 sec.
iter 360850 || Loss: 0.5270 || timer: 0.1025 sec.
iter 360860 || Loss: 0.6364 || timer: 0.0822 sec.
iter 360870 || Loss: 0.6570 || timer: 0.0896 sec.
iter 360880 || Loss: 0.6081 || timer: 0.0848 sec.
iter 360890 || Loss: 0.7824 || timer: 0.0991 sec.
iter 360900 || Loss: 0.8052 || timer: 0.0895 sec.
iter 360910 || Loss: 0.5767 || timer: 0.0225 sec.
iter 360920 || Loss: 0.3996 || timer: 0.0895 sec.
iter 360930 || Loss: 1.0742 || timer: 0.0825 sec.
iter 360940 || Loss: 0.8290 || timer: 0.0808 sec.
iter 360950 || Loss: 0.7591 || timer: 0.0828 sec.
iter 360960 || Loss: 0.6090 || timer: 0.0826 sec.
iter 360970 || Loss: 0.6935 || timer: 0.0896 sec.
iter 360980 || Loss: 0.8012 || timer: 0.0825 sec.
iter 360990 || Loss: 0.7009 || timer: 0.0823 sec.
iter 361000 || Loss: 0.8415 || timer: 0.0828 sec.
iter 361010 || Loss: 0.8246 || timer: 0.0955 sec.
iter 361020 || Loss: 0.6879 || timer: 0.1046 sec.
iter 361030 || Loss: 0.5182 || timer: 0.0889 sec.
iter 361040 || Loss: 0.8022 || timer: 0.0844 sec.
iter 361050 || Loss: 0.7435 || timer: 0.0841 sec.
iter 361060 || Loss: 0.6205 || timer: 0.0988 sec.
iter 361070 || Loss: 0.6538 || timer: 0.0860 sec.
iter 361080 || Loss: 0.5036 || timer: 0.0902 sec.
iter 361090 || Loss: 0.6739 || timer: 0.0876 sec.
iter 361100 || Loss: 0.5870 || timer: 0.0936 sec.
iter 361110 || Loss: 0.6358 || timer: 0.0826 sec.
iter 361120 || Loss: 0.9690 || timer: 0.0895 sec.
iter 361130 || Loss: 0.5568 || timer: 0.0900 sec.
iter 361140 || Loss: 0.6833 || timer: 0.0732 sec.
iter 361150 || Loss: 0.5342 || timer: 0.0826 sec.
iter 361160 || Loss: 0.5168 || timer: 0.0889 sec.
iter 361170 || Loss: 0.7234 || timer: 0.0886 sec.
iter 361180 || Loss: 0.7220 || timer: 0.0747 sec.
iter 361190 || Loss: 0.8107 || timer: 0.0747 sec.
iter 361200 || Loss: 0.6128 || timer: 0.0887 sec.
iter 361210 || Loss: 0.5897 || timer: 0.0901 sec.
iter 361220 || Loss: 0.6646 || timer: 0.0818 sec.
iter 361230 || Loss: 0.6460 || timer: 0.0813 sec.
iter 361240 || Loss: 0.4334 || timer: 0.0215 sec.
iter 361250 || Loss: 0.8205 || timer: 0.0897 sec.
iter 361260 || Loss: 0.5592 || timer: 0.0888 sec.
iter 361270 || Loss: 0.7657 || timer: 0.0886 sec.
iter 361280 || Loss: 0.4796 || timer: 0.0756 sec.
iter 361290 || Loss: 0.7396 || timer: 0.0846 sec.
iter 361300 || Loss: 0.5806 || timer: 0.0890 sec.
iter 361310 || Loss: 0.5934 || timer: 0.0889 sec.
iter 361320 || Loss: 0.6206 || timer: 0.0883 sec.
iter 361330 || Loss: 0.7702 || timer: 0.0853 sec.
iter 361340 || Loss: 0.6546 || timer: 0.0876 sec.
iter 361350 || Loss: 0.8783 || timer: 0.0910 sec.
iter 361360 || Loss: 0.8399 || timer: 0.0825 sec.
iter 361370 || Loss: 0.4375 || timer: 0.0813 sec.
iter 361380 || Loss: 0.4295 || timer: 0.0885 sec.
iter 361390 || Loss: 0.7155 || timer: 0.0886 sec.
iter 361400 || Loss: 0.8477 || timer: 0.0881 sec.
iter 361410 || Loss: 0.6287 || timer: 0.0818 sec.
iter 361420 || Loss: 0.4740 || timer: 0.0904 sec.
iter 361430 || Loss: 0.6665 || timer: 0.0828 sec.
iter 361440 || Loss: 0.8567 || timer: 0.0885 sec.
iter 361450 || Loss: 0.7754 || timer: 0.0858 sec.
iter 361460 || Loss: 0.8525 || timer: 0.0897 sec.
iter 361470 || Loss: 0.8642 || timer: 0.0720 sec.
iter 361480 || Loss: 0.5390 || timer: 0.0893 sec.
iter 361490 || Loss: 0.5312 || timer: 0.0899 sec.
iter 361500 || Loss: 0.6997 || timer: 0.1086 sec.
iter 361510 || Loss: 0.6680 || timer: 0.0827 sec.
iter 361520 || Loss: 0.5148 || timer: 0.0927 sec.
iter 361530 || Loss: 0.5456 || timer: 0.0814 sec.
iter 361540 || Loss: 0.8305 || timer: 0.0822 sec.
iter 361550 || Loss: 0.6069 || timer: 0.0822 sec.
iter 361560 || Loss: 0.7325 || timer: 0.0876 sec.
iter 361570 || Loss: 0.7830 || timer: 0.0230 sec.
iter 361580 || Loss: 0.5006 || timer: 0.1093 sec.
iter 361590 || Loss: 0.7047 || timer: 0.1177 sec.
iter 361600 || Loss: 0.9315 || timer: 0.0887 sec.
iter 361610 || Loss: 0.5431 || timer: 0.0874 sec.
iter 361620 || Loss: 0.7514 || timer: 0.0894 sec.
iter 361630 || Loss: 0.6661 || timer: 0.0898 sec.
iter 361640 || Loss: 0.4562 || timer: 0.0827 sec.
iter 361650 || Loss: 0.7384 || timer: 0.0901 sec.
iter 361660 || Loss: 0.5464 || timer: 0.0823 sec.
iter 361670 || Loss: 0.7270 || timer: 0.1170 sec.
iter 361680 || Loss: 0.6014 || timer: 0.0884 sec.
iter 361690 || Loss: 0.4822 || timer: 0.0890 sec.
iter 361700 || Loss: 0.6423 || timer: 0.0889 sec.
iter 361710 || Loss: 1.1132 || timer: 0.0900 sec.
iter 361720 || Loss: 0.8040 || timer: 0.0912 sec.
iter 361730 || Loss: 0.6934 || timer: 0.0905 sec.
iter 361740 || Loss: 0.7393 || timer: 0.1095 sec.
iter 361750 || Loss: 0.7177 || timer: 0.0832 sec.
iter 361760 || Loss: 1.0453 || timer: 0.0908 sec.
iter 361770 || Loss: 0.4478 || timer: 0.0972 sec.
iter 361780 || Loss: 0.4903 || timer: 0.0827 sec.
iter 361790 || Loss: 0.9199 || timer: 0.1019 sec.
iter 361800 || Loss: 0.5048 || timer: 0.0856 sec.
iter 361810 || Loss: 0.7052 || timer: 0.1000 sec.
iter 361820 || Loss: 0.8126 || timer: 0.0838 sec.
iter 361830 || Loss: 0.6370 || timer: 0.1006 sec.
iter 361840 || Loss: 0.5982 || timer: 0.0919 sec.
iter 361850 || Loss: 0.7662 || timer: 0.0932 sec.
iter 361860 || Loss: 0.6531 || timer: 0.0898 sec.
iter 361870 || Loss: 0.6676 || timer: 0.0828 sec.
iter 361880 || Loss: 0.7464 || timer: 0.0891 sec.
iter 361890 || Loss: 0.6928 || timer: 0.0921 sec.
iter 361900 || Loss: 0.5098 || timer: 0.0209 sec.
iter 361910 || Loss: 0.2044 || timer: 0.0916 sec.
iter 361920 || Loss: 0.5223 || timer: 0.0896 sec.
iter 361930 || Loss: 0.8295 || timer: 0.0814 sec.
iter 361940 || Loss: 0.5904 || timer: 0.0899 sec.
iter 361950 || Loss: 0.7492 || timer: 0.0874 sec.
iter 361960 || Loss: 0.8348 || timer: 0.0877 sec.
iter 361970 || Loss: 0.6721 || timer: 0.0830 sec.
iter 361980 || Loss: 0.7147 || timer: 0.0898 sec.
iter 361990 || Loss: 0.5910 || timer: 0.0902 sec.
iter 362000 || Loss: 0.5083 || timer: 0.0898 sec.
iter 362010 || Loss: 0.7053 || timer: 0.0740 sec.
iter 362020 || Loss: 0.6942 || timer: 0.0791 sec.
iter 362030 || Loss: 0.7291 || timer: 0.0734 sec.
iter 362040 || Loss: 0.6371 || timer: 0.0905 sec.
iter 362050 || Loss: 0.6056 || timer: 0.0822 sec.
iter 362060 || Loss: 0.6698 || timer: 0.0822 sec.
iter 362070 || Loss: 0.7487 || timer: 0.1055 sec.
iter 362080 || Loss: 0.6523 || timer: 0.1112 sec.
iter 362090 || Loss: 0.5980 || timer: 0.0854 sec.
iter 362100 || Loss: 0.7025 || timer: 0.0910 sec.
iter 362110 || Loss: 0.7165 || timer: 0.0960 sec.
iter 362120 || Loss: 0.4555 || timer: 0.0830 sec.
iter 362130 || Loss: 0.4423 || timer: 0.0889 sec.
iter 362140 || Loss: 0.7503 || timer: 0.0912 sec.
iter 362150 || Loss: 0.8837 || timer: 0.0894 sec.
iter 362160 || Loss: 0.5790 || timer: 0.0820 sec.
iter 362170 || Loss: 0.5479 || timer: 0.0832 sec.
iter 362180 || Loss: 0.5141 || timer: 0.0976 sec.
iter 362190 || Loss: 0.7007 || timer: 0.0867 sec.
iter 362200 || Loss: 0.5479 || timer: 0.0891 sec.
iter 362210 || Loss: 0.6019 || timer: 0.0889 sec.
iter 362220 || Loss: 0.5034 || timer: 0.0830 sec.
iter 362230 || Loss: 0.4824 || timer: 0.0250 sec.
iter 362240 || Loss: 0.0792 || timer: 0.0825 sec.
iter 362250 || Loss: 0.5825 || timer: 0.0968 sec.
iter 362260 || Loss: 0.4319 || timer: 0.1023 sec.
iter 362270 || Loss: 0.6451 || timer: 0.0754 sec.
iter 362280 || Loss: 0.8567 || timer: 0.0992 sec.
iter 362290 || Loss: 0.7403 || timer: 0.0972 sec.
iter 362300 || Loss: 0.4160 || timer: 0.1003 sec.
iter 362310 || Loss: 0.6078 || timer: 0.0905 sec.
iter 362320 || Loss: 0.5584 || timer: 0.0908 sec.
iter 362330 || Loss: 0.5194 || timer: 0.1006 sec.
iter 362340 || Loss: 0.6259 || timer: 0.0897 sec.
iter 362350 || Loss: 0.5560 || timer: 0.0832 sec.
iter 362360 || Loss: 0.5739 || timer: 0.0887 sec.
iter 362370 || Loss: 0.4423 || timer: 0.0835 sec.
iter 362380 || Loss: 0.5145 || timer: 0.0893 sec.
iter 362390 || Loss: 0.7480 || timer: 0.1033 sec.
iter 362400 || Loss: 0.4566 || timer: 0.0892 sec.
iter 362410 || Loss: 0.5325 || timer: 0.0920 sec.
iter 362420 || Loss: 0.7590 || timer: 0.0954 sec.
iter 362430 || Loss: 0.7046 || timer: 0.0760 sec.
iter 362440 || Loss: 0.8700 || timer: 0.0840 sec.
iter 362450 || Loss: 0.6423 || timer: 0.0834 sec.
iter 362460 || Loss: 0.7330 || timer: 0.0826 sec.
iter 362470 || Loss: 0.9129 || timer: 0.0903 sec.
iter 362480 || Loss: 0.6234 || timer: 0.0753 sec.
iter 362490 || Loss: 1.1545 || timer: 0.0815 sec.
iter 362500 || Loss: 0.5363 || timer: 0.0835 sec.
iter 362510 || Loss: 0.6635 || timer: 0.0758 sec.
iter 362520 || Loss: 0.6440 || timer: 0.0825 sec.
iter 362530 || Loss: 0.7574 || timer: 0.0907 sec.
iter 362540 || Loss: 0.7199 || timer: 0.0819 sec.
iter 362550 || Loss: 0.6848 || timer: 0.0894 sec.
iter 362560 || Loss: 1.0076 || timer: 0.0255 sec.
iter 362570 || Loss: 0.9580 || timer: 0.0863 sec.
iter 362580 || Loss: 0.5929 || timer: 0.1154 sec.
iter 362590 || Loss: 0.6570 || timer: 0.0932 sec.
iter 362600 || Loss: 0.7880 || timer: 0.0944 sec.
iter 362610 || Loss: 0.6296 || timer: 0.0856 sec.
iter 362620 || Loss: 0.7625 || timer: 0.0831 sec.
iter 362630 || Loss: 0.6926 || timer: 0.1006 sec.
iter 362640 || Loss: 0.6471 || timer: 0.0841 sec.
iter 362650 || Loss: 0.5822 || timer: 0.0897 sec.
iter 362660 || Loss: 0.5905 || timer: 0.1057 sec.
iter 362670 || Loss: 0.6315 || timer: 0.0840 sec.
iter 362680 || Loss: 0.9462 || timer: 0.1066 sec.
iter 362690 || Loss: 0.8536 || timer: 0.0831 sec.
iter 362700 || Loss: 0.5606 || timer: 0.0898 sec.
iter 362710 || Loss: 0.7187 || timer: 0.0999 sec.
iter 362720 || Loss: 0.6828 || timer: 0.0897 sec.
iter 362730 || Loss: 0.9174 || timer: 0.0897 sec.
iter 362740 || Loss: 0.7637 || timer: 0.0915 sec.
iter 362750 || Loss: 1.1879 || timer: 0.0905 sec.
iter 362760 || Loss: 0.5324 || timer: 0.0915 sec.
iter 362770 || Loss: 0.5018 || timer: 0.1124 sec.
iter 362780 || Loss: 0.8681 || timer: 0.0834 sec.
iter 362790 || Loss: 0.9360 || timer: 0.0902 sec.
iter 362800 || Loss: 0.6783 || timer: 0.0853 sec.
iter 362810 || Loss: 0.7243 || timer: 0.0882 sec.
iter 362820 || Loss: 0.8271 || timer: 0.1023 sec.
iter 362830 || Loss: 0.6736 || timer: 0.1051 sec.
iter 362840 || Loss: 0.5313 || timer: 0.0834 sec.
iter 362850 || Loss: 0.7473 || timer: 0.0914 sec.
iter 362860 || Loss: 0.6944 || timer: 0.0897 sec.
iter 362870 || Loss: 0.7367 || timer: 0.0892 sec.
iter 362880 || Loss: 0.6939 || timer: 0.0919 sec.
iter 362890 || Loss: 0.7591 || timer: 0.0170 sec.
iter 362900 || Loss: 0.6734 || timer: 0.0905 sec.
iter 362910 || Loss: 0.6131 || timer: 0.0923 sec.
iter 362920 || Loss: 0.6502 || timer: 0.0946 sec.
iter 362930 || Loss: 0.8660 || timer: 0.0839 sec.
iter 362940 || Loss: 0.5907 || timer: 0.0820 sec.
iter 362950 || Loss: 0.6346 || timer: 0.0920 sec.
iter 362960 || Loss: 0.9522 || timer: 0.0919 sec.
iter 362970 || Loss: 0.9471 || timer: 0.0938 sec.
iter 362980 || Loss: 0.6167 || timer: 0.0939 sec.
iter 362990 || Loss: 0.9252 || timer: 0.1014 sec.
iter 363000 || Loss: 0.6070 || timer: 0.1044 sec.
iter 363010 || Loss: 0.9950 || timer: 0.0951 sec.
iter 363020 || Loss: 0.9772 || timer: 0.0839 sec.
iter 363030 || Loss: 0.6253 || timer: 0.0824 sec.
iter 363040 || Loss: 0.8454 || timer: 0.0825 sec.
iter 363050 || Loss: 0.7524 || timer: 0.0920 sec.
iter 363060 || Loss: 0.7189 || timer: 0.0912 sec.
iter 363070 || Loss: 0.9193 || timer: 0.0863 sec.
iter 363080 || Loss: 0.6446 || timer: 0.0882 sec.
iter 363090 || Loss: 0.8391 || timer: 0.0841 sec.
iter 363100 || Loss: 0.6231 || timer: 0.0939 sec.
iter 363110 || Loss: 1.1057 || timer: 0.0906 sec.
iter 363120 || Loss: 0.5611 || timer: 0.0883 sec.
iter 363130 || Loss: 0.7199 || timer: 0.0923 sec.
iter 363140 || Loss: 0.7137 || timer: 0.0904 sec.
iter 363150 || Loss: 0.4615 || timer: 0.0768 sec.
iter 363160 || Loss: 0.5526 || timer: 0.0881 sec.
iter 363170 || Loss: 0.7895 || timer: 0.0827 sec.
iter 363180 || Loss: 0.4146 || timer: 0.0915 sec.
iter 363190 || Loss: 0.8262 || timer: 0.0917 sec.
iter 363200 || Loss: 0.6192 || timer: 0.1113 sec.
iter 363210 || Loss: 0.6754 || timer: 0.0934 sec.
iter 363220 || Loss: 0.6511 || timer: 0.0239 sec.
iter 363230 || Loss: 0.8299 || timer: 0.0907 sec.
iter 363240 || Loss: 0.5905 || timer: 0.0897 sec.
iter 363250 || Loss: 0.5135 || timer: 0.0914 sec.
iter 363260 || Loss: 0.6668 || timer: 0.0823 sec.
iter 363270 || Loss: 0.7056 || timer: 0.1012 sec.
iter 363280 || Loss: 0.6619 || timer: 0.0826 sec.
iter 363290 || Loss: 0.7840 || timer: 0.0904 sec.
iter 363300 || Loss: 0.7800 || timer: 0.0884 sec.
iter 363310 || Loss: 0.5593 || timer: 0.0849 sec.
iter 363320 || Loss: 0.4648 || timer: 0.0955 sec.
iter 363330 || Loss: 0.5616 || timer: 0.0942 sec.
iter 363340 || Loss: 1.3386 || timer: 0.0841 sec.
iter 363350 || Loss: 0.6640 || timer: 0.0913 sec.
iter 363360 || Loss: 0.6253 || timer: 0.0903 sec.
iter 363370 || Loss: 1.2314 || timer: 0.0907 sec.
iter 363380 || Loss: 0.4462 || timer: 0.0854 sec.
iter 363390 || Loss: 0.7922 || timer: 0.0949 sec.
iter 363400 || Loss: 0.7417 || timer: 0.0906 sec.
iter 363410 || Loss: 0.5417 || timer: 0.0831 sec.
iter 363420 || Loss: 0.5185 || timer: 0.0891 sec.
iter 363430 || Loss: 0.5097 || timer: 0.0900 sec.
iter 363440 || Loss: 0.8489 || timer: 0.0919 sec.
iter 363450 || Loss: 0.8439 || timer: 0.0844 sec.
iter 363460 || Loss: 0.6371 || timer: 0.0908 sec.
iter 363470 || Loss: 0.6514 || timer: 0.1249 sec.
iter 363480 || Loss: 0.8721 || timer: 0.0966 sec.
iter 363490 || Loss: 0.6518 || timer: 0.0883 sec.
iter 363500 || Loss: 0.6645 || timer: 0.0897 sec.
iter 363510 || Loss: 0.6562 || timer: 0.0821 sec.
iter 363520 || Loss: 0.6417 || timer: 0.0902 sec.
iter 363530 || Loss: 0.7040 || timer: 0.0900 sec.
iter 363540 || Loss: 0.8780 || timer: 0.1116 sec.
iter 363550 || Loss: 0.6738 || timer: 0.0176 sec.
iter 363560 || Loss: 0.6800 || timer: 0.0836 sec.
iter 363570 || Loss: 0.5724 || timer: 0.0912 sec.
iter 363580 || Loss: 0.8534 || timer: 0.0921 sec.
iter 363590 || Loss: 1.0742 || timer: 0.0847 sec.
iter 363600 || Loss: 0.8676 || timer: 0.0852 sec.
iter 363610 || Loss: 0.4461 || timer: 0.0909 sec.
iter 363620 || Loss: 0.8026 || timer: 0.0844 sec.
iter 363630 || Loss: 0.5090 || timer: 0.0906 sec.
iter 363640 || Loss: 0.6788 || timer: 0.0893 sec.
iter 363650 || Loss: 1.0886 || timer: 0.1118 sec.
iter 363660 || Loss: 0.5956 || timer: 0.0834 sec.
iter 363670 || Loss: 0.4718 || timer: 0.1017 sec.
iter 363680 || Loss: 0.5600 || timer: 0.0900 sec.
iter 363690 || Loss: 0.6283 || timer: 0.0841 sec.
iter 363700 || Loss: 0.7377 || timer: 0.0920 sec.
iter 363710 || Loss: 0.8291 || timer: 0.0953 sec.
iter 363720 || Loss: 0.6619 || timer: 0.0898 sec.
iter 363730 || Loss: 0.7569 || timer: 0.0860 sec.
iter 363740 || Loss: 0.7124 || timer: 0.0997 sec.
iter 363750 || Loss: 0.5627 || timer: 0.0920 sec.
iter 363760 || Loss: 0.4194 || timer: 0.0914 sec.
iter 363770 || Loss: 0.6781 || timer: 0.0941 sec.
iter 363780 || Loss: 0.8012 || timer: 0.0836 sec.
iter 363790 || Loss: 0.7384 || timer: 0.1026 sec.
iter 363800 || Loss: 0.5466 || timer: 0.0881 sec.
iter 363810 || Loss: 0.6067 || timer: 0.0918 sec.
iter 363820 || Loss: 1.0122 || timer: 0.1112 sec.
iter 363830 || Loss: 0.7077 || timer: 0.0917 sec.
iter 363840 || Loss: 0.9510 || timer: 0.0902 sec.
iter 363850 || Loss: 0.8679 || timer: 0.0904 sec.
iter 363860 || Loss: 0.4971 || timer: 0.1026 sec.
iter 363870 || Loss: 0.7864 || timer: 0.1059 sec.
iter 363880 || Loss: 0.5213 || timer: 0.0200 sec.
iter 363890 || Loss: 0.6060 || timer: 0.0824 sec.
iter 363900 || Loss: 0.6091 || timer: 0.0913 sec.
iter 363910 || Loss: 0.5166 || timer: 0.0886 sec.
iter 363920 || Loss: 0.7534 || timer: 0.0927 sec.
iter 363930 || Loss: 0.7365 || timer: 0.0918 sec.
iter 363940 || Loss: 1.0413 || timer: 0.0865 sec.
iter 363950 || Loss: 0.4222 || timer: 0.0865 sec.
iter 363960 || Loss: 1.0840 || timer: 0.0829 sec.
iter 363970 || Loss: 0.5645 || timer: 0.0834 sec.
iter 363980 || Loss: 0.7257 || timer: 0.1002 sec.
iter 363990 || Loss: 0.6765 || timer: 0.0929 sec.
iter 364000 || Loss: 0.7773 || timer: 0.0908 sec.
iter 364010 || Loss: 0.6518 || timer: 0.0821 sec.
iter 364020 || Loss: 0.5536 || timer: 0.0904 sec.
iter 364030 || Loss: 0.6330 || timer: 0.0889 sec.
iter 364040 || Loss: 0.5923 || timer: 0.1072 sec.
iter 364050 || Loss: 0.7176 || timer: 0.0924 sec.
iter 364060 || Loss: 0.6890 || timer: 0.0961 sec.
iter 364070 || Loss: 0.6558 || timer: 0.0831 sec.
iter 364080 || Loss: 0.6138 || timer: 0.0829 sec.
iter 364090 || Loss: 0.8146 || timer: 0.0887 sec.
iter 364100 || Loss: 0.8667 || timer: 0.0832 sec.
iter 364110 || Loss: 0.9234 || timer: 0.0904 sec.
iter 364120 || Loss: 0.6561 || timer: 0.0924 sec.
iter 364130 || Loss: 0.6841 || timer: 0.0897 sec.
iter 364140 || Loss: 0.5795 || timer: 0.0919 sec.
iter 364150 || Loss: 0.6746 || timer: 0.0904 sec.
iter 364160 || Loss: 0.6577 || timer: 0.0966 sec.
iter 364170 || Loss: 0.5890 || timer: 0.0876 sec.
iter 364180 || Loss: 0.8115 || timer: 0.0906 sec.
iter 364190 || Loss: 0.7339 || timer: 0.1036 sec.
iter 364200 || Loss: 0.8045 || timer: 0.0925 sec.
iter 364210 || Loss: 0.6025 || timer: 0.0213 sec.
iter 364220 || Loss: 0.1713 || timer: 0.0829 sec.
iter 364230 || Loss: 0.8518 || timer: 0.1053 sec.
iter 364240 || Loss: 0.7138 || timer: 0.0907 sec.
iter 364250 || Loss: 0.9663 || timer: 0.0828 sec.
iter 364260 || Loss: 0.6412 || timer: 0.1105 sec.
iter 364270 || Loss: 1.2628 || timer: 0.0914 sec.
iter 364280 || Loss: 0.4384 || timer: 0.0983 sec.
iter 364290 || Loss: 0.5099 || timer: 0.0830 sec.
iter 364300 || Loss: 0.6605 || timer: 0.1072 sec.
iter 364310 || Loss: 0.9690 || timer: 0.0968 sec.
iter 364320 || Loss: 0.5628 || timer: 0.0916 sec.
iter 364330 || Loss: 0.7206 || timer: 0.0921 sec.
iter 364340 || Loss: 0.7624 || timer: 0.0846 sec.
iter 364350 || Loss: 0.4830 || timer: 0.0834 sec.
iter 364360 || Loss: 0.5503 || timer: 0.0836 sec.
iter 364370 || Loss: 0.5882 || timer: 0.1056 sec.
iter 364380 || Loss: 0.6304 || timer: 0.0823 sec.
iter 364390 || Loss: 0.8273 || timer: 0.0913 sec.
iter 364400 || Loss: 0.6857 || timer: 0.1024 sec.
iter 364410 || Loss: 0.5575 || timer: 0.0844 sec.
iter 364420 || Loss: 0.5638 || timer: 0.0828 sec.
iter 364430 || Loss: 0.5902 || timer: 0.0912 sec.
iter 364440 || Loss: 1.1060 || timer: 0.0841 sec.
iter 364450 || Loss: 0.7332 || timer: 0.0862 sec.
iter 364460 || Loss: 0.5025 || timer: 0.0914 sec.
iter 364470 || Loss: 0.6608 || timer: 0.1009 sec.
iter 364480 || Loss: 1.0831 || timer: 0.1036 sec.
iter 364490 || Loss: 0.6417 || timer: 0.0848 sec.
iter 364500 || Loss: 1.1926 || timer: 0.0898 sec.
iter 364510 || Loss: 0.7601 || timer: 0.0895 sec.
iter 364520 || Loss: 0.7409 || timer: 0.0821 sec.
iter 364530 || Loss: 0.7117 || timer: 0.0941 sec.
iter 364540 || Loss: 0.5007 || timer: 0.0209 sec.
iter 364550 || Loss: 0.4391 || timer: 0.0837 sec.
iter 364560 || Loss: 0.7822 || timer: 0.0840 sec.
iter 364570 || Loss: 0.7879 || timer: 0.0850 sec.
iter 364580 || Loss: 0.5555 || timer: 0.0935 sec.
iter 364590 || Loss: 0.7380 || timer: 0.0826 sec.
iter 364600 || Loss: 0.6421 || timer: 0.0917 sec.
iter 364610 || Loss: 0.8460 || timer: 0.0839 sec.
iter 364620 || Loss: 0.6174 || timer: 0.0827 sec.
iter 364630 || Loss: 0.6092 || timer: 0.0837 sec.
iter 364640 || Loss: 0.6110 || timer: 0.1035 sec.
iter 364650 || Loss: 0.8510 || timer: 0.0887 sec.
iter 364660 || Loss: 0.8484 || timer: 0.0958 sec.
iter 364670 || Loss: 0.6368 || timer: 0.0823 sec.
iter 364680 || Loss: 0.7276 || timer: 0.0961 sec.
iter 364690 || Loss: 0.5427 || timer: 0.0896 sec.
iter 364700 || Loss: 0.6572 || timer: 0.0906 sec.
iter 364710 || Loss: 0.7524 || timer: 0.0927 sec.
iter 364720 || Loss: 0.5884 || timer: 0.0899 sec.
iter 364730 || Loss: 0.8304 || timer: 0.0940 sec.
iter 364740 || Loss: 0.4831 || timer: 0.1073 sec.
iter 364750 || Loss: 0.8215 || timer: 0.0832 sec.
iter 364760 || Loss: 0.6427 || timer: 0.1096 sec.
iter 364770 || Loss: 0.6361 || timer: 0.0890 sec.
iter 364780 || Loss: 0.7084 || timer: 0.0921 sec.
iter 364790 || Loss: 0.5727 || timer: 0.0891 sec.
iter 364800 || Loss: 0.5261 || timer: 0.0913 sec.
iter 364810 || Loss: 0.3995 || timer: 0.1044 sec.
iter 364820 || Loss: 0.6142 || timer: 0.0854 sec.
iter 364830 || Loss: 0.8320 || timer: 0.0900 sec.
iter 364840 || Loss: 0.6420 || timer: 0.0912 sec.
iter 364850 || Loss: 0.5362 || timer: 0.0934 sec.
iter 364860 || Loss: 0.5828 || timer: 0.1059 sec.
iter 364870 || Loss: 0.6016 || timer: 0.0192 sec.
iter 364880 || Loss: 0.6919 || timer: 0.0889 sec.
iter 364890 || Loss: 0.6429 || timer: 0.0919 sec.
iter 364900 || Loss: 0.6568 || timer: 0.0897 sec.
iter 364910 || Loss: 0.7208 || timer: 0.0755 sec.
iter 364920 || Loss: 0.5526 || timer: 0.0912 sec.
iter 364930 || Loss: 0.9491 || timer: 0.0823 sec.
iter 364940 || Loss: 0.7464 || timer: 0.0844 sec.
iter 364950 || Loss: 0.9404 || timer: 0.0837 sec.
iter 364960 || Loss: 0.6099 || timer: 0.1072 sec.
iter 364970 || Loss: 0.7161 || timer: 0.1187 sec.
iter 364980 || Loss: 0.5812 || timer: 0.0833 sec.
iter 364990 || Loss: 0.8385 || timer: 0.0923 sec.
iter 365000 || Loss: 0.5373 || Saving state, iter: 365000
timer: 0.0831 sec.
iter 365010 || Loss: 0.4971 || timer: 0.0916 sec.
iter 365020 || Loss: 0.6165 || timer: 0.0925 sec.
iter 365030 || Loss: 0.8244 || timer: 0.0998 sec.
iter 365040 || Loss: 0.5419 || timer: 0.1072 sec.
iter 365050 || Loss: 0.7824 || timer: 0.0825 sec.
iter 365060 || Loss: 0.5701 || timer: 0.0881 sec.
iter 365070 || Loss: 0.5675 || timer: 0.0909 sec.
iter 365080 || Loss: 0.4858 || timer: 0.0917 sec.
iter 365090 || Loss: 0.8940 || timer: 0.0837 sec.
iter 365100 || Loss: 0.5111 || timer: 0.0877 sec.
iter 365110 || Loss: 0.5878 || timer: 0.0970 sec.
iter 365120 || Loss: 0.7502 || timer: 0.1000 sec.
iter 365130 || Loss: 0.7236 || timer: 0.1089 sec.
iter 365140 || Loss: 0.4745 || timer: 0.0920 sec.
iter 365150 || Loss: 0.6916 || timer: 0.0881 sec.
iter 365160 || Loss: 0.7129 || timer: 0.1041 sec.
iter 365170 || Loss: 0.6549 || timer: 0.0899 sec.
iter 365180 || Loss: 0.5044 || timer: 0.0919 sec.
iter 365190 || Loss: 0.5584 || timer: 0.1042 sec.
iter 365200 || Loss: 0.7897 || timer: 0.0233 sec.
iter 365210 || Loss: 1.2651 || timer: 0.1065 sec.
iter 365220 || Loss: 0.5682 || timer: 0.1047 sec.
iter 365230 || Loss: 0.9014 || timer: 0.0870 sec.
iter 365240 || Loss: 0.6948 || timer: 0.0899 sec.
iter 365250 || Loss: 0.9170 || timer: 0.0911 sec.
iter 365260 || Loss: 0.5134 || timer: 0.0831 sec.
iter 365270 || Loss: 0.6289 || timer: 0.0906 sec.
iter 365280 || Loss: 0.5224 || timer: 0.1245 sec.
iter 365290 || Loss: 0.8143 || timer: 0.0837 sec.
iter 365300 || Loss: 0.6400 || timer: 0.0969 sec.
iter 365310 || Loss: 0.8178 || timer: 0.0842 sec.
iter 365320 || Loss: 0.8584 || timer: 0.0914 sec.
iter 365330 || Loss: 0.9691 || timer: 0.0824 sec.
iter 365340 || Loss: 0.5510 || timer: 0.1204 sec.
iter 365350 || Loss: 0.6725 || timer: 0.0979 sec.
iter 365360 || Loss: 0.7151 || timer: 0.1160 sec.
iter 365370 || Loss: 0.7498 || timer: 0.0905 sec.
iter 365380 || Loss: 0.4199 || timer: 0.0905 sec.
iter 365390 || Loss: 0.9763 || timer: 0.0911 sec.
iter 365400 || Loss: 0.7040 || timer: 0.0835 sec.
iter 365410 || Loss: 0.6156 || timer: 0.0867 sec.
iter 365420 || Loss: 0.8521 || timer: 0.1038 sec.
iter 365430 || Loss: 0.6336 || timer: 0.0865 sec.
iter 365440 || Loss: 0.7763 || timer: 0.0900 sec.
iter 365450 || Loss: 0.7076 || timer: 0.0830 sec.
iter 365460 || Loss: 0.8442 || timer: 0.0860 sec.
iter 365470 || Loss: 0.7279 || timer: 0.1004 sec.
iter 365480 || Loss: 0.5492 || timer: 0.0915 sec.
iter 365490 || Loss: 0.4951 || timer: 0.1047 sec.
iter 365500 || Loss: 0.5454 || timer: 0.0905 sec.
iter 365510 || Loss: 0.5493 || timer: 0.0894 sec.
iter 365520 || Loss: 0.4890 || timer: 0.0828 sec.
iter 365530 || Loss: 0.4756 || timer: 0.0162 sec.
iter 365540 || Loss: 0.4703 || timer: 0.0929 sec.
iter 365550 || Loss: 0.9194 || timer: 0.0826 sec.
iter 365560 || Loss: 0.5060 || timer: 0.0899 sec.
iter 365570 || Loss: 0.8847 || timer: 0.0823 sec.
iter 365580 || Loss: 0.5390 || timer: 0.0983 sec.
iter 365590 || Loss: 0.5671 || timer: 0.0906 sec.
iter 365600 || Loss: 0.8191 || timer: 0.0870 sec.
iter 365610 || Loss: 0.5839 || timer: 0.0818 sec.
iter 365620 || Loss: 0.6766 || timer: 0.0810 sec.
iter 365630 || Loss: 0.7318 || timer: 0.1021 sec.
iter 365640 || Loss: 0.6184 || timer: 0.0905 sec.
iter 365650 || Loss: 0.3574 || timer: 0.0891 sec.
iter 365660 || Loss: 0.6115 || timer: 0.0914 sec.
iter 365670 || Loss: 0.9471 || timer: 0.0961 sec.
iter 365680 || Loss: 0.9452 || timer: 0.1049 sec.
iter 365690 || Loss: 0.7961 || timer: 0.0818 sec.
iter 365700 || Loss: 0.9548 || timer: 0.1056 sec.
iter 365710 || Loss: 0.7676 || timer: 0.0835 sec.
iter 365720 || Loss: 0.8518 || timer: 0.0918 sec.
iter 365730 || Loss: 0.7757 || timer: 0.0909 sec.
iter 365740 || Loss: 0.7202 || timer: 0.0767 sec.
iter 365750 || Loss: 0.6472 || timer: 0.0930 sec.
iter 365760 || Loss: 0.6545 || timer: 0.0829 sec.
iter 365770 || Loss: 0.5577 || timer: 0.0926 sec.
iter 365780 || Loss: 0.5108 || timer: 0.0903 sec.
iter 365790 || Loss: 0.6174 || timer: 0.0855 sec.
iter 365800 || Loss: 0.8042 || timer: 0.0914 sec.
iter 365810 || Loss: 0.8805 || timer: 0.0826 sec.
iter 365820 || Loss: 0.7827 || timer: 0.0922 sec.
iter 365830 || Loss: 0.6777 || timer: 0.0886 sec.
iter 365840 || Loss: 0.5972 || timer: 0.0943 sec.
iter 365850 || Loss: 0.9333 || timer: 0.0835 sec.
iter 365860 || Loss: 0.8884 || timer: 0.0258 sec.
iter 365870 || Loss: 0.2308 || timer: 0.0829 sec.
iter 365880 || Loss: 0.6824 || timer: 0.1204 sec.
iter 365890 || Loss: 0.7527 || timer: 0.0897 sec.
iter 365900 || Loss: 0.6064 || timer: 0.1118 sec.
iter 365910 || Loss: 0.6901 || timer: 0.0912 sec.
iter 365920 || Loss: 0.5339 || timer: 0.1064 sec.
iter 365930 || Loss: 0.9260 || timer: 0.0893 sec.
iter 365940 || Loss: 0.7431 || timer: 0.0921 sec.
iter 365950 || Loss: 0.6626 || timer: 0.0913 sec.
iter 365960 || Loss: 0.7590 || timer: 0.1308 sec.
iter 365970 || Loss: 0.5252 || timer: 0.0830 sec.
iter 365980 || Loss: 0.5758 || timer: 0.0914 sec.
iter 365990 || Loss: 0.4619 || timer: 0.0935 sec.
iter 366000 || Loss: 0.6510 || timer: 0.0835 sec.
iter 366010 || Loss: 0.4488 || timer: 0.1073 sec.
iter 366020 || Loss: 0.6736 || timer: 0.0903 sec.
iter 366030 || Loss: 0.6969 || timer: 0.1085 sec.
iter 366040 || Loss: 0.8723 || timer: 0.0891 sec.
iter 366050 || Loss: 0.7049 || timer: 0.1034 sec.
iter 366060 || Loss: 0.8683 || timer: 0.0853 sec.
iter 366070 || Loss: 0.6793 || timer: 0.0962 sec.
iter 366080 || Loss: 0.6139 || timer: 0.0923 sec.
iter 366090 || Loss: 0.8353 || timer: 0.0864 sec.
iter 366100 || Loss: 0.9047 || timer: 0.1009 sec.
iter 366110 || Loss: 0.5959 || timer: 0.0906 sec.
iter 366120 || Loss: 0.6302 || timer: 0.0892 sec.
iter 366130 || Loss: 0.7218 || timer: 0.0827 sec.
iter 366140 || Loss: 0.8136 || timer: 0.0906 sec.
iter 366150 || Loss: 0.7265 || timer: 0.0918 sec.
iter 366160 || Loss: 0.7277 || timer: 0.0831 sec.
iter 366170 || Loss: 0.6116 || timer: 0.0933 sec.
iter 366180 || Loss: 0.8608 || timer: 0.1049 sec.
iter 366190 || Loss: 0.5188 || timer: 0.0251 sec.
iter 366200 || Loss: 0.4826 || timer: 0.0959 sec.
iter 366210 || Loss: 0.7440 || timer: 0.0951 sec.
iter 366220 || Loss: 0.6434 || timer: 0.0940 sec.
iter 366230 || Loss: 0.7512 || timer: 0.0916 sec.
iter 366240 || Loss: 0.6634 || timer: 0.0837 sec.
iter 366250 || Loss: 0.9634 || timer: 0.0915 sec.
iter 366260 || Loss: 0.7340 || timer: 0.0949 sec.
iter 366270 || Loss: 0.7087 || timer: 0.0824 sec.
iter 366280 || Loss: 0.6351 || timer: 0.0921 sec.
iter 366290 || Loss: 0.4890 || timer: 0.0997 sec.
iter 366300 || Loss: 0.4292 || timer: 0.1047 sec.
iter 366310 || Loss: 0.7061 || timer: 0.0828 sec.
iter 366320 || Loss: 0.7010 || timer: 0.0875 sec.
iter 366330 || Loss: 0.7591 || timer: 0.0911 sec.
iter 366340 || Loss: 0.6873 || timer: 0.0905 sec.
iter 366350 || Loss: 0.7780 || timer: 0.0961 sec.
iter 366360 || Loss: 0.5432 || timer: 0.1143 sec.
iter 366370 || Loss: 0.6710 || timer: 0.0918 sec.
iter 366380 || Loss: 0.7911 || timer: 0.0930 sec.
iter 366390 || Loss: 0.7505 || timer: 0.0824 sec.
iter 366400 || Loss: 0.8815 || timer: 0.0831 sec.
iter 366410 || Loss: 0.6333 || timer: 0.0876 sec.
iter 366420 || Loss: 0.8859 || timer: 0.0923 sec.
iter 366430 || Loss: 0.6678 || timer: 0.1139 sec.
iter 366440 || Loss: 0.8916 || timer: 0.0908 sec.
iter 366450 || Loss: 0.6334 || timer: 0.0824 sec.
iter 366460 || Loss: 0.7688 || timer: 0.0825 sec.
iter 366470 || Loss: 0.6013 || timer: 0.0910 sec.
iter 366480 || Loss: 0.7218 || timer: 0.0912 sec.
iter 366490 || Loss: 0.4698 || timer: 0.0896 sec.
iter 366500 || Loss: 0.5776 || timer: 0.0911 sec.
iter 366510 || Loss: 0.5308 || timer: 0.0825 sec.
iter 366520 || Loss: 0.4787 || timer: 0.0166 sec.
iter 366530 || Loss: 0.3098 || timer: 0.0887 sec.
iter 366540 || Loss: 0.6735 || timer: 0.1077 sec.
iter 366550 || Loss: 0.5788 || timer: 0.0840 sec.
iter 366560 || Loss: 0.7382 || timer: 0.0893 sec.
iter 366570 || Loss: 0.4376 || timer: 0.0825 sec.
iter 366580 || Loss: 0.6531 || timer: 0.0935 sec.
iter 366590 || Loss: 0.6922 || timer: 0.0967 sec.
iter 366600 || Loss: 0.6200 || timer: 0.1121 sec.
iter 366610 || Loss: 0.7384 || timer: 0.0907 sec.
iter 366620 || Loss: 0.8695 || timer: 0.1062 sec.
iter 366630 || Loss: 0.6000 || timer: 0.0919 sec.
iter 366640 || Loss: 0.5692 || timer: 0.0836 sec.
iter 366650 || Loss: 0.7902 || timer: 0.0916 sec.
iter 366660 || Loss: 0.5215 || timer: 0.0836 sec.
iter 366670 || Loss: 0.8598 || timer: 0.0831 sec.
iter 366680 || Loss: 0.9407 || timer: 0.0928 sec.
iter 366690 || Loss: 0.5748 || timer: 0.0896 sec.
iter 366700 || Loss: 0.7787 || timer: 0.0902 sec.
iter 366710 || Loss: 0.8078 || timer: 0.1098 sec.
iter 366720 || Loss: 0.5455 || timer: 0.0868 sec.
iter 366730 || Loss: 0.8652 || timer: 0.0889 sec.
iter 366740 || Loss: 0.4171 || timer: 0.0873 sec.
iter 366750 || Loss: 0.7490 || timer: 0.0904 sec.
iter 366760 || Loss: 0.7732 || timer: 0.0818 sec.
iter 366770 || Loss: 0.5825 || timer: 0.0918 sec.
iter 366780 || Loss: 0.4981 || timer: 0.1023 sec.
iter 366790 || Loss: 0.5660 || timer: 0.0826 sec.
iter 366800 || Loss: 0.5331 || timer: 0.1119 sec.
iter 366810 || Loss: 0.5124 || timer: 0.1066 sec.
iter 366820 || Loss: 0.7031 || timer: 0.0905 sec.
iter 366830 || Loss: 0.6240 || timer: 0.0833 sec.
iter 366840 || Loss: 0.7058 || timer: 0.0905 sec.
iter 366850 || Loss: 0.5775 || timer: 0.0261 sec.
iter 366860 || Loss: 1.0956 || timer: 0.0899 sec.
iter 366870 || Loss: 0.6727 || timer: 0.0918 sec.
iter 366880 || Loss: 0.7129 || timer: 0.0924 sec.
iter 366890 || Loss: 0.8679 || timer: 0.0829 sec.
iter 366900 || Loss: 0.6465 || timer: 0.0910 sec.
iter 366910 || Loss: 0.5271 || timer: 0.1082 sec.
iter 366920 || Loss: 0.8752 || timer: 0.0837 sec.
iter 366930 || Loss: 0.7681 || timer: 0.0977 sec.
iter 366940 || Loss: 0.6238 || timer: 0.0909 sec.
iter 366950 || Loss: 1.0443 || timer: 0.0980 sec.
iter 366960 || Loss: 0.6186 || timer: 0.0874 sec.
iter 366970 || Loss: 0.6293 || timer: 0.0910 sec.
iter 366980 || Loss: 0.5602 || timer: 0.0837 sec.
iter 366990 || Loss: 0.4701 || timer: 0.0863 sec.
iter 367000 || Loss: 0.4962 || timer: 0.0902 sec.
iter 367010 || Loss: 0.7046 || timer: 0.0924 sec.
iter 367020 || Loss: 0.6037 || timer: 0.0900 sec.
iter 367030 || Loss: 0.7261 || timer: 0.0825 sec.
iter 367040 || Loss: 0.8420 || timer: 0.0976 sec.
iter 367050 || Loss: 0.7813 || timer: 0.0897 sec.
iter 367060 || Loss: 0.6389 || timer: 0.0855 sec.
iter 367070 || Loss: 0.6272 || timer: 0.0921 sec.
iter 367080 || Loss: 0.6376 || timer: 0.0897 sec.
iter 367090 || Loss: 0.7822 || timer: 0.0891 sec.
iter 367100 || Loss: 0.5712 || timer: 0.0880 sec.
iter 367110 || Loss: 0.4320 || timer: 0.0923 sec.
iter 367120 || Loss: 0.6813 || timer: 0.0875 sec.
iter 367130 || Loss: 0.7414 || timer: 0.0923 sec.
iter 367140 || Loss: 0.8413 || timer: 0.0967 sec.
iter 367150 || Loss: 0.8709 || timer: 0.0898 sec.
iter 367160 || Loss: 0.7097 || timer: 0.0884 sec.
iter 367170 || Loss: 0.3925 || timer: 0.0887 sec.
iter 367180 || Loss: 0.6580 || timer: 0.0184 sec.
iter 367190 || Loss: 0.5599 || timer: 0.1036 sec.
iter 367200 || Loss: 0.5528 || timer: 0.0904 sec.
iter 367210 || Loss: 0.5524 || timer: 0.0977 sec.
iter 367220 || Loss: 0.4475 || timer: 0.1119 sec.
iter 367230 || Loss: 0.8094 || timer: 0.0827 sec.
iter 367240 || Loss: 0.5142 || timer: 0.1084 sec.
iter 367250 || Loss: 0.6367 || timer: 0.0903 sec.
iter 367260 || Loss: 0.7064 || timer: 0.1081 sec.
iter 367270 || Loss: 0.8548 || timer: 0.0844 sec.
iter 367280 || Loss: 0.6540 || timer: 0.0958 sec.
iter 367290 || Loss: 0.5808 || timer: 0.1067 sec.
iter 367300 || Loss: 0.6047 || timer: 0.0825 sec.
iter 367310 || Loss: 0.5164 || timer: 0.0913 sec.
iter 367320 || Loss: 0.5754 || timer: 0.0918 sec.
iter 367330 || Loss: 0.7470 || timer: 0.0897 sec.
iter 367340 || Loss: 0.7760 || timer: 0.0915 sec.
iter 367350 || Loss: 0.5576 || timer: 0.0879 sec.
iter 367360 || Loss: 0.7084 || timer: 0.0835 sec.
iter 367370 || Loss: 0.6219 || timer: 0.1269 sec.
iter 367380 || Loss: 0.8393 || timer: 0.0849 sec.
iter 367390 || Loss: 0.8023 || timer: 0.0977 sec.
iter 367400 || Loss: 0.9836 || timer: 0.0939 sec.
iter 367410 || Loss: 0.6401 || timer: 0.0830 sec.
iter 367420 || Loss: 0.5253 || timer: 0.0917 sec.
iter 367430 || Loss: 0.5365 || timer: 0.0861 sec.
iter 367440 || Loss: 0.9280 || timer: 0.0900 sec.
iter 367450 || Loss: 0.6307 || timer: 0.0824 sec.
iter 367460 || Loss: 0.7389 || timer: 0.0918 sec.
iter 367470 || Loss: 0.7422 || timer: 0.0837 sec.
iter 367480 || Loss: 0.7182 || timer: 0.0828 sec.
iter 367490 || Loss: 0.7545 || timer: 0.0832 sec.
iter 367500 || Loss: 0.9193 || timer: 0.0823 sec.
iter 367510 || Loss: 0.5996 || timer: 0.0251 sec.
iter 367520 || Loss: 1.5557 || timer: 0.1053 sec.
iter 367530 || Loss: 0.5664 || timer: 0.0905 sec.
iter 367540 || Loss: 0.6540 || timer: 0.0950 sec.
iter 367550 || Loss: 0.6587 || timer: 0.0893 sec.
iter 367560 || Loss: 0.4660 || timer: 0.0928 sec.
iter 367570 || Loss: 0.8706 || timer: 0.0905 sec.
iter 367580 || Loss: 0.5943 || timer: 0.1206 sec.
iter 367590 || Loss: 0.7939 || timer: 0.0909 sec.
iter 367600 || Loss: 0.7446 || timer: 0.1151 sec.
iter 367610 || Loss: 0.8207 || timer: 0.1093 sec.
iter 367620 || Loss: 0.6492 || timer: 0.0907 sec.
iter 367630 || Loss: 0.6483 || timer: 0.0839 sec.
iter 367640 || Loss: 0.7236 || timer: 0.0942 sec.
iter 367650 || Loss: 0.5804 || timer: 0.1066 sec.
iter 367660 || Loss: 0.8026 || timer: 0.0887 sec.
iter 367670 || Loss: 0.6586 || timer: 0.0898 sec.
iter 367680 || Loss: 0.5217 || timer: 0.0896 sec.
iter 367690 || Loss: 0.8059 || timer: 0.0828 sec.
iter 367700 || Loss: 0.6592 || timer: 0.1035 sec.
iter 367710 || Loss: 0.7468 || timer: 0.0883 sec.
iter 367720 || Loss: 0.6623 || timer: 0.0904 sec.
iter 367730 || Loss: 0.6983 || timer: 0.0998 sec.
iter 367740 || Loss: 0.6733 || timer: 0.0890 sec.
iter 367750 || Loss: 0.6155 || timer: 0.0816 sec.
iter 367760 || Loss: 0.8109 || timer: 0.1018 sec.
iter 367770 || Loss: 0.8045 || timer: 0.0891 sec.
iter 367780 || Loss: 0.6637 || timer: 0.0942 sec.
iter 367790 || Loss: 0.8542 || timer: 0.0842 sec.
iter 367800 || Loss: 0.6320 || timer: 0.0830 sec.
iter 367810 || Loss: 0.6309 || timer: 0.0898 sec.
iter 367820 || Loss: 0.9580 || timer: 0.0915 sec.
iter 367830 || Loss: 0.4289 || timer: 0.0913 sec.
iter 367840 || Loss: 0.5506 || timer: 0.0250 sec.
iter 367850 || Loss: 0.9335 || timer: 0.0821 sec.
iter 367860 || Loss: 0.7954 || timer: 0.0926 sec.
iter 367870 || Loss: 0.6074 || timer: 0.1095 sec.
iter 367880 || Loss: 0.7975 || timer: 0.0895 sec.
iter 367890 || Loss: 0.7961 || timer: 0.0977 sec.
iter 367900 || Loss: 0.7802 || timer: 0.0857 sec.
iter 367910 || Loss: 0.6642 || timer: 0.0921 sec.
iter 367920 || Loss: 0.6833 || timer: 0.0897 sec.
iter 367930 || Loss: 0.7147 || timer: 0.0830 sec.
iter 367940 || Loss: 0.8026 || timer: 0.1115 sec.
iter 367950 || Loss: 0.6447 || timer: 0.0856 sec.
iter 367960 || Loss: 0.5790 || timer: 0.1092 sec.
iter 367970 || Loss: 0.6519 || timer: 0.1004 sec.
iter 367980 || Loss: 0.8252 || timer: 0.0947 sec.
iter 367990 || Loss: 0.7975 || timer: 0.0916 sec.
iter 368000 || Loss: 0.5876 || timer: 0.0882 sec.
iter 368010 || Loss: 0.7635 || timer: 0.0920 sec.
iter 368020 || Loss: 0.8948 || timer: 0.0960 sec.
iter 368030 || Loss: 0.8898 || timer: 0.0852 sec.
iter 368040 || Loss: 0.6811 || timer: 0.0898 sec.
iter 368050 || Loss: 0.5009 || timer: 0.0909 sec.
iter 368060 || Loss: 0.6814 || timer: 0.0898 sec.
iter 368070 || Loss: 0.4913 || timer: 0.0917 sec.
iter 368080 || Loss: 0.8715 || timer: 0.0900 sec.
iter 368090 || Loss: 0.7297 || timer: 0.0910 sec.
iter 368100 || Loss: 0.7173 || timer: 0.0903 sec.
iter 368110 || Loss: 0.6458 || timer: 0.0910 sec.
iter 368120 || Loss: 0.7523 || timer: 0.0905 sec.
iter 368130 || Loss: 0.7235 || timer: 0.0903 sec.
iter 368140 || Loss: 0.5128 || timer: 0.1044 sec.
iter 368150 || Loss: 0.9179 || timer: 0.0915 sec.
iter 368160 || Loss: 0.8693 || timer: 0.0910 sec.
iter 368170 || Loss: 0.6431 || timer: 0.0267 sec.
iter 368180 || Loss: 1.7058 || timer: 0.0914 sec.
iter 368190 || Loss: 0.7897 || timer: 0.0842 sec.
iter 368200 || Loss: 0.7056 || timer: 0.0908 sec.
iter 368210 || Loss: 0.5472 || timer: 0.1025 sec.
iter 368220 || Loss: 0.5969 || timer: 0.0932 sec.
iter 368230 || Loss: 0.6680 || timer: 0.0918 sec.
iter 368240 || Loss: 0.6893 || timer: 0.0826 sec.
iter 368250 || Loss: 0.9662 || timer: 0.0850 sec.
iter 368260 || Loss: 0.6955 || timer: 0.0896 sec.
iter 368270 || Loss: 0.9377 || timer: 0.1034 sec.
iter 368280 || Loss: 0.7938 || timer: 0.0882 sec.
iter 368290 || Loss: 0.3848 || timer: 0.0836 sec.
iter 368300 || Loss: 0.5495 || timer: 0.0835 sec.
iter 368310 || Loss: 0.4932 || timer: 0.0904 sec.
iter 368320 || Loss: 0.5260 || timer: 0.0914 sec.
iter 368330 || Loss: 0.6972 || timer: 0.0876 sec.
iter 368340 || Loss: 0.5609 || timer: 0.0990 sec.
iter 368350 || Loss: 0.7945 || timer: 0.0916 sec.
iter 368360 || Loss: 0.8144 || timer: 0.0902 sec.
iter 368370 || Loss: 0.5967 || timer: 0.1021 sec.
iter 368380 || Loss: 0.6583 || timer: 0.1025 sec.
iter 368390 || Loss: 0.8048 || timer: 0.0905 sec.
iter 368400 || Loss: 0.6135 || timer: 0.0834 sec.
iter 368410 || Loss: 0.8049 || timer: 0.0891 sec.
iter 368420 || Loss: 1.1187 || timer: 0.0994 sec.
iter 368430 || Loss: 0.4630 || timer: 0.0815 sec.
iter 368440 || Loss: 0.5442 || timer: 0.0836 sec.
iter 368450 || Loss: 0.9323 || timer: 0.0908 sec.
iter 368460 || Loss: 0.6366 || timer: 0.0881 sec.
iter 368470 || Loss: 0.5148 || timer: 0.0919 sec.
iter 368480 || Loss: 0.5440 || timer: 0.0890 sec.
iter 368490 || Loss: 0.5796 || timer: 0.0885 sec.
iter 368500 || Loss: 0.5916 || timer: 0.0157 sec.
iter 368510 || Loss: 0.3205 || timer: 0.0906 sec.
iter 368520 || Loss: 0.6853 || timer: 0.0902 sec.
iter 368530 || Loss: 0.4516 || timer: 0.0916 sec.
iter 368540 || Loss: 0.7269 || timer: 0.0898 sec.
iter 368550 || Loss: 0.7390 || timer: 0.0867 sec.
iter 368560 || Loss: 0.8401 || timer: 0.0819 sec.
iter 368570 || Loss: 0.7755 || timer: 0.0824 sec.
iter 368580 || Loss: 0.6845 || timer: 0.0895 sec.
iter 368590 || Loss: 0.5854 || timer: 0.0964 sec.
iter 368600 || Loss: 0.6132 || timer: 0.0960 sec.
iter 368610 || Loss: 0.5959 || timer: 0.0890 sec.
iter 368620 || Loss: 0.6644 || timer: 0.0897 sec.
iter 368630 || Loss: 0.6093 || timer: 0.0918 sec.
iter 368640 || Loss: 0.7880 || timer: 0.0818 sec.
iter 368650 || Loss: 0.6883 || timer: 0.0908 sec.
iter 368660 || Loss: 0.6188 || timer: 0.0851 sec.
iter 368670 || Loss: 0.7308 || timer: 0.1193 sec.
iter 368680 || Loss: 0.5614 || timer: 0.0898 sec.
iter 368690 || Loss: 0.4210 || timer: 0.0888 sec.
iter 368700 || Loss: 0.7491 || timer: 0.0872 sec.
iter 368710 || Loss: 0.7792 || timer: 0.0823 sec.
iter 368720 || Loss: 0.6712 || timer: 0.0813 sec.
iter 368730 || Loss: 0.6510 || timer: 0.0906 sec.
iter 368740 || Loss: 0.4630 || timer: 0.1052 sec.
iter 368750 || Loss: 0.5082 || timer: 0.0939 sec.
iter 368760 || Loss: 0.4918 || timer: 0.0833 sec.
iter 368770 || Loss: 0.5972 || timer: 0.0900 sec.
iter 368780 || Loss: 0.7599 || timer: 0.0934 sec.
iter 368790 || Loss: 0.4771 || timer: 0.0837 sec.
iter 368800 || Loss: 0.7568 || timer: 0.0880 sec.
iter 368810 || Loss: 0.8539 || timer: 0.0874 sec.
iter 368820 || Loss: 0.6219 || timer: 0.0864 sec.
iter 368830 || Loss: 0.9178 || timer: 0.0285 sec.
iter 368840 || Loss: 0.7142 || timer: 0.1103 sec.
iter 368850 || Loss: 0.6361 || timer: 0.0834 sec.
iter 368860 || Loss: 0.5123 || timer: 0.0980 sec.
iter 368870 || Loss: 1.0277 || timer: 0.1041 sec.
iter 368880 || Loss: 0.5694 || timer: 0.1096 sec.
iter 368890 || Loss: 0.7028 || timer: 0.0846 sec.
iter 368900 || Loss: 0.6674 || timer: 0.0900 sec.
iter 368910 || Loss: 0.7534 || timer: 0.0907 sec.
iter 368920 || Loss: 0.5429 || timer: 0.0945 sec.
iter 368930 || Loss: 0.5243 || timer: 0.1149 sec.
iter 368940 || Loss: 0.7013 || timer: 0.0896 sec.
iter 368950 || Loss: 0.7154 || timer: 0.1113 sec.
iter 368960 || Loss: 0.8350 || timer: 0.0915 sec.
iter 368970 || Loss: 0.6333 || timer: 0.0831 sec.
iter 368980 || Loss: 0.6576 || timer: 0.0875 sec.
iter 368990 || Loss: 0.4283 || timer: 0.0853 sec.
iter 369000 || Loss: 0.7102 || timer: 0.1249 sec.
iter 369010 || Loss: 0.7882 || timer: 0.0928 sec.
iter 369020 || Loss: 0.4246 || timer: 0.1055 sec.
iter 369030 || Loss: 0.6723 || timer: 0.0833 sec.
iter 369040 || Loss: 0.7173 || timer: 0.0916 sec.
iter 369050 || Loss: 0.5150 || timer: 0.0896 sec.
iter 369060 || Loss: 0.7235 || timer: 0.0839 sec.
iter 369070 || Loss: 0.6499 || timer: 0.0900 sec.
iter 369080 || Loss: 0.4947 || timer: 0.0895 sec.
iter 369090 || Loss: 0.5578 || timer: 0.0849 sec.
iter 369100 || Loss: 0.7279 || timer: 0.0901 sec.
iter 369110 || Loss: 0.6989 || timer: 0.1220 sec.
iter 369120 || Loss: 0.5027 || timer: 0.0914 sec.
iter 369130 || Loss: 0.9360 || timer: 0.0910 sec.
iter 369140 || Loss: 0.6988 || timer: 0.0905 sec.
iter 369150 || Loss: 0.7507 || timer: 0.0840 sec.
iter 369160 || Loss: 0.9020 || timer: 0.0271 sec.
iter 369170 || Loss: 0.5624 || timer: 0.0840 sec.
iter 369180 || Loss: 0.8665 || timer: 0.1178 sec.
iter 369190 || Loss: 0.6661 || timer: 0.0901 sec.
iter 369200 || Loss: 0.9476 || timer: 0.0830 sec.
iter 369210 || Loss: 0.5673 || timer: 0.0911 sec.
iter 369220 || Loss: 0.5212 || timer: 0.0946 sec.
iter 369230 || Loss: 0.7610 || timer: 0.1042 sec.
iter 369240 || Loss: 0.5492 || timer: 0.0820 sec.
iter 369250 || Loss: 0.6767 || timer: 0.1170 sec.
iter 369260 || Loss: 0.7576 || timer: 0.1212 sec.
iter 369270 || Loss: 0.7072 || timer: 0.0833 sec.
iter 369280 || Loss: 0.7322 || timer: 0.1057 sec.
iter 369290 || Loss: 0.8336 || timer: 0.0913 sec.
iter 369300 || Loss: 0.7238 || timer: 0.0888 sec.
iter 369310 || Loss: 0.4451 || timer: 0.0905 sec.
iter 369320 || Loss: 0.4763 || timer: 0.0837 sec.
iter 369330 || Loss: 0.6542 || timer: 0.1156 sec.
iter 369340 || Loss: 0.6787 || timer: 0.1030 sec.
iter 369350 || Loss: 0.5157 || timer: 0.0838 sec.
iter 369360 || Loss: 0.5664 || timer: 0.0835 sec.
iter 369370 || Loss: 0.6118 || timer: 0.0825 sec.
iter 369380 || Loss: 0.5553 || timer: 0.0896 sec.
iter 369390 || Loss: 0.9644 || timer: 0.0889 sec.
iter 369400 || Loss: 0.6954 || timer: 0.0912 sec.
iter 369410 || Loss: 0.8689 || timer: 0.0869 sec.
iter 369420 || Loss: 0.5773 || timer: 0.0984 sec.
iter 369430 || Loss: 0.6885 || timer: 0.0879 sec.
iter 369440 || Loss: 0.6339 || timer: 0.0916 sec.
iter 369450 || Loss: 0.6252 || timer: 0.0821 sec.
iter 369460 || Loss: 0.4769 || timer: 0.1021 sec.
iter 369470 || Loss: 0.6471 || timer: 0.0907 sec.
iter 369480 || Loss: 0.7323 || timer: 0.0912 sec.
iter 369490 || Loss: 0.8729 || timer: 0.0278 sec.
iter 369500 || Loss: 0.4339 || timer: 0.0909 sec.
iter 369510 || Loss: 0.7539 || timer: 0.0888 sec.
iter 369520 || Loss: 0.6747 || timer: 0.0910 sec.
iter 369530 || Loss: 0.6030 || timer: 0.0925 sec.
iter 369540 || Loss: 0.8039 || timer: 0.0916 sec.
iter 369550 || Loss: 0.7636 || timer: 0.0881 sec.
iter 369560 || Loss: 1.0031 || timer: 0.0929 sec.
iter 369570 || Loss: 0.6664 || timer: 0.0835 sec.
iter 369580 || Loss: 0.7084 || timer: 0.0898 sec.
iter 369590 || Loss: 0.6345 || timer: 0.1127 sec.
iter 369600 || Loss: 0.5878 || timer: 0.1005 sec.
iter 369610 || Loss: 0.7469 || timer: 0.0928 sec.
iter 369620 || Loss: 0.5966 || timer: 0.0830 sec.
iter 369630 || Loss: 0.6691 || timer: 0.0916 sec.
iter 369640 || Loss: 0.6143 || timer: 0.0887 sec.
iter 369650 || Loss: 0.7056 || timer: 0.1121 sec.
iter 369660 || Loss: 0.8623 || timer: 0.0917 sec.
iter 369670 || Loss: 0.6735 || timer: 0.0861 sec.
iter 369680 || Loss: 0.5925 || timer: 0.0910 sec.
iter 369690 || Loss: 0.5555 || timer: 0.0833 sec.
iter 369700 || Loss: 0.4728 || timer: 0.0839 sec.
iter 369710 || Loss: 0.6319 || timer: 0.1065 sec.
iter 369720 || Loss: 0.7071 || timer: 0.0858 sec.
iter 369730 || Loss: 0.8050 || timer: 0.0954 sec.
iter 369740 || Loss: 0.5873 || timer: 0.0898 sec.
iter 369750 || Loss: 0.6235 || timer: 0.0877 sec.
iter 369760 || Loss: 0.6047 || timer: 0.0921 sec.
iter 369770 || Loss: 0.8201 || timer: 0.0827 sec.
iter 369780 || Loss: 0.6408 || timer: 0.0880 sec.
iter 369790 || Loss: 0.5597 || timer: 0.0895 sec.
iter 369800 || Loss: 0.5883 || timer: 0.0888 sec.
iter 369810 || Loss: 0.6833 || timer: 0.1095 sec.
iter 369820 || Loss: 0.6841 || timer: 0.0172 sec.
iter 369830 || Loss: 0.5690 || timer: 0.0864 sec.
iter 369840 || Loss: 0.4591 || timer: 0.0864 sec.
iter 369850 || Loss: 0.5836 || timer: 0.0910 sec.
iter 369860 || Loss: 0.4992 || timer: 0.0845 sec.
iter 369870 || Loss: 0.4838 || timer: 0.0823 sec.
iter 369880 || Loss: 1.1143 || timer: 0.0823 sec.
iter 369890 || Loss: 0.5821 || timer: 0.1129 sec.
iter 369900 || Loss: 0.6982 || timer: 0.0915 sec.
iter 369910 || Loss: 0.8050 || timer: 0.0823 sec.
iter 369920 || Loss: 0.7865 || timer: 0.1372 sec.
iter 369930 || Loss: 0.6676 || timer: 0.0924 sec.
iter 369940 || Loss: 0.6211 || timer: 0.0898 sec.
iter 369950 || Loss: 0.7929 || timer: 0.1010 sec.
iter 369960 || Loss: 0.5207 || timer: 0.0890 sec.
iter 369970 || Loss: 0.6253 || timer: 0.0829 sec.
iter 369980 || Loss: 0.8922 || timer: 0.0878 sec.
iter 369990 || Loss: 0.7347 || timer: 0.0927 sec.
iter 370000 || Loss: 0.6894 || Saving state, iter: 370000
timer: 0.0857 sec.
iter 370010 || Loss: 0.7764 || timer: 0.0884 sec.
iter 370020 || Loss: 0.5024 || timer: 0.1048 sec.
iter 370030 || Loss: 0.9194 || timer: 0.1463 sec.
iter 370040 || Loss: 0.7453 || timer: 0.0831 sec.
iter 370050 || Loss: 0.6540 || timer: 0.0922 sec.
iter 370060 || Loss: 0.5955 || timer: 0.0978 sec.
iter 370070 || Loss: 0.7992 || timer: 0.0899 sec.
iter 370080 || Loss: 0.6998 || timer: 0.1058 sec.
iter 370090 || Loss: 0.7001 || timer: 0.0908 sec.
iter 370100 || Loss: 0.5724 || timer: 0.0892 sec.
iter 370110 || Loss: 0.8233 || timer: 0.1093 sec.
iter 370120 || Loss: 0.5687 || timer: 0.0917 sec.
iter 370130 || Loss: 0.5058 || timer: 0.0907 sec.
iter 370140 || Loss: 0.6008 || timer: 0.0875 sec.
iter 370150 || Loss: 0.6684 || timer: 0.0182 sec.
iter 370160 || Loss: 0.6433 || timer: 0.0906 sec.
iter 370170 || Loss: 0.6574 || timer: 0.0877 sec.
iter 370180 || Loss: 0.7921 || timer: 0.0920 sec.
iter 370190 || Loss: 0.5431 || timer: 0.0905 sec.
iter 370200 || Loss: 0.6584 || timer: 0.0854 sec.
iter 370210 || Loss: 1.0543 || timer: 0.0832 sec.
iter 370220 || Loss: 0.7019 || timer: 0.0861 sec.
iter 370230 || Loss: 0.3402 || timer: 0.0835 sec.
iter 370240 || Loss: 0.8487 || timer: 0.1025 sec.
iter 370250 || Loss: 0.8308 || timer: 0.1124 sec.
iter 370260 || Loss: 0.5858 || timer: 0.0861 sec.
iter 370270 || Loss: 0.6263 || timer: 0.0978 sec.
iter 370280 || Loss: 0.9923 || timer: 0.0890 sec.
iter 370290 || Loss: 0.8280 || timer: 0.0933 sec.
iter 370300 || Loss: 0.6445 || timer: 0.0911 sec.
iter 370310 || Loss: 0.7161 || timer: 0.0903 sec.
iter 370320 || Loss: 0.6062 || timer: 0.1051 sec.
iter 370330 || Loss: 0.6399 || timer: 0.1060 sec.
iter 370340 || Loss: 0.8093 || timer: 0.0869 sec.
iter 370350 || Loss: 0.6381 || timer: 0.0902 sec.
iter 370360 || Loss: 0.7105 || timer: 0.0828 sec.
iter 370370 || Loss: 0.5565 || timer: 0.0936 sec.
iter 370380 || Loss: 0.5987 || timer: 0.0861 sec.
iter 370390 || Loss: 0.5589 || timer: 0.0913 sec.
iter 370400 || Loss: 0.5161 || timer: 0.0924 sec.
iter 370410 || Loss: 0.8004 || timer: 0.0897 sec.
iter 370420 || Loss: 0.7991 || timer: 0.0917 sec.
iter 370430 || Loss: 0.7286 || timer: 0.0873 sec.
iter 370440 || Loss: 0.4357 || timer: 0.0918 sec.
iter 370450 || Loss: 0.6311 || timer: 0.0838 sec.
iter 370460 || Loss: 0.6697 || timer: 0.0904 sec.
iter 370470 || Loss: 0.6549 || timer: 0.1189 sec.
iter 370480 || Loss: 0.5505 || timer: 0.0195 sec.
iter 370490 || Loss: 1.2826 || timer: 0.0896 sec.
iter 370500 || Loss: 0.5407 || timer: 0.0979 sec.
iter 370510 || Loss: 0.8816 || timer: 0.0905 sec.
iter 370520 || Loss: 0.8396 || timer: 0.0927 sec.
iter 370530 || Loss: 0.9178 || timer: 0.0824 sec.
iter 370540 || Loss: 0.7728 || timer: 0.0820 sec.
iter 370550 || Loss: 0.9224 || timer: 0.0883 sec.
iter 370560 || Loss: 0.9626 || timer: 0.0872 sec.
iter 370570 || Loss: 0.5901 || timer: 0.0905 sec.
iter 370580 || Loss: 0.6650 || timer: 0.0989 sec.
iter 370590 || Loss: 0.6277 || timer: 0.1257 sec.
iter 370600 || Loss: 0.7040 || timer: 0.0894 sec.
iter 370610 || Loss: 0.7964 || timer: 0.0911 sec.
iter 370620 || Loss: 0.5460 || timer: 0.0939 sec.
iter 370630 || Loss: 0.8483 || timer: 0.0911 sec.
iter 370640 || Loss: 0.7766 || timer: 0.0913 sec.
iter 370650 || Loss: 0.5216 || timer: 0.1203 sec.
iter 370660 || Loss: 0.7323 || timer: 0.0917 sec.
iter 370670 || Loss: 0.4857 || timer: 0.0896 sec.
iter 370680 || Loss: 0.5532 || timer: 0.0894 sec.
iter 370690 || Loss: 0.7113 || timer: 0.0848 sec.
iter 370700 || Loss: 0.9331 || timer: 0.0841 sec.
iter 370710 || Loss: 0.5469 || timer: 0.0826 sec.
iter 370720 || Loss: 0.5145 || timer: 0.1038 sec.
iter 370730 || Loss: 0.6100 || timer: 0.0844 sec.
iter 370740 || Loss: 0.7559 || timer: 0.1030 sec.
iter 370750 || Loss: 0.5332 || timer: 0.0830 sec.
iter 370760 || Loss: 0.8580 || timer: 0.0889 sec.
iter 370770 || Loss: 0.6531 || timer: 0.0843 sec.
iter 370780 || Loss: 0.7266 || timer: 0.0903 sec.
iter 370790 || Loss: 0.8756 || timer: 0.0906 sec.
iter 370800 || Loss: 0.9048 || timer: 0.0920 sec.
iter 370810 || Loss: 0.7619 || timer: 0.0275 sec.
iter 370820 || Loss: 0.4783 || timer: 0.0995 sec.
iter 370830 || Loss: 0.7906 || timer: 0.1131 sec.
iter 370840 || Loss: 0.4643 || timer: 0.0899 sec.
iter 370850 || Loss: 0.7064 || timer: 0.0939 sec.
iter 370860 || Loss: 0.6326 || timer: 0.0948 sec.
iter 370870 || Loss: 0.7562 || timer: 0.0837 sec.
iter 370880 || Loss: 0.8656 || timer: 0.0862 sec.
iter 370890 || Loss: 0.5949 || timer: 0.1070 sec.
iter 370900 || Loss: 0.6277 || timer: 0.0848 sec.
iter 370910 || Loss: 0.8824 || timer: 0.1011 sec.
iter 370920 || Loss: 0.5249 || timer: 0.0847 sec.
iter 370930 || Loss: 0.6759 || timer: 0.0888 sec.
iter 370940 || Loss: 0.5920 || timer: 0.0899 sec.
iter 370950 || Loss: 0.7372 || timer: 0.0923 sec.
iter 370960 || Loss: 0.7954 || timer: 0.0907 sec.
iter 370970 || Loss: 0.7624 || timer: 0.1122 sec.
iter 370980 || Loss: 0.4772 || timer: 0.1046 sec.
iter 370990 || Loss: 0.5367 || timer: 0.0838 sec.
iter 371000 || Loss: 0.7725 || timer: 0.0892 sec.
iter 371010 || Loss: 0.6014 || timer: 0.0915 sec.
iter 371020 || Loss: 0.7008 || timer: 0.0906 sec.
iter 371030 || Loss: 0.7766 || timer: 0.0910 sec.
iter 371040 || Loss: 0.5599 || timer: 0.0910 sec.
iter 371050 || Loss: 0.7963 || timer: 0.0897 sec.
iter 371060 || Loss: 0.6102 || timer: 0.0950 sec.
iter 371070 || Loss: 0.9302 || timer: 0.0823 sec.
iter 371080 || Loss: 0.6490 || timer: 0.0870 sec.
iter 371090 || Loss: 0.5261 || timer: 0.1028 sec.
iter 371100 || Loss: 0.8823 || timer: 0.0903 sec.
iter 371110 || Loss: 0.4666 || timer: 0.0918 sec.
iter 371120 || Loss: 0.6687 || timer: 0.0973 sec.
iter 371130 || Loss: 0.5887 || timer: 0.0892 sec.
iter 371140 || Loss: 0.6577 || timer: 0.0262 sec.
iter 371150 || Loss: 0.9289 || timer: 0.0924 sec.
iter 371160 || Loss: 0.5366 || timer: 0.0853 sec.
iter 371170 || Loss: 0.6806 || timer: 0.0910 sec.
iter 371180 || Loss: 0.7261 || timer: 0.0842 sec.
iter 371190 || Loss: 1.1395 || timer: 0.1213 sec.
iter 371200 || Loss: 0.4798 || timer: 0.1048 sec.
iter 371210 || Loss: 0.8681 || timer: 0.0908 sec.
iter 371220 || Loss: 0.5845 || timer: 0.0901 sec.
iter 371230 || Loss: 0.5935 || timer: 0.0891 sec.
iter 371240 || Loss: 0.4593 || timer: 0.1025 sec.
iter 371250 || Loss: 0.5640 || timer: 0.0895 sec.
iter 371260 || Loss: 0.7026 || timer: 0.0842 sec.
iter 371270 || Loss: 0.7530 || timer: 0.1088 sec.
iter 371280 || Loss: 0.9380 || timer: 0.1117 sec.
iter 371290 || Loss: 0.6052 || timer: 0.0919 sec.
iter 371300 || Loss: 0.7510 || timer: 0.0910 sec.
iter 371310 || Loss: 0.5681 || timer: 0.0887 sec.
iter 371320 || Loss: 0.7238 || timer: 0.0823 sec.
iter 371330 || Loss: 0.8248 || timer: 0.0941 sec.
iter 371340 || Loss: 0.6201 || timer: 0.0847 sec.
iter 371350 || Loss: 0.7146 || timer: 0.0917 sec.
iter 371360 || Loss: 0.8148 || timer: 0.0918 sec.
iter 371370 || Loss: 0.6464 || timer: 0.0919 sec.
iter 371380 || Loss: 0.6366 || timer: 0.1031 sec.
iter 371390 || Loss: 0.6376 || timer: 0.0980 sec.
iter 371400 || Loss: 0.6322 || timer: 0.0828 sec.
iter 371410 || Loss: 0.5790 || timer: 0.0849 sec.
iter 371420 || Loss: 0.3514 || timer: 0.0912 sec.
iter 371430 || Loss: 0.6521 || timer: 0.0902 sec.
iter 371440 || Loss: 0.5001 || timer: 0.0818 sec.
iter 371450 || Loss: 0.9092 || timer: 0.0828 sec.
iter 371460 || Loss: 0.7087 || timer: 0.1033 sec.
iter 371470 || Loss: 0.7139 || timer: 0.0253 sec.
iter 371480 || Loss: 1.0884 || timer: 0.0833 sec.
iter 371490 || Loss: 0.5849 || timer: 0.0921 sec.
iter 371500 || Loss: 0.7892 || timer: 0.0923 sec.
iter 371510 || Loss: 0.7107 || timer: 0.1084 sec.
iter 371520 || Loss: 0.5866 || timer: 0.0894 sec.
iter 371530 || Loss: 0.7773 || timer: 0.1056 sec.
iter 371540 || Loss: 0.4226 || timer: 0.0976 sec.
iter 371550 || Loss: 0.5750 || timer: 0.0930 sec.
iter 371560 || Loss: 0.6999 || timer: 0.1020 sec.
iter 371570 || Loss: 0.4922 || timer: 0.1195 sec.
iter 371580 || Loss: 0.5397 || timer: 0.0916 sec.
iter 371590 || Loss: 0.7968 || timer: 0.0840 sec.
iter 371600 || Loss: 0.6530 || timer: 0.0834 sec.
iter 371610 || Loss: 0.7407 || timer: 0.0920 sec.
iter 371620 || Loss: 0.5976 || timer: 0.0967 sec.
iter 371630 || Loss: 0.8415 || timer: 0.0860 sec.
iter 371640 || Loss: 0.4789 || timer: 0.0901 sec.
iter 371650 || Loss: 0.6322 || timer: 0.1072 sec.
iter 371660 || Loss: 0.7645 || timer: 0.0826 sec.
iter 371670 || Loss: 0.5324 || timer: 0.1274 sec.
iter 371680 || Loss: 0.6912 || timer: 0.0886 sec.
iter 371690 || Loss: 0.9433 || timer: 0.0980 sec.
iter 371700 || Loss: 0.5804 || timer: 0.0839 sec.
iter 371710 || Loss: 0.6838 || timer: 0.0910 sec.
iter 371720 || Loss: 0.7981 || timer: 0.0824 sec.
iter 371730 || Loss: 0.5696 || timer: 0.1015 sec.
iter 371740 || Loss: 0.7580 || timer: 0.0883 sec.
iter 371750 || Loss: 0.9032 || timer: 0.0932 sec.
iter 371760 || Loss: 0.5360 || timer: 0.0830 sec.
iter 371770 || Loss: 0.4619 || timer: 0.0844 sec.
iter 371780 || Loss: 0.6557 || timer: 0.0915 sec.
iter 371790 || Loss: 0.6165 || timer: 0.0829 sec.
iter 371800 || Loss: 0.8091 || timer: 0.0201 sec.
iter 371810 || Loss: 0.7312 || timer: 0.0817 sec.
iter 371820 || Loss: 0.4619 || timer: 0.1058 sec.
iter 371830 || Loss: 0.5728 || timer: 0.0837 sec.
iter 371840 || Loss: 0.7661 || timer: 0.1170 sec.
iter 371850 || Loss: 0.7601 || timer: 0.0910 sec.
iter 371860 || Loss: 0.6327 || timer: 0.0898 sec.
iter 371870 || Loss: 0.7991 || timer: 0.0886 sec.
iter 371880 || Loss: 0.6388 || timer: 0.0854 sec.
iter 371890 || Loss: 0.5575 || timer: 0.0939 sec.
iter 371900 || Loss: 0.6128 || timer: 0.1137 sec.
iter 371910 || Loss: 0.7342 || timer: 0.1073 sec.
iter 371920 || Loss: 0.6892 || timer: 0.1127 sec.
iter 371930 || Loss: 0.5403 || timer: 0.0942 sec.
iter 371940 || Loss: 0.8180 || timer: 0.0850 sec.
iter 371950 || Loss: 0.8059 || timer: 0.0841 sec.
iter 371960 || Loss: 0.6103 || timer: 0.1051 sec.
iter 371970 || Loss: 0.5135 || timer: 0.0944 sec.
iter 371980 || Loss: 1.0714 || timer: 0.1164 sec.
iter 371990 || Loss: 0.5549 || timer: 0.0838 sec.
iter 372000 || Loss: 0.6677 || timer: 0.1079 sec.
iter 372010 || Loss: 0.8375 || timer: 0.0847 sec.
iter 372020 || Loss: 0.6940 || timer: 0.0847 sec.
iter 372030 || Loss: 0.6452 || timer: 0.0828 sec.
iter 372040 || Loss: 0.6398 || timer: 0.0905 sec.
iter 372050 || Loss: 0.5992 || timer: 0.1135 sec.
iter 372060 || Loss: 0.6725 || timer: 0.0912 sec.
iter 372070 || Loss: 0.5881 || timer: 0.0894 sec.
iter 372080 || Loss: 0.7037 || timer: 0.0839 sec.
iter 372090 || Loss: 0.7897 || timer: 0.0909 sec.
iter 372100 || Loss: 0.6387 || timer: 0.0830 sec.
iter 372110 || Loss: 0.9789 || timer: 0.0833 sec.
iter 372120 || Loss: 0.9349 || timer: 0.1073 sec.
iter 372130 || Loss: 0.8303 || timer: 0.0213 sec.
iter 372140 || Loss: 0.1745 || timer: 0.0877 sec.
iter 372150 || Loss: 0.6813 || timer: 0.0827 sec.
iter 372160 || Loss: 0.9213 || timer: 0.0841 sec.
iter 372170 || Loss: 0.6041 || timer: 0.0906 sec.
iter 372180 || Loss: 0.7054 || timer: 0.1127 sec.
iter 372190 || Loss: 0.7210 || timer: 0.1084 sec.
iter 372200 || Loss: 1.0248 || timer: 0.1101 sec.
iter 372210 || Loss: 0.5918 || timer: 0.0829 sec.
iter 372220 || Loss: 0.6383 || timer: 0.0838 sec.
iter 372230 || Loss: 0.5652 || timer: 0.1196 sec.
iter 372240 || Loss: 0.7209 || timer: 0.0841 sec.
iter 372250 || Loss: 0.5222 || timer: 0.0906 sec.
iter 372260 || Loss: 0.8518 || timer: 0.0925 sec.
iter 372270 || Loss: 0.8286 || timer: 0.0821 sec.
iter 372280 || Loss: 0.7416 || timer: 0.0905 sec.
iter 372290 || Loss: 0.7504 || timer: 0.0872 sec.
iter 372300 || Loss: 0.7216 || timer: 0.0828 sec.
iter 372310 || Loss: 0.5375 || timer: 0.1064 sec.
iter 372320 || Loss: 0.5279 || timer: 0.0816 sec.
iter 372330 || Loss: 0.5444 || timer: 0.1134 sec.
iter 372340 || Loss: 0.4507 || timer: 0.0828 sec.
iter 372350 || Loss: 0.7697 || timer: 0.1030 sec.
iter 372360 || Loss: 0.4295 || timer: 0.0833 sec.
iter 372370 || Loss: 0.5821 || timer: 0.0927 sec.
iter 372380 || Loss: 0.6527 || timer: 0.0893 sec.
iter 372390 || Loss: 0.6335 || timer: 0.0945 sec.
iter 372400 || Loss: 0.6497 || timer: 0.0902 sec.
iter 372410 || Loss: 0.5502 || timer: 0.0897 sec.
iter 372420 || Loss: 0.5455 || timer: 0.1052 sec.
iter 372430 || Loss: 0.4513 || timer: 0.0836 sec.
iter 372440 || Loss: 0.4704 || timer: 0.0981 sec.
iter 372450 || Loss: 0.8122 || timer: 0.0915 sec.
iter 372460 || Loss: 0.7088 || timer: 0.0172 sec.
iter 372470 || Loss: 0.6921 || timer: 0.1034 sec.
iter 372480 || Loss: 0.7534 || timer: 0.0837 sec.
iter 372490 || Loss: 0.6170 || timer: 0.0919 sec.
iter 372500 || Loss: 0.8414 || timer: 0.0926 sec.
iter 372510 || Loss: 0.6980 || timer: 0.1084 sec.
iter 372520 || Loss: 0.5597 || timer: 0.0846 sec.
iter 372530 || Loss: 0.5846 || timer: 0.0880 sec.
iter 372540 || Loss: 0.6855 || timer: 0.0841 sec.
iter 372550 || Loss: 0.4818 || timer: 0.0839 sec.
iter 372560 || Loss: 0.6628 || timer: 0.0954 sec.
iter 372570 || Loss: 0.8293 || timer: 0.0897 sec.
iter 372580 || Loss: 0.6226 || timer: 0.0894 sec.
iter 372590 || Loss: 0.6747 || timer: 0.0941 sec.
iter 372600 || Loss: 0.6039 || timer: 0.0823 sec.
iter 372610 || Loss: 0.4373 || timer: 0.0935 sec.
iter 372620 || Loss: 0.4798 || timer: 0.0933 sec.
iter 372630 || Loss: 0.6070 || timer: 0.1092 sec.
iter 372640 || Loss: 0.6658 || timer: 0.0827 sec.
iter 372650 || Loss: 0.7372 || timer: 0.0917 sec.
iter 372660 || Loss: 0.5679 || timer: 0.0842 sec.
iter 372670 || Loss: 0.6610 || timer: 0.0904 sec.
iter 372680 || Loss: 0.4290 || timer: 0.1018 sec.
iter 372690 || Loss: 0.5889 || timer: 0.0893 sec.
iter 372700 || Loss: 0.6082 || timer: 0.0895 sec.
iter 372710 || Loss: 0.7206 || timer: 0.0907 sec.
iter 372720 || Loss: 0.6794 || timer: 0.0822 sec.
iter 372730 || Loss: 0.6952 || timer: 0.0930 sec.
iter 372740 || Loss: 0.8779 || timer: 0.0845 sec.
iter 372750 || Loss: 0.5556 || timer: 0.0914 sec.
iter 372760 || Loss: 0.6111 || timer: 0.0894 sec.
iter 372770 || Loss: 0.8042 || timer: 0.0818 sec.
iter 372780 || Loss: 0.6781 || timer: 0.1004 sec.
iter 372790 || Loss: 0.6757 || timer: 0.0252 sec.
iter 372800 || Loss: 1.5230 || timer: 0.0926 sec.
iter 372810 || Loss: 0.6491 || timer: 0.0915 sec.
iter 372820 || Loss: 0.5733 || timer: 0.0857 sec.
iter 372830 || Loss: 0.5341 || timer: 0.0823 sec.
iter 372840 || Loss: 0.8509 || timer: 0.0901 sec.
iter 372850 || Loss: 0.6014 || timer: 0.1051 sec.
iter 372860 || Loss: 0.8322 || timer: 0.0941 sec.
iter 372870 || Loss: 0.9805 || timer: 0.0882 sec.
iter 372880 || Loss: 0.7200 || timer: 0.0825 sec.
iter 372890 || Loss: 0.6008 || timer: 0.1155 sec.
iter 372900 || Loss: 0.6154 || timer: 0.0834 sec.
iter 372910 || Loss: 0.7493 || timer: 0.0840 sec.
iter 372920 || Loss: 0.7856 || timer: 0.0919 sec.
iter 372930 || Loss: 0.5432 || timer: 0.0835 sec.
iter 372940 || Loss: 0.7155 || timer: 0.0838 sec.
iter 372950 || Loss: 0.6101 || timer: 0.0895 sec.
iter 372960 || Loss: 0.5385 || timer: 0.1094 sec.
iter 372970 || Loss: 0.8523 || timer: 0.0893 sec.
iter 372980 || Loss: 0.5634 || timer: 0.0896 sec.
iter 372990 || Loss: 0.7302 || timer: 0.0967 sec.
iter 373000 || Loss: 0.6468 || timer: 0.1119 sec.
iter 373010 || Loss: 0.5318 || timer: 0.1052 sec.
iter 373020 || Loss: 0.5219 || timer: 0.0877 sec.
iter 373030 || Loss: 0.7301 || timer: 0.0753 sec.
iter 373040 || Loss: 0.7194 || timer: 0.1081 sec.
iter 373050 || Loss: 0.6906 || timer: 0.0844 sec.
iter 373060 || Loss: 0.5219 || timer: 0.0829 sec.
iter 373070 || Loss: 0.7506 || timer: 0.0918 sec.
iter 373080 || Loss: 0.4100 || timer: 0.0822 sec.
iter 373090 || Loss: 0.7024 || timer: 0.0906 sec.
iter 373100 || Loss: 0.6369 || timer: 0.0861 sec.
iter 373110 || Loss: 0.8935 || timer: 0.1094 sec.
iter 373120 || Loss: 0.6645 || timer: 0.0233 sec.
iter 373130 || Loss: 0.4186 || timer: 0.0832 sec.
iter 373140 || Loss: 0.5898 || timer: 0.1115 sec.
iter 373150 || Loss: 0.6570 || timer: 0.0873 sec.
iter 373160 || Loss: 0.6402 || timer: 0.0831 sec.
iter 373170 || Loss: 0.8145 || timer: 0.0893 sec.
iter 373180 || Loss: 0.4934 || timer: 0.1124 sec.
iter 373190 || Loss: 0.6416 || timer: 0.0828 sec.
iter 373200 || Loss: 0.4994 || timer: 0.0934 sec.
iter 373210 || Loss: 0.6825 || timer: 0.0920 sec.
iter 373220 || Loss: 0.5656 || timer: 0.1060 sec.
iter 373230 || Loss: 0.6398 || timer: 0.0841 sec.
iter 373240 || Loss: 0.9225 || timer: 0.0892 sec.
iter 373250 || Loss: 0.7832 || timer: 0.0889 sec.
iter 373260 || Loss: 0.5968 || timer: 0.1015 sec.
iter 373270 || Loss: 0.8326 || timer: 0.0829 sec.
iter 373280 || Loss: 0.4375 || timer: 0.0963 sec.
iter 373290 || Loss: 0.6722 || timer: 0.0897 sec.
iter 373300 || Loss: 0.8119 || timer: 0.0932 sec.
iter 373310 || Loss: 0.9156 || timer: 0.0841 sec.
iter 373320 || Loss: 0.7882 || timer: 0.0887 sec.
iter 373330 || Loss: 0.5420 || timer: 0.0922 sec.
iter 373340 || Loss: 0.6100 || timer: 0.1092 sec.
iter 373350 || Loss: 0.6916 || timer: 0.0942 sec.
iter 373360 || Loss: 0.4997 || timer: 0.1090 sec.
iter 373370 || Loss: 0.4076 || timer: 0.0910 sec.
iter 373380 || Loss: 0.6906 || timer: 0.0954 sec.
iter 373390 || Loss: 0.7790 || timer: 0.1083 sec.
iter 373400 || Loss: 0.6012 || timer: 0.1043 sec.
iter 373410 || Loss: 0.5671 || timer: 0.0915 sec.
iter 373420 || Loss: 0.7798 || timer: 0.0926 sec.
iter 373430 || Loss: 0.5216 || timer: 0.0929 sec.
iter 373440 || Loss: 0.6953 || timer: 0.0926 sec.
iter 373450 || Loss: 0.6370 || timer: 0.0176 sec.
iter 373460 || Loss: 0.0940 || timer: 0.0834 sec.
iter 373470 || Loss: 0.6563 || timer: 0.0906 sec.
iter 373480 || Loss: 0.7233 || timer: 0.0955 sec.
iter 373490 || Loss: 0.5692 || timer: 0.0838 sec.
iter 373500 || Loss: 0.7804 || timer: 0.0887 sec.
iter 373510 || Loss: 0.5745 || timer: 0.1140 sec.
iter 373520 || Loss: 0.5133 || timer: 0.0922 sec.
iter 373530 || Loss: 0.8200 || timer: 0.0934 sec.
iter 373540 || Loss: 0.5020 || timer: 0.0915 sec.
iter 373550 || Loss: 0.5690 || timer: 0.0980 sec.
iter 373560 || Loss: 0.7196 || timer: 0.0832 sec.
iter 373570 || Loss: 0.8578 || timer: 0.0822 sec.
iter 373580 || Loss: 0.7125 || timer: 0.0885 sec.
iter 373590 || Loss: 0.8510 || timer: 0.1227 sec.
iter 373600 || Loss: 0.6198 || timer: 0.0824 sec.
iter 373610 || Loss: 0.7107 || timer: 0.0922 sec.
iter 373620 || Loss: 0.8636 || timer: 0.0905 sec.
iter 373630 || Loss: 0.7338 || timer: 0.0919 sec.
iter 373640 || Loss: 1.0019 || timer: 0.1091 sec.
iter 373650 || Loss: 0.8239 || timer: 0.0966 sec.
iter 373660 || Loss: 0.4040 || timer: 0.1043 sec.
iter 373670 || Loss: 0.9538 || timer: 0.1133 sec.
iter 373680 || Loss: 0.7162 || timer: 0.0903 sec.
iter 373690 || Loss: 0.8068 || timer: 0.0828 sec.
iter 373700 || Loss: 0.7359 || timer: 0.0922 sec.
iter 373710 || Loss: 0.6216 || timer: 0.0938 sec.
iter 373720 || Loss: 0.6224 || timer: 0.0926 sec.
iter 373730 || Loss: 0.6426 || timer: 0.0900 sec.
iter 373740 || Loss: 0.8908 || timer: 0.0943 sec.
iter 373750 || Loss: 0.9757 || timer: 0.0915 sec.
iter 373760 || Loss: 0.9857 || timer: 0.1089 sec.
iter 373770 || Loss: 0.6451 || timer: 0.0922 sec.
iter 373780 || Loss: 0.5924 || timer: 0.0261 sec.
iter 373790 || Loss: 0.6186 || timer: 0.1025 sec.
iter 373800 || Loss: 0.6521 || timer: 0.1037 sec.
iter 373810 || Loss: 0.8167 || timer: 0.0879 sec.
iter 373820 || Loss: 0.9735 || timer: 0.0834 sec.
iter 373830 || Loss: 0.6304 || timer: 0.0957 sec.
iter 373840 || Loss: 0.4460 || timer: 0.0965 sec.
iter 373850 || Loss: 0.6483 || timer: 0.0911 sec.
iter 373860 || Loss: 0.7503 || timer: 0.0811 sec.
iter 373870 || Loss: 0.6937 || timer: 0.0822 sec.
iter 373880 || Loss: 0.5447 || timer: 0.1009 sec.
iter 373890 || Loss: 0.7334 || timer: 0.0837 sec.
iter 373900 || Loss: 0.6301 || timer: 0.0903 sec.
iter 373910 || Loss: 0.4738 || timer: 0.0914 sec.
iter 373920 || Loss: 0.8270 || timer: 0.0828 sec.
iter 373930 || Loss: 0.6995 || timer: 0.0872 sec.
iter 373940 || Loss: 1.2880 || timer: 0.0842 sec.
iter 373950 || Loss: 0.6404 || timer: 0.0834 sec.
iter 373960 || Loss: 0.7553 || timer: 0.0883 sec.
iter 373970 || Loss: 0.5478 || timer: 0.0865 sec.
iter 373980 || Loss: 0.6278 || timer: 0.0827 sec.
iter 373990 || Loss: 0.7566 || timer: 0.0831 sec.
iter 374000 || Loss: 0.7901 || timer: 0.0931 sec.
iter 374010 || Loss: 0.4835 || timer: 0.0910 sec.
iter 374020 || Loss: 0.5499 || timer: 0.0908 sec.
iter 374030 || Loss: 0.8065 || timer: 0.0914 sec.
iter 374040 || Loss: 0.5442 || timer: 0.0929 sec.
iter 374050 || Loss: 0.6555 || timer: 0.0941 sec.
iter 374060 || Loss: 0.5769 || timer: 0.0822 sec.
iter 374070 || Loss: 0.5748 || timer: 0.0895 sec.
iter 374080 || Loss: 0.7438 || timer: 0.0827 sec.
iter 374090 || Loss: 0.7476 || timer: 0.1070 sec.
iter 374100 || Loss: 0.6441 || timer: 0.0877 sec.
iter 374110 || Loss: 0.6920 || timer: 0.0263 sec.
iter 374120 || Loss: 0.5391 || timer: 0.0879 sec.
iter 374130 || Loss: 0.6901 || timer: 0.1089 sec.
iter 374140 || Loss: 0.7085 || timer: 0.1121 sec.
iter 374150 || Loss: 0.6756 || timer: 0.0883 sec.
iter 374160 || Loss: 0.6298 || timer: 0.0907 sec.
iter 374170 || Loss: 0.7615 || timer: 0.0884 sec.
iter 374180 || Loss: 0.5834 || timer: 0.1017 sec.
iter 374190 || Loss: 0.5862 || timer: 0.0915 sec.
iter 374200 || Loss: 0.8290 || timer: 0.0890 sec.
iter 374210 || Loss: 0.6006 || timer: 0.1000 sec.
iter 374220 || Loss: 0.6761 || timer: 0.1002 sec.
iter 374230 || Loss: 0.6230 || timer: 0.0925 sec.
iter 374240 || Loss: 0.7180 || timer: 0.0838 sec.
iter 374250 || Loss: 0.7327 || timer: 0.0880 sec.
iter 374260 || Loss: 0.7003 || timer: 0.1040 sec.
iter 374270 || Loss: 0.6799 || timer: 0.0881 sec.
iter 374280 || Loss: 0.8243 || timer: 0.0923 sec.
iter 374290 || Loss: 0.9428 || timer: 0.1040 sec.
iter 374300 || Loss: 0.6834 || timer: 0.0916 sec.
iter 374310 || Loss: 0.7472 || timer: 0.1090 sec.
iter 374320 || Loss: 0.9142 || timer: 0.0835 sec.
iter 374330 || Loss: 0.9847 || timer: 0.0926 sec.
iter 374340 || Loss: 0.7500 || timer: 0.0882 sec.
iter 374350 || Loss: 0.7443 || timer: 0.1055 sec.
iter 374360 || Loss: 0.7226 || timer: 0.1059 sec.
iter 374370 || Loss: 0.6320 || timer: 0.0938 sec.
iter 374380 || Loss: 0.5071 || timer: 0.0921 sec.
iter 374390 || Loss: 0.6686 || timer: 0.1119 sec.
iter 374400 || Loss: 0.8306 || timer: 0.0842 sec.
iter 374410 || Loss: 0.7718 || timer: 0.0887 sec.
iter 374420 || Loss: 0.7408 || timer: 0.0885 sec.
iter 374430 || Loss: 0.7735 || timer: 0.0833 sec.
iter 374440 || Loss: 0.4016 || timer: 0.0242 sec.
iter 374450 || Loss: 0.5616 || timer: 0.0852 sec.
iter 374460 || Loss: 0.6371 || timer: 0.0822 sec.
iter 374470 || Loss: 0.5797 || timer: 0.0904 sec.
iter 374480 || Loss: 0.7775 || timer: 0.0912 sec.
iter 374490 || Loss: 0.6568 || timer: 0.0927 sec.
iter 374500 || Loss: 0.6669 || timer: 0.0883 sec.
iter 374510 || Loss: 0.5941 || timer: 0.0896 sec.
iter 374520 || Loss: 0.5830 || timer: 0.0903 sec.
iter 374530 || Loss: 1.2541 || timer: 0.0933 sec.
iter 374540 || Loss: 0.5009 || timer: 0.1208 sec.
iter 374550 || Loss: 0.6026 || timer: 0.0897 sec.
iter 374560 || Loss: 0.6651 || timer: 0.0921 sec.
iter 374570 || Loss: 0.7565 || timer: 0.0917 sec.
iter 374580 || Loss: 0.6083 || timer: 0.0833 sec.
iter 374590 || Loss: 0.7694 || timer: 0.0901 sec.
iter 374600 || Loss: 0.4615 || timer: 0.1153 sec.
iter 374610 || Loss: 0.5417 || timer: 0.0934 sec.
iter 374620 || Loss: 0.7798 || timer: 0.0916 sec.
iter 374630 || Loss: 0.7494 || timer: 0.0909 sec.
iter 374640 || Loss: 0.7882 || timer: 0.1110 sec.
iter 374650 || Loss: 0.6142 || timer: 0.0832 sec.
iter 374660 || Loss: 0.6666 || timer: 0.0901 sec.
iter 374670 || Loss: 0.6885 || timer: 0.0899 sec.
iter 374680 || Loss: 0.6397 || timer: 0.0925 sec.
iter 374690 || Loss: 0.5687 || timer: 0.0891 sec.
iter 374700 || Loss: 0.6426 || timer: 0.0886 sec.
iter 374710 || Loss: 0.6146 || timer: 0.0880 sec.
iter 374720 || Loss: 1.0004 || timer: 0.1098 sec.
iter 374730 || Loss: 0.6656 || timer: 0.0838 sec.
iter 374740 || Loss: 0.6625 || timer: 0.0832 sec.
iter 374750 || Loss: 0.7723 || timer: 0.0924 sec.
iter 374760 || Loss: 0.9159 || timer: 0.0898 sec.
iter 374770 || Loss: 0.7811 || timer: 0.0263 sec.
iter 374780 || Loss: 1.1799 || timer: 0.0912 sec.
iter 374790 || Loss: 0.7275 || timer: 0.0832 sec.
iter 374800 || Loss: 0.9095 || timer: 0.0871 sec.
iter 374810 || Loss: 0.7167 || timer: 0.1006 sec.
iter 374820 || Loss: 0.7971 || timer: 0.0903 sec.
iter 374830 || Loss: 0.9863 || timer: 0.0915 sec.
iter 374840 || Loss: 0.7097 || timer: 0.1052 sec.
iter 374850 || Loss: 0.6202 || timer: 0.0902 sec.
iter 374860 || Loss: 0.8421 || timer: 0.0907 sec.
iter 374870 || Loss: 0.5676 || timer: 0.0967 sec.
iter 374880 || Loss: 0.7418 || timer: 0.0922 sec.
iter 374890 || Loss: 0.7079 || timer: 0.0831 sec.
iter 374900 || Loss: 0.7391 || timer: 0.0898 sec.
iter 374910 || Loss: 0.7528 || timer: 0.0904 sec.
iter 374920 || Loss: 0.6330 || timer: 0.0900 sec.
iter 374930 || Loss: 0.6412 || timer: 0.0827 sec.
iter 374940 || Loss: 0.7641 || timer: 0.0831 sec.
iter 374950 || Loss: 0.7710 || timer: 0.0895 sec.
iter 374960 || Loss: 0.8561 || timer: 0.0922 sec.
iter 374970 || Loss: 0.5853 || timer: 0.1025 sec.
iter 374980 || Loss: 0.7558 || timer: 0.0894 sec.
iter 374990 || Loss: 0.6772 || timer: 0.0834 sec.
iter 375000 || Loss: 0.6233 || Saving state, iter: 375000
timer: 0.0962 sec.
iter 375010 || Loss: 0.8345 || timer: 0.0884 sec.
iter 375020 || Loss: 0.6952 || timer: 0.0896 sec.
iter 375030 || Loss: 0.6708 || timer: 0.0889 sec.
iter 375040 || Loss: 0.7511 || timer: 0.0902 sec.
iter 375050 || Loss: 0.5566 || timer: 0.0815 sec.
iter 375060 || Loss: 0.9939 || timer: 0.0904 sec.
iter 375070 || Loss: 0.8185 || timer: 0.0913 sec.
iter 375080 || Loss: 0.7197 || timer: 0.1154 sec.
iter 375090 || Loss: 0.6635 || timer: 0.0922 sec.
iter 375100 || Loss: 0.8982 || timer: 0.0216 sec.
iter 375110 || Loss: 0.1619 || timer: 0.0924 sec.
iter 375120 || Loss: 0.9344 || timer: 0.0904 sec.
iter 375130 || Loss: 0.5939 || timer: 0.0920 sec.
iter 375140 || Loss: 0.7222 || timer: 0.0923 sec.
iter 375150 || Loss: 0.7459 || timer: 0.1125 sec.
iter 375160 || Loss: 0.6568 || timer: 0.0845 sec.
iter 375170 || Loss: 0.7764 || timer: 0.0853 sec.
iter 375180 || Loss: 0.7249 || timer: 0.1148 sec.
iter 375190 || Loss: 0.6386 || timer: 0.1048 sec.
iter 375200 || Loss: 0.8918 || timer: 0.0979 sec.
iter 375210 || Loss: 0.4246 || timer: 0.0871 sec.
iter 375220 || Loss: 0.9241 || timer: 0.0892 sec.
iter 375230 || Loss: 0.5156 || timer: 0.0844 sec.
iter 375240 || Loss: 0.8196 || timer: 0.0838 sec.
iter 375250 || Loss: 0.7060 || timer: 0.1005 sec.
iter 375260 || Loss: 0.7758 || timer: 0.0903 sec.
iter 375270 || Loss: 0.5477 || timer: 0.1000 sec.
iter 375280 || Loss: 0.4416 || timer: 0.0834 sec.
iter 375290 || Loss: 0.7904 || timer: 0.0996 sec.
iter 375300 || Loss: 0.6305 || timer: 0.0833 sec.
iter 375310 || Loss: 0.6768 || timer: 0.0817 sec.
iter 375320 || Loss: 0.6913 || timer: 0.0889 sec.
iter 375330 || Loss: 0.6140 || timer: 0.0920 sec.
iter 375340 || Loss: 0.9146 || timer: 0.0922 sec.
iter 375350 || Loss: 0.6596 || timer: 0.0834 sec.
iter 375360 || Loss: 0.8018 || timer: 0.0904 sec.
iter 375370 || Loss: 0.6717 || timer: 0.0978 sec.
iter 375380 || Loss: 0.9617 || timer: 0.0954 sec.
iter 375390 || Loss: 0.7558 || timer: 0.1111 sec.
iter 375400 || Loss: 0.8699 || timer: 0.1063 sec.
iter 375410 || Loss: 0.6995 || timer: 0.1016 sec.
iter 375420 || Loss: 0.5576 || timer: 0.0898 sec.
iter 375430 || Loss: 0.5793 || timer: 0.0155 sec.
iter 375440 || Loss: 0.3798 || timer: 0.0973 sec.
iter 375450 || Loss: 0.6542 || timer: 0.1098 sec.
iter 375460 || Loss: 0.5137 || timer: 0.0827 sec.
iter 375470 || Loss: 0.7008 || timer: 0.0921 sec.
iter 375480 || Loss: 0.5683 || timer: 0.0918 sec.
iter 375490 || Loss: 0.7450 || timer: 0.0917 sec.
iter 375500 || Loss: 0.7504 || timer: 0.0867 sec.
iter 375510 || Loss: 0.7854 || timer: 0.0835 sec.
iter 375520 || Loss: 0.6075 || timer: 0.0905 sec.
iter 375530 || Loss: 0.8629 || timer: 0.0951 sec.
iter 375540 || Loss: 0.7341 || timer: 0.0770 sec.
iter 375550 || Loss: 0.6992 || timer: 0.1036 sec.
iter 375560 || Loss: 0.6549 || timer: 0.0840 sec.
iter 375570 || Loss: 0.3969 || timer: 0.0964 sec.
iter 375580 || Loss: 0.6745 || timer: 0.0928 sec.
iter 375590 || Loss: 0.6469 || timer: 0.0834 sec.
iter 375600 || Loss: 0.5685 || timer: 0.0868 sec.
iter 375610 || Loss: 0.8059 || timer: 0.0908 sec.
iter 375620 || Loss: 0.6823 || timer: 0.0939 sec.
iter 375630 || Loss: 0.8704 || timer: 0.1087 sec.
iter 375640 || Loss: 0.5513 || timer: 0.0893 sec.
iter 375650 || Loss: 0.7969 || timer: 0.0933 sec.
iter 375660 || Loss: 0.5793 || timer: 0.0893 sec.
iter 375670 || Loss: 0.9525 || timer: 0.1018 sec.
iter 375680 || Loss: 0.7348 || timer: 0.0958 sec.
iter 375690 || Loss: 0.8007 || timer: 0.1186 sec.
iter 375700 || Loss: 0.9341 || timer: 0.1010 sec.
iter 375710 || Loss: 1.0935 || timer: 0.0937 sec.
iter 375720 || Loss: 0.6235 || timer: 0.0907 sec.
iter 375730 || Loss: 0.7652 || timer: 0.1020 sec.
iter 375740 || Loss: 0.8081 || timer: 0.0787 sec.
iter 375750 || Loss: 0.8067 || timer: 0.1047 sec.
iter 375760 || Loss: 0.7283 || timer: 0.0259 sec.
iter 375770 || Loss: 0.4707 || timer: 0.0983 sec.
iter 375780 || Loss: 0.6841 || timer: 0.0845 sec.
iter 375790 || Loss: 0.6537 || timer: 0.0840 sec.
iter 375800 || Loss: 0.6370 || timer: 0.0824 sec.
iter 375810 || Loss: 0.8315 || timer: 0.0881 sec.
iter 375820 || Loss: 0.5853 || timer: 0.0879 sec.
iter 375830 || Loss: 0.5710 || timer: 0.1180 sec.
iter 375840 || Loss: 0.4446 || timer: 0.1005 sec.
iter 375850 || Loss: 0.5848 || timer: 0.0911 sec.
iter 375860 || Loss: 0.8675 || timer: 0.1179 sec.
iter 375870 || Loss: 0.7802 || timer: 0.1098 sec.
iter 375880 || Loss: 0.6979 || timer: 0.0920 sec.
iter 375890 || Loss: 0.5890 || timer: 0.0867 sec.
iter 375900 || Loss: 0.9913 || timer: 0.0948 sec.
iter 375910 || Loss: 0.5703 || timer: 0.0884 sec.
iter 375920 || Loss: 0.8157 || timer: 0.0920 sec.
iter 375930 || Loss: 0.6432 || timer: 0.0956 sec.
iter 375940 || Loss: 0.6016 || timer: 0.0949 sec.
iter 375950 || Loss: 0.6288 || timer: 0.0814 sec.
iter 375960 || Loss: 0.5209 || timer: 0.0915 sec.
iter 375970 || Loss: 0.7790 || timer: 0.0891 sec.
iter 375980 || Loss: 0.6296 || timer: 0.0883 sec.
iter 375990 || Loss: 0.9121 || timer: 0.0813 sec.
iter 376000 || Loss: 0.9693 || timer: 0.0899 sec.
iter 376010 || Loss: 0.6892 || timer: 0.0932 sec.
iter 376020 || Loss: 0.6286 || timer: 0.0895 sec.
iter 376030 || Loss: 0.5136 || timer: 0.0900 sec.
iter 376040 || Loss: 0.8031 || timer: 0.0920 sec.
iter 376050 || Loss: 0.5840 || timer: 0.0994 sec.
iter 376060 || Loss: 0.8322 || timer: 0.0863 sec.
iter 376070 || Loss: 0.8010 || timer: 0.0907 sec.
iter 376080 || Loss: 0.7117 || timer: 0.0870 sec.
iter 376090 || Loss: 0.5791 || timer: 0.0178 sec.
iter 376100 || Loss: 0.5827 || timer: 0.1027 sec.
iter 376110 || Loss: 0.5356 || timer: 0.0813 sec.
iter 376120 || Loss: 0.6198 || timer: 0.1016 sec.
iter 376130 || Loss: 0.5028 || timer: 0.0967 sec.
iter 376140 || Loss: 0.4474 || timer: 0.0823 sec.
iter 376150 || Loss: 0.6996 || timer: 0.0913 sec.
iter 376160 || Loss: 0.6407 || timer: 0.0959 sec.
iter 376170 || Loss: 0.7457 || timer: 0.0872 sec.
iter 376180 || Loss: 0.6736 || timer: 0.0832 sec.
iter 376190 || Loss: 0.5691 || timer: 0.0985 sec.
iter 376200 || Loss: 0.9476 || timer: 0.0894 sec.
iter 376210 || Loss: 0.6227 || timer: 0.0884 sec.
iter 376220 || Loss: 0.5520 || timer: 0.0861 sec.
iter 376230 || Loss: 0.6882 || timer: 0.0909 sec.
iter 376240 || Loss: 0.7313 || timer: 0.0895 sec.
iter 376250 || Loss: 0.6009 || timer: 0.0932 sec.
iter 376260 || Loss: 0.7228 || timer: 0.0904 sec.
iter 376270 || Loss: 0.6280 || timer: 0.0826 sec.
iter 376280 || Loss: 0.5734 || timer: 0.0882 sec.
iter 376290 || Loss: 0.7011 || timer: 0.0819 sec.
iter 376300 || Loss: 0.5605 || timer: 0.1123 sec.
iter 376310 || Loss: 0.3606 || timer: 0.0891 sec.
iter 376320 || Loss: 0.5791 || timer: 0.0976 sec.
iter 376330 || Loss: 0.6303 || timer: 0.1181 sec.
iter 376340 || Loss: 0.7122 || timer: 0.0887 sec.
iter 376350 || Loss: 0.5447 || timer: 0.0891 sec.
iter 376360 || Loss: 0.8263 || timer: 0.0817 sec.
iter 376370 || Loss: 0.7744 || timer: 0.0909 sec.
iter 376380 || Loss: 0.8243 || timer: 0.0821 sec.
iter 376390 || Loss: 0.6981 || timer: 0.0828 sec.
iter 376400 || Loss: 0.9187 || timer: 0.0887 sec.
iter 376410 || Loss: 0.7018 || timer: 0.0815 sec.
iter 376420 || Loss: 0.4796 || timer: 0.0214 sec.
iter 376430 || Loss: 0.2627 || timer: 0.0805 sec.
iter 376440 || Loss: 0.5975 || timer: 0.0947 sec.
iter 376450 || Loss: 0.7276 || timer: 0.1304 sec.
iter 376460 || Loss: 0.8592 || timer: 0.0962 sec.
iter 376470 || Loss: 0.7196 || timer: 0.0826 sec.
iter 376480 || Loss: 0.8619 || timer: 0.0894 sec.
iter 376490 || Loss: 0.4782 || timer: 0.0892 sec.
iter 376500 || Loss: 0.5989 || timer: 0.0917 sec.
iter 376510 || Loss: 0.5437 || timer: 0.0823 sec.
iter 376520 || Loss: 0.7060 || timer: 0.1231 sec.
iter 376530 || Loss: 0.6585 || timer: 0.0823 sec.
iter 376540 || Loss: 0.6501 || timer: 0.0999 sec.
iter 376550 || Loss: 0.7908 || timer: 0.0884 sec.
iter 376560 || Loss: 0.6469 || timer: 0.0874 sec.
iter 376570 || Loss: 0.6749 || timer: 0.1032 sec.
iter 376580 || Loss: 0.5921 || timer: 0.1044 sec.
iter 376590 || Loss: 0.8621 || timer: 0.0910 sec.
iter 376600 || Loss: 0.8294 || timer: 0.0899 sec.
iter 376610 || Loss: 0.7117 || timer: 0.0873 sec.
iter 376620 || Loss: 0.6891 || timer: 0.0886 sec.
iter 376630 || Loss: 0.8852 || timer: 0.0820 sec.
iter 376640 || Loss: 0.7349 || timer: 0.0865 sec.
iter 376650 || Loss: 0.7052 || timer: 0.0817 sec.
iter 376660 || Loss: 0.6672 || timer: 0.1045 sec.
iter 376670 || Loss: 0.6916 || timer: 0.0904 sec.
iter 376680 || Loss: 0.6680 || timer: 0.0803 sec.
iter 376690 || Loss: 0.5123 || timer: 0.1064 sec.
iter 376700 || Loss: 0.8283 || timer: 0.0893 sec.
iter 376710 || Loss: 0.7166 || timer: 0.1111 sec.
iter 376720 || Loss: 0.6629 || timer: 0.0805 sec.
iter 376730 || Loss: 0.6211 || timer: 0.0915 sec.
iter 376740 || Loss: 0.7911 || timer: 0.1062 sec.
iter 376750 || Loss: 0.6770 || timer: 0.0164 sec.
iter 376760 || Loss: 0.1883 || timer: 0.0986 sec.
iter 376770 || Loss: 0.4732 || timer: 0.0816 sec.
iter 376780 || Loss: 0.6496 || timer: 0.1066 sec.
iter 376790 || Loss: 0.6471 || timer: 0.0899 sec.
iter 376800 || Loss: 0.5091 || timer: 0.1154 sec.
iter 376810 || Loss: 0.6632 || timer: 0.0963 sec.
iter 376820 || Loss: 0.6445 || timer: 0.0842 sec.
iter 376830 || Loss: 0.6912 || timer: 0.0855 sec.
iter 376840 || Loss: 0.5741 || timer: 0.0839 sec.
iter 376850 || Loss: 0.6388 || timer: 0.0928 sec.
iter 376860 || Loss: 0.7430 || timer: 0.0876 sec.
iter 376870 || Loss: 0.7555 || timer: 0.0917 sec.
iter 376880 || Loss: 0.8186 || timer: 0.0802 sec.
iter 376890 || Loss: 0.8326 || timer: 0.0981 sec.
iter 376900 || Loss: 0.6186 || timer: 0.0901 sec.
iter 376910 || Loss: 0.4815 || timer: 0.0972 sec.
iter 376920 || Loss: 0.6721 || timer: 0.0886 sec.
iter 376930 || Loss: 0.4900 || timer: 0.0901 sec.
iter 376940 || Loss: 0.7319 || timer: 0.0862 sec.
iter 376950 || Loss: 0.6437 || timer: 0.0896 sec.
iter 376960 || Loss: 0.7270 || timer: 0.0909 sec.
iter 376970 || Loss: 0.7683 || timer: 0.0921 sec.
iter 376980 || Loss: 0.6285 || timer: 0.0884 sec.
iter 376990 || Loss: 0.6457 || timer: 0.0885 sec.
iter 377000 || Loss: 0.5233 || timer: 0.0910 sec.
iter 377010 || Loss: 0.8663 || timer: 0.0879 sec.
iter 377020 || Loss: 0.6601 || timer: 0.0799 sec.
iter 377030 || Loss: 0.7857 || timer: 0.0920 sec.
iter 377040 || Loss: 0.5446 || timer: 0.0827 sec.
iter 377050 || Loss: 0.8654 || timer: 0.0897 sec.
iter 377060 || Loss: 0.4933 || timer: 0.0980 sec.
iter 377070 || Loss: 1.1013 || timer: 0.0892 sec.
iter 377080 || Loss: 0.8281 || timer: 0.0211 sec.
iter 377090 || Loss: 0.5750 || timer: 0.1009 sec.
iter 377100 || Loss: 0.6897 || timer: 0.0819 sec.
iter 377110 || Loss: 0.6755 || timer: 0.0903 sec.
iter 377120 || Loss: 0.6642 || timer: 0.0895 sec.
iter 377130 || Loss: 0.7909 || timer: 0.1090 sec.
iter 377140 || Loss: 0.6118 || timer: 0.0815 sec.
iter 377150 || Loss: 0.5016 || timer: 0.0818 sec.
iter 377160 || Loss: 0.6756 || timer: 0.0857 sec.
iter 377170 || Loss: 0.4619 || timer: 0.0905 sec.
iter 377180 || Loss: 0.5797 || timer: 0.1153 sec.
iter 377190 || Loss: 0.6852 || timer: 0.0928 sec.
iter 377200 || Loss: 0.7491 || timer: 0.0893 sec.
iter 377210 || Loss: 0.5517 || timer: 0.0815 sec.
iter 377220 || Loss: 0.6055 || timer: 0.0944 sec.
iter 377230 || Loss: 0.7853 || timer: 0.0911 sec.
iter 377240 || Loss: 0.6063 || timer: 0.0827 sec.
iter 377250 || Loss: 0.4651 || timer: 0.0979 sec.
iter 377260 || Loss: 0.5307 || timer: 0.0890 sec.
iter 377270 || Loss: 0.8556 || timer: 0.0886 sec.
iter 377280 || Loss: 0.8772 || timer: 0.0893 sec.
iter 377290 || Loss: 0.6614 || timer: 0.0823 sec.
iter 377300 || Loss: 0.7580 || timer: 0.0913 sec.
iter 377310 || Loss: 0.8904 || timer: 0.0819 sec.
iter 377320 || Loss: 0.5499 || timer: 0.0956 sec.
iter 377330 || Loss: 0.5883 || timer: 0.0889 sec.
iter 377340 || Loss: 0.7527 || timer: 0.1025 sec.
iter 377350 || Loss: 0.5089 || timer: 0.0811 sec.
iter 377360 || Loss: 0.7092 || timer: 0.0934 sec.
iter 377370 || Loss: 0.6205 || timer: 0.0835 sec.
iter 377380 || Loss: 0.6304 || timer: 0.1078 sec.
iter 377390 || Loss: 0.6693 || timer: 0.1044 sec.
iter 377400 || Loss: 0.7028 || timer: 0.0821 sec.
iter 377410 || Loss: 0.7288 || timer: 0.0169 sec.
iter 377420 || Loss: 0.8456 || timer: 0.0863 sec.
iter 377430 || Loss: 0.6897 || timer: 0.0928 sec.
iter 377440 || Loss: 0.4042 || timer: 0.0820 sec.
iter 377450 || Loss: 0.7304 || timer: 0.0910 sec.
iter 377460 || Loss: 0.5625 || timer: 0.1018 sec.
iter 377470 || Loss: 0.5464 || timer: 0.0898 sec.
iter 377480 || Loss: 0.7316 || timer: 0.0829 sec.
iter 377490 || Loss: 0.7471 || timer: 0.0882 sec.
iter 377500 || Loss: 0.4819 || timer: 0.0853 sec.
iter 377510 || Loss: 0.3420 || timer: 0.1165 sec.
iter 377520 || Loss: 0.7254 || timer: 0.0931 sec.
iter 377530 || Loss: 0.7799 || timer: 0.0888 sec.
iter 377540 || Loss: 0.6823 || timer: 0.0810 sec.
iter 377550 || Loss: 0.7772 || timer: 0.0901 sec.
iter 377560 || Loss: 0.7684 || timer: 0.0993 sec.
iter 377570 || Loss: 0.5174 || timer: 0.0887 sec.
iter 377580 || Loss: 0.6750 || timer: 0.0908 sec.
iter 377590 || Loss: 0.7082 || timer: 0.1113 sec.
iter 377600 || Loss: 0.9547 || timer: 0.0902 sec.
iter 377610 || Loss: 0.7621 || timer: 0.0821 sec.
iter 377620 || Loss: 0.6805 || timer: 0.0807 sec.
iter 377630 || Loss: 0.5525 || timer: 0.0802 sec.
iter 377640 || Loss: 0.6403 || timer: 0.0817 sec.
iter 377650 || Loss: 0.8845 || timer: 0.0885 sec.
iter 377660 || Loss: 0.6021 || timer: 0.0816 sec.
iter 377670 || Loss: 0.6914 || timer: 0.1030 sec.
iter 377680 || Loss: 0.7537 || timer: 0.0896 sec.
iter 377690 || Loss: 0.4574 || timer: 0.0914 sec.
iter 377700 || Loss: 0.8326 || timer: 0.0931 sec.
iter 377710 || Loss: 0.7146 || timer: 0.1113 sec.
iter 377720 || Loss: 0.5983 || timer: 0.0993 sec.
iter 377730 || Loss: 0.5958 || timer: 0.0913 sec.
iter 377740 || Loss: 0.5679 || timer: 0.0174 sec.
iter 377750 || Loss: 0.4390 || timer: 0.0809 sec.
iter 377760 || Loss: 0.8061 || timer: 0.1218 sec.
iter 377770 || Loss: 0.5885 || timer: 0.0883 sec.
iter 377780 || Loss: 0.6817 || timer: 0.0824 sec.
iter 377790 || Loss: 0.4700 || timer: 0.0828 sec.
iter 377800 || Loss: 0.6313 || timer: 0.0993 sec.
iter 377810 || Loss: 0.6782 || timer: 0.0876 sec.
iter 377820 || Loss: 0.7605 || timer: 0.0927 sec.
iter 377830 || Loss: 0.5087 || timer: 0.0959 sec.
iter 377840 || Loss: 0.8501 || timer: 0.0962 sec.
iter 377850 || Loss: 0.7339 || timer: 0.0870 sec.
iter 377860 || Loss: 0.8374 || timer: 0.1061 sec.
iter 377870 || Loss: 0.3892 || timer: 0.0906 sec.
iter 377880 || Loss: 0.5582 || timer: 0.0891 sec.
iter 377890 || Loss: 0.7689 || timer: 0.0885 sec.
iter 377900 || Loss: 0.7445 || timer: 0.0820 sec.
iter 377910 || Loss: 0.5942 || timer: 0.0822 sec.
iter 377920 || Loss: 0.8305 || timer: 0.0880 sec.
iter 377930 || Loss: 0.7454 || timer: 0.0861 sec.
iter 377940 || Loss: 0.6254 || timer: 0.0809 sec.
iter 377950 || Loss: 0.7418 || timer: 0.1027 sec.
iter 377960 || Loss: 0.6208 || timer: 0.0833 sec.
iter 377970 || Loss: 0.8639 || timer: 0.0935 sec.
iter 377980 || Loss: 0.7183 || timer: 0.0847 sec.
iter 377990 || Loss: 0.5130 || timer: 0.0869 sec.
iter 378000 || Loss: 0.9309 || timer: 0.0954 sec.
iter 378010 || Loss: 0.8770 || timer: 0.0823 sec.
iter 378020 || Loss: 0.6364 || timer: 0.1112 sec.
iter 378030 || Loss: 0.4582 || timer: 0.0885 sec.
iter 378040 || Loss: 0.6022 || timer: 0.0915 sec.
iter 378050 || Loss: 1.0150 || timer: 0.0908 sec.
iter 378060 || Loss: 0.7323 || timer: 0.0877 sec.
iter 378070 || Loss: 0.4885 || timer: 0.0162 sec.
iter 378080 || Loss: 0.2864 || timer: 0.0903 sec.
iter 378090 || Loss: 0.4517 || timer: 0.0854 sec.
iter 378100 || Loss: 0.5334 || timer: 0.0845 sec.
iter 378110 || Loss: 0.6277 || timer: 0.0941 sec.
iter 378120 || Loss: 0.8457 || timer: 0.1021 sec.
iter 378130 || Loss: 0.6327 || timer: 0.1071 sec.
iter 378140 || Loss: 0.6640 || timer: 0.0910 sec.
iter 378150 || Loss: 0.6399 || timer: 0.0835 sec.
iter 378160 || Loss: 0.7742 || timer: 0.1198 sec.
iter 378170 || Loss: 0.8136 || timer: 0.1235 sec.
iter 378180 || Loss: 0.7403 || timer: 0.0854 sec.
iter 378190 || Loss: 0.7587 || timer: 0.0899 sec.
iter 378200 || Loss: 0.6688 || timer: 0.0906 sec.
iter 378210 || Loss: 0.6776 || timer: 0.1022 sec.
iter 378220 || Loss: 0.4635 || timer: 0.0809 sec.
iter 378230 || Loss: 0.9131 || timer: 0.1090 sec.
iter 378240 || Loss: 0.4922 || timer: 0.1148 sec.
iter 378250 || Loss: 0.7148 || timer: 0.1030 sec.
iter 378260 || Loss: 0.5774 || timer: 0.0890 sec.
iter 378270 || Loss: 0.5317 || timer: 0.0883 sec.
iter 378280 || Loss: 0.7367 || timer: 0.0740 sec.
iter 378290 || Loss: 0.8261 || timer: 0.0809 sec.
iter 378300 || Loss: 0.5137 || timer: 0.0895 sec.
iter 378310 || Loss: 0.6161 || timer: 0.0809 sec.
iter 378320 || Loss: 0.6039 || timer: 0.0897 sec.
iter 378330 || Loss: 0.6779 || timer: 0.0814 sec.
iter 378340 || Loss: 0.7610 || timer: 0.0830 sec.
iter 378350 || Loss: 0.6327 || timer: 0.1013 sec.
iter 378360 || Loss: 0.5881 || timer: 0.0821 sec.
iter 378370 || Loss: 0.5158 || timer: 0.0822 sec.
iter 378380 || Loss: 0.6075 || timer: 0.0933 sec.
iter 378390 || Loss: 0.5749 || timer: 0.0821 sec.
iter 378400 || Loss: 0.6473 || timer: 0.0240 sec.
iter 378410 || Loss: 0.5087 || timer: 0.0835 sec.
iter 378420 || Loss: 0.5811 || timer: 0.0949 sec.
iter 378430 || Loss: 0.7056 || timer: 0.0907 sec.
iter 378440 || Loss: 0.9599 || timer: 0.0858 sec.
iter 378450 || Loss: 0.5275 || timer: 0.1157 sec.
iter 378460 || Loss: 0.7882 || timer: 0.0854 sec.
iter 378470 || Loss: 0.7489 || timer: 0.0979 sec.
iter 378480 || Loss: 0.6573 || timer: 0.0814 sec.
iter 378490 || Loss: 0.8049 || timer: 0.0884 sec.
iter 378500 || Loss: 0.6534 || timer: 0.0953 sec.
iter 378510 || Loss: 0.5192 || timer: 0.1123 sec.
iter 378520 || Loss: 0.6184 || timer: 0.0819 sec.
iter 378530 || Loss: 0.6922 || timer: 0.0811 sec.
iter 378540 || Loss: 0.6665 || timer: 0.0821 sec.
iter 378550 || Loss: 0.7335 || timer: 0.0885 sec.
iter 378560 || Loss: 0.9293 || timer: 0.0829 sec.
iter 378570 || Loss: 0.8157 || timer: 0.1218 sec.
iter 378580 || Loss: 0.6356 || timer: 0.0836 sec.
iter 378590 || Loss: 0.8432 || timer: 0.0822 sec.
iter 378600 || Loss: 0.7334 || timer: 0.1121 sec.
iter 378610 || Loss: 0.7232 || timer: 0.0987 sec.
iter 378620 || Loss: 0.6687 || timer: 0.0909 sec.
iter 378630 || Loss: 0.3896 || timer: 0.0904 sec.
iter 378640 || Loss: 0.4476 || timer: 0.0811 sec.
iter 378650 || Loss: 0.6580 || timer: 0.1172 sec.
iter 378660 || Loss: 0.7104 || timer: 0.0923 sec.
iter 378670 || Loss: 0.6475 || timer: 0.0814 sec.
iter 378680 || Loss: 0.6824 || timer: 0.1136 sec.
iter 378690 || Loss: 0.6824 || timer: 0.0828 sec.
iter 378700 || Loss: 0.7692 || timer: 0.0889 sec.
iter 378710 || Loss: 0.6454 || timer: 0.0864 sec.
iter 378720 || Loss: 0.5926 || timer: 0.0811 sec.
iter 378730 || Loss: 0.5784 || timer: 0.0164 sec.
iter 378740 || Loss: 0.5133 || timer: 0.0895 sec.
iter 378750 || Loss: 0.9032 || timer: 0.0937 sec.
iter 378760 || Loss: 1.0467 || timer: 0.0919 sec.
iter 378770 || Loss: 0.8153 || timer: 0.0881 sec.
iter 378780 || Loss: 0.6973 || timer: 0.1062 sec.
iter 378790 || Loss: 0.8378 || timer: 0.0820 sec.
iter 378800 || Loss: 0.6928 || timer: 0.0828 sec.
iter 378810 || Loss: 0.3729 || timer: 0.0842 sec.
iter 378820 || Loss: 0.5187 || timer: 0.0907 sec.
iter 378830 || Loss: 0.6902 || timer: 0.0966 sec.
iter 378840 || Loss: 0.6311 || timer: 0.0844 sec.
iter 378850 || Loss: 0.5062 || timer: 0.0880 sec.
iter 378860 || Loss: 0.5724 || timer: 0.0801 sec.
iter 378870 || Loss: 0.9163 || timer: 0.0912 sec.
iter 378880 || Loss: 0.6666 || timer: 0.0848 sec.
iter 378890 || Loss: 0.7574 || timer: 0.1059 sec.
iter 378900 || Loss: 0.6647 || timer: 0.1014 sec.
iter 378910 || Loss: 0.4743 || timer: 0.0825 sec.
iter 378920 || Loss: 0.6288 || timer: 0.0824 sec.
iter 378930 || Loss: 0.7192 || timer: 0.0886 sec.
iter 378940 || Loss: 0.6307 || timer: 0.0966 sec.
iter 378950 || Loss: 0.7196 || timer: 0.0908 sec.
iter 378960 || Loss: 0.5846 || timer: 0.0839 sec.
iter 378970 || Loss: 0.5567 || timer: 0.0894 sec.
iter 378980 || Loss: 0.5791 || timer: 0.0817 sec.
iter 378990 || Loss: 0.5710 || timer: 0.0917 sec.
iter 379000 || Loss: 0.7495 || timer: 0.0808 sec.
iter 379010 || Loss: 0.6234 || timer: 0.0879 sec.
iter 379020 || Loss: 0.4395 || timer: 0.0882 sec.
iter 379030 || Loss: 0.5710 || timer: 0.0863 sec.
iter 379040 || Loss: 0.6894 || timer: 0.0901 sec.
iter 379050 || Loss: 0.9380 || timer: 0.0983 sec.
iter 379060 || Loss: 0.6108 || timer: 0.0248 sec.
iter 379070 || Loss: 1.2543 || timer: 0.0931 sec.
iter 379080 || Loss: 0.5429 || timer: 0.0965 sec.
iter 379090 || Loss: 0.6928 || timer: 0.0910 sec.
iter 379100 || Loss: 0.8848 || timer: 0.0892 sec.
iter 379110 || Loss: 0.6873 || timer: 0.0824 sec.
iter 379120 || Loss: 1.0013 || timer: 0.0823 sec.
iter 379130 || Loss: 0.8221 || timer: 0.1004 sec.
iter 379140 || Loss: 0.6641 || timer: 0.0871 sec.
iter 379150 || Loss: 0.7099 || timer: 0.0821 sec.
iter 379160 || Loss: 0.5694 || timer: 0.0991 sec.
iter 379170 || Loss: 0.7185 || timer: 0.1135 sec.
iter 379180 || Loss: 0.4739 || timer: 0.0889 sec.
iter 379190 || Loss: 0.6569 || timer: 0.0813 sec.
iter 379200 || Loss: 0.7218 || timer: 0.0901 sec.
iter 379210 || Loss: 0.5748 || timer: 0.0885 sec.
iter 379220 || Loss: 0.6524 || timer: 0.0889 sec.
iter 379230 || Loss: 0.7432 || timer: 0.0921 sec.
iter 379240 || Loss: 0.7691 || timer: 0.0883 sec.
iter 379250 || Loss: 0.6816 || timer: 0.1067 sec.
iter 379260 || Loss: 0.6113 || timer: 0.0926 sec.
iter 379270 || Loss: 0.7568 || timer: 0.0986 sec.
iter 379280 || Loss: 0.9443 || timer: 0.0810 sec.
iter 379290 || Loss: 0.7044 || timer: 0.0915 sec.
iter 379300 || Loss: 0.6425 || timer: 0.0909 sec.
iter 379310 || Loss: 0.4498 || timer: 0.0872 sec.
iter 379320 || Loss: 0.5889 || timer: 0.0918 sec.
iter 379330 || Loss: 0.7654 || timer: 0.0871 sec.
iter 379340 || Loss: 0.5356 || timer: 0.1181 sec.
iter 379350 || Loss: 0.5386 || timer: 0.0828 sec.
iter 379360 || Loss: 0.6090 || timer: 0.0903 sec.
iter 379370 || Loss: 0.5956 || timer: 0.0866 sec.
iter 379380 || Loss: 0.7065 || timer: 0.1086 sec.
iter 379390 || Loss: 0.4668 || timer: 0.0242 sec.
iter 379400 || Loss: 0.5437 || timer: 0.0817 sec.
iter 379410 || Loss: 0.9663 || timer: 0.0895 sec.
iter 379420 || Loss: 0.6497 || timer: 0.0863 sec.
iter 379430 || Loss: 0.7310 || timer: 0.0881 sec.
iter 379440 || Loss: 0.5013 || timer: 0.0808 sec.
iter 379450 || Loss: 0.4462 || timer: 0.0903 sec.
iter 379460 || Loss: 0.7598 || timer: 0.0925 sec.
iter 379470 || Loss: 0.6147 || timer: 0.0917 sec.
iter 379480 || Loss: 0.6919 || timer: 0.1101 sec.
iter 379490 || Loss: 0.6238 || timer: 0.0957 sec.
iter 379500 || Loss: 0.7181 || timer: 0.0829 sec.
iter 379510 || Loss: 0.6649 || timer: 0.1049 sec.
iter 379520 || Loss: 0.8588 || timer: 0.0891 sec.
iter 379530 || Loss: 0.5759 || timer: 0.0894 sec.
iter 379540 || Loss: 0.8088 || timer: 0.0896 sec.
iter 379550 || Loss: 0.7047 || timer: 0.0827 sec.
iter 379560 || Loss: 0.8051 || timer: 0.0807 sec.
iter 379570 || Loss: 0.7986 || timer: 0.1085 sec.
iter 379580 || Loss: 0.6056 || timer: 0.0832 sec.
iter 379590 || Loss: 0.7730 || timer: 0.0833 sec.
iter 379600 || Loss: 0.6135 || timer: 0.0914 sec.
iter 379610 || Loss: 0.6685 || timer: 0.0904 sec.
iter 379620 || Loss: 0.5698 || timer: 0.0911 sec.
iter 379630 || Loss: 0.7222 || timer: 0.0890 sec.
iter 379640 || Loss: 0.6600 || timer: 0.1009 sec.
iter 379650 || Loss: 0.7456 || timer: 0.0946 sec.
iter 379660 || Loss: 0.6310 || timer: 0.0890 sec.
iter 379670 || Loss: 0.4659 || timer: 0.1010 sec.
iter 379680 || Loss: 0.6034 || timer: 0.0829 sec.
iter 379690 || Loss: 0.7786 || timer: 0.0882 sec.
iter 379700 || Loss: 0.5238 || timer: 0.0814 sec.
iter 379710 || Loss: 0.4269 || timer: 0.0873 sec.
iter 379720 || Loss: 0.7006 || timer: 0.0214 sec.
iter 379730 || Loss: 0.3913 || timer: 0.1203 sec.
iter 379740 || Loss: 0.7572 || timer: 0.0924 sec.
iter 379750 || Loss: 0.6222 || timer: 0.1097 sec.
iter 379760 || Loss: 0.5164 || timer: 0.0892 sec.
iter 379770 || Loss: 0.5434 || timer: 0.0935 sec.
iter 379780 || Loss: 0.4663 || timer: 0.0917 sec.
iter 379790 || Loss: 0.5585 || timer: 0.0921 sec.
iter 379800 || Loss: 0.4753 || timer: 0.0824 sec.
iter 379810 || Loss: 0.8493 || timer: 0.0945 sec.
iter 379820 || Loss: 0.4475 || timer: 0.0993 sec.
iter 379830 || Loss: 0.8699 || timer: 0.0864 sec.
iter 379840 || Loss: 0.4539 || timer: 0.0897 sec.
iter 379850 || Loss: 0.7149 || timer: 0.0908 sec.
iter 379860 || Loss: 0.5694 || timer: 0.0810 sec.
iter 379870 || Loss: 0.7563 || timer: 0.0829 sec.
iter 379880 || Loss: 0.6785 || timer: 0.0902 sec.
iter 379890 || Loss: 0.6371 || timer: 0.0912 sec.
iter 379900 || Loss: 0.6445 || timer: 0.0899 sec.
iter 379910 || Loss: 0.6465 || timer: 0.0899 sec.
iter 379920 || Loss: 0.4344 || timer: 0.1016 sec.
iter 379930 || Loss: 0.6035 || timer: 0.0957 sec.
iter 379940 || Loss: 0.8473 || timer: 0.1114 sec.
iter 379950 || Loss: 0.5800 || timer: 0.0837 sec.
iter 379960 || Loss: 0.7531 || timer: 0.0909 sec.
iter 379970 || Loss: 0.8704 || timer: 0.0828 sec.
iter 379980 || Loss: 0.7241 || timer: 0.0881 sec.
iter 379990 || Loss: 0.5699 || timer: 0.1024 sec.
iter 380000 || Loss: 0.5604 || Saving state, iter: 380000
timer: 0.0827 sec.
iter 380010 || Loss: 0.5343 || timer: 0.0950 sec.
iter 380020 || Loss: 0.6090 || timer: 0.0848 sec.
iter 380030 || Loss: 0.8732 || timer: 0.0819 sec.
iter 380040 || Loss: 0.5140 || timer: 0.1040 sec.
iter 380050 || Loss: 0.6991 || timer: 0.0235 sec.
iter 380060 || Loss: 0.9570 || timer: 0.0816 sec.
iter 380070 || Loss: 0.8559 || timer: 0.0896 sec.
iter 380080 || Loss: 0.5524 || timer: 0.0892 sec.
iter 380090 || Loss: 0.6775 || timer: 0.0906 sec.
iter 380100 || Loss: 0.7123 || timer: 0.0887 sec.
iter 380110 || Loss: 0.6049 || timer: 0.0873 sec.
iter 380120 || Loss: 0.4810 || timer: 0.0969 sec.
iter 380130 || Loss: 0.4919 || timer: 0.0875 sec.
iter 380140 || Loss: 0.7108 || timer: 0.1035 sec.
iter 380150 || Loss: 0.7304 || timer: 0.0962 sec.
iter 380160 || Loss: 0.8350 || timer: 0.0890 sec.
iter 380170 || Loss: 0.7217 || timer: 0.0847 sec.
iter 380180 || Loss: 0.8742 || timer: 0.0875 sec.
iter 380190 || Loss: 0.6778 || timer: 0.0989 sec.
iter 380200 || Loss: 0.6160 || timer: 0.0847 sec.
iter 380210 || Loss: 0.4620 || timer: 0.0837 sec.
iter 380220 || Loss: 0.5836 || timer: 0.0888 sec.
iter 380230 || Loss: 0.5052 || timer: 0.0915 sec.
iter 380240 || Loss: 1.1083 || timer: 0.0905 sec.
iter 380250 || Loss: 0.5858 || timer: 0.0818 sec.
iter 380260 || Loss: 0.5677 || timer: 0.0888 sec.
iter 380270 || Loss: 1.3162 || timer: 0.0915 sec.
iter 380280 || Loss: 0.6881 || timer: 0.0995 sec.
iter 380290 || Loss: 0.7207 || timer: 0.0848 sec.
iter 380300 || Loss: 0.6220 || timer: 0.0855 sec.
iter 380310 || Loss: 0.6765 || timer: 0.0995 sec.
iter 380320 || Loss: 0.5715 || timer: 0.0886 sec.
iter 380330 || Loss: 0.6632 || timer: 0.0812 sec.
iter 380340 || Loss: 0.5637 || timer: 0.0884 sec.
iter 380350 || Loss: 0.6337 || timer: 0.0877 sec.
iter 380360 || Loss: 0.6548 || timer: 0.0966 sec.
iter 380370 || Loss: 0.5156 || timer: 0.0919 sec.
iter 380380 || Loss: 0.8904 || timer: 0.0176 sec.
iter 380390 || Loss: 0.1348 || timer: 0.0847 sec.
iter 380400 || Loss: 0.7657 || timer: 0.0955 sec.
iter 380410 || Loss: 0.6745 || timer: 0.0893 sec.
iter 380420 || Loss: 0.7879 || timer: 0.1027 sec.
iter 380430 || Loss: 0.9757 || timer: 0.0904 sec.
iter 380440 || Loss: 0.5185 || timer: 0.0987 sec.
iter 380450 || Loss: 0.7544 || timer: 0.0995 sec.
iter 380460 || Loss: 0.6864 || timer: 0.0882 sec.
iter 380470 || Loss: 0.5152 || timer: 0.1009 sec.
iter 380480 || Loss: 0.5496 || timer: 0.1145 sec.
iter 380490 || Loss: 0.6659 || timer: 0.0927 sec.
iter 380500 || Loss: 0.8030 || timer: 0.0922 sec.
iter 380510 || Loss: 0.6409 || timer: 0.0876 sec.
iter 380520 || Loss: 0.9495 || timer: 0.1127 sec.
iter 380530 || Loss: 0.6392 || timer: 0.0908 sec.
iter 380540 || Loss: 0.5259 || timer: 0.0879 sec.
iter 380550 || Loss: 0.7093 || timer: 0.0893 sec.
iter 380560 || Loss: 0.4933 || timer: 0.0873 sec.
iter 380570 || Loss: 0.6207 || timer: 0.0815 sec.
iter 380580 || Loss: 0.5430 || timer: 0.0810 sec.
iter 380590 || Loss: 0.6217 || timer: 0.0879 sec.
iter 380600 || Loss: 0.4746 || timer: 0.0830 sec.
iter 380610 || Loss: 0.8062 || timer: 0.0814 sec.
iter 380620 || Loss: 0.5931 || timer: 0.0880 sec.
iter 380630 || Loss: 0.8226 || timer: 0.1011 sec.
iter 380640 || Loss: 0.4068 || timer: 0.1139 sec.
iter 380650 || Loss: 0.6664 || timer: 0.0894 sec.
iter 380660 || Loss: 0.7196 || timer: 0.0891 sec.
iter 380670 || Loss: 0.7430 || timer: 0.0873 sec.
iter 380680 || Loss: 0.7534 || timer: 0.0890 sec.
iter 380690 || Loss: 0.6893 || timer: 0.0824 sec.
iter 380700 || Loss: 0.6328 || timer: 0.0959 sec.
iter 380710 || Loss: 0.7232 || timer: 0.0163 sec.
iter 380720 || Loss: 0.6362 || timer: 0.1055 sec.
iter 380730 || Loss: 0.5400 || timer: 0.0846 sec.
iter 380740 || Loss: 0.7437 || timer: 0.0814 sec.
iter 380750 || Loss: 0.6749 || timer: 0.0890 sec.
iter 380760 || Loss: 0.7295 || timer: 0.0870 sec.
iter 380770 || Loss: 0.4871 || timer: 0.1124 sec.
iter 380780 || Loss: 0.6493 || timer: 0.0823 sec.
iter 380790 || Loss: 0.5379 || timer: 0.0885 sec.
iter 380800 || Loss: 0.8710 || timer: 0.1041 sec.
iter 380810 || Loss: 0.8865 || timer: 0.0940 sec.
iter 380820 || Loss: 0.6692 || timer: 0.0941 sec.
iter 380830 || Loss: 0.5551 || timer: 0.1006 sec.
iter 380840 || Loss: 0.7198 || timer: 0.0886 sec.
iter 380850 || Loss: 0.6435 || timer: 0.0843 sec.
iter 380860 || Loss: 0.7425 || timer: 0.0891 sec.
iter 380870 || Loss: 0.7917 || timer: 0.0820 sec.
iter 380880 || Loss: 0.6789 || timer: 0.0920 sec.
iter 380890 || Loss: 0.6828 || timer: 0.0888 sec.
iter 380900 || Loss: 0.6883 || timer: 0.0978 sec.
iter 380910 || Loss: 0.6186 || timer: 0.0964 sec.
iter 380920 || Loss: 0.7007 || timer: 0.1147 sec.
iter 380930 || Loss: 0.8088 || timer: 0.0907 sec.
iter 380940 || Loss: 0.7441 || timer: 0.0938 sec.
iter 380950 || Loss: 0.5285 || timer: 0.0825 sec.
iter 380960 || Loss: 0.4992 || timer: 0.0917 sec.
iter 380970 || Loss: 0.6154 || timer: 0.0914 sec.
iter 380980 || Loss: 0.7950 || timer: 0.0809 sec.
iter 380990 || Loss: 0.7534 || timer: 0.1078 sec.
iter 381000 || Loss: 0.8534 || timer: 0.0809 sec.
iter 381010 || Loss: 0.4304 || timer: 0.0917 sec.
iter 381020 || Loss: 0.6597 || timer: 0.0891 sec.
iter 381030 || Loss: 0.7610 || timer: 0.0821 sec.
iter 381040 || Loss: 0.7806 || timer: 0.0248 sec.
iter 381050 || Loss: 0.1129 || timer: 0.0815 sec.
iter 381060 || Loss: 0.7097 || timer: 0.0842 sec.
iter 381070 || Loss: 0.7566 || timer: 0.0819 sec.
iter 381080 || Loss: 0.8355 || timer: 0.1054 sec.
iter 381090 || Loss: 0.5239 || timer: 0.0904 sec.
iter 381100 || Loss: 0.6436 || timer: 0.0820 sec.
iter 381110 || Loss: 0.9863 || timer: 0.0811 sec.
iter 381120 || Loss: 0.8275 || timer: 0.0808 sec.
iter 381130 || Loss: 0.7880 || timer: 0.0912 sec.
iter 381140 || Loss: 0.4572 || timer: 0.0967 sec.
iter 381150 || Loss: 0.9207 || timer: 0.0882 sec.
iter 381160 || Loss: 0.7000 || timer: 0.1116 sec.
iter 381170 || Loss: 0.6189 || timer: 0.0897 sec.
iter 381180 || Loss: 0.7083 || timer: 0.0861 sec.
iter 381190 || Loss: 0.7565 || timer: 0.0877 sec.
iter 381200 || Loss: 0.8480 || timer: 0.0982 sec.
iter 381210 || Loss: 0.6016 || timer: 0.0892 sec.
iter 381220 || Loss: 0.6483 || timer: 0.0818 sec.
iter 381230 || Loss: 0.6431 || timer: 0.0954 sec.
iter 381240 || Loss: 0.5612 || timer: 0.0955 sec.
iter 381250 || Loss: 0.6207 || timer: 0.0884 sec.
iter 381260 || Loss: 0.7773 || timer: 0.0799 sec.
iter 381270 || Loss: 0.6541 || timer: 0.0821 sec.
iter 381280 || Loss: 0.6549 || timer: 0.1235 sec.
iter 381290 || Loss: 0.4434 || timer: 0.0983 sec.
iter 381300 || Loss: 0.6204 || timer: 0.0896 sec.
iter 381310 || Loss: 0.6306 || timer: 0.0815 sec.
iter 381320 || Loss: 0.6209 || timer: 0.0831 sec.
iter 381330 || Loss: 0.6208 || timer: 0.0891 sec.
iter 381340 || Loss: 0.8336 || timer: 0.0816 sec.
iter 381350 || Loss: 0.6668 || timer: 0.1134 sec.
iter 381360 || Loss: 0.6829 || timer: 0.0879 sec.
iter 381370 || Loss: 0.3744 || timer: 0.0167 sec.
iter 381380 || Loss: 0.2220 || timer: 0.0813 sec.
iter 381390 || Loss: 0.9894 || timer: 0.0906 sec.
iter 381400 || Loss: 0.6117 || timer: 0.0870 sec.
iter 381410 || Loss: 0.6697 || timer: 0.0895 sec.
iter 381420 || Loss: 0.6283 || timer: 0.0895 sec.
iter 381430 || Loss: 0.5270 || timer: 0.0892 sec.
iter 381440 || Loss: 0.5421 || timer: 0.0927 sec.
iter 381450 || Loss: 0.5402 || timer: 0.0887 sec.
iter 381460 || Loss: 0.5751 || timer: 0.0825 sec.
iter 381470 || Loss: 0.6474 || timer: 0.1071 sec.
iter 381480 || Loss: 0.6607 || timer: 0.0886 sec.
iter 381490 || Loss: 0.7563 || timer: 0.1101 sec.
iter 381500 || Loss: 0.5128 || timer: 0.0992 sec.
iter 381510 || Loss: 0.9750 || timer: 0.1059 sec.
iter 381520 || Loss: 0.4927 || timer: 0.0899 sec.
iter 381530 || Loss: 0.5376 || timer: 0.1014 sec.
iter 381540 || Loss: 0.5012 || timer: 0.0901 sec.
iter 381550 || Loss: 0.7254 || timer: 0.0789 sec.
iter 381560 || Loss: 0.7451 || timer: 0.0867 sec.
iter 381570 || Loss: 0.6345 || timer: 0.0822 sec.
iter 381580 || Loss: 0.3953 || timer: 0.0819 sec.
iter 381590 || Loss: 0.7359 || timer: 0.1055 sec.
iter 381600 || Loss: 0.6947 || timer: 0.0881 sec.
iter 381610 || Loss: 0.8301 || timer: 0.0875 sec.
iter 381620 || Loss: 0.8969 || timer: 0.0976 sec.
iter 381630 || Loss: 0.7974 || timer: 0.0965 sec.
iter 381640 || Loss: 0.5531 || timer: 0.0871 sec.
iter 381650 || Loss: 0.5848 || timer: 0.0845 sec.
iter 381660 || Loss: 0.8075 || timer: 0.0827 sec.
iter 381670 || Loss: 0.7297 || timer: 0.0824 sec.
iter 381680 || Loss: 0.7265 || timer: 0.0919 sec.
iter 381690 || Loss: 0.4435 || timer: 0.0827 sec.
iter 381700 || Loss: 0.5173 || timer: 0.0172 sec.
iter 381710 || Loss: 0.7751 || timer: 0.1130 sec.
iter 381720 || Loss: 0.6043 || timer: 0.0888 sec.
iter 381730 || Loss: 0.5273 || timer: 0.1049 sec.
iter 381740 || Loss: 0.8548 || timer: 0.1048 sec.
iter 381750 || Loss: 0.7463 || timer: 0.0979 sec.
iter 381760 || Loss: 0.7731 || timer: 0.0977 sec.
iter 381770 || Loss: 0.7672 || timer: 0.0883 sec.
iter 381780 || Loss: 0.5807 || timer: 0.1025 sec.
iter 381790 || Loss: 0.7050 || timer: 0.0887 sec.
iter 381800 || Loss: 0.5349 || timer: 0.1067 sec.
iter 381810 || Loss: 0.6585 || timer: 0.1072 sec.
iter 381820 || Loss: 0.6326 || timer: 0.0821 sec.
iter 381830 || Loss: 0.6143 || timer: 0.0835 sec.
iter 381840 || Loss: 0.4515 || timer: 0.1031 sec.
iter 381850 || Loss: 0.7034 || timer: 0.0870 sec.
iter 381860 || Loss: 0.6694 || timer: 0.1027 sec.
iter 381870 || Loss: 0.3733 || timer: 0.0872 sec.
iter 381880 || Loss: 0.5927 || timer: 0.0835 sec.
iter 381890 || Loss: 0.9844 || timer: 0.1178 sec.
iter 381900 || Loss: 0.8901 || timer: 0.0813 sec.
iter 381910 || Loss: 0.5664 || timer: 0.1138 sec.
iter 381920 || Loss: 0.5041 || timer: 0.0823 sec.
iter 381930 || Loss: 0.8638 || timer: 0.0813 sec.
iter 381940 || Loss: 0.9038 || timer: 0.0874 sec.
iter 381950 || Loss: 0.7731 || timer: 0.0913 sec.
iter 381960 || Loss: 0.8206 || timer: 0.1054 sec.
iter 381970 || Loss: 0.6260 || timer: 0.0994 sec.
iter 381980 || Loss: 0.7429 || timer: 0.0892 sec.
iter 381990 || Loss: 0.8127 || timer: 0.1066 sec.
iter 382000 || Loss: 0.7047 || timer: 0.0881 sec.
iter 382010 || Loss: 0.7291 || timer: 0.0863 sec.
iter 382020 || Loss: 0.6058 || timer: 0.0884 sec.
iter 382030 || Loss: 0.4123 || timer: 0.0163 sec.
iter 382040 || Loss: 1.5869 || timer: 0.0894 sec.
iter 382050 || Loss: 0.6925 || timer: 0.0810 sec.
iter 382060 || Loss: 0.5368 || timer: 0.0893 sec.
iter 382070 || Loss: 0.7223 || timer: 0.0808 sec.
iter 382080 || Loss: 0.6051 || timer: 0.0874 sec.
iter 382090 || Loss: 0.5215 || timer: 0.0910 sec.
iter 382100 || Loss: 0.7118 || timer: 0.0899 sec.
iter 382110 || Loss: 1.2298 || timer: 0.0846 sec.
iter 382120 || Loss: 0.7497 || timer: 0.0889 sec.
iter 382130 || Loss: 0.6688 || timer: 0.0956 sec.
iter 382140 || Loss: 0.6668 || timer: 0.0825 sec.
iter 382150 || Loss: 0.4773 || timer: 0.0820 sec.
iter 382160 || Loss: 1.1324 || timer: 0.1055 sec.
iter 382170 || Loss: 0.4230 || timer: 0.0884 sec.
iter 382180 || Loss: 0.5192 || timer: 0.0823 sec.
iter 382190 || Loss: 0.5595 || timer: 0.0927 sec.
iter 382200 || Loss: 0.7236 || timer: 0.0812 sec.
iter 382210 || Loss: 0.7162 || timer: 0.0822 sec.
iter 382220 || Loss: 0.6312 || timer: 0.0893 sec.
iter 382230 || Loss: 0.6874 || timer: 0.0824 sec.
iter 382240 || Loss: 0.5323 || timer: 0.0911 sec.
iter 382250 || Loss: 0.6887 || timer: 0.0808 sec.
iter 382260 || Loss: 1.0081 || timer: 0.0878 sec.
iter 382270 || Loss: 0.8153 || timer: 0.1025 sec.
iter 382280 || Loss: 0.6219 || timer: 0.0842 sec.
iter 382290 || Loss: 1.1755 || timer: 0.0851 sec.
iter 382300 || Loss: 0.5999 || timer: 0.0816 sec.
iter 382310 || Loss: 0.6107 || timer: 0.0935 sec.
iter 382320 || Loss: 0.6885 || timer: 0.0912 sec.
iter 382330 || Loss: 0.9376 || timer: 0.0906 sec.
iter 382340 || Loss: 0.4381 || timer: 0.0820 sec.
iter 382350 || Loss: 0.7052 || timer: 0.1143 sec.
iter 382360 || Loss: 0.6233 || timer: 0.0301 sec.
iter 382370 || Loss: 0.6678 || timer: 0.0849 sec.
iter 382380 || Loss: 0.7219 || timer: 0.0902 sec.
iter 382390 || Loss: 0.5968 || timer: 0.0920 sec.
iter 382400 || Loss: 0.7567 || timer: 0.1140 sec.
iter 382410 || Loss: 0.6003 || timer: 0.0896 sec.
iter 382420 || Loss: 0.9145 || timer: 0.0880 sec.
iter 382430 || Loss: 0.6991 || timer: 0.1102 sec.
iter 382440 || Loss: 0.6048 || timer: 0.0894 sec.
iter 382450 || Loss: 0.6860 || timer: 0.0863 sec.
iter 382460 || Loss: 0.5998 || timer: 0.0947 sec.
iter 382470 || Loss: 0.8557 || timer: 0.0880 sec.
iter 382480 || Loss: 0.5155 || timer: 0.0945 sec.
iter 382490 || Loss: 0.8688 || timer: 0.0877 sec.
iter 382500 || Loss: 0.6150 || timer: 0.0935 sec.
iter 382510 || Loss: 0.4688 || timer: 0.0913 sec.
iter 382520 || Loss: 0.7578 || timer: 0.0812 sec.
iter 382530 || Loss: 0.7920 || timer: 0.1074 sec.
iter 382540 || Loss: 0.9759 || timer: 0.0845 sec.
iter 382550 || Loss: 0.6167 || timer: 0.0864 sec.
iter 382560 || Loss: 0.6996 || timer: 0.0870 sec.
iter 382570 || Loss: 0.5929 || timer: 0.1055 sec.
iter 382580 || Loss: 1.1132 || timer: 0.0892 sec.
iter 382590 || Loss: 0.7333 || timer: 0.0861 sec.
iter 382600 || Loss: 0.7326 || timer: 0.0923 sec.
iter 382610 || Loss: 0.6225 || timer: 0.1038 sec.
iter 382620 || Loss: 0.5622 || timer: 0.0821 sec.
iter 382630 || Loss: 0.7245 || timer: 0.0900 sec.
iter 382640 || Loss: 0.5649 || timer: 0.0827 sec.
iter 382650 || Loss: 0.9124 || timer: 0.0894 sec.
iter 382660 || Loss: 0.8803 || timer: 0.0843 sec.
iter 382670 || Loss: 0.5621 || timer: 0.0817 sec.
iter 382680 || Loss: 0.4423 || timer: 0.0831 sec.
iter 382690 || Loss: 0.5991 || timer: 0.0165 sec.
iter 382700 || Loss: 1.2309 || timer: 0.0914 sec.
iter 382710 || Loss: 1.2524 || timer: 0.0935 sec.
iter 382720 || Loss: 0.6020 || timer: 0.0818 sec.
iter 382730 || Loss: 0.5628 || timer: 0.0884 sec.
iter 382740 || Loss: 0.6843 || timer: 0.0813 sec.
iter 382750 || Loss: 0.4733 || timer: 0.1065 sec.
iter 382760 || Loss: 1.1070 || timer: 0.0830 sec.
iter 382770 || Loss: 0.6562 || timer: 0.1009 sec.
iter 382780 || Loss: 0.5281 || timer: 0.0813 sec.
iter 382790 || Loss: 0.7227 || timer: 0.1156 sec.
iter 382800 || Loss: 0.5281 || timer: 0.0825 sec.
iter 382810 || Loss: 0.6862 || timer: 0.0848 sec.
iter 382820 || Loss: 0.4635 || timer: 0.0736 sec.
iter 382830 || Loss: 0.5208 || timer: 0.0977 sec.
iter 382840 || Loss: 0.5437 || timer: 0.0901 sec.
iter 382850 || Loss: 0.4499 || timer: 0.0941 sec.
iter 382860 || Loss: 0.7371 || timer: 0.0918 sec.
iter 382870 || Loss: 0.4978 || timer: 0.0894 sec.
iter 382880 || Loss: 0.6574 || timer: 0.0888 sec.
iter 382890 || Loss: 0.5656 || timer: 0.0939 sec.
iter 382900 || Loss: 0.7516 || timer: 0.1450 sec.
iter 382910 || Loss: 0.7717 || timer: 0.0925 sec.
iter 382920 || Loss: 0.4704 || timer: 0.0907 sec.
iter 382930 || Loss: 0.7480 || timer: 0.0947 sec.
iter 382940 || Loss: 0.4931 || timer: 0.0760 sec.
iter 382950 || Loss: 0.7316 || timer: 0.0819 sec.
iter 382960 || Loss: 0.6214 || timer: 0.0887 sec.
iter 382970 || Loss: 0.7710 || timer: 0.0923 sec.
iter 382980 || Loss: 0.6126 || timer: 0.0864 sec.
iter 382990 || Loss: 0.6777 || timer: 0.0913 sec.
iter 383000 || Loss: 0.6204 || timer: 0.0872 sec.
iter 383010 || Loss: 0.5128 || timer: 0.0919 sec.
iter 383020 || Loss: 0.4445 || timer: 0.0168 sec.
iter 383030 || Loss: 0.3253 || timer: 0.0861 sec.
iter 383040 || Loss: 0.7752 || timer: 0.0849 sec.
iter 383050 || Loss: 0.5826 || timer: 0.0832 sec.
iter 383060 || Loss: 0.5519 || timer: 0.0811 sec.
iter 383070 || Loss: 0.7463 || timer: 0.1141 sec.
iter 383080 || Loss: 0.7541 || timer: 0.0987 sec.
iter 383090 || Loss: 0.7340 || timer: 0.0826 sec.
iter 383100 || Loss: 0.6121 || timer: 0.1257 sec.
iter 383110 || Loss: 0.4542 || timer: 0.0998 sec.
iter 383120 || Loss: 0.6538 || timer: 0.0933 sec.
iter 383130 || Loss: 0.8629 || timer: 0.0866 sec.
iter 383140 || Loss: 0.9685 || timer: 0.0918 sec.
iter 383150 || Loss: 0.5978 || timer: 0.0877 sec.
iter 383160 || Loss: 0.3839 || timer: 0.0864 sec.
iter 383170 || Loss: 0.9530 || timer: 0.1102 sec.
iter 383180 || Loss: 0.6782 || timer: 0.1205 sec.
iter 383190 || Loss: 0.7474 || timer: 0.0804 sec.
iter 383200 || Loss: 0.6349 || timer: 0.0859 sec.
iter 383210 || Loss: 0.6851 || timer: 0.1099 sec.
iter 383220 || Loss: 0.6941 || timer: 0.0827 sec.
iter 383230 || Loss: 0.7736 || timer: 0.0814 sec.
iter 383240 || Loss: 0.5176 || timer: 0.0818 sec.
iter 383250 || Loss: 0.6019 || timer: 0.0884 sec.
iter 383260 || Loss: 1.0813 || timer: 0.0894 sec.
iter 383270 || Loss: 0.6255 || timer: 0.1041 sec.
iter 383280 || Loss: 0.6319 || timer: 0.0943 sec.
iter 383290 || Loss: 0.5541 || timer: 0.0825 sec.
iter 383300 || Loss: 0.8952 || timer: 0.0833 sec.
iter 383310 || Loss: 0.9206 || timer: 0.1095 sec.
iter 383320 || Loss: 0.5575 || timer: 0.0935 sec.
iter 383330 || Loss: 0.7350 || timer: 0.0868 sec.
iter 383340 || Loss: 0.7229 || timer: 0.0831 sec.
iter 383350 || Loss: 0.6893 || timer: 0.0198 sec.
iter 383360 || Loss: 0.7669 || timer: 0.0848 sec.
iter 383370 || Loss: 0.9202 || timer: 0.0924 sec.
iter 383380 || Loss: 0.5155 || timer: 0.0885 sec.
iter 383390 || Loss: 0.4780 || timer: 0.1037 sec.
iter 383400 || Loss: 0.4993 || timer: 0.1070 sec.
iter 383410 || Loss: 0.5709 || timer: 0.0924 sec.
iter 383420 || Loss: 0.7486 || timer: 0.0924 sec.
iter 383430 || Loss: 0.7219 || timer: 0.0828 sec.
iter 383440 || Loss: 0.5991 || timer: 0.0904 sec.
iter 383450 || Loss: 0.7087 || timer: 0.1158 sec.
iter 383460 || Loss: 0.6746 || timer: 0.0830 sec.
iter 383470 || Loss: 0.6395 || timer: 0.0914 sec.
iter 383480 || Loss: 0.6368 || timer: 0.0930 sec.
iter 383490 || Loss: 0.7173 || timer: 0.0892 sec.
iter 383500 || Loss: 0.6125 || timer: 0.0828 sec.
iter 383510 || Loss: 0.8396 || timer: 0.1038 sec.
iter 383520 || Loss: 0.7286 || timer: 0.0967 sec.
iter 383530 || Loss: 0.7525 || timer: 0.0836 sec.
iter 383540 || Loss: 0.8481 || timer: 0.1075 sec.
iter 383550 || Loss: 0.7145 || timer: 0.1032 sec.
iter 383560 || Loss: 0.5524 || timer: 0.0908 sec.
iter 383570 || Loss: 0.7061 || timer: 0.0907 sec.
iter 383580 || Loss: 1.1696 || timer: 0.0901 sec.
iter 383590 || Loss: 0.5044 || timer: 0.0882 sec.
iter 383600 || Loss: 0.5724 || timer: 0.0946 sec.
iter 383610 || Loss: 0.5494 || timer: 0.0978 sec.
iter 383620 || Loss: 0.6332 || timer: 0.0887 sec.
iter 383630 || Loss: 0.8925 || timer: 0.0890 sec.
iter 383640 || Loss: 0.4006 || timer: 0.0913 sec.
iter 383650 || Loss: 0.8623 || timer: 0.0835 sec.
iter 383660 || Loss: 0.6887 || timer: 0.0928 sec.
iter 383670 || Loss: 0.5135 || timer: 0.0900 sec.
iter 383680 || Loss: 0.6398 || timer: 0.0239 sec.
iter 383690 || Loss: 1.1150 || timer: 0.1067 sec.
iter 383700 || Loss: 0.7507 || timer: 0.1052 sec.
iter 383710 || Loss: 0.7717 || timer: 0.0996 sec.
iter 383720 || Loss: 0.6467 || timer: 0.0940 sec.
iter 383730 || Loss: 0.6436 || timer: 0.1055 sec.
iter 383740 || Loss: 0.7219 || timer: 0.0862 sec.
iter 383750 || Loss: 0.6425 || timer: 0.0876 sec.
iter 383760 || Loss: 0.7032 || timer: 0.0931 sec.
iter 383770 || Loss: 0.5280 || timer: 0.0906 sec.
iter 383780 || Loss: 0.5308 || timer: 0.1163 sec.
iter 383790 || Loss: 0.7646 || timer: 0.0928 sec.
iter 383800 || Loss: 0.7150 || timer: 0.0836 sec.
iter 383810 || Loss: 0.5594 || timer: 0.0843 sec.
iter 383820 || Loss: 0.4779 || timer: 0.0876 sec.
iter 383830 || Loss: 0.8020 || timer: 0.0891 sec.
iter 383840 || Loss: 0.4945 || timer: 0.1060 sec.
iter 383850 || Loss: 0.5562 || timer: 0.0927 sec.
iter 383860 || Loss: 0.6045 || timer: 0.0824 sec.
iter 383870 || Loss: 0.6855 || timer: 0.1117 sec.
iter 383880 || Loss: 0.5853 || timer: 0.0838 sec.
iter 383890 || Loss: 0.8370 || timer: 0.0892 sec.
iter 383900 || Loss: 0.9383 || timer: 0.0982 sec.
iter 383910 || Loss: 0.5626 || timer: 0.0839 sec.
iter 383920 || Loss: 0.7196 || timer: 0.1008 sec.
iter 383930 || Loss: 0.7249 || timer: 0.0884 sec.
iter 383940 || Loss: 0.6587 || timer: 0.0823 sec.
iter 383950 || Loss: 1.0861 || timer: 0.0914 sec.
iter 383960 || Loss: 0.5701 || timer: 0.0838 sec.
iter 383970 || Loss: 0.6494 || timer: 0.0950 sec.
iter 383980 || Loss: 0.4897 || timer: 0.0925 sec.
iter 383990 || Loss: 0.6668 || timer: 0.1231 sec.
iter 384000 || Loss: 0.6244 || timer: 0.0828 sec.
iter 384010 || Loss: 0.5275 || timer: 0.0312 sec.
iter 384020 || Loss: 0.2018 || timer: 0.0875 sec.
iter 384030 || Loss: 0.3996 || timer: 0.0909 sec.
iter 384040 || Loss: 0.5804 || timer: 0.0895 sec.
iter 384050 || Loss: 0.5157 || timer: 0.0918 sec.
iter 384060 || Loss: 0.7223 || timer: 0.0953 sec.
iter 384070 || Loss: 0.4250 || timer: 0.1050 sec.
iter 384080 || Loss: 0.4811 || timer: 0.0864 sec.
iter 384090 || Loss: 0.9959 || timer: 0.0840 sec.
iter 384100 || Loss: 0.8438 || timer: 0.0835 sec.
iter 384110 || Loss: 0.5953 || timer: 0.1100 sec.
iter 384120 || Loss: 0.8806 || timer: 0.0931 sec.
iter 384130 || Loss: 0.7701 || timer: 0.0888 sec.
iter 384140 || Loss: 0.4717 || timer: 0.0849 sec.
iter 384150 || Loss: 0.6026 || timer: 0.0828 sec.
iter 384160 || Loss: 0.7164 || timer: 0.0904 sec.
iter 384170 || Loss: 0.6756 || timer: 0.0896 sec.
iter 384180 || Loss: 0.7385 || timer: 0.0964 sec.
iter 384190 || Loss: 0.6724 || timer: 0.1010 sec.
iter 384200 || Loss: 0.7042 || timer: 0.0900 sec.
iter 384210 || Loss: 0.6231 || timer: 0.0833 sec.
iter 384220 || Loss: 0.6058 || timer: 0.0925 sec.
iter 384230 || Loss: 0.8443 || timer: 0.0906 sec.
iter 384240 || Loss: 0.8603 || timer: 0.0836 sec.
iter 384250 || Loss: 0.6527 || timer: 0.1053 sec.
iter 384260 || Loss: 0.5564 || timer: 0.0830 sec.
iter 384270 || Loss: 0.8488 || timer: 0.0878 sec.
iter 384280 || Loss: 0.5633 || timer: 0.0939 sec.
iter 384290 || Loss: 1.0102 || timer: 0.0926 sec.
iter 384300 || Loss: 0.4766 || timer: 0.0866 sec.
iter 384310 || Loss: 0.7909 || timer: 0.0885 sec.
iter 384320 || Loss: 0.8903 || timer: 0.0909 sec.
iter 384330 || Loss: 0.7046 || timer: 0.0862 sec.
iter 384340 || Loss: 0.4038 || timer: 0.0155 sec.
iter 384350 || Loss: 0.7875 || timer: 0.1006 sec.
iter 384360 || Loss: 0.6828 || timer: 0.0835 sec.
iter 384370 || Loss: 0.4833 || timer: 0.0921 sec.
iter 384380 || Loss: 0.5293 || timer: 0.0926 sec.
iter 384390 || Loss: 0.4377 || timer: 0.0829 sec.
iter 384400 || Loss: 0.6744 || timer: 0.0844 sec.
iter 384410 || Loss: 0.7460 || timer: 0.0919 sec.
iter 384420 || Loss: 0.5732 || timer: 0.0945 sec.
iter 384430 || Loss: 0.8090 || timer: 0.0903 sec.
iter 384440 || Loss: 0.7248 || timer: 0.1101 sec.
iter 384450 || Loss: 0.5680 || timer: 0.1041 sec.
iter 384460 || Loss: 0.7091 || timer: 0.0926 sec.
iter 384470 || Loss: 0.7090 || timer: 0.0929 sec.
iter 384480 || Loss: 0.6947 || timer: 0.0904 sec.
iter 384490 || Loss: 0.5861 || timer: 0.1035 sec.
iter 384500 || Loss: 0.5518 || timer: 0.0870 sec.
iter 384510 || Loss: 0.6715 || timer: 0.0901 sec.
iter 384520 || Loss: 0.7537 || timer: 0.1130 sec.
iter 384530 || Loss: 0.7858 || timer: 0.0885 sec.
iter 384540 || Loss: 0.5975 || timer: 0.0827 sec.
iter 384550 || Loss: 0.4256 || timer: 0.0900 sec.
iter 384560 || Loss: 0.5796 || timer: 0.1036 sec.
iter 384570 || Loss: 0.7031 || timer: 0.0835 sec.
iter 384580 || Loss: 0.5687 || timer: 0.0916 sec.
iter 384590 || Loss: 0.7204 || timer: 0.1047 sec.
iter 384600 || Loss: 0.6942 || timer: 0.1049 sec.
iter 384610 || Loss: 0.7980 || timer: 0.0933 sec.
iter 384620 || Loss: 0.7361 || timer: 0.0932 sec.
iter 384630 || Loss: 0.7209 || timer: 0.0921 sec.
iter 384640 || Loss: 0.6176 || timer: 0.0900 sec.
iter 384650 || Loss: 0.8305 || timer: 0.1112 sec.
iter 384660 || Loss: 0.5188 || timer: 0.0929 sec.
iter 384670 || Loss: 1.0270 || timer: 0.0242 sec.
iter 384680 || Loss: 0.2087 || timer: 0.0921 sec.
iter 384690 || Loss: 0.7204 || timer: 0.0920 sec.
iter 384700 || Loss: 0.5512 || timer: 0.0931 sec.
iter 384710 || Loss: 0.7203 || timer: 0.0912 sec.
iter 384720 || Loss: 0.5839 || timer: 0.0912 sec.
iter 384730 || Loss: 0.6263 || timer: 0.0908 sec.
iter 384740 || Loss: 0.6392 || timer: 0.0910 sec.
iter 384750 || Loss: 0.4005 || timer: 0.0917 sec.
iter 384760 || Loss: 0.5751 || timer: 0.0926 sec.
iter 384770 || Loss: 0.8140 || timer: 0.1016 sec.
iter 384780 || Loss: 0.5799 || timer: 0.0915 sec.
iter 384790 || Loss: 0.6064 || timer: 0.0853 sec.
iter 384800 || Loss: 0.7269 || timer: 0.0835 sec.
iter 384810 || Loss: 0.4438 || timer: 0.0914 sec.
iter 384820 || Loss: 0.8058 || timer: 0.0904 sec.
iter 384830 || Loss: 0.4046 || timer: 0.0937 sec.
iter 384840 || Loss: 0.5882 || timer: 0.0911 sec.
iter 384850 || Loss: 0.4033 || timer: 0.0910 sec.
iter 384860 || Loss: 0.6174 || timer: 0.1077 sec.
iter 384870 || Loss: 0.5919 || timer: 0.0895 sec.
iter 384880 || Loss: 0.7364 || timer: 0.1041 sec.
iter 384890 || Loss: 0.7171 || timer: 0.0925 sec.
iter 384900 || Loss: 0.5958 || timer: 0.0833 sec.
iter 384910 || Loss: 0.9329 || timer: 0.0828 sec.
iter 384920 || Loss: 0.8096 || timer: 0.0910 sec.
iter 384930 || Loss: 0.7620 || timer: 0.0904 sec.
iter 384940 || Loss: 0.6343 || timer: 0.0908 sec.
iter 384950 || Loss: 0.6115 || timer: 0.0854 sec.
iter 384960 || Loss: 1.0740 || timer: 0.0899 sec.
iter 384970 || Loss: 0.7257 || timer: 0.0905 sec.
iter 384980 || Loss: 0.5757 || timer: 0.0846 sec.
iter 384990 || Loss: 0.4799 || timer: 0.0837 sec.
iter 385000 || Loss: 0.7916 || Saving state, iter: 385000
timer: 0.0171 sec.
iter 385010 || Loss: 0.4299 || timer: 0.1200 sec.
iter 385020 || Loss: 0.7067 || timer: 0.1014 sec.
iter 385030 || Loss: 0.4815 || timer: 0.0881 sec.
iter 385040 || Loss: 0.8064 || timer: 0.0827 sec.
iter 385050 || Loss: 0.8007 || timer: 0.0880 sec.
iter 385060 || Loss: 0.8320 || timer: 0.0862 sec.
iter 385070 || Loss: 0.5661 || timer: 0.0957 sec.
iter 385080 || Loss: 0.5824 || timer: 0.0882 sec.
iter 385090 || Loss: 0.6009 || timer: 0.0853 sec.
iter 385100 || Loss: 0.5188 || timer: 0.0990 sec.
iter 385110 || Loss: 0.9359 || timer: 0.0834 sec.
iter 385120 || Loss: 0.7566 || timer: 0.0932 sec.
iter 385130 || Loss: 0.6078 || timer: 0.0941 sec.
iter 385140 || Loss: 0.8803 || timer: 0.1083 sec.
iter 385150 || Loss: 0.6859 || timer: 0.0926 sec.
iter 385160 || Loss: 0.5767 || timer: 0.0843 sec.
iter 385170 || Loss: 0.7937 || timer: 0.0903 sec.
iter 385180 || Loss: 0.6130 || timer: 0.0839 sec.
iter 385190 || Loss: 0.6004 || timer: 0.0917 sec.
iter 385200 || Loss: 0.7533 || timer: 0.0817 sec.
iter 385210 || Loss: 0.6008 || timer: 0.1088 sec.
iter 385220 || Loss: 1.0107 || timer: 0.0828 sec.
iter 385230 || Loss: 0.7522 || timer: 0.0897 sec.
iter 385240 || Loss: 0.7048 || timer: 0.0922 sec.
iter 385250 || Loss: 0.7418 || timer: 0.0923 sec.
iter 385260 || Loss: 0.8520 || timer: 0.0927 sec.
iter 385270 || Loss: 0.7215 || timer: 0.0826 sec.
iter 385280 || Loss: 0.7040 || timer: 0.1063 sec.
iter 385290 || Loss: 0.7652 || timer: 0.0848 sec.
iter 385300 || Loss: 0.7517 || timer: 0.0894 sec.
iter 385310 || Loss: 0.7239 || timer: 0.1068 sec.
iter 385320 || Loss: 0.6123 || timer: 0.0840 sec.
iter 385330 || Loss: 0.5742 || timer: 0.0221 sec.
iter 385340 || Loss: 0.3745 || timer: 0.0845 sec.
iter 385350 || Loss: 0.6952 || timer: 0.0920 sec.
iter 385360 || Loss: 0.4931 || timer: 0.0897 sec.
iter 385370 || Loss: 0.7344 || timer: 0.1022 sec.
iter 385380 || Loss: 0.6435 || timer: 0.0827 sec.
iter 385390 || Loss: 0.5345 || timer: 0.0945 sec.
iter 385400 || Loss: 0.7177 || timer: 0.0838 sec.
iter 385410 || Loss: 0.7035 || timer: 0.0904 sec.
iter 385420 || Loss: 0.6823 || timer: 0.0906 sec.
iter 385430 || Loss: 1.1198 || timer: 0.1328 sec.
iter 385440 || Loss: 0.6039 || timer: 0.0767 sec.
iter 385450 || Loss: 0.6233 || timer: 0.0864 sec.
iter 385460 || Loss: 0.7973 || timer: 0.0886 sec.
iter 385470 || Loss: 0.9251 || timer: 0.1068 sec.
iter 385480 || Loss: 0.7919 || timer: 0.0905 sec.
iter 385490 || Loss: 0.9101 || timer: 0.0908 sec.
iter 385500 || Loss: 0.7064 || timer: 0.0908 sec.
iter 385510 || Loss: 0.6330 || timer: 0.0835 sec.
iter 385520 || Loss: 0.5039 || timer: 0.0825 sec.
iter 385530 || Loss: 0.6830 || timer: 0.0892 sec.
iter 385540 || Loss: 0.5692 || timer: 0.1115 sec.
iter 385550 || Loss: 0.5523 || timer: 0.0897 sec.
iter 385560 || Loss: 0.6857 || timer: 0.0895 sec.
iter 385570 || Loss: 0.7881 || timer: 0.0920 sec.
iter 385580 || Loss: 0.8615 || timer: 0.1099 sec.
iter 385590 || Loss: 0.9124 || timer: 0.0922 sec.
iter 385600 || Loss: 0.8284 || timer: 0.0884 sec.
iter 385610 || Loss: 0.6553 || timer: 0.0893 sec.
iter 385620 || Loss: 0.7996 || timer: 0.0976 sec.
iter 385630 || Loss: 0.5965 || timer: 0.0905 sec.
iter 385640 || Loss: 0.7484 || timer: 0.0912 sec.
iter 385650 || Loss: 1.1118 || timer: 0.0834 sec.
iter 385660 || Loss: 0.7371 || timer: 0.0240 sec.
iter 385670 || Loss: 0.0877 || timer: 0.0984 sec.
iter 385680 || Loss: 1.1579 || timer: 0.0906 sec.
iter 385690 || Loss: 0.8147 || timer: 0.0882 sec.
iter 385700 || Loss: 0.9370 || timer: 0.0916 sec.
iter 385710 || Loss: 0.7319 || timer: 0.1094 sec.
iter 385720 || Loss: 0.6018 || timer: 0.0849 sec.
iter 385730 || Loss: 0.8937 || timer: 0.0842 sec.
iter 385740 || Loss: 0.5840 || timer: 0.0856 sec.
iter 385750 || Loss: 0.6407 || timer: 0.0899 sec.
iter 385760 || Loss: 0.7315 || timer: 0.1144 sec.
iter 385770 || Loss: 0.8014 || timer: 0.0929 sec.
iter 385780 || Loss: 0.5869 || timer: 0.0843 sec.
iter 385790 || Loss: 0.8342 || timer: 0.0920 sec.
iter 385800 || Loss: 0.7186 || timer: 0.0893 sec.
iter 385810 || Loss: 0.7510 || timer: 0.0911 sec.
iter 385820 || Loss: 0.9286 || timer: 0.0830 sec.
iter 385830 || Loss: 0.6488 || timer: 0.0933 sec.
iter 385840 || Loss: 0.8167 || timer: 0.0924 sec.
iter 385850 || Loss: 0.7917 || timer: 0.0907 sec.
iter 385860 || Loss: 0.6175 || timer: 0.0909 sec.
iter 385870 || Loss: 0.6203 || timer: 0.0830 sec.
iter 385880 || Loss: 0.5100 || timer: 0.0839 sec.
iter 385890 || Loss: 0.6918 || timer: 0.0914 sec.
iter 385900 || Loss: 0.5897 || timer: 0.0831 sec.
iter 385910 || Loss: 0.6233 || timer: 0.0900 sec.
iter 385920 || Loss: 0.7165 || timer: 0.0876 sec.
iter 385930 || Loss: 0.6733 || timer: 0.0901 sec.
iter 385940 || Loss: 0.9410 || timer: 0.1147 sec.
iter 385950 || Loss: 1.0548 || timer: 0.0941 sec.
iter 385960 || Loss: 0.5959 || timer: 0.0860 sec.
iter 385970 || Loss: 0.4924 || timer: 0.0825 sec.
iter 385980 || Loss: 0.8007 || timer: 0.0917 sec.
iter 385990 || Loss: 0.4461 || timer: 0.0267 sec.
iter 386000 || Loss: 0.8206 || timer: 0.0828 sec.
iter 386010 || Loss: 0.5895 || timer: 0.0886 sec.
iter 386020 || Loss: 0.6059 || timer: 0.0849 sec.
iter 386030 || Loss: 0.6609 || timer: 0.0828 sec.
iter 386040 || Loss: 0.4336 || timer: 0.0839 sec.
iter 386050 || Loss: 0.5775 || timer: 0.1031 sec.
iter 386060 || Loss: 0.5205 || timer: 0.0904 sec.
iter 386070 || Loss: 0.6677 || timer: 0.0910 sec.
iter 386080 || Loss: 0.5881 || timer: 0.0918 sec.
iter 386090 || Loss: 0.5076 || timer: 0.1014 sec.
iter 386100 || Loss: 0.5611 || timer: 0.0847 sec.
iter 386110 || Loss: 0.8220 || timer: 0.0858 sec.
iter 386120 || Loss: 0.6901 || timer: 0.0902 sec.
iter 386130 || Loss: 0.6130 || timer: 0.0900 sec.
iter 386140 || Loss: 0.8185 || timer: 0.0839 sec.
iter 386150 || Loss: 0.7596 || timer: 0.0948 sec.
iter 386160 || Loss: 0.8929 || timer: 0.0821 sec.
iter 386170 || Loss: 0.8482 || timer: 0.0890 sec.
iter 386180 || Loss: 0.7040 || timer: 0.0964 sec.
iter 386190 || Loss: 0.7094 || timer: 0.0883 sec.
iter 386200 || Loss: 0.8198 || timer: 0.0903 sec.
iter 386210 || Loss: 0.6803 || timer: 0.1059 sec.
iter 386220 || Loss: 0.8031 || timer: 0.0918 sec.
iter 386230 || Loss: 0.6381 || timer: 0.0902 sec.
iter 386240 || Loss: 0.7393 || timer: 0.0876 sec.
iter 386250 || Loss: 0.4921 || timer: 0.0931 sec.
iter 386260 || Loss: 0.6478 || timer: 0.0866 sec.
iter 386270 || Loss: 0.9506 || timer: 0.0915 sec.
iter 386280 || Loss: 0.7136 || timer: 0.0892 sec.
iter 386290 || Loss: 0.6607 || timer: 0.0910 sec.
iter 386300 || Loss: 0.5525 || timer: 0.0899 sec.
iter 386310 || Loss: 0.8181 || timer: 0.0901 sec.
iter 386320 || Loss: 1.1489 || timer: 0.0292 sec.
iter 386330 || Loss: 0.2029 || timer: 0.0825 sec.
iter 386340 || Loss: 0.8717 || timer: 0.0846 sec.
iter 386350 || Loss: 0.5363 || timer: 0.0821 sec.
iter 386360 || Loss: 0.6755 || timer: 0.0822 sec.
iter 386370 || Loss: 0.4371 || timer: 0.1184 sec.
iter 386380 || Loss: 0.5359 || timer: 0.0922 sec.
iter 386390 || Loss: 1.1103 || timer: 0.0927 sec.
iter 386400 || Loss: 0.6561 || timer: 0.1045 sec.
iter 386410 || Loss: 0.6990 || timer: 0.0824 sec.
iter 386420 || Loss: 0.7189 || timer: 0.1184 sec.
iter 386430 || Loss: 0.8996 || timer: 0.0913 sec.
iter 386440 || Loss: 1.0256 || timer: 0.0828 sec.
iter 386450 || Loss: 0.6021 || timer: 0.0832 sec.
iter 386460 || Loss: 0.5049 || timer: 0.0917 sec.
iter 386470 || Loss: 0.5971 || timer: 0.0907 sec.
iter 386480 || Loss: 0.6341 || timer: 0.0894 sec.
iter 386490 || Loss: 0.5375 || timer: 0.1084 sec.
iter 386500 || Loss: 0.9565 || timer: 0.0894 sec.
iter 386510 || Loss: 0.5461 || timer: 0.0833 sec.
iter 386520 || Loss: 0.7226 || timer: 0.0830 sec.
iter 386530 || Loss: 0.5692 || timer: 0.1042 sec.
iter 386540 || Loss: 1.1065 || timer: 0.1005 sec.
iter 386550 || Loss: 0.6630 || timer: 0.0909 sec.
iter 386560 || Loss: 0.7133 || timer: 0.0908 sec.
iter 386570 || Loss: 0.6885 || timer: 0.0840 sec.
iter 386580 || Loss: 0.8625 || timer: 0.0905 sec.
iter 386590 || Loss: 0.8333 || timer: 0.0900 sec.
iter 386600 || Loss: 0.7426 || timer: 0.0880 sec.
iter 386610 || Loss: 0.8585 || timer: 0.0897 sec.
iter 386620 || Loss: 0.5563 || timer: 0.0871 sec.
iter 386630 || Loss: 0.5676 || timer: 0.0839 sec.
iter 386640 || Loss: 1.0330 || timer: 0.0896 sec.
iter 386650 || Loss: 0.6378 || timer: 0.0254 sec.
iter 386660 || Loss: 0.8412 || timer: 0.1073 sec.
iter 386670 || Loss: 0.6892 || timer: 0.0985 sec.
iter 386680 || Loss: 0.5304 || timer: 0.0877 sec.
iter 386690 || Loss: 0.4369 || timer: 0.1069 sec.
iter 386700 || Loss: 0.5700 || timer: 0.0838 sec.
iter 386710 || Loss: 0.6611 || timer: 0.0928 sec.
iter 386720 || Loss: 0.6391 || timer: 0.0918 sec.
iter 386730 || Loss: 0.6599 || timer: 0.0957 sec.
iter 386740 || Loss: 0.4974 || timer: 0.0839 sec.
iter 386750 || Loss: 0.5513 || timer: 0.1134 sec.
iter 386760 || Loss: 0.5882 || timer: 0.0917 sec.
iter 386770 || Loss: 0.5150 || timer: 0.0841 sec.
iter 386780 || Loss: 0.6268 || timer: 0.0872 sec.
iter 386790 || Loss: 0.6407 || timer: 0.0901 sec.
iter 386800 || Loss: 0.5729 || timer: 0.0927 sec.
iter 386810 || Loss: 0.3799 || timer: 0.0823 sec.
iter 386820 || Loss: 0.6242 || timer: 0.0936 sec.
iter 386830 || Loss: 0.6423 || timer: 0.0887 sec.
iter 386840 || Loss: 0.7670 || timer: 0.0992 sec.
iter 386850 || Loss: 0.6762 || timer: 0.0952 sec.
iter 386860 || Loss: 0.5643 || timer: 0.0905 sec.
iter 386870 || Loss: 0.6055 || timer: 0.0835 sec.
iter 386880 || Loss: 0.8561 || timer: 0.0922 sec.
iter 386890 || Loss: 0.7083 || timer: 0.0901 sec.
iter 386900 || Loss: 0.5755 || timer: 0.0904 sec.
iter 386910 || Loss: 0.6809 || timer: 0.1096 sec.
iter 386920 || Loss: 0.8614 || timer: 0.0897 sec.
iter 386930 || Loss: 1.0907 || timer: 0.1041 sec.
iter 386940 || Loss: 0.7006 || timer: 0.0969 sec.
iter 386950 || Loss: 0.7428 || timer: 0.0905 sec.
iter 386960 || Loss: 0.6077 || timer: 0.0911 sec.
iter 386970 || Loss: 0.7073 || timer: 0.0839 sec.
iter 386980 || Loss: 0.8978 || timer: 0.0180 sec.
iter 386990 || Loss: 1.0171 || timer: 0.0830 sec.
iter 387000 || Loss: 0.6469 || timer: 0.0907 sec.
iter 387010 || Loss: 0.6566 || timer: 0.0841 sec.
iter 387020 || Loss: 0.9512 || timer: 0.0912 sec.
iter 387030 || Loss: 0.7501 || timer: 0.0918 sec.
iter 387040 || Loss: 0.7591 || timer: 0.0900 sec.
iter 387050 || Loss: 0.6109 || timer: 0.0903 sec.
iter 387060 || Loss: 0.8871 || timer: 0.0919 sec.
iter 387070 || Loss: 0.5184 || timer: 0.1156 sec.
iter 387080 || Loss: 0.7437 || timer: 0.1370 sec.
iter 387090 || Loss: 0.7787 || timer: 0.0910 sec.
iter 387100 || Loss: 0.5754 || timer: 0.1117 sec.
iter 387110 || Loss: 0.4566 || timer: 0.0829 sec.
iter 387120 || Loss: 0.9966 || timer: 0.0915 sec.
iter 387130 || Loss: 0.8096 || timer: 0.0917 sec.
iter 387140 || Loss: 0.6908 || timer: 0.0918 sec.
iter 387150 || Loss: 0.6772 || timer: 0.1031 sec.
iter 387160 || Loss: 0.7720 || timer: 0.0911 sec.
iter 387170 || Loss: 0.8468 || timer: 0.0834 sec.
iter 387180 || Loss: 0.6983 || timer: 0.0937 sec.
iter 387190 || Loss: 0.6654 || timer: 0.0928 sec.
iter 387200 || Loss: 0.5798 || timer: 0.1079 sec.
iter 387210 || Loss: 0.8497 || timer: 0.0906 sec.
iter 387220 || Loss: 0.4331 || timer: 0.1059 sec.
iter 387230 || Loss: 0.5380 || timer: 0.0854 sec.
iter 387240 || Loss: 0.7027 || timer: 0.1160 sec.
iter 387250 || Loss: 0.6152 || timer: 0.0873 sec.
iter 387260 || Loss: 0.8146 || timer: 0.0830 sec.
iter 387270 || Loss: 0.7142 || timer: 0.0980 sec.
iter 387280 || Loss: 0.8080 || timer: 0.0828 sec.
iter 387290 || Loss: 0.8487 || timer: 0.0829 sec.
iter 387300 || Loss: 1.0010 || timer: 0.1099 sec.
iter 387310 || Loss: 0.5379 || timer: 0.0171 sec.
iter 387320 || Loss: 0.3046 || timer: 0.0893 sec.
iter 387330 || Loss: 0.5317 || timer: 0.0949 sec.
iter 387340 || Loss: 0.5926 || timer: 0.0836 sec.
iter 387350 || Loss: 0.6137 || timer: 0.0893 sec.
iter 387360 || Loss: 0.5774 || timer: 0.0931 sec.
iter 387370 || Loss: 0.6850 || timer: 0.0892 sec.
iter 387380 || Loss: 0.4546 || timer: 0.0842 sec.
iter 387390 || Loss: 0.6009 || timer: 0.0888 sec.
iter 387400 || Loss: 0.5947 || timer: 0.0845 sec.
iter 387410 || Loss: 1.1853 || timer: 0.1129 sec.
iter 387420 || Loss: 0.8852 || timer: 0.0900 sec.
iter 387430 || Loss: 0.7546 || timer: 0.0938 sec.
iter 387440 || Loss: 0.7987 || timer: 0.0835 sec.
iter 387450 || Loss: 0.7437 || timer: 0.0913 sec.
iter 387460 || Loss: 0.7822 || timer: 0.0844 sec.
iter 387470 || Loss: 0.7097 || timer: 0.0898 sec.
iter 387480 || Loss: 0.5437 || timer: 0.1048 sec.
iter 387490 || Loss: 0.7684 || timer: 0.1040 sec.
iter 387500 || Loss: 0.7434 || timer: 0.1005 sec.
iter 387510 || Loss: 0.7324 || timer: 0.0967 sec.
iter 387520 || Loss: 0.9889 || timer: 0.0907 sec.
iter 387530 || Loss: 0.3385 || timer: 0.0901 sec.
iter 387540 || Loss: 0.6417 || timer: 0.0923 sec.
iter 387550 || Loss: 0.6273 || timer: 0.0906 sec.
iter 387560 || Loss: 0.5647 || timer: 0.0897 sec.
iter 387570 || Loss: 0.5856 || timer: 0.0960 sec.
iter 387580 || Loss: 0.5883 || timer: 0.0910 sec.
iter 387590 || Loss: 0.9242 || timer: 0.0916 sec.
iter 387600 || Loss: 1.1138 || timer: 0.0853 sec.
iter 387610 || Loss: 0.5267 || timer: 0.0869 sec.
iter 387620 || Loss: 0.6521 || timer: 0.0987 sec.
iter 387630 || Loss: 0.6049 || timer: 0.0837 sec.
iter 387640 || Loss: 0.6042 || timer: 0.0189 sec.
iter 387650 || Loss: 2.2237 || timer: 0.0899 sec.
iter 387660 || Loss: 0.4945 || timer: 0.0904 sec.
iter 387670 || Loss: 0.6289 || timer: 0.0908 sec.
iter 387680 || Loss: 0.7721 || timer: 0.0892 sec.
iter 387690 || Loss: 0.5520 || timer: 0.0890 sec.
iter 387700 || Loss: 0.7398 || timer: 0.0844 sec.
iter 387710 || Loss: 0.5491 || timer: 0.0954 sec.
iter 387720 || Loss: 0.7212 || timer: 0.0927 sec.
iter 387730 || Loss: 0.5319 || timer: 0.0832 sec.
iter 387740 || Loss: 0.8960 || timer: 0.0919 sec.
iter 387750 || Loss: 0.5643 || timer: 0.0905 sec.
iter 387760 || Loss: 0.7173 || timer: 0.1091 sec.
iter 387770 || Loss: 0.7679 || timer: 0.0821 sec.
iter 387780 || Loss: 0.3881 || timer: 0.0922 sec.
iter 387790 || Loss: 0.7610 || timer: 0.1116 sec.
iter 387800 || Loss: 0.8002 || timer: 0.0892 sec.
iter 387810 || Loss: 0.6350 || timer: 0.1085 sec.
iter 387820 || Loss: 0.7814 || timer: 0.1065 sec.
iter 387830 || Loss: 0.4863 || timer: 0.1118 sec.
iter 387840 || Loss: 0.7954 || timer: 0.0834 sec.
iter 387850 || Loss: 0.6402 || timer: 0.0905 sec.
iter 387860 || Loss: 0.9493 || timer: 0.0989 sec.
iter 387870 || Loss: 0.6063 || timer: 0.0831 sec.
iter 387880 || Loss: 0.6063 || timer: 0.0981 sec.
iter 387890 || Loss: 0.5002 || timer: 0.0900 sec.
iter 387900 || Loss: 0.6020 || timer: 0.0911 sec.
iter 387910 || Loss: 0.6037 || timer: 0.1136 sec.
iter 387920 || Loss: 0.7981 || timer: 0.1109 sec.
iter 387930 || Loss: 0.6253 || timer: 0.0835 sec.
iter 387940 || Loss: 0.7905 || timer: 0.0965 sec.
iter 387950 || Loss: 0.7250 || timer: 0.0913 sec.
iter 387960 || Loss: 0.6561 || timer: 0.0907 sec.
iter 387970 || Loss: 0.7853 || timer: 0.0281 sec.
iter 387980 || Loss: 0.7621 || timer: 0.0889 sec.
iter 387990 || Loss: 0.4567 || timer: 0.0830 sec.
iter 388000 || Loss: 0.4130 || timer: 0.0903 sec.
iter 388010 || Loss: 0.6205 || timer: 0.0904 sec.
iter 388020 || Loss: 0.5704 || timer: 0.1189 sec.
iter 388030 || Loss: 0.5857 || timer: 0.0828 sec.
iter 388040 || Loss: 0.6138 || timer: 0.1064 sec.
iter 388050 || Loss: 0.7971 || timer: 0.0913 sec.
iter 388060 || Loss: 0.6492 || timer: 0.0907 sec.
iter 388070 || Loss: 0.5873 || timer: 0.0938 sec.
iter 388080 || Loss: 0.8287 || timer: 0.0921 sec.
iter 388090 || Loss: 0.4829 || timer: 0.0834 sec.
iter 388100 || Loss: 0.7443 || timer: 0.0841 sec.
iter 388110 || Loss: 0.8794 || timer: 0.0825 sec.
iter 388120 || Loss: 0.6638 || timer: 0.0919 sec.
iter 388130 || Loss: 0.7473 || timer: 0.1027 sec.
iter 388140 || Loss: 0.5615 || timer: 0.0861 sec.
iter 388150 || Loss: 0.5039 || timer: 0.0955 sec.
iter 388160 || Loss: 0.5185 || timer: 0.1004 sec.
iter 388170 || Loss: 0.5783 || timer: 0.0936 sec.
iter 388180 || Loss: 0.8444 || timer: 0.1129 sec.
iter 388190 || Loss: 0.6594 || timer: 0.0924 sec.
iter 388200 || Loss: 0.5249 || timer: 0.0841 sec.
iter 388210 || Loss: 0.6599 || timer: 0.0823 sec.
iter 388220 || Loss: 0.7853 || timer: 0.0929 sec.
iter 388230 || Loss: 0.5765 || timer: 0.0841 sec.
iter 388240 || Loss: 0.7377 || timer: 0.0906 sec.
iter 388250 || Loss: 0.5393 || timer: 0.1037 sec.
iter 388260 || Loss: 0.6865 || timer: 0.0960 sec.
iter 388270 || Loss: 0.7909 || timer: 0.0914 sec.
iter 388280 || Loss: 0.6403 || timer: 0.0915 sec.
iter 388290 || Loss: 0.8334 || timer: 0.0932 sec.
iter 388300 || Loss: 0.7505 || timer: 0.0233 sec.
iter 388310 || Loss: 0.9611 || timer: 0.0908 sec.
iter 388320 || Loss: 0.9049 || timer: 0.0840 sec.
iter 388330 || Loss: 0.8413 || timer: 0.0828 sec.
iter 388340 || Loss: 0.8111 || timer: 0.0826 sec.
iter 388350 || Loss: 0.6142 || timer: 0.0890 sec.
iter 388360 || Loss: 0.5929 || timer: 0.1059 sec.
iter 388370 || Loss: 0.5904 || timer: 0.0827 sec.
iter 388380 || Loss: 0.4923 || timer: 0.0961 sec.
iter 388390 || Loss: 0.8151 || timer: 0.0892 sec.
iter 388400 || Loss: 0.9286 || timer: 0.0942 sec.
iter 388410 || Loss: 0.8541 || timer: 0.0891 sec.
iter 388420 || Loss: 0.4789 || timer: 0.0905 sec.
iter 388430 || Loss: 0.8358 || timer: 0.1055 sec.
iter 388440 || Loss: 0.6212 || timer: 0.1145 sec.
iter 388450 || Loss: 0.6694 || timer: 0.0914 sec.
iter 388460 || Loss: 0.5541 || timer: 0.0844 sec.
iter 388470 || Loss: 1.1261 || timer: 0.0898 sec.
iter 388480 || Loss: 0.5899 || timer: 0.0880 sec.
iter 388490 || Loss: 0.8936 || timer: 0.1081 sec.
iter 388500 || Loss: 0.5586 || timer: 0.0869 sec.
iter 388510 || Loss: 0.7702 || timer: 0.0845 sec.
iter 388520 || Loss: 0.6125 || timer: 0.0843 sec.
iter 388530 || Loss: 0.8636 || timer: 0.0915 sec.
iter 388540 || Loss: 0.6182 || timer: 0.0884 sec.
iter 388550 || Loss: 0.7941 || timer: 0.0915 sec.
iter 388560 || Loss: 0.7147 || timer: 0.0873 sec.
iter 388570 || Loss: 0.4643 || timer: 0.0838 sec.
iter 388580 || Loss: 0.8688 || timer: 0.0843 sec.
iter 388590 || Loss: 0.7646 || timer: 0.0835 sec.
iter 388600 || Loss: 0.8053 || timer: 0.0958 sec.
iter 388610 || Loss: 0.6200 || timer: 0.1047 sec.
iter 388620 || Loss: 0.7843 || timer: 0.1073 sec.
iter 388630 || Loss: 0.7301 || timer: 0.0233 sec.
iter 388640 || Loss: 0.7580 || timer: 0.0828 sec.
iter 388650 || Loss: 0.4451 || timer: 0.0920 sec.
iter 388660 || Loss: 0.6536 || timer: 0.0890 sec.
iter 388670 || Loss: 0.6484 || timer: 0.0913 sec.
iter 388680 || Loss: 0.5469 || timer: 0.0890 sec.
iter 388690 || Loss: 0.9071 || timer: 0.0842 sec.
iter 388700 || Loss: 0.8550 || timer: 0.1145 sec.
iter 388710 || Loss: 0.5945 || timer: 0.1005 sec.
iter 388720 || Loss: 0.5081 || timer: 0.0911 sec.
iter 388730 || Loss: 0.9711 || timer: 0.0981 sec.
iter 388740 || Loss: 0.7855 || timer: 0.0834 sec.
iter 388750 || Loss: 0.5480 || timer: 0.0894 sec.
iter 388760 || Loss: 0.6244 || timer: 0.0869 sec.
iter 388770 || Loss: 0.7916 || timer: 0.0898 sec.
iter 388780 || Loss: 0.4742 || timer: 0.0898 sec.
iter 388790 || Loss: 0.4998 || timer: 0.0875 sec.
iter 388800 || Loss: 0.6902 || timer: 0.0907 sec.
iter 388810 || Loss: 0.6347 || timer: 0.0831 sec.
iter 388820 || Loss: 0.9750 || timer: 0.0811 sec.
iter 388830 || Loss: 0.5461 || timer: 0.0829 sec.
iter 388840 || Loss: 0.5522 || timer: 0.0882 sec.
iter 388850 || Loss: 0.5596 || timer: 0.0902 sec.
iter 388860 || Loss: 0.6018 || timer: 0.0911 sec.
iter 388870 || Loss: 0.5893 || timer: 0.0904 sec.
iter 388880 || Loss: 1.0485 || timer: 0.0913 sec.
iter 388890 || Loss: 0.6911 || timer: 0.0921 sec.
iter 388900 || Loss: 0.8434 || timer: 0.0841 sec.
iter 388910 || Loss: 0.7687 || timer: 0.0939 sec.
iter 388920 || Loss: 0.9152 || timer: 0.0838 sec.
iter 388930 || Loss: 0.7324 || timer: 0.0921 sec.
iter 388940 || Loss: 0.9109 || timer: 0.0898 sec.
iter 388950 || Loss: 0.6408 || timer: 0.0839 sec.
iter 388960 || Loss: 0.8437 || timer: 0.0243 sec.
iter 388970 || Loss: 0.8636 || timer: 0.1216 sec.
iter 388980 || Loss: 0.8056 || timer: 0.0908 sec.
iter 388990 || Loss: 0.5538 || timer: 0.0924 sec.
iter 389000 || Loss: 0.5603 || timer: 0.0835 sec.
iter 389010 || Loss: 0.8986 || timer: 0.0855 sec.
iter 389020 || Loss: 0.6116 || timer: 0.1044 sec.
iter 389030 || Loss: 1.0777 || timer: 0.0894 sec.
iter 389040 || Loss: 0.7558 || timer: 0.0842 sec.
iter 389050 || Loss: 0.6784 || timer: 0.0857 sec.
iter 389060 || Loss: 0.6238 || timer: 0.1336 sec.
iter 389070 || Loss: 0.6002 || timer: 0.1148 sec.
iter 389080 || Loss: 0.8338 || timer: 0.0911 sec.
iter 389090 || Loss: 0.7445 || timer: 0.0910 sec.
iter 389100 || Loss: 0.7123 || timer: 0.0914 sec.
iter 389110 || Loss: 0.7272 || timer: 0.0877 sec.
iter 389120 || Loss: 0.6344 || timer: 0.0935 sec.
iter 389130 || Loss: 0.6564 || timer: 0.1061 sec.
iter 389140 || Loss: 0.8576 || timer: 0.0953 sec.
iter 389150 || Loss: 0.4914 || timer: 0.0875 sec.
iter 389160 || Loss: 0.6859 || timer: 0.0898 sec.
iter 389170 || Loss: 0.6546 || timer: 0.1124 sec.
iter 389180 || Loss: 0.6049 || timer: 0.0902 sec.
iter 389190 || Loss: 0.5546 || timer: 0.0902 sec.
iter 389200 || Loss: 0.8316 || timer: 0.0902 sec.
iter 389210 || Loss: 0.8260 || timer: 0.0874 sec.
iter 389220 || Loss: 0.5350 || timer: 0.1131 sec.
iter 389230 || Loss: 0.4493 || timer: 0.0829 sec.
iter 389240 || Loss: 0.5878 || timer: 0.0897 sec.
iter 389250 || Loss: 0.8809 || timer: 0.0850 sec.
iter 389260 || Loss: 0.6036 || timer: 0.0834 sec.
iter 389270 || Loss: 0.7355 || timer: 0.0892 sec.
iter 389280 || Loss: 0.8970 || timer: 0.0826 sec.
iter 389290 || Loss: 0.7055 || timer: 0.0210 sec.
iter 389300 || Loss: 0.6210 || timer: 0.0827 sec.
iter 389310 || Loss: 0.5520 || timer: 0.0930 sec.
iter 389320 || Loss: 0.5830 || timer: 0.0930 sec.
iter 389330 || Loss: 0.6601 || timer: 0.1167 sec.
iter 389340 || Loss: 0.8633 || timer: 0.0887 sec.
iter 389350 || Loss: 0.6621 || timer: 0.0864 sec.
iter 389360 || Loss: 0.7895 || timer: 0.0895 sec.
iter 389370 || Loss: 0.7238 || timer: 0.0838 sec.
iter 389380 || Loss: 0.6499 || timer: 0.0875 sec.
iter 389390 || Loss: 0.6295 || timer: 0.1000 sec.
iter 389400 || Loss: 0.6629 || timer: 0.0771 sec.
iter 389410 || Loss: 0.7233 || timer: 0.0860 sec.
iter 389420 || Loss: 0.7500 || timer: 0.0911 sec.
iter 389430 || Loss: 0.5852 || timer: 0.0869 sec.
iter 389440 || Loss: 0.8971 || timer: 0.0907 sec.
iter 389450 || Loss: 0.6141 || timer: 0.0907 sec.
iter 389460 || Loss: 0.7747 || timer: 0.1036 sec.
iter 389470 || Loss: 0.8879 || timer: 0.1141 sec.
iter 389480 || Loss: 0.3678 || timer: 0.1073 sec.
iter 389490 || Loss: 0.7223 || timer: 0.0893 sec.
iter 389500 || Loss: 1.6267 || timer: 0.0828 sec.
iter 389510 || Loss: 0.6745 || timer: 0.0906 sec.
iter 389520 || Loss: 0.9438 || timer: 0.0897 sec.
iter 389530 || Loss: 0.6601 || timer: 0.0894 sec.
iter 389540 || Loss: 0.7102 || timer: 0.1062 sec.
iter 389550 || Loss: 0.9043 || timer: 0.1035 sec.
iter 389560 || Loss: 0.7578 || timer: 0.0917 sec.
iter 389570 || Loss: 0.9335 || timer: 0.1025 sec.
iter 389580 || Loss: 0.7867 || timer: 0.1057 sec.
iter 389590 || Loss: 0.7399 || timer: 0.0767 sec.
iter 389600 || Loss: 0.5426 || timer: 0.0926 sec.
iter 389610 || Loss: 0.5045 || timer: 0.0906 sec.
iter 389620 || Loss: 0.6173 || timer: 0.0218 sec.
iter 389630 || Loss: 1.1708 || timer: 0.0850 sec.
iter 389640 || Loss: 0.5549 || timer: 0.1050 sec.
iter 389650 || Loss: 0.8242 || timer: 0.0900 sec.
iter 389660 || Loss: 0.9611 || timer: 0.0825 sec.
iter 389670 || Loss: 0.7640 || timer: 0.0910 sec.
iter 389680 || Loss: 0.6081 || timer: 0.0828 sec.
iter 389690 || Loss: 0.5222 || timer: 0.0860 sec.
iter 389700 || Loss: 0.5836 || timer: 0.0918 sec.
iter 389710 || Loss: 0.7287 || timer: 0.0835 sec.
iter 389720 || Loss: 0.7047 || timer: 0.1258 sec.
iter 389730 || Loss: 0.6467 || timer: 0.0897 sec.
iter 389740 || Loss: 0.8352 || timer: 0.0903 sec.
iter 389750 || Loss: 0.6295 || timer: 0.0886 sec.
iter 389760 || Loss: 0.6665 || timer: 0.0926 sec.
iter 389770 || Loss: 0.7426 || timer: 0.0897 sec.
iter 389780 || Loss: 0.7295 || timer: 0.0927 sec.
iter 389790 || Loss: 0.7884 || timer: 0.0867 sec.
iter 389800 || Loss: 0.5909 || timer: 0.1081 sec.
iter 389810 || Loss: 0.6127 || timer: 0.0893 sec.
iter 389820 || Loss: 0.6243 || timer: 0.0825 sec.
iter 389830 || Loss: 0.6897 || timer: 0.0838 sec.
iter 389840 || Loss: 0.6922 || timer: 0.0902 sec.
iter 389850 || Loss: 0.5194 || timer: 0.0897 sec.
iter 389860 || Loss: 0.7416 || timer: 0.0905 sec.
iter 389870 || Loss: 0.5762 || timer: 0.0839 sec.
iter 389880 || Loss: 0.6495 || timer: 0.0836 sec.
iter 389890 || Loss: 0.5702 || timer: 0.0911 sec.
iter 389900 || Loss: 0.6426 || timer: 0.0863 sec.
iter 389910 || Loss: 0.7038 || timer: 0.0840 sec.
iter 389920 || Loss: 0.5444 || timer: 0.1071 sec.
iter 389930 || Loss: 0.4187 || timer: 0.0913 sec.
iter 389940 || Loss: 0.5537 || timer: 0.0896 sec.
iter 389950 || Loss: 0.6153 || timer: 0.0211 sec.
iter 389960 || Loss: 0.8003 || timer: 0.0839 sec.
iter 389970 || Loss: 0.4956 || timer: 0.1073 sec.
iter 389980 || Loss: 0.5346 || timer: 0.0921 sec.
iter 389990 || Loss: 0.5205 || timer: 0.0968 sec.
iter 390000 || Loss: 0.7798 || Saving state, iter: 390000
timer: 0.0856 sec.
iter 390010 || Loss: 0.8382 || timer: 0.0916 sec.
iter 390020 || Loss: 0.9503 || timer: 0.0846 sec.
iter 390030 || Loss: 0.6337 || timer: 0.1213 sec.
iter 390040 || Loss: 0.6376 || timer: 0.0906 sec.
iter 390050 || Loss: 0.7926 || timer: 0.0974 sec.
iter 390060 || Loss: 0.6850 || timer: 0.0831 sec.
iter 390070 || Loss: 0.5990 || timer: 0.1034 sec.
iter 390080 || Loss: 0.5060 || timer: 0.0828 sec.
iter 390090 || Loss: 0.7347 || timer: 0.0931 sec.
iter 390100 || Loss: 0.8863 || timer: 0.0958 sec.
iter 390110 || Loss: 0.8064 || timer: 0.0840 sec.
iter 390120 || Loss: 0.5090 || timer: 0.0914 sec.
iter 390130 || Loss: 0.8584 || timer: 0.0898 sec.
iter 390140 || Loss: 0.6375 || timer: 0.0904 sec.
iter 390150 || Loss: 0.6914 || timer: 0.0967 sec.
iter 390160 || Loss: 0.4979 || timer: 0.0921 sec.
iter 390170 || Loss: 0.6638 || timer: 0.0904 sec.
iter 390180 || Loss: 0.5134 || timer: 0.0835 sec.
iter 390190 || Loss: 0.7714 || timer: 0.0879 sec.
iter 390200 || Loss: 0.5986 || timer: 0.0913 sec.
iter 390210 || Loss: 0.5787 || timer: 0.0909 sec.
iter 390220 || Loss: 0.5339 || timer: 0.0947 sec.
iter 390230 || Loss: 0.8836 || timer: 0.0834 sec.
iter 390240 || Loss: 0.6594 || timer: 0.0856 sec.
iter 390250 || Loss: 0.6721 || timer: 0.0824 sec.
iter 390260 || Loss: 0.6500 || timer: 0.0861 sec.
iter 390270 || Loss: 0.6865 || timer: 0.0836 sec.
iter 390280 || Loss: 0.6442 || timer: 0.0238 sec.
iter 390290 || Loss: 0.4777 || timer: 0.1032 sec.
iter 390300 || Loss: 0.9105 || timer: 0.0923 sec.
iter 390310 || Loss: 0.8747 || timer: 0.0943 sec.
iter 390320 || Loss: 0.5035 || timer: 0.0953 sec.
iter 390330 || Loss: 0.7299 || timer: 0.0891 sec.
iter 390340 || Loss: 0.7016 || timer: 0.0932 sec.
iter 390350 || Loss: 0.5952 || timer: 0.0828 sec.
iter 390360 || Loss: 0.7288 || timer: 0.0822 sec.
iter 390370 || Loss: 0.4483 || timer: 0.0874 sec.
iter 390380 || Loss: 0.4975 || timer: 0.1013 sec.
iter 390390 || Loss: 0.5691 || timer: 0.0814 sec.
iter 390400 || Loss: 0.5723 || timer: 0.0923 sec.
iter 390410 || Loss: 0.5465 || timer: 0.0988 sec.
iter 390420 || Loss: 0.5067 || timer: 0.1115 sec.
iter 390430 || Loss: 0.6215 || timer: 0.0920 sec.
iter 390440 || Loss: 0.7647 || timer: 0.0840 sec.
iter 390450 || Loss: 0.5995 || timer: 0.0928 sec.
iter 390460 || Loss: 0.6453 || timer: 0.0911 sec.
iter 390470 || Loss: 0.5512 || timer: 0.0892 sec.
iter 390480 || Loss: 0.5713 || timer: 0.0832 sec.
iter 390490 || Loss: 0.7371 || timer: 0.0829 sec.
iter 390500 || Loss: 0.7005 || timer: 0.0853 sec.
iter 390510 || Loss: 0.7592 || timer: 0.0835 sec.
iter 390520 || Loss: 0.6938 || timer: 0.0926 sec.
iter 390530 || Loss: 0.7444 || timer: 0.0839 sec.
iter 390540 || Loss: 0.7279 || timer: 0.0925 sec.
iter 390550 || Loss: 0.5458 || timer: 0.1127 sec.
iter 390560 || Loss: 0.5528 || timer: 0.0901 sec.
iter 390570 || Loss: 0.5055 || timer: 0.0824 sec.
iter 390580 || Loss: 0.6377 || timer: 0.0858 sec.
iter 390590 || Loss: 0.7197 || timer: 0.0906 sec.
iter 390600 || Loss: 0.6102 || timer: 0.1082 sec.
iter 390610 || Loss: 0.6754 || timer: 0.0208 sec.
iter 390620 || Loss: 0.2288 || timer: 0.0922 sec.
iter 390630 || Loss: 0.5469 || timer: 0.0924 sec.
iter 390640 || Loss: 0.8969 || timer: 0.0901 sec.
iter 390650 || Loss: 0.5833 || timer: 0.0912 sec.
iter 390660 || Loss: 0.6531 || timer: 0.0837 sec.
iter 390670 || Loss: 0.6678 || timer: 0.0878 sec.
iter 390680 || Loss: 0.5692 || timer: 0.0908 sec.
iter 390690 || Loss: 0.6299 || timer: 0.1037 sec.
iter 390700 || Loss: 0.7829 || timer: 0.0954 sec.
iter 390710 || Loss: 0.6460 || timer: 0.0935 sec.
iter 390720 || Loss: 0.5651 || timer: 0.0877 sec.
iter 390730 || Loss: 0.8986 || timer: 0.1061 sec.
iter 390740 || Loss: 0.7228 || timer: 0.0906 sec.
iter 390750 || Loss: 0.6410 || timer: 0.0864 sec.
iter 390760 || Loss: 0.7197 || timer: 0.0831 sec.
iter 390770 || Loss: 0.8110 || timer: 0.1083 sec.
iter 390780 || Loss: 0.6018 || timer: 0.1010 sec.
iter 390790 || Loss: 0.7331 || timer: 0.0834 sec.
iter 390800 || Loss: 0.6977 || timer: 0.0887 sec.
iter 390810 || Loss: 0.7522 || timer: 0.1013 sec.
iter 390820 || Loss: 0.8496 || timer: 0.0920 sec.
iter 390830 || Loss: 0.8863 || timer: 0.0840 sec.
iter 390840 || Loss: 0.7500 || timer: 0.0901 sec.
iter 390850 || Loss: 0.5977 || timer: 0.0919 sec.
iter 390860 || Loss: 0.5194 || timer: 0.0829 sec.
iter 390870 || Loss: 0.8053 || timer: 0.0967 sec.
iter 390880 || Loss: 0.5459 || timer: 0.0833 sec.
iter 390890 || Loss: 0.6140 || timer: 0.1054 sec.
iter 390900 || Loss: 0.9178 || timer: 0.0839 sec.
iter 390910 || Loss: 0.5061 || timer: 0.1165 sec.
iter 390920 || Loss: 0.8915 || timer: 0.0904 sec.
iter 390930 || Loss: 0.8049 || timer: 0.0956 sec.
iter 390940 || Loss: 0.5423 || timer: 0.0169 sec.
iter 390950 || Loss: 2.7452 || timer: 0.0909 sec.
iter 390960 || Loss: 0.6872 || timer: 0.0838 sec.
iter 390970 || Loss: 0.5290 || timer: 0.0840 sec.
iter 390980 || Loss: 0.4193 || timer: 0.0905 sec.
iter 390990 || Loss: 0.6467 || timer: 0.1040 sec.
iter 391000 || Loss: 1.1098 || timer: 0.0907 sec.
iter 391010 || Loss: 0.4668 || timer: 0.0908 sec.
iter 391020 || Loss: 0.4464 || timer: 0.0829 sec.
iter 391030 || Loss: 0.8029 || timer: 0.0921 sec.
iter 391040 || Loss: 0.6731 || timer: 0.1028 sec.
iter 391050 || Loss: 0.7262 || timer: 0.0896 sec.
iter 391060 || Loss: 0.8469 || timer: 0.0994 sec.
iter 391070 || Loss: 0.6049 || timer: 0.0831 sec.
iter 391080 || Loss: 0.7225 || timer: 0.0924 sec.
iter 391090 || Loss: 0.6602 || timer: 0.0890 sec.
iter 391100 || Loss: 0.7038 || timer: 0.0905 sec.
iter 391110 || Loss: 0.6221 || timer: 0.1023 sec.
iter 391120 || Loss: 0.5532 || timer: 0.0876 sec.
iter 391130 || Loss: 0.5832 || timer: 0.0841 sec.
iter 391140 || Loss: 0.5194 || timer: 0.0837 sec.
iter 391150 || Loss: 0.5831 || timer: 0.0845 sec.
iter 391160 || Loss: 0.7828 || timer: 0.0841 sec.
iter 391170 || Loss: 0.6802 || timer: 0.0917 sec.
iter 391180 || Loss: 0.5495 || timer: 0.0912 sec.
iter 391190 || Loss: 0.8265 || timer: 0.0834 sec.
iter 391200 || Loss: 0.7852 || timer: 0.1083 sec.
iter 391210 || Loss: 0.6253 || timer: 0.0841 sec.
iter 391220 || Loss: 0.5075 || timer: 0.0910 sec.
iter 391230 || Loss: 0.8100 || timer: 0.0837 sec.
iter 391240 || Loss: 0.5802 || timer: 0.0960 sec.
iter 391250 || Loss: 0.5981 || timer: 0.0924 sec.
iter 391260 || Loss: 0.5995 || timer: 0.0837 sec.
iter 391270 || Loss: 0.7529 || timer: 0.0237 sec.
iter 391280 || Loss: 1.8903 || timer: 0.0883 sec.
iter 391290 || Loss: 0.8616 || timer: 0.0898 sec.
iter 391300 || Loss: 0.5037 || timer: 0.0962 sec.
iter 391310 || Loss: 0.8998 || timer: 0.0840 sec.
iter 391320 || Loss: 0.7118 || timer: 0.0871 sec.
iter 391330 || Loss: 0.5609 || timer: 0.0923 sec.
iter 391340 || Loss: 0.4979 || timer: 0.1122 sec.
iter 391350 || Loss: 0.7600 || timer: 0.0860 sec.
iter 391360 || Loss: 0.5398 || timer: 0.0903 sec.
iter 391370 || Loss: 0.8069 || timer: 0.1196 sec.
iter 391380 || Loss: 0.6182 || timer: 0.0824 sec.
iter 391390 || Loss: 0.7046 || timer: 0.0909 sec.
iter 391400 || Loss: 0.5668 || timer: 0.0837 sec.
iter 391410 || Loss: 0.4187 || timer: 0.0866 sec.
iter 391420 || Loss: 0.5037 || timer: 0.0977 sec.
iter 391430 || Loss: 0.5056 || timer: 0.0889 sec.
iter 391440 || Loss: 0.6076 || timer: 0.1064 sec.
iter 391450 || Loss: 0.6242 || timer: 0.0897 sec.
iter 391460 || Loss: 0.6802 || timer: 0.0831 sec.
iter 391470 || Loss: 0.7398 || timer: 0.0828 sec.
iter 391480 || Loss: 0.4653 || timer: 0.0903 sec.
iter 391490 || Loss: 0.7251 || timer: 0.0902 sec.
iter 391500 || Loss: 0.5609 || timer: 0.0905 sec.
iter 391510 || Loss: 0.5191 || timer: 0.0885 sec.
iter 391520 || Loss: 0.6988 || timer: 0.0968 sec.
iter 391530 || Loss: 0.4687 || timer: 0.0901 sec.
iter 391540 || Loss: 0.5723 || timer: 0.0829 sec.
iter 391550 || Loss: 0.7745 || timer: 0.0900 sec.
iter 391560 || Loss: 0.7749 || timer: 0.0958 sec.
iter 391570 || Loss: 0.6755 || timer: 0.0912 sec.
iter 391580 || Loss: 0.6109 || timer: 0.1080 sec.
iter 391590 || Loss: 0.5735 || timer: 0.0950 sec.
iter 391600 || Loss: 0.5599 || timer: 0.0198 sec.
iter 391610 || Loss: 0.1252 || timer: 0.1018 sec.
iter 391620 || Loss: 0.5931 || timer: 0.0900 sec.
iter 391630 || Loss: 0.7782 || timer: 0.0881 sec.
iter 391640 || Loss: 0.7313 || timer: 0.1274 sec.
iter 391650 || Loss: 0.6323 || timer: 0.0952 sec.
iter 391660 || Loss: 0.6211 || timer: 0.0900 sec.
iter 391670 || Loss: 0.4347 || timer: 0.0903 sec.
iter 391680 || Loss: 0.3882 || timer: 0.0905 sec.
iter 391690 || Loss: 0.6024 || timer: 0.0920 sec.
iter 391700 || Loss: 0.7590 || timer: 0.1200 sec.
iter 391710 || Loss: 0.6736 || timer: 0.0831 sec.
iter 391720 || Loss: 0.9919 || timer: 0.0895 sec.
iter 391730 || Loss: 0.7804 || timer: 0.1052 sec.
iter 391740 || Loss: 0.7815 || timer: 0.0855 sec.
iter 391750 || Loss: 0.7016 || timer: 0.0902 sec.
iter 391760 || Loss: 0.8297 || timer: 0.0894 sec.
iter 391770 || Loss: 0.7477 || timer: 0.0912 sec.
iter 391780 || Loss: 0.6304 || timer: 0.0850 sec.
iter 391790 || Loss: 0.6888 || timer: 0.1071 sec.
iter 391800 || Loss: 0.6826 || timer: 0.0835 sec.
iter 391810 || Loss: 0.6337 || timer: 0.0900 sec.
iter 391820 || Loss: 0.9221 || timer: 0.0903 sec.
iter 391830 || Loss: 0.5529 || timer: 0.0948 sec.
iter 391840 || Loss: 0.5719 || timer: 0.1102 sec.
iter 391850 || Loss: 0.8531 || timer: 0.0837 sec.
iter 391860 || Loss: 0.7693 || timer: 0.0892 sec.
iter 391870 || Loss: 0.5368 || timer: 0.0894 sec.
iter 391880 || Loss: 0.8827 || timer: 0.0901 sec.
iter 391890 || Loss: 0.5069 || timer: 0.0927 sec.
iter 391900 || Loss: 0.7291 || timer: 0.0874 sec.
iter 391910 || Loss: 0.5620 || timer: 0.0916 sec.
iter 391920 || Loss: 0.6794 || timer: 0.1023 sec.
iter 391930 || Loss: 0.7423 || timer: 0.0203 sec.
iter 391940 || Loss: 0.2830 || timer: 0.0837 sec.
iter 391950 || Loss: 0.4646 || timer: 0.0846 sec.
iter 391960 || Loss: 0.6858 || timer: 0.0938 sec.
iter 391970 || Loss: 0.7861 || timer: 0.0937 sec.
iter 391980 || Loss: 1.0544 || timer: 0.0919 sec.
iter 391990 || Loss: 0.8608 || timer: 0.1020 sec.
iter 392000 || Loss: 0.6741 || timer: 0.0841 sec.
iter 392010 || Loss: 0.8162 || timer: 0.0827 sec.
iter 392020 || Loss: 0.6462 || timer: 0.0914 sec.
iter 392030 || Loss: 0.6113 || timer: 0.0985 sec.
iter 392040 || Loss: 0.6369 || timer: 0.0896 sec.
iter 392050 || Loss: 0.7599 || timer: 0.0892 sec.
iter 392060 || Loss: 0.4206 || timer: 0.0835 sec.
iter 392070 || Loss: 0.6818 || timer: 0.0852 sec.
iter 392080 || Loss: 0.5290 || timer: 0.0839 sec.
iter 392090 || Loss: 0.7811 || timer: 0.1050 sec.
iter 392100 || Loss: 0.5837 || timer: 0.1029 sec.
iter 392110 || Loss: 0.6684 || timer: 0.0914 sec.
iter 392120 || Loss: 0.5206 || timer: 0.0877 sec.
iter 392130 || Loss: 0.6515 || timer: 0.0966 sec.
iter 392140 || Loss: 0.6767 || timer: 0.0843 sec.
iter 392150 || Loss: 0.7820 || timer: 0.0907 sec.
iter 392160 || Loss: 0.6972 || timer: 0.0919 sec.
iter 392170 || Loss: 0.5815 || timer: 0.0905 sec.
iter 392180 || Loss: 0.7254 || timer: 0.1019 sec.
iter 392190 || Loss: 0.7663 || timer: 0.0830 sec.
iter 392200 || Loss: 0.6360 || timer: 0.0915 sec.
iter 392210 || Loss: 0.8338 || timer: 0.0910 sec.
iter 392220 || Loss: 0.7020 || timer: 0.0899 sec.
iter 392230 || Loss: 0.6841 || timer: 0.0895 sec.
iter 392240 || Loss: 0.8091 || timer: 0.0899 sec.
iter 392250 || Loss: 0.8367 || timer: 0.0839 sec.
iter 392260 || Loss: 0.7133 || timer: 0.0275 sec.
iter 392270 || Loss: 1.8788 || timer: 0.0842 sec.
iter 392280 || Loss: 0.7861 || timer: 0.1022 sec.
iter 392290 || Loss: 0.6693 || timer: 0.0890 sec.
iter 392300 || Loss: 0.5550 || timer: 0.0945 sec.
iter 392310 || Loss: 0.9885 || timer: 0.0893 sec.
iter 392320 || Loss: 0.6629 || timer: 0.0883 sec.
iter 392330 || Loss: 0.5874 || timer: 0.1061 sec.
iter 392340 || Loss: 0.5021 || timer: 0.0895 sec.
iter 392350 || Loss: 0.7375 || timer: 0.0858 sec.
iter 392360 || Loss: 0.6452 || timer: 0.1065 sec.
iter 392370 || Loss: 0.8273 || timer: 0.0907 sec.
iter 392380 || Loss: 0.5224 || timer: 0.0987 sec.
iter 392390 || Loss: 0.5649 || timer: 0.0890 sec.
iter 392400 || Loss: 0.6337 || timer: 0.0883 sec.
iter 392410 || Loss: 0.7229 || timer: 0.1045 sec.
iter 392420 || Loss: 0.6788 || timer: 0.0906 sec.
iter 392430 || Loss: 0.6291 || timer: 0.1001 sec.
iter 392440 || Loss: 0.6673 || timer: 0.1036 sec.
iter 392450 || Loss: 0.4119 || timer: 0.0893 sec.
iter 392460 || Loss: 0.9317 || timer: 0.0810 sec.
iter 392470 || Loss: 1.2994 || timer: 0.1026 sec.
iter 392480 || Loss: 0.5120 || timer: 0.0806 sec.
iter 392490 || Loss: 0.5410 || timer: 0.0916 sec.
iter 392500 || Loss: 0.5819 || timer: 0.0930 sec.
iter 392510 || Loss: 0.7153 || timer: 0.0904 sec.
iter 392520 || Loss: 0.5234 || timer: 0.1024 sec.
iter 392530 || Loss: 0.5332 || timer: 0.0810 sec.
iter 392540 || Loss: 0.8069 || timer: 0.0883 sec.
iter 392550 || Loss: 0.4618 || timer: 0.0825 sec.
iter 392560 || Loss: 0.8724 || timer: 0.0895 sec.
iter 392570 || Loss: 0.6304 || timer: 0.0816 sec.
iter 392580 || Loss: 0.6982 || timer: 0.0901 sec.
iter 392590 || Loss: 0.5445 || timer: 0.0250 sec.
iter 392600 || Loss: 0.6227 || timer: 0.0813 sec.
iter 392610 || Loss: 0.7477 || timer: 0.0873 sec.
iter 392620 || Loss: 0.9906 || timer: 0.0792 sec.
iter 392630 || Loss: 0.7123 || timer: 0.0882 sec.
iter 392640 || Loss: 0.7183 || timer: 0.1041 sec.
iter 392650 || Loss: 0.6219 || timer: 0.0996 sec.
iter 392660 || Loss: 0.8244 || timer: 0.1369 sec.
iter 392670 || Loss: 0.8255 || timer: 0.0898 sec.
iter 392680 || Loss: 0.6539 || timer: 0.1022 sec.
iter 392690 || Loss: 0.6419 || timer: 0.0970 sec.
iter 392700 || Loss: 0.6256 || timer: 0.0819 sec.
iter 392710 || Loss: 0.5870 || timer: 0.0892 sec.
iter 392720 || Loss: 0.6911 || timer: 0.0920 sec.
iter 392730 || Loss: 0.5564 || timer: 0.1001 sec.
iter 392740 || Loss: 0.7496 || timer: 0.1374 sec.
iter 392750 || Loss: 0.6550 || timer: 0.0838 sec.
iter 392760 || Loss: 0.6458 || timer: 0.0896 sec.
iter 392770 || Loss: 0.4592 || timer: 0.0813 sec.
iter 392780 || Loss: 0.5805 || timer: 0.0967 sec.
iter 392790 || Loss: 0.7125 || timer: 0.0917 sec.
iter 392800 || Loss: 0.5489 || timer: 0.1014 sec.
iter 392810 || Loss: 0.6396 || timer: 0.1011 sec.
iter 392820 || Loss: 0.6032 || timer: 0.0824 sec.
iter 392830 || Loss: 0.6380 || timer: 0.0883 sec.
iter 392840 || Loss: 0.9065 || timer: 0.0835 sec.
iter 392850 || Loss: 0.5567 || timer: 0.1019 sec.
iter 392860 || Loss: 0.7364 || timer: 0.0825 sec.
iter 392870 || Loss: 0.7749 || timer: 0.0826 sec.
iter 392880 || Loss: 0.6629 || timer: 0.0810 sec.
iter 392890 || Loss: 0.8331 || timer: 0.0828 sec.
iter 392900 || Loss: 1.0600 || timer: 0.1008 sec.
iter 392910 || Loss: 0.6050 || timer: 0.0838 sec.
iter 392920 || Loss: 0.8287 || timer: 0.0197 sec.
iter 392930 || Loss: 1.0811 || timer: 0.0859 sec.
iter 392940 || Loss: 0.6525 || timer: 0.0904 sec.
iter 392950 || Loss: 0.6072 || timer: 0.0915 sec.
iter 392960 || Loss: 0.7452 || timer: 0.0862 sec.
iter 392970 || Loss: 0.6641 || timer: 0.1064 sec.
iter 392980 || Loss: 0.8420 || timer: 0.0873 sec.
iter 392990 || Loss: 0.6154 || timer: 0.0855 sec.
iter 393000 || Loss: 0.7247 || timer: 0.0838 sec.
iter 393010 || Loss: 0.7479 || timer: 0.0833 sec.
iter 393020 || Loss: 0.7155 || timer: 0.0994 sec.
iter 393030 || Loss: 0.4677 || timer: 0.0921 sec.
iter 393040 || Loss: 0.5185 || timer: 0.0977 sec.
iter 393050 || Loss: 0.7707 || timer: 0.1064 sec.
iter 393060 || Loss: 0.5512 || timer: 0.0803 sec.
iter 393070 || Loss: 0.5821 || timer: 0.0906 sec.
iter 393080 || Loss: 0.6322 || timer: 0.0963 sec.
iter 393090 || Loss: 0.7892 || timer: 0.0899 sec.
iter 393100 || Loss: 0.5023 || timer: 0.0961 sec.
iter 393110 || Loss: 0.5900 || timer: 0.0993 sec.
iter 393120 || Loss: 0.7452 || timer: 0.0884 sec.
iter 393130 || Loss: 0.6014 || timer: 0.0825 sec.
iter 393140 || Loss: 0.7002 || timer: 0.0884 sec.
iter 393150 || Loss: 0.6841 || timer: 0.0875 sec.
iter 393160 || Loss: 0.8217 || timer: 0.0913 sec.
iter 393170 || Loss: 0.5834 || timer: 0.0901 sec.
iter 393180 || Loss: 0.7822 || timer: 0.0886 sec.
iter 393190 || Loss: 0.6829 || timer: 0.1301 sec.
iter 393200 || Loss: 0.6143 || timer: 0.0948 sec.
iter 393210 || Loss: 0.6717 || timer: 0.0910 sec.
iter 393220 || Loss: 0.7162 || timer: 0.1112 sec.
iter 393230 || Loss: 0.4633 || timer: 0.0864 sec.
iter 393240 || Loss: 0.4975 || timer: 0.0900 sec.
iter 393250 || Loss: 0.6084 || timer: 0.0196 sec.
iter 393260 || Loss: 1.0242 || timer: 0.0912 sec.
iter 393270 || Loss: 0.5497 || timer: 0.0882 sec.
iter 393280 || Loss: 0.6063 || timer: 0.0884 sec.
iter 393290 || Loss: 0.5518 || timer: 0.0834 sec.
iter 393300 || Loss: 0.5317 || timer: 0.0882 sec.
iter 393310 || Loss: 0.6698 || timer: 0.0968 sec.
iter 393320 || Loss: 0.6503 || timer: 0.0835 sec.
iter 393330 || Loss: 0.8022 || timer: 0.0826 sec.
iter 393340 || Loss: 0.4353 || timer: 0.0826 sec.
iter 393350 || Loss: 0.4845 || timer: 0.1069 sec.
iter 393360 || Loss: 0.8521 || timer: 0.1005 sec.
iter 393370 || Loss: 0.6188 || timer: 0.0885 sec.
iter 393380 || Loss: 0.7473 || timer: 0.0884 sec.
iter 393390 || Loss: 0.8183 || timer: 0.0836 sec.
iter 393400 || Loss: 0.6412 || timer: 0.1144 sec.
iter 393410 || Loss: 0.7253 || timer: 0.0992 sec.
iter 393420 || Loss: 0.6101 || timer: 0.0829 sec.
iter 393430 || Loss: 0.6072 || timer: 0.0960 sec.
iter 393440 || Loss: 0.7591 || timer: 0.0969 sec.
iter 393450 || Loss: 0.8997 || timer: 0.0973 sec.
iter 393460 || Loss: 0.6914 || timer: 0.0909 sec.
iter 393470 || Loss: 0.8059 || timer: 0.0903 sec.
iter 393480 || Loss: 0.7020 || timer: 0.0920 sec.
iter 393490 || Loss: 0.3070 || timer: 0.0902 sec.
iter 393500 || Loss: 0.7151 || timer: 0.0873 sec.
iter 393510 || Loss: 0.7151 || timer: 0.0867 sec.
iter 393520 || Loss: 0.6346 || timer: 0.0827 sec.
iter 393530 || Loss: 0.6158 || timer: 0.1251 sec.
iter 393540 || Loss: 0.5946 || timer: 0.1048 sec.
iter 393550 || Loss: 0.5955 || timer: 0.0864 sec.
iter 393560 || Loss: 0.6681 || timer: 0.0926 sec.
iter 393570 || Loss: 0.8909 || timer: 0.0899 sec.
iter 393580 || Loss: 0.5738 || timer: 0.0197 sec.
iter 393590 || Loss: 2.2067 || timer: 0.0933 sec.
iter 393600 || Loss: 0.9014 || timer: 0.0784 sec.
iter 393610 || Loss: 0.4599 || timer: 0.0758 sec.
iter 393620 || Loss: 0.7056 || timer: 0.0827 sec.
iter 393630 || Loss: 0.7541 || timer: 0.0826 sec.
iter 393640 || Loss: 0.5845 || timer: 0.1091 sec.
iter 393650 || Loss: 0.8433 || timer: 0.0931 sec.
iter 393660 || Loss: 0.5481 || timer: 0.0828 sec.
iter 393670 || Loss: 0.4594 || timer: 0.0831 sec.
iter 393680 || Loss: 0.4520 || timer: 0.0956 sec.
iter 393690 || Loss: 0.8030 || timer: 0.0892 sec.
iter 393700 || Loss: 0.8692 || timer: 0.0950 sec.
iter 393710 || Loss: 0.5879 || timer: 0.0927 sec.
iter 393720 || Loss: 0.6384 || timer: 0.0835 sec.
iter 393730 || Loss: 0.4757 || timer: 0.0887 sec.
iter 393740 || Loss: 0.7166 || timer: 0.0883 sec.
iter 393750 || Loss: 0.5682 || timer: 0.0833 sec.
iter 393760 || Loss: 0.7872 || timer: 0.1035 sec.
iter 393770 || Loss: 0.8590 || timer: 0.0901 sec.
iter 393780 || Loss: 0.6071 || timer: 0.0917 sec.
iter 393790 || Loss: 0.9070 || timer: 0.1198 sec.
iter 393800 || Loss: 0.6070 || timer: 0.1091 sec.
iter 393810 || Loss: 0.5867 || timer: 0.0832 sec.
iter 393820 || Loss: 0.6923 || timer: 0.0912 sec.
iter 393830 || Loss: 0.5545 || timer: 0.0838 sec.
iter 393840 || Loss: 0.8006 || timer: 0.0964 sec.
iter 393850 || Loss: 0.6845 || timer: 0.0894 sec.
iter 393860 || Loss: 0.5548 || timer: 0.0894 sec.
iter 393870 || Loss: 0.7190 || timer: 0.0906 sec.
iter 393880 || Loss: 0.8315 || timer: 0.0904 sec.
iter 393890 || Loss: 0.6429 || timer: 0.1065 sec.
iter 393900 || Loss: 0.8455 || timer: 0.0918 sec.
iter 393910 || Loss: 0.5552 || timer: 0.0283 sec.
iter 393920 || Loss: 1.6945 || timer: 0.0831 sec.
iter 393930 || Loss: 0.6127 || timer: 0.0825 sec.
iter 393940 || Loss: 0.8796 || timer: 0.0834 sec.
iter 393950 || Loss: 0.5693 || timer: 0.0825 sec.
iter 393960 || Loss: 0.6479 || timer: 0.0910 sec.
iter 393970 || Loss: 0.5514 || timer: 0.0891 sec.
iter 393980 || Loss: 0.5304 || timer: 0.1018 sec.
iter 393990 || Loss: 0.6833 || timer: 0.0928 sec.
iter 394000 || Loss: 0.6680 || timer: 0.0902 sec.
iter 394010 || Loss: 0.6616 || timer: 0.1006 sec.
iter 394020 || Loss: 0.6177 || timer: 0.0928 sec.
iter 394030 || Loss: 0.4267 || timer: 0.0960 sec.
iter 394040 || Loss: 0.6346 || timer: 0.0916 sec.
iter 394050 || Loss: 0.8282 || timer: 0.0832 sec.
iter 394060 || Loss: 0.6832 || timer: 0.0917 sec.
iter 394070 || Loss: 0.7211 || timer: 0.0876 sec.
iter 394080 || Loss: 0.8331 || timer: 0.0954 sec.
iter 394090 || Loss: 0.4952 || timer: 0.0827 sec.
iter 394100 || Loss: 0.7969 || timer: 0.1035 sec.
iter 394110 || Loss: 0.5924 || timer: 0.0827 sec.
iter 394120 || Loss: 0.5622 || timer: 0.0867 sec.
iter 394130 || Loss: 0.6342 || timer: 0.1026 sec.
iter 394140 || Loss: 0.6183 || timer: 0.0844 sec.
iter 394150 || Loss: 0.6393 || timer: 0.0871 sec.
iter 394160 || Loss: 0.5808 || timer: 0.0827 sec.
iter 394170 || Loss: 0.8310 || timer: 0.0898 sec.
iter 394180 || Loss: 0.5478 || timer: 0.0917 sec.
iter 394190 || Loss: 0.6544 || timer: 0.0875 sec.
iter 394200 || Loss: 0.6782 || timer: 0.0919 sec.
iter 394210 || Loss: 0.7327 || timer: 0.0916 sec.
iter 394220 || Loss: 0.3996 || timer: 0.0908 sec.
iter 394230 || Loss: 0.6539 || timer: 0.0978 sec.
iter 394240 || Loss: 0.6230 || timer: 0.0227 sec.
iter 394250 || Loss: 0.2138 || timer: 0.0828 sec.
iter 394260 || Loss: 0.5049 || timer: 0.0904 sec.
iter 394270 || Loss: 0.7777 || timer: 0.0825 sec.
iter 394280 || Loss: 0.5366 || timer: 0.0891 sec.
iter 394290 || Loss: 0.7588 || timer: 0.0923 sec.
iter 394300 || Loss: 0.5125 || timer: 0.0910 sec.
iter 394310 || Loss: 0.9268 || timer: 0.0905 sec.
iter 394320 || Loss: 0.6915 || timer: 0.0915 sec.
iter 394330 || Loss: 0.6276 || timer: 0.1013 sec.
iter 394340 || Loss: 0.6244 || timer: 0.0986 sec.
iter 394350 || Loss: 0.4643 || timer: 0.0930 sec.
iter 394360 || Loss: 0.7079 || timer: 0.0903 sec.
iter 394370 || Loss: 0.7208 || timer: 0.0924 sec.
iter 394380 || Loss: 0.6555 || timer: 0.1105 sec.
iter 394390 || Loss: 0.4941 || timer: 0.0891 sec.
iter 394400 || Loss: 0.6461 || timer: 0.1064 sec.
iter 394410 || Loss: 0.8226 || timer: 0.1036 sec.
iter 394420 || Loss: 0.7044 || timer: 0.0822 sec.
iter 394430 || Loss: 0.8938 || timer: 0.1292 sec.
iter 394440 || Loss: 0.6892 || timer: 0.0841 sec.
iter 394450 || Loss: 0.6472 || timer: 0.0897 sec.
iter 394460 || Loss: 0.7403 || timer: 0.0905 sec.
iter 394470 || Loss: 0.5096 || timer: 0.0907 sec.
iter 394480 || Loss: 0.7005 || timer: 0.0886 sec.
iter 394490 || Loss: 0.5664 || timer: 0.0995 sec.
iter 394500 || Loss: 0.4642 || timer: 0.0999 sec.
iter 394510 || Loss: 0.6227 || timer: 0.0982 sec.
iter 394520 || Loss: 0.9184 || timer: 0.0920 sec.
iter 394530 || Loss: 0.7288 || timer: 0.0937 sec.
iter 394540 || Loss: 0.6901 || timer: 0.0896 sec.
iter 394550 || Loss: 0.7883 || timer: 0.0902 sec.
iter 394560 || Loss: 0.4229 || timer: 0.0792 sec.
iter 394570 || Loss: 0.7810 || timer: 0.0194 sec.
iter 394580 || Loss: 1.0634 || timer: 0.0904 sec.
iter 394590 || Loss: 0.4881 || timer: 0.0937 sec.
iter 394600 || Loss: 0.6359 || timer: 0.1057 sec.
iter 394610 || Loss: 0.7000 || timer: 0.0906 sec.
iter 394620 || Loss: 1.1266 || timer: 0.1065 sec.
iter 394630 || Loss: 0.5714 || timer: 0.1099 sec.
iter 394640 || Loss: 0.5837 || timer: 0.0933 sec.
iter 394650 || Loss: 0.7618 || timer: 0.0904 sec.
iter 394660 || Loss: 0.6259 || timer: 0.1133 sec.
iter 394670 || Loss: 0.8034 || timer: 0.1310 sec.
iter 394680 || Loss: 0.6824 || timer: 0.0885 sec.
iter 394690 || Loss: 0.6532 || timer: 0.1066 sec.
iter 394700 || Loss: 0.7650 || timer: 0.0916 sec.
iter 394710 || Loss: 0.5563 || timer: 0.1080 sec.
iter 394720 || Loss: 0.6159 || timer: 0.0861 sec.
iter 394730 || Loss: 0.6744 || timer: 0.1103 sec.
iter 394740 || Loss: 0.5451 || timer: 0.0917 sec.
iter 394750 || Loss: 0.5812 || timer: 0.0892 sec.
iter 394760 || Loss: 0.6364 || timer: 0.0898 sec.
iter 394770 || Loss: 0.6566 || timer: 0.0984 sec.
iter 394780 || Loss: 0.7486 || timer: 0.1049 sec.
iter 394790 || Loss: 0.8298 || timer: 0.0899 sec.
iter 394800 || Loss: 0.8800 || timer: 0.0839 sec.
iter 394810 || Loss: 0.6170 || timer: 0.0947 sec.
iter 394820 || Loss: 0.6728 || timer: 0.0827 sec.
iter 394830 || Loss: 0.7002 || timer: 0.0832 sec.
iter 394840 || Loss: 0.5442 || timer: 0.0980 sec.
iter 394850 || Loss: 0.6160 || timer: 0.0845 sec.
iter 394860 || Loss: 0.5975 || timer: 0.0969 sec.
iter 394870 || Loss: 1.0603 || timer: 0.0916 sec.
iter 394880 || Loss: 0.6963 || timer: 0.0906 sec.
iter 394890 || Loss: 0.6490 || timer: 0.0904 sec.
iter 394900 || Loss: 0.6525 || timer: 0.0255 sec.
iter 394910 || Loss: 0.5096 || timer: 0.0983 sec.
iter 394920 || Loss: 0.6698 || timer: 0.1058 sec.
iter 394930 || Loss: 0.9865 || timer: 0.0894 sec.
iter 394940 || Loss: 0.6396 || timer: 0.0825 sec.
iter 394950 || Loss: 0.5739 || timer: 0.0845 sec.
iter 394960 || Loss: 0.6370 || timer: 0.1073 sec.
iter 394970 || Loss: 0.9447 || timer: 0.0834 sec.
iter 394980 || Loss: 0.5857 || timer: 0.1004 sec.
iter 394990 || Loss: 0.6920 || timer: 0.0903 sec.
iter 395000 || Loss: 0.8501 || Saving state, iter: 395000
timer: 0.1124 sec.
iter 395010 || Loss: 0.7062 || timer: 0.0829 sec.
iter 395020 || Loss: 0.7583 || timer: 0.0824 sec.
iter 395030 || Loss: 0.7585 || timer: 0.0826 sec.
iter 395040 || Loss: 0.6073 || timer: 0.0832 sec.
iter 395050 || Loss: 0.6568 || timer: 0.0830 sec.
iter 395060 || Loss: 0.6270 || timer: 0.0835 sec.
iter 395070 || Loss: 0.5014 || timer: 0.1088 sec.
iter 395080 || Loss: 0.6887 || timer: 0.0904 sec.
iter 395090 || Loss: 0.5878 || timer: 0.0968 sec.
iter 395100 || Loss: 0.6699 || timer: 0.0897 sec.
iter 395110 || Loss: 0.6149 || timer: 0.0898 sec.
iter 395120 || Loss: 0.7067 || timer: 0.0861 sec.
iter 395130 || Loss: 0.8462 || timer: 0.0922 sec.
iter 395140 || Loss: 0.7033 || timer: 0.0911 sec.
iter 395150 || Loss: 0.5256 || timer: 0.0835 sec.
iter 395160 || Loss: 0.6044 || timer: 0.0907 sec.
iter 395170 || Loss: 0.7540 || timer: 0.0904 sec.
iter 395180 || Loss: 0.8048 || timer: 0.0992 sec.
iter 395190 || Loss: 0.6960 || timer: 0.1072 sec.
iter 395200 || Loss: 0.6011 || timer: 0.0840 sec.
iter 395210 || Loss: 0.6691 || timer: 0.0875 sec.
iter 395220 || Loss: 0.8992 || timer: 0.0863 sec.
iter 395230 || Loss: 0.7160 || timer: 0.0255 sec.
iter 395240 || Loss: 0.8754 || timer: 0.0829 sec.
iter 395250 || Loss: 0.5141 || timer: 0.0830 sec.
iter 395260 || Loss: 0.8652 || timer: 0.0900 sec.
iter 395270 || Loss: 0.6147 || timer: 0.0906 sec.
iter 395280 || Loss: 0.6793 || timer: 0.0998 sec.
iter 395290 || Loss: 0.5756 || timer: 0.1099 sec.
iter 395300 || Loss: 0.5838 || timer: 0.0824 sec.
iter 395310 || Loss: 0.8363 || timer: 0.1125 sec.
iter 395320 || Loss: 0.6727 || timer: 0.0855 sec.
iter 395330 || Loss: 0.7954 || timer: 0.1126 sec.
iter 395340 || Loss: 0.7567 || timer: 0.0906 sec.
iter 395350 || Loss: 1.1254 || timer: 0.0887 sec.
iter 395360 || Loss: 0.6237 || timer: 0.0913 sec.
iter 395370 || Loss: 0.6843 || timer: 0.0913 sec.
iter 395380 || Loss: 0.5498 || timer: 0.1046 sec.
iter 395390 || Loss: 0.6057 || timer: 0.0893 sec.
iter 395400 || Loss: 0.4486 || timer: 0.1013 sec.
iter 395410 || Loss: 0.5862 || timer: 0.0881 sec.
iter 395420 || Loss: 0.7314 || timer: 0.1027 sec.
iter 395430 || Loss: 0.5497 || timer: 0.0840 sec.
iter 395440 || Loss: 0.7092 || timer: 0.0890 sec.
iter 395450 || Loss: 0.7298 || timer: 0.0836 sec.
iter 395460 || Loss: 0.6148 || timer: 0.0860 sec.
iter 395470 || Loss: 0.5320 || timer: 0.0908 sec.
iter 395480 || Loss: 0.7881 || timer: 0.0877 sec.
iter 395490 || Loss: 0.5689 || timer: 0.0999 sec.
iter 395500 || Loss: 0.8326 || timer: 0.0831 sec.
iter 395510 || Loss: 0.5768 || timer: 0.0914 sec.
iter 395520 || Loss: 0.5406 || timer: 0.0919 sec.
iter 395530 || Loss: 0.8844 || timer: 0.0897 sec.
iter 395540 || Loss: 0.5058 || timer: 0.1044 sec.
iter 395550 || Loss: 0.7802 || timer: 0.0909 sec.
iter 395560 || Loss: 0.5315 || timer: 0.0212 sec.
iter 395570 || Loss: 0.2296 || timer: 0.0829 sec.
iter 395580 || Loss: 0.6656 || timer: 0.0832 sec.
iter 395590 || Loss: 0.4644 || timer: 0.0902 sec.
iter 395600 || Loss: 0.6582 || timer: 0.0895 sec.
iter 395610 || Loss: 0.6207 || timer: 0.0840 sec.
iter 395620 || Loss: 0.7189 || timer: 0.0874 sec.
iter 395630 || Loss: 0.7297 || timer: 0.0893 sec.
iter 395640 || Loss: 0.7576 || timer: 0.0877 sec.
iter 395650 || Loss: 0.9592 || timer: 0.0891 sec.
iter 395660 || Loss: 0.6715 || timer: 0.0949 sec.
iter 395670 || Loss: 0.9167 || timer: 0.1003 sec.
iter 395680 || Loss: 0.9559 || timer: 0.0937 sec.
iter 395690 || Loss: 0.5608 || timer: 0.0834 sec.
iter 395700 || Loss: 0.7717 || timer: 0.0996 sec.
iter 395710 || Loss: 0.6416 || timer: 0.0915 sec.
iter 395720 || Loss: 0.6592 || timer: 0.0957 sec.
iter 395730 || Loss: 0.6801 || timer: 0.0817 sec.
iter 395740 || Loss: 0.8061 || timer: 0.0901 sec.
iter 395750 || Loss: 0.9286 || timer: 0.0935 sec.
iter 395760 || Loss: 0.5460 || timer: 0.0834 sec.
iter 395770 || Loss: 0.5865 || timer: 0.0877 sec.
iter 395780 || Loss: 0.8678 || timer: 0.0838 sec.
iter 395790 || Loss: 0.7783 || timer: 0.0832 sec.
iter 395800 || Loss: 0.6210 || timer: 0.1180 sec.
iter 395810 || Loss: 0.7354 || timer: 0.0878 sec.
iter 395820 || Loss: 0.7188 || timer: 0.0922 sec.
iter 395830 || Loss: 0.5431 || timer: 0.0827 sec.
iter 395840 || Loss: 0.7167 || timer: 0.0906 sec.
iter 395850 || Loss: 0.8112 || timer: 0.0961 sec.
iter 395860 || Loss: 0.5900 || timer: 0.0920 sec.
iter 395870 || Loss: 0.5350 || timer: 0.0922 sec.
iter 395880 || Loss: 0.8735 || timer: 0.1036 sec.
iter 395890 || Loss: 0.7364 || timer: 0.0172 sec.
iter 395900 || Loss: 2.3984 || timer: 0.0840 sec.
iter 395910 || Loss: 0.6071 || timer: 0.0901 sec.
iter 395920 || Loss: 0.5504 || timer: 0.0941 sec.
iter 395930 || Loss: 0.7299 || timer: 0.0891 sec.
iter 395940 || Loss: 0.6253 || timer: 0.0914 sec.
iter 395950 || Loss: 0.6316 || timer: 0.0921 sec.
iter 395960 || Loss: 0.5627 || timer: 0.0880 sec.
iter 395970 || Loss: 0.4329 || timer: 0.1006 sec.
iter 395980 || Loss: 0.4629 || timer: 0.0984 sec.
iter 395990 || Loss: 0.3112 || timer: 0.1125 sec.
iter 396000 || Loss: 0.5126 || timer: 0.0903 sec.
iter 396010 || Loss: 0.8222 || timer: 0.1035 sec.
iter 396020 || Loss: 0.7630 || timer: 0.0884 sec.
iter 396030 || Loss: 0.4859 || timer: 0.1161 sec.
iter 396040 || Loss: 0.7996 || timer: 0.0823 sec.
iter 396050 || Loss: 0.7642 || timer: 0.0953 sec.
iter 396060 || Loss: 0.5081 || timer: 0.0885 sec.
iter 396070 || Loss: 0.7316 || timer: 0.1014 sec.
iter 396080 || Loss: 0.7654 || timer: 0.0835 sec.
iter 396090 || Loss: 0.7753 || timer: 0.0828 sec.
iter 396100 || Loss: 0.6262 || timer: 0.0834 sec.
iter 396110 || Loss: 0.7691 || timer: 0.0869 sec.
iter 396120 || Loss: 0.7401 || timer: 0.0888 sec.
iter 396130 || Loss: 0.7098 || timer: 0.0821 sec.
iter 396140 || Loss: 0.8231 || timer: 0.0825 sec.
iter 396150 || Loss: 0.7204 || timer: 0.0893 sec.
iter 396160 || Loss: 0.4504 || timer: 0.0908 sec.
iter 396170 || Loss: 0.6458 || timer: 0.1021 sec.
iter 396180 || Loss: 0.5832 || timer: 0.0876 sec.
iter 396190 || Loss: 0.7393 || timer: 0.0898 sec.
iter 396200 || Loss: 0.7857 || timer: 0.0876 sec.
iter 396210 || Loss: 0.6435 || timer: 0.0867 sec.
iter 396220 || Loss: 0.6979 || timer: 0.0168 sec.
iter 396230 || Loss: 0.1451 || timer: 0.0895 sec.
iter 396240 || Loss: 0.7318 || timer: 0.1003 sec.
iter 396250 || Loss: 0.7267 || timer: 0.0899 sec.
iter 396260 || Loss: 0.6752 || timer: 0.0836 sec.
iter 396270 || Loss: 0.5130 || timer: 0.0839 sec.
iter 396280 || Loss: 0.5182 || timer: 0.1115 sec.
iter 396290 || Loss: 0.4012 || timer: 0.0890 sec.
iter 396300 || Loss: 0.9561 || timer: 0.0997 sec.
iter 396310 || Loss: 0.8802 || timer: 0.1012 sec.
iter 396320 || Loss: 0.5190 || timer: 0.0960 sec.
iter 396330 || Loss: 0.4908 || timer: 0.1037 sec.
iter 396340 || Loss: 0.5992 || timer: 0.0898 sec.
iter 396350 || Loss: 0.5039 || timer: 0.0969 sec.
iter 396360 || Loss: 0.7334 || timer: 0.1056 sec.
iter 396370 || Loss: 0.5691 || timer: 0.0830 sec.
iter 396380 || Loss: 0.9342 || timer: 0.1193 sec.
iter 396390 || Loss: 0.7977 || timer: 0.0887 sec.
iter 396400 || Loss: 0.4219 || timer: 0.0900 sec.
iter 396410 || Loss: 0.4990 || timer: 0.1024 sec.
iter 396420 || Loss: 0.7562 || timer: 0.1071 sec.
iter 396430 || Loss: 0.6967 || timer: 0.0831 sec.
iter 396440 || Loss: 0.9252 || timer: 0.0884 sec.
iter 396450 || Loss: 0.7588 || timer: 0.0833 sec.
iter 396460 || Loss: 0.4985 || timer: 0.0839 sec.
iter 396470 || Loss: 0.7427 || timer: 0.0844 sec.
iter 396480 || Loss: 0.5956 || timer: 0.0922 sec.
iter 396490 || Loss: 0.5283 || timer: 0.0930 sec.
iter 396500 || Loss: 0.7735 || timer: 0.0947 sec.
iter 396510 || Loss: 0.8486 || timer: 0.0932 sec.
iter 396520 || Loss: 0.6812 || timer: 0.0839 sec.
iter 396530 || Loss: 0.4622 || timer: 0.0897 sec.
iter 396540 || Loss: 0.4735 || timer: 0.1077 sec.
iter 396550 || Loss: 0.4707 || timer: 0.0221 sec.
iter 396560 || Loss: 0.2707 || timer: 0.0901 sec.
iter 396570 || Loss: 0.6917 || timer: 0.0850 sec.
iter 396580 || Loss: 0.5759 || timer: 0.0911 sec.
iter 396590 || Loss: 0.7194 || timer: 0.0919 sec.
iter 396600 || Loss: 0.5366 || timer: 0.1149 sec.
iter 396610 || Loss: 0.7142 || timer: 0.0869 sec.
iter 396620 || Loss: 0.8458 || timer: 0.0834 sec.
iter 396630 || Loss: 0.4836 || timer: 0.1043 sec.
iter 396640 || Loss: 0.7035 || timer: 0.0972 sec.
iter 396650 || Loss: 0.5454 || timer: 0.1332 sec.
iter 396660 || Loss: 0.6585 || timer: 0.0906 sec.
iter 396670 || Loss: 0.7257 || timer: 0.0949 sec.
iter 396680 || Loss: 0.7880 || timer: 0.0852 sec.
iter 396690 || Loss: 0.5701 || timer: 0.0865 sec.
iter 396700 || Loss: 1.0007 || timer: 0.0955 sec.
iter 396710 || Loss: 0.5772 || timer: 0.0987 sec.
iter 396720 || Loss: 0.6591 || timer: 0.0953 sec.
iter 396730 || Loss: 0.5080 || timer: 0.0973 sec.
iter 396740 || Loss: 0.6678 || timer: 0.0919 sec.
iter 396750 || Loss: 0.3885 || timer: 0.0828 sec.
iter 396760 || Loss: 0.7396 || timer: 0.1114 sec.
iter 396770 || Loss: 0.7871 || timer: 0.0825 sec.
iter 396780 || Loss: 0.7686 || timer: 0.0848 sec.
iter 396790 || Loss: 0.8291 || timer: 0.0978 sec.
iter 396800 || Loss: 0.5660 || timer: 0.1049 sec.
iter 396810 || Loss: 0.7488 || timer: 0.0891 sec.
iter 396820 || Loss: 0.8113 || timer: 0.0895 sec.
iter 396830 || Loss: 0.8911 || timer: 0.1002 sec.
iter 396840 || Loss: 0.9200 || timer: 0.1045 sec.
iter 396850 || Loss: 0.5628 || timer: 0.0890 sec.
iter 396860 || Loss: 0.6529 || timer: 0.0898 sec.
iter 396870 || Loss: 0.7187 || timer: 0.0903 sec.
iter 396880 || Loss: 0.8671 || timer: 0.0239 sec.
iter 396890 || Loss: 3.2151 || timer: 0.0908 sec.
iter 396900 || Loss: 0.5684 || timer: 0.1130 sec.
iter 396910 || Loss: 0.4353 || timer: 0.0961 sec.
iter 396920 || Loss: 0.5752 || timer: 0.0826 sec.
iter 396930 || Loss: 0.6756 || timer: 0.0905 sec.
iter 396940 || Loss: 0.7720 || timer: 0.0840 sec.
iter 396950 || Loss: 0.5764 || timer: 0.0882 sec.
iter 396960 || Loss: 0.6118 || timer: 0.0829 sec.
iter 396970 || Loss: 0.7722 || timer: 0.0824 sec.
iter 396980 || Loss: 0.6362 || timer: 0.0972 sec.
iter 396990 || Loss: 0.8795 || timer: 0.0828 sec.
iter 397000 || Loss: 0.5482 || timer: 0.0882 sec.
iter 397010 || Loss: 0.5161 || timer: 0.0835 sec.
iter 397020 || Loss: 0.8393 || timer: 0.0977 sec.
iter 397030 || Loss: 0.9276 || timer: 0.0927 sec.
iter 397040 || Loss: 0.7237 || timer: 0.0906 sec.
iter 397050 || Loss: 0.7278 || timer: 0.0923 sec.
iter 397060 || Loss: 0.7728 || timer: 0.0966 sec.
iter 397070 || Loss: 0.5377 || timer: 0.0895 sec.
iter 397080 || Loss: 0.9078 || timer: 0.0835 sec.
iter 397090 || Loss: 0.5014 || timer: 0.0841 sec.
iter 397100 || Loss: 0.5213 || timer: 0.0868 sec.
iter 397110 || Loss: 0.6094 || timer: 0.0891 sec.
iter 397120 || Loss: 0.7868 || timer: 0.0882 sec.
iter 397130 || Loss: 0.4878 || timer: 0.0930 sec.
iter 397140 || Loss: 0.9099 || timer: 0.0859 sec.
iter 397150 || Loss: 0.8226 || timer: 0.0873 sec.
iter 397160 || Loss: 1.0089 || timer: 0.0839 sec.
iter 397170 || Loss: 0.7772 || timer: 0.0829 sec.
iter 397180 || Loss: 0.6971 || timer: 0.0894 sec.
iter 397190 || Loss: 0.7518 || timer: 0.0901 sec.
iter 397200 || Loss: 0.6002 || timer: 0.0830 sec.
iter 397210 || Loss: 0.5586 || timer: 0.0170 sec.
iter 397220 || Loss: 1.5843 || timer: 0.0911 sec.
iter 397230 || Loss: 0.7237 || timer: 0.0831 sec.
iter 397240 || Loss: 0.6102 || timer: 0.0884 sec.
iter 397250 || Loss: 0.5737 || timer: 0.0873 sec.
iter 397260 || Loss: 0.7451 || timer: 0.1078 sec.
iter 397270 || Loss: 0.6363 || timer: 0.0920 sec.
iter 397280 || Loss: 0.8596 || timer: 0.0825 sec.
iter 397290 || Loss: 0.5186 || timer: 0.0876 sec.
iter 397300 || Loss: 0.6482 || timer: 0.0918 sec.
iter 397310 || Loss: 0.4703 || timer: 0.1182 sec.
iter 397320 || Loss: 0.6876 || timer: 0.0909 sec.
iter 397330 || Loss: 0.3896 || timer: 0.0917 sec.
iter 397340 || Loss: 0.7203 || timer: 0.0943 sec.
iter 397350 || Loss: 0.9017 || timer: 0.1084 sec.
iter 397360 || Loss: 0.6856 || timer: 0.1026 sec.
iter 397370 || Loss: 0.6692 || timer: 0.0898 sec.
iter 397380 || Loss: 0.6353 || timer: 0.1087 sec.
iter 397390 || Loss: 0.6813 || timer: 0.0872 sec.
iter 397400 || Loss: 0.5932 || timer: 0.0912 sec.
iter 397410 || Loss: 0.8120 || timer: 0.0882 sec.
iter 397420 || Loss: 0.5860 || timer: 0.0827 sec.
iter 397430 || Loss: 0.6273 || timer: 0.0889 sec.
iter 397440 || Loss: 0.7906 || timer: 0.0901 sec.
iter 397450 || Loss: 0.6319 || timer: 0.1183 sec.
iter 397460 || Loss: 0.7876 || timer: 0.0843 sec.
iter 397470 || Loss: 0.8172 || timer: 0.0843 sec.
iter 397480 || Loss: 0.6886 || timer: 0.0831 sec.
iter 397490 || Loss: 0.8838 || timer: 0.1060 sec.
iter 397500 || Loss: 0.6282 || timer: 0.1032 sec.
iter 397510 || Loss: 0.9177 || timer: 0.0907 sec.
iter 397520 || Loss: 0.4862 || timer: 0.1050 sec.
iter 397530 || Loss: 0.5637 || timer: 0.0900 sec.
iter 397540 || Loss: 0.4434 || timer: 0.0226 sec.
iter 397550 || Loss: 0.6270 || timer: 0.0820 sec.
iter 397560 || Loss: 0.7331 || timer: 0.0829 sec.
iter 397570 || Loss: 0.5036 || timer: 0.0918 sec.
iter 397580 || Loss: 0.6715 || timer: 0.1038 sec.
iter 397590 || Loss: 0.8011 || timer: 0.0780 sec.
iter 397600 || Loss: 1.0306 || timer: 0.0915 sec.
iter 397610 || Loss: 0.9346 || timer: 0.0917 sec.
iter 397620 || Loss: 0.6528 || timer: 0.1049 sec.
iter 397630 || Loss: 0.6620 || timer: 0.0835 sec.
iter 397640 || Loss: 0.6367 || timer: 0.0971 sec.
iter 397650 || Loss: 0.5950 || timer: 0.1042 sec.
iter 397660 || Loss: 0.5772 || timer: 0.0904 sec.
iter 397670 || Loss: 0.5768 || timer: 0.0990 sec.
iter 397680 || Loss: 0.6104 || timer: 0.0840 sec.
iter 397690 || Loss: 0.8146 || timer: 0.0919 sec.
iter 397700 || Loss: 0.9407 || timer: 0.0918 sec.
iter 397710 || Loss: 0.5664 || timer: 0.0891 sec.
iter 397720 || Loss: 0.8398 || timer: 0.0891 sec.
iter 397730 || Loss: 0.5997 || timer: 0.0848 sec.
iter 397740 || Loss: 0.8011 || timer: 0.0914 sec.
iter 397750 || Loss: 0.5884 || timer: 0.0915 sec.
iter 397760 || Loss: 0.5556 || timer: 0.1064 sec.
iter 397770 || Loss: 0.8423 || timer: 0.0922 sec.
iter 397780 || Loss: 0.4609 || timer: 0.0919 sec.
iter 397790 || Loss: 0.7995 || timer: 0.0929 sec.
iter 397800 || Loss: 0.6316 || timer: 0.0931 sec.
iter 397810 || Loss: 0.5936 || timer: 0.0916 sec.
iter 397820 || Loss: 0.7536 || timer: 0.0838 sec.
iter 397830 || Loss: 0.6595 || timer: 0.0956 sec.
iter 397840 || Loss: 0.5703 || timer: 0.0912 sec.
iter 397850 || Loss: 0.7830 || timer: 0.0900 sec.
iter 397860 || Loss: 0.5884 || timer: 0.1199 sec.
iter 397870 || Loss: 0.7367 || timer: 0.0248 sec.
iter 397880 || Loss: 1.3773 || timer: 0.0902 sec.
iter 397890 || Loss: 0.4915 || timer: 0.1034 sec.
iter 397900 || Loss: 0.5740 || timer: 0.0896 sec.
iter 397910 || Loss: 0.5132 || timer: 0.0879 sec.
iter 397920 || Loss: 0.6758 || timer: 0.0986 sec.
iter 397930 || Loss: 1.0107 || timer: 0.0933 sec.
iter 397940 || Loss: 0.4296 || timer: 0.0907 sec.
iter 397950 || Loss: 0.9199 || timer: 0.0834 sec.
iter 397960 || Loss: 0.7677 || timer: 0.0848 sec.
iter 397970 || Loss: 0.5066 || timer: 0.0954 sec.
iter 397980 || Loss: 0.6952 || timer: 0.0771 sec.
iter 397990 || Loss: 0.5757 || timer: 0.0851 sec.
iter 398000 || Loss: 0.6983 || timer: 0.0838 sec.
iter 398010 || Loss: 0.6262 || timer: 0.0878 sec.
iter 398020 || Loss: 0.7352 || timer: 0.0911 sec.
iter 398030 || Loss: 0.7808 || timer: 0.0938 sec.
iter 398040 || Loss: 0.6169 || timer: 0.0925 sec.
iter 398050 || Loss: 0.5673 || timer: 0.0911 sec.
iter 398060 || Loss: 0.4847 || timer: 0.0904 sec.
iter 398070 || Loss: 0.4982 || timer: 0.0834 sec.
iter 398080 || Loss: 0.6363 || timer: 0.0900 sec.
iter 398090 || Loss: 0.8885 || timer: 0.0886 sec.
iter 398100 || Loss: 0.6154 || timer: 0.0941 sec.
iter 398110 || Loss: 0.5781 || timer: 0.0909 sec.
iter 398120 || Loss: 0.7577 || timer: 0.0894 sec.
iter 398130 || Loss: 0.5427 || timer: 0.0902 sec.
iter 398140 || Loss: 0.5953 || timer: 0.0819 sec.
iter 398150 || Loss: 0.7472 || timer: 0.0834 sec.
iter 398160 || Loss: 0.7476 || timer: 0.0888 sec.
iter 398170 || Loss: 0.5546 || timer: 0.1078 sec.
iter 398180 || Loss: 0.7381 || timer: 0.0918 sec.
iter 398190 || Loss: 0.7819 || timer: 0.0939 sec.
iter 398200 || Loss: 0.6245 || timer: 0.0288 sec.
iter 398210 || Loss: 0.3457 || timer: 0.0936 sec.
iter 398220 || Loss: 0.6623 || timer: 0.0866 sec.
iter 398230 || Loss: 0.4997 || timer: 0.0862 sec.
iter 398240 || Loss: 0.6982 || timer: 0.0922 sec.
iter 398250 || Loss: 0.5471 || timer: 0.0849 sec.
iter 398260 || Loss: 0.5332 || timer: 0.0916 sec.
iter 398270 || Loss: 0.7995 || timer: 0.0895 sec.
iter 398280 || Loss: 0.8943 || timer: 0.1064 sec.
iter 398290 || Loss: 0.6464 || timer: 0.0814 sec.
iter 398300 || Loss: 0.7483 || timer: 0.1205 sec.
iter 398310 || Loss: 0.6118 || timer: 0.1031 sec.
iter 398320 || Loss: 0.5390 || timer: 0.0914 sec.
iter 398330 || Loss: 0.7402 || timer: 0.0912 sec.
iter 398340 || Loss: 0.8446 || timer: 0.0830 sec.
iter 398350 || Loss: 0.7563 || timer: 0.0835 sec.
iter 398360 || Loss: 0.7352 || timer: 0.1022 sec.
iter 398370 || Loss: 0.7228 || timer: 0.0963 sec.
iter 398380 || Loss: 0.6089 || timer: 0.0913 sec.
iter 398390 || Loss: 0.8075 || timer: 0.0894 sec.
iter 398400 || Loss: 0.4971 || timer: 0.0848 sec.
iter 398410 || Loss: 0.5976 || timer: 0.0888 sec.
iter 398420 || Loss: 0.6777 || timer: 0.0837 sec.
iter 398430 || Loss: 0.7916 || timer: 0.0918 sec.
iter 398440 || Loss: 0.7680 || timer: 0.1071 sec.
iter 398450 || Loss: 0.6763 || timer: 0.0883 sec.
iter 398460 || Loss: 0.6834 || timer: 0.0911 sec.
iter 398470 || Loss: 0.9926 || timer: 0.0923 sec.
iter 398480 || Loss: 0.5533 || timer: 0.0913 sec.
iter 398490 || Loss: 0.6650 || timer: 0.0808 sec.
iter 398500 || Loss: 0.5999 || timer: 0.0897 sec.
iter 398510 || Loss: 0.5897 || timer: 0.0839 sec.
iter 398520 || Loss: 0.8258 || timer: 0.0919 sec.
iter 398530 || Loss: 0.6138 || timer: 0.0275 sec.
iter 398540 || Loss: 0.2054 || timer: 0.0842 sec.
iter 398550 || Loss: 0.7803 || timer: 0.0844 sec.
iter 398560 || Loss: 0.8786 || timer: 0.1004 sec.
iter 398570 || Loss: 0.7209 || timer: 0.1049 sec.
iter 398580 || Loss: 0.6467 || timer: 0.1051 sec.
iter 398590 || Loss: 0.8466 || timer: 0.0914 sec.
iter 398600 || Loss: 0.4598 || timer: 0.0949 sec.
iter 398610 || Loss: 0.5133 || timer: 0.0899 sec.
iter 398620 || Loss: 0.5392 || timer: 0.0951 sec.
iter 398630 || Loss: 0.5995 || timer: 0.1030 sec.
iter 398640 || Loss: 0.5699 || timer: 0.1008 sec.
iter 398650 || Loss: 0.5526 || timer: 0.0837 sec.
iter 398660 || Loss: 0.5421 || timer: 0.0825 sec.
iter 398670 || Loss: 0.4991 || timer: 0.0938 sec.
iter 398680 || Loss: 0.8849 || timer: 0.0834 sec.
iter 398690 || Loss: 0.5605 || timer: 0.1014 sec.
iter 398700 || Loss: 0.7532 || timer: 0.0911 sec.
iter 398710 || Loss: 0.6294 || timer: 0.0937 sec.
iter 398720 || Loss: 0.6094 || timer: 0.1062 sec.
iter 398730 || Loss: 0.5397 || timer: 0.0768 sec.
iter 398740 || Loss: 0.8081 || timer: 0.0828 sec.
iter 398750 || Loss: 0.6753 || timer: 0.0875 sec.
iter 398760 || Loss: 0.5511 || timer: 0.0844 sec.
iter 398770 || Loss: 0.6642 || timer: 0.0807 sec.
iter 398780 || Loss: 0.5406 || timer: 0.0826 sec.
iter 398790 || Loss: 0.6010 || timer: 0.0934 sec.
iter 398800 || Loss: 0.6209 || timer: 0.0883 sec.
iter 398810 || Loss: 0.6471 || timer: 0.0849 sec.
iter 398820 || Loss: 0.5834 || timer: 0.0820 sec.
iter 398830 || Loss: 1.0322 || timer: 0.0813 sec.
iter 398840 || Loss: 0.7035 || timer: 0.0893 sec.
iter 398850 || Loss: 0.7214 || timer: 0.0890 sec.
iter 398860 || Loss: 0.4075 || timer: 0.0324 sec.
iter 398870 || Loss: 0.6495 || timer: 0.0872 sec.
iter 398880 || Loss: 0.6350 || timer: 0.0820 sec.
iter 398890 || Loss: 0.7648 || timer: 0.0875 sec.
iter 398900 || Loss: 0.6766 || timer: 0.0810 sec.
iter 398910 || Loss: 0.7137 || timer: 0.0880 sec.
iter 398920 || Loss: 0.8093 || timer: 0.0884 sec.
iter 398930 || Loss: 0.8031 || timer: 0.1068 sec.
iter 398940 || Loss: 0.5835 || timer: 0.0827 sec.
iter 398950 || Loss: 0.4727 || timer: 0.0823 sec.
iter 398960 || Loss: 0.7263 || timer: 0.1232 sec.
iter 398970 || Loss: 0.7047 || timer: 0.0931 sec.
iter 398980 || Loss: 0.6681 || timer: 0.1276 sec.
iter 398990 || Loss: 0.8925 || timer: 0.0827 sec.
iter 399000 || Loss: 0.8989 || timer: 0.0941 sec.
iter 399010 || Loss: 0.7073 || timer: 0.1205 sec.
iter 399020 || Loss: 0.6218 || timer: 0.0992 sec.
iter 399030 || Loss: 0.5719 || timer: 0.0923 sec.
iter 399040 || Loss: 0.9613 || timer: 0.0883 sec.
iter 399050 || Loss: 0.4151 || timer: 0.0977 sec.
iter 399060 || Loss: 0.7534 || timer: 0.1219 sec.
iter 399070 || Loss: 0.6246 || timer: 0.1062 sec.
iter 399080 || Loss: 0.5578 || timer: 0.0921 sec.
iter 399090 || Loss: 0.9019 || timer: 0.0906 sec.
iter 399100 || Loss: 0.7713 || timer: 0.0876 sec.
iter 399110 || Loss: 0.7036 || timer: 0.0822 sec.
iter 399120 || Loss: 0.6239 || timer: 0.0895 sec.
iter 399130 || Loss: 0.6342 || timer: 0.0877 sec.
iter 399140 || Loss: 0.6900 || timer: 0.0918 sec.
iter 399150 || Loss: 0.7043 || timer: 0.0891 sec.
iter 399160 || Loss: 0.5885 || timer: 0.0892 sec.
iter 399170 || Loss: 0.7010 || timer: 0.0833 sec.
iter 399180 || Loss: 0.8546 || timer: 0.0939 sec.
iter 399190 || Loss: 0.4804 || timer: 0.0163 sec.
iter 399200 || Loss: 1.2709 || timer: 0.0876 sec.
iter 399210 || Loss: 0.5884 || timer: 0.0905 sec.
iter 399220 || Loss: 0.5864 || timer: 0.0890 sec.
iter 399230 || Loss: 0.6762 || timer: 0.0811 sec.
iter 399240 || Loss: 0.6588 || timer: 0.0978 sec.
iter 399250 || Loss: 0.5826 || timer: 0.0879 sec.
iter 399260 || Loss: 0.4765 || timer: 0.0933 sec.
iter 399270 || Loss: 0.6694 || timer: 0.0904 sec.
iter 399280 || Loss: 0.7941 || timer: 0.0811 sec.
iter 399290 || Loss: 0.8585 || timer: 0.1075 sec.
iter 399300 || Loss: 0.6950 || timer: 0.0893 sec.
iter 399310 || Loss: 0.8152 || timer: 0.0922 sec.
iter 399320 || Loss: 0.8408 || timer: 0.0898 sec.
iter 399330 || Loss: 0.6432 || timer: 0.0909 sec.
iter 399340 || Loss: 0.9197 || timer: 0.0999 sec.
iter 399350 || Loss: 0.8022 || timer: 0.0891 sec.
iter 399360 || Loss: 0.7570 || timer: 0.0886 sec.
iter 399370 || Loss: 0.5886 || timer: 0.0884 sec.
iter 399380 || Loss: 0.5305 || timer: 0.0890 sec.
iter 399390 || Loss: 0.9716 || timer: 0.1047 sec.
iter 399400 || Loss: 0.7599 || timer: 0.0932 sec.
iter 399410 || Loss: 0.6148 || timer: 0.0925 sec.
iter 399420 || Loss: 0.6725 || timer: 0.0912 sec.
iter 399430 || Loss: 0.6606 || timer: 0.1053 sec.
iter 399440 || Loss: 0.8484 || timer: 0.0976 sec.
iter 399450 || Loss: 0.8108 || timer: 0.0873 sec.
iter 399460 || Loss: 0.5366 || timer: 0.0809 sec.
iter 399470 || Loss: 0.5796 || timer: 0.0813 sec.
iter 399480 || Loss: 0.8070 || timer: 0.0836 sec.
iter 399490 || Loss: 0.7509 || timer: 0.0813 sec.
iter 399500 || Loss: 0.6540 || timer: 0.1035 sec.
iter 399510 || Loss: 0.9098 || timer: 0.0872 sec.
iter 399520 || Loss: 0.6071 || timer: 0.0300 sec.
iter 399530 || Loss: 1.1268 || timer: 0.0879 sec.
iter 399540 || Loss: 0.4713 || timer: 0.0824 sec.
iter 399550 || Loss: 0.6042 || timer: 0.0882 sec.
iter 399560 || Loss: 0.7648 || timer: 0.0899 sec.
iter 399570 || Loss: 0.5751 || timer: 0.0860 sec.
iter 399580 || Loss: 0.7943 || timer: 0.0920 sec.
iter 399590 || Loss: 0.5670 || timer: 0.0919 sec.
iter 399600 || Loss: 0.5341 || timer: 0.0905 sec.
iter 399610 || Loss: 0.5799 || timer: 0.0843 sec.
iter 399620 || Loss: 0.5933 || timer: 0.0997 sec.
iter 399630 || Loss: 0.5485 || timer: 0.1150 sec.
iter 399640 || Loss: 0.9664 || timer: 0.1021 sec.
iter 399650 || Loss: 0.6445 || timer: 0.0898 sec.
iter 399660 || Loss: 0.5682 || timer: 0.0843 sec.
iter 399670 || Loss: 0.7335 || timer: 0.0867 sec.
iter 399680 || Loss: 0.7816 || timer: 0.0915 sec.
iter 399690 || Loss: 0.9013 || timer: 0.0943 sec.
iter 399700 || Loss: 0.6597 || timer: 0.0880 sec.
iter 399710 || Loss: 0.5331 || timer: 0.0859 sec.
iter 399720 || Loss: 0.8140 || timer: 0.0886 sec.
iter 399730 || Loss: 0.6488 || timer: 0.0891 sec.
iter 399740 || Loss: 0.5821 || timer: 0.0890 sec.
iter 399750 || Loss: 0.7220 || timer: 0.0816 sec.
iter 399760 || Loss: 0.4992 || timer: 0.0867 sec.
iter 399770 || Loss: 0.5983 || timer: 0.0820 sec.
iter 399780 || Loss: 0.6635 || timer: 0.0913 sec.
iter 399790 || Loss: 0.6095 || timer: 0.0913 sec.
iter 399800 || Loss: 0.6491 || timer: 0.0830 sec.
iter 399810 || Loss: 0.7063 || timer: 0.0855 sec.
iter 399820 || Loss: 0.5623 || timer: 0.0853 sec.
iter 399830 || Loss: 0.5596 || timer: 0.0876 sec.
iter 399840 || Loss: 0.7132 || timer: 0.0835 sec.
iter 399850 || Loss: 0.6041 || timer: 0.0177 sec.
iter 399860 || Loss: 0.9098 || timer: 0.0818 sec.
iter 399870 || Loss: 0.4567 || timer: 0.0846 sec.
iter 399880 || Loss: 0.5716 || timer: 0.1195 sec.
iter 399890 || Loss: 0.5532 || timer: 0.0873 sec.
iter 399900 || Loss: 0.4174 || timer: 0.0851 sec.
iter 399910 || Loss: 0.7513 || timer: 0.0893 sec.
iter 399920 || Loss: 0.6706 || timer: 0.0894 sec.
iter 399930 || Loss: 0.5779 || timer: 0.0870 sec.
iter 399940 || Loss: 0.5303 || timer: 0.0910 sec.
iter 399950 || Loss: 0.6735 || timer: 0.1053 sec.
iter 399960 || Loss: 0.5701 || timer: 0.0886 sec.
iter 399970 || Loss: 0.6366 || timer: 0.0900 sec.
iter 399980 || Loss: 0.6701 || timer: 0.0839 sec.
iter 399990 || Loss: 0.6912 || /home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
/home/xj/xu/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  self.priors = Variable(self.priorbox.forward(), volatile=True)
/home/xj/xu/ssd.pytorch/train.py:225: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  init.xavier_uniform(param)
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading base network...
Initializing weights...
Loading the dataset...
Training SSD on: MS COCO
Using the specified args:
Namespace(dataset='COCO', dataset_root='/home/xj/xu/ssd.pytorch/COCO_250423_the_thrid_optimization_datasets', basenet='vgg16_reducedfc.pth', batch_size=32, resume=None, start_iter=0, num_workers=4, cuda=True, lr=0.001, momentum=0.9, weight_decay=0.0005, gamma=0.1, visdom=False, save_folder='weights/')
/home/xj/xu/ssd.pytorch/train.py:180: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  targets = [Variable(ann.cuda(), volatile=True) for ann in targets]
/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
timer: 5.2772 sec.
iter 0 || Loss: 24.7378 || timer: 0.0795 sec.
iter 10 || Loss: 11.9974 || timer: 0.0804 sec.
iter 20 || Loss: 13.2041 || timer: 0.0728 sec.
iter 30 || Loss: 11.5908 || timer: 0.1038 sec.
iter 40 || Loss: 13.2684 || timer: 0.0831 sec.
iter 50 || Loss: 15.9066 || timer: 0.0751 sec.
iter 60 || Loss: 10.3735 || timer: 0.0860 sec.
iter 70 || Loss: 10.3025 || timer: 0.0815 sec.
iter 80 || Loss: 7.8550 || timer: 0.0739 sec.
iter 90 || Loss: 7.7550 || timer: 0.0908 sec.
iter 100 || Loss: 7.7266 || timer: 0.0832 sec.
iter 110 || Loss: 9.0642 || timer: 0.0814 sec.
iter 120 || Loss: 7.1320 || timer: 0.0852 sec.
iter 130 || Loss: 6.1388 || timer: 0.0828 sec.
iter 140 || Loss: 6.7866 || timer: 0.0918 sec.
iter 150 || Loss: 7.9138 || timer: 0.0810 sec.
iter 160 || Loss: 6.1901 || timer: 0.0818 sec.
iter 170 || Loss: 7.9127 || timer: 0.0822 sec.
iter 180 || Loss: 10.2494 || timer: 0.0971 sec.
iter 190 || Loss: 7.5907 || timer: 0.0741 sec.
iter 200 || Loss: 5.3221 || timer: 0.0736 sec.
iter 210 || Loss: 5.8300 || timer: 0.0804 sec.
iter 220 || Loss: 5.1765 || timer: 0.0169 sec.
iter 230 || Loss: 5.6334 || timer: 0.0755 sec.
iter 240 || Loss: 6.0537 || timer: 0.0772 sec.
iter 250 || Loss: 5.0585 || timer: 0.0805 sec.
iter 260 || Loss: 5.9661 || timer: 0.0931 sec.
iter 270 || Loss: 5.8143 || timer: 0.0746 sec.
iter 280 || Loss: 5.6630 || timer: 0.0680 sec.
iter 290 || Loss: 6.3741 || timer: 0.0813 sec.
iter 300 || Loss: 5.9440 || timer: 0.0866 sec.
iter 310 || Loss: 5.0930 || timer: 0.0756 sec.
iter 320 || Loss: 5.8803 || timer: 0.0886 sec.
iter 330 || Loss: 5.3456 || timer: 0.0989 sec.
iter 340 || Loss: 5.9971 || timer: 0.0824 sec.
iter 350 || Loss: 5.4854 || timer: 0.0818 sec.
iter 360 || Loss: 5.3880 || timer: 0.0793 sec.
iter 370 || Loss: 4.8997 || timer: 0.0738 sec.
iter 380 || Loss: 5.0302 || timer: 0.0796 sec.
iter 390 || Loss: 5.2986 || timer: 0.0809 sec.
iter 400 || Loss: 5.1716 || timer: 0.0835 sec.
iter 410 || Loss: 4.7959 || timer: 0.0866 sec.
iter 420 || Loss: 5.0884 || timer: 0.0759 sec.
iter 430 || Loss: 5.9650 || timer: 0.0997 sec.
iter 440 || Loss: 5.1658 || timer: 0.0817 sec.
iter 450 || Loss: 4.9980 || timer: 0.0735 sec.
iter 460 || Loss: 6.8951 || timer: 0.0753 sec.
iter 470 || Loss: 6.4517 || timer: 0.0828 sec.
iter 480 || Loss: 4.8721 || timer: 0.0860 sec.
iter 490 || Loss: 5.1336 || timer: 0.0814 sec.
iter 500 || Loss: 4.7575 || timer: 0.0827 sec.
iter 510 || Loss: 4.9299 || timer: 0.0765 sec.
iter 520 || Loss: 5.0817 || timer: 0.0766 sec.
iter 530 || Loss: 5.2977 || timer: 0.0770 sec.
iter 540 || Loss: 5.2171 || timer: 0.0804 sec.
iter 550 || Loss: 5.0898 || timer: 0.0170 sec.
iter 560 || Loss: 6.3895 || timer: 0.0833 sec.
iter 570 || Loss: 4.8035 || timer: 0.0746 sec.
iter 580 || Loss: 4.7002 || timer: 0.0873 sec.
iter 590 || Loss: 4.4351 || timer: 0.0724 sec.
iter 600 || Loss: 6.4560 || timer: 0.0698 sec.
iter 610 || Loss: 5.1947 || timer: 0.0666 sec.
iter 620 || Loss: 4.7359 || timer: 0.0814 sec.
iter 630 || Loss: 4.9451 || timer: 0.0777 sec.
iter 640 || Loss: 3.8626 || timer: 0.0770 sec.
iter 650 || Loss: 4.7456 || timer: 0.0806 sec.
iter 660 || Loss: 4.7729 || timer: 0.0873 sec.
iter 670 || Loss: 4.9502 || timer: 0.0673 sec.
iter 680 || Loss: 4.4593 || timer: 0.0805 sec.
iter 690 || Loss: 4.3173 || timer: 0.0740 sec.
iter 700 || Loss: 4.9016 || timer: 0.1017 sec.
iter 710 || Loss: 4.2257 || timer: 0.0833 sec.
iter 720 || Loss: 4.1081 || timer: 0.0910 sec.
iter 730 || Loss: 4.7621 || timer: 0.1012 sec.
iter 740 || Loss: 4.9715 || timer: 0.0812 sec.
iter 750 || Loss: 4.8369 || timer: 0.0840 sec.
iter 760 || Loss: 4.0992 || timer: 0.0852 sec.
iter 770 || Loss: 4.5618 || timer: 0.0968 sec.
iter 780 || Loss: 4.3713 || timer: 0.0815 sec.
iter 790 || Loss: 4.1946 || timer: 0.0826 sec.
iter 800 || Loss: 4.3966 || timer: 0.0739 sec.
iter 810 || Loss: 4.5742 || timer: 0.0967 sec.
iter 820 || Loss: 3.8996 || timer: 0.0928 sec.
iter 830 || Loss: 5.2012 || timer: 0.0812 sec.
iter 840 || Loss: 3.9644 || timer: 0.0831 sec.
iter 850 || Loss: 4.2106 || timer: 0.0725 sec.
iter 860 || Loss: 5.1210 || timer: 0.0809 sec.
iter 870 || Loss: 4.8530 || timer: 0.0826 sec.
iter 880 || Loss: 4.8878 || timer: 0.0259 sec.
iter 890 || Loss: 5.4110 || timer: 0.0820 sec.
iter 900 || Loss: 4.9267 || timer: 0.0738 sec.
iter 910 || Loss: 4.2647 || timer: 0.0831 sec.
iter 920 || Loss: 3.8807 || timer: 0.0833 sec.
iter 930 || Loss: 4.6545 || timer: 0.0793 sec.
iter 940 || Loss: 4.3512 || timer: 0.0811 sec.
iter 950 || Loss: 3.9101 || timer: 0.0975 sec.
iter 960 || Loss: 4.5252 || timer: 0.0821 sec.
iter 970 || Loss: 4.2820 || timer: 0.0945 sec.
iter 980 || Loss: 4.0308 || timer: 0.0881 sec.
iter 990 || Loss: 4.1169 || timer: 0.0815 sec.
iter 1000 || Loss: 4.6393 || timer: 0.0829 sec.
iter 1010 || Loss: 4.7137 || timer: 0.0800 sec.
iter 1020 || Loss: 3.9701 || timer: 0.0812 sec.
iter 1030 || Loss: 4.1650 || timer: 0.0835 sec.
iter 1040 || Loss: 4.1674 || timer: 0.0796 sec.
iter 1050 || Loss: 4.0488 || timer: 0.0945 sec.
iter 1060 || Loss: 4.1276 || timer: 0.0799 sec.
iter 1070 || Loss: 4.7679 || timer: 0.0775 sec.
iter 1080 || Loss: 3.6655 || timer: 0.0831 sec.
iter 1090 || Loss: 3.7829 || timer: 0.0821 sec.
iter 1100 || Loss: 3.7485 || timer: 0.1084 sec.
iter 1110 || Loss: 4.3809 || timer: 0.0793 sec.
iter 1120 || Loss: 3.1331 || timer: 0.0806 sec.
iter 1130 || Loss: 4.5707 || timer: 0.0812 sec.
iter 1140 || Loss: 3.7517 || timer: 0.0810 sec.
iter 1150 || Loss: 3.4139 || timer: 0.0820 sec.
iter 1160 || Loss: 3.6462 || timer: 0.0960 sec.
iter 1170 || Loss: 3.3882 || timer: 0.0777 sec.
iter 1180 || Loss: 3.8103 || timer: 0.0780 sec.
iter 1190 || Loss: 3.7043 || timer: 0.0825 sec.
iter 1200 || Loss: 3.5619 || timer: 0.0822 sec.
iter 1210 || Loss: 4.3837 || timer: 0.0288 sec.
iter 1220 || Loss: 1.4911 || timer: 0.0984 sec.
iter 1230 || Loss: 3.6529 || timer: 0.0820 sec.
iter 1240 || Loss: 3.7807 || timer: 0.0837 sec.
iter 1250 || Loss: 3.0390 || timer: 0.0793 sec.
iter 1260 || Loss: 4.3636 || timer: 0.0854 sec.
iter 1270 || Loss: 3.9712 || timer: 0.0818 sec.
iter 1280 || Loss: 3.4714 || timer: 0.0858 sec.
iter 1290 || Loss: 4.2124 || timer: 0.0814 sec.
iter 1300 || Loss: 4.1139 || timer: 0.0821 sec.
iter 1310 || Loss: 3.9357 || timer: 0.0971 sec.
iter 1320 || Loss: 3.1368 || timer: 0.0923 sec.
iter 1330 || Loss: 3.6065 || timer: 0.0816 sec.
iter 1340 || Loss: 3.6497 || timer: 0.0946 sec.
iter 1350 || Loss: 3.3399 || timer: 0.0740 sec.
iter 1360 || Loss: 3.4151 || timer: 0.0731 sec.
iter 1370 || Loss: 3.1775 || timer: 0.0799 sec.
iter 1380 || Loss: 3.0351 || timer: 0.0815 sec.
iter 1390 || Loss: 5.1109 || timer: 0.0780 sec.
iter 1400 || Loss: 4.1952 || timer: 0.0737 sec.
iter 1410 || Loss: 3.6684 || timer: 0.0761 sec.
iter 1420 || Loss: 3.2592 || timer: 0.0914 sec.
iter 1430 || Loss: 3.4919 || timer: 0.0841 sec.
iter 1440 || Loss: 3.4327 || timer: 0.0813 sec.
iter 1450 || Loss: 3.1748 || timer: 0.0831 sec.
iter 1460 || Loss: 3.9241 || timer: 0.0779 sec.
iter 1470 || Loss: 3.8639 || timer: 0.0876 sec.
iter 1480 || Loss: 3.3730 || timer: 0.0784 sec.
iter 1490 || Loss: 3.9969 || timer: 0.1063 sec.
iter 1500 || Loss: 3.9971 || timer: 0.0988 sec.
iter 1510 || Loss: 3.4447 || timer: 0.0847 sec.
iter 1520 || Loss: 4.5269 || timer: 0.0789 sec.
iter 1530 || Loss: 3.6992 || timer: 0.0770 sec.
iter 1540 || Loss: 3.7405 || timer: 0.0162 sec.
iter 1550 || Loss: 2.0302 || timer: 0.0791 sec.
iter 1560 || Loss: 3.2437 || timer: 0.0832 sec.
iter 1570 || Loss: 3.6481 || timer: 0.0729 sec.
iter 1580 || Loss: 3.5983 || timer: 0.0735 sec.
iter 1590 || Loss: 5.2800 || timer: 0.0816 sec.
iter 1600 || Loss: 3.7807 || timer: 0.0936 sec.
iter 1610 || Loss: 3.8058 || timer: 0.0723 sec.
iter 1620 || Loss: 3.8047 || timer: 0.0818 sec.
iter 1630 || Loss: 3.5991 || timer: 0.0870 sec.
iter 1640 || Loss: 3.2946 || timer: 0.0931 sec.
iter 1650 || Loss: 3.5118 || timer: 0.0743 sec.
iter 1660 || Loss: 4.6495 || timer: 0.0831 sec.
iter 1670 || Loss: 2.9969 || timer: 0.0732 sec.
iter 1680 || Loss: 3.1911 || timer: 0.1042 sec.
iter 1690 || Loss: 4.1298 || timer: 0.0829 sec.
iter 1700 || Loss: 3.7128 || timer: 0.0824 sec.
iter 1710 || Loss: 3.1688 || timer: 0.0721 sec.
iter 1720 || Loss: 3.7173 || timer: 0.0725 sec.
iter 1730 || Loss: 3.8315 || timer: 0.0854 sec.
iter 1740 || Loss: 3.2503 || timer: 0.0872 sec.
iter 1750 || Loss: 2.9176 || timer: 0.0782 sec.
iter 1760 || Loss: 3.6183 || timer: 0.0943 sec.
iter 1770 || Loss: 3.6986 || timer: 0.0793 sec.
iter 1780 || Loss: 3.1865 || timer: 0.0946 sec.
iter 1790 || Loss: 3.6433 || timer: 0.0945 sec.
iter 1800 || Loss: 3.4381 || timer: 0.0692 sec.
iter 1810 || Loss: 3.4884 || timer: 0.0811 sec.
iter 1820 || Loss: 3.8462 || timer: 0.0816 sec.
iter 1830 || Loss: 3.0802 || timer: 0.0766 sec.
iter 1840 || Loss: 3.0829 || timer: 0.0835 sec.
iter 1850 || Loss: 3.3061 || timer: 0.0828 sec.
iter 1860 || Loss: 3.1599 || timer: 0.0806 sec.
iter 1870 || Loss: 3.1205 || timer: 0.0157 sec.
iter 1880 || Loss: 3.6743 || timer: 0.0845 sec.
iter 1890 || Loss: 3.6290 || timer: 0.0972 sec.
iter 1900 || Loss: 3.1801 || timer: 0.0818 sec.
iter 1910 || Loss: 3.4068 || timer: 0.0775 sec.
iter 1920 || Loss: 3.8359 || timer: 0.0814 sec.
iter 1930 || Loss: 2.7455 || timer: 0.0795 sec.
iter 1940 || Loss: 3.2063 || timer: 0.0732 sec.
iter 1950 || Loss: 4.1780 || timer: 0.0770 sec.
iter 1960 || Loss: 4.5830 || timer: 0.0875 sec.
iter 1970 || Loss: 3.2874 || timer: 0.0867 sec.
iter 1980 || Loss: 2.8157 || timer: 0.0808 sec.
iter 1990 || Loss: 3.0624 || timer: 0.0803 sec.
iter 2000 || Loss: 3.4473 || timer: 0.0799 sec.
iter 2010 || Loss: 2.9781 || timer: 0.1026 sec.
iter 2020 || Loss: 2.8141 || timer: 0.0776 sec.
iter 2030 || Loss: 3.4068 || timer: 0.0805 sec.
iter 2040 || Loss: 3.5534 || timer: 0.0994 sec.
iter 2050 || Loss: 3.6350 || timer: 0.0806 sec.
iter 2060 || Loss: 3.1536 || timer: 0.0825 sec.
iter 2070 || Loss: 3.2399 || timer: 0.0976 sec.
iter 2080 || Loss: 3.1787 || timer: 0.0830 sec.
iter 2090 || Loss: 3.2115 || timer: 0.0816 sec.
iter 2100 || Loss: 3.2150 || timer: 0.0810 sec.
iter 2110 || Loss: 2.6952 || timer: 0.0833 sec.
iter 2120 || Loss: 3.2798 || timer: 0.0810 sec.
iter 2130 || Loss: 3.1796 || timer: 0.0880 sec.
iter 2140 || Loss: 2.5158 || timer: 0.0799 sec.
iter 2150 || Loss: 3.6627 || timer: 0.0811 sec.
iter 2160 || Loss: 4.4126 || timer: 0.0821 sec.
iter 2170 || Loss: 3.7494 || timer: 0.0810 sec.
iter 2180 || Loss: 4.0418 || timer: 0.0845 sec.
iter 2190 || Loss: 3.1318 || timer: 0.1005 sec.
iter 2200 || Loss: 2.9279 || timer: 0.0181 sec.
iter 2210 || Loss: 2.3457 || timer: 0.0991 sec.
iter 2220 || Loss: 3.2974 || timer: 0.0723 sec.
iter 2230 || Loss: 3.2132 || timer: 0.0773 sec.
iter 2240 || Loss: 3.3639 || timer: 0.0802 sec.
iter 2250 || Loss: 2.7363 || timer: 0.0800 sec.
iter 2260 || Loss: 2.7691 || timer: 0.0830 sec.
iter 2270 || Loss: 2.6386 || timer: 0.0736 sec.
iter 2280 || Loss: 3.4642 || timer: 0.0815 sec.
iter 2290 || Loss: 3.6040 || timer: 0.0824 sec.
iter 2300 || Loss: 2.9506 || timer: 0.0883 sec.
iter 2310 || Loss: 3.1488 || timer: 0.0798 sec.
iter 2320 || Loss: 3.1813 || timer: 0.0807 sec.
iter 2330 || Loss: 2.6412 || timer: 0.0862 sec.
iter 2340 || Loss: 2.9163 || timer: 0.0741 sec.
iter 2350 || Loss: 2.5563 || timer: 0.0783 sec.
iter 2360 || Loss: 2.8392 || timer: 0.0777 sec.
iter 2370 || Loss: 2.7586 || timer: 0.0815 sec.
iter 2380 || Loss: 3.1538 || timer: 0.0762 sec.
iter 2390 || Loss: 2.9799 || timer: 0.1044 sec.
iter 2400 || Loss: 2.8461 || timer: 0.0912 sec.
iter 2410 || Loss: 2.9287 || timer: 0.0795 sec.
iter 2420 || Loss: 2.7607 || timer: 0.0786 sec.
iter 2430 || Loss: 3.0571 || timer: 0.0815 sec.
iter 2440 || Loss: 2.3172 || timer: 0.0833 sec.
iter 2450 || Loss: 3.1242 || timer: 0.0739 sec.
iter 2460 || Loss: 2.4685 || timer: 0.0870 sec.
iter 2470 || Loss: 3.0366 || timer: 0.0721 sec.
iter 2480 || Loss: 3.0271 || timer: 0.0804 sec.
iter 2490 || Loss: 2.7557 || timer: 0.0795 sec.
iter 2500 || Loss: 2.5608 || timer: 0.0735 sec.
iter 2510 || Loss: 2.6846 || timer: 0.0737 sec.
iter 2520 || Loss: 2.9849 || timer: 0.0947 sec.
iter 2530 || Loss: 2.8778 || timer: 0.0240 sec.
iter 2540 || Loss: 2.8177 || timer: 0.0809 sec.
iter 2550 || Loss: 3.0302 || timer: 0.0864 sec.
iter 2560 || Loss: 2.8691 || timer: 0.0737 sec.
iter 2570 || Loss: 2.3464 || timer: 0.0796 sec.
iter 2580 || Loss: 2.7373 || timer: 0.0788 sec.
iter 2590 || Loss: 2.6743 || timer: 0.0820 sec.
iter 2600 || Loss: 2.4559 || timer: 0.0858 sec.
iter 2610 || Loss: 2.6011 || timer: 0.0797 sec.
iter 2620 || Loss: 2.8550 || timer: 0.0800 sec.
iter 2630 || Loss: 2.7487 || timer: 0.1045 sec.
iter 2640 || Loss: 2.9276 || timer: 0.0759 sec.
iter 2650 || Loss: 2.6989 || timer: 0.0733 sec.
iter 2660 || Loss: 2.8811 || timer: 0.0769 sec.
iter 2670 || Loss: 2.8887 || timer: 0.0726 sec.
iter 2680 || Loss: 2.9756 || timer: 0.1066 sec.
iter 2690 || Loss: 2.8400 || timer: 0.0962 sec.
iter 2700 || Loss: 2.6502 || timer: 0.0800 sec.
iter 2710 || Loss: 3.5780 || timer: 0.0840 sec.
iter 2720 || Loss: 2.8040 || timer: 0.0786 sec.
iter 2730 || Loss: 2.7743 || timer: 0.0910 sec.
iter 2740 || Loss: 3.9059 || timer: 0.0924 sec.
iter 2750 || Loss: 3.3885 || timer: 0.0748 sec.
iter 2760 || Loss: 3.5269 || timer: 0.0829 sec.
iter 2770 || Loss: 3.5933 || timer: 0.0757 sec.
iter 2780 || Loss: 3.1413 || timer: 0.0738 sec.
iter 2790 || Loss: 2.5452 || timer: 0.0917 sec.
iter 2800 || Loss: 2.6779 || timer: 0.0757 sec.
iter 2810 || Loss: 3.2371 || timer: 0.0811 sec.
iter 2820 || Loss: 2.8419 || timer: 0.0802 sec.
iter 2830 || Loss: 3.2850 || timer: 0.0758 sec.
iter 2840 || Loss: 2.7279 || timer: 0.0828 sec.
iter 2850 || Loss: 3.1732 || timer: 0.0742 sec.
iter 2860 || Loss: 3.4371 || timer: 0.0269 sec.
iter 2870 || Loss: 5.1173 || timer: 0.0830 sec.
iter 2880 || Loss: 3.7733 || timer: 0.0819 sec.
iter 2890 || Loss: 3.4038 || timer: 0.0834 sec.
iter 2900 || Loss: 3.3507 || timer: 0.0820 sec.
iter 2910 || Loss: 3.1363 || timer: 0.0937 sec.
iter 2920 || Loss: 2.8332 || timer: 0.0800 sec.
iter 2930 || Loss: 2.5766 || timer: 0.0860 sec.
iter 2940 || Loss: 2.4918 || timer: 0.0751 sec.
iter 2950 || Loss: 3.1099 || timer: 0.0824 sec.
iter 2960 || Loss: 2.5149 || timer: 0.1090 sec.
iter 2970 || Loss: 3.1737 || timer: 0.1015 sec.
iter 2980 || Loss: 2.4310 || timer: 0.0833 sec.
iter 2990 || Loss: 3.0330 || timer: 0.0785 sec.
iter 3000 || Loss: 2.6170 || timer: 0.0811 sec.
iter 3010 || Loss: 2.6863 || timer: 0.0789 sec.
iter 3020 || Loss: 2.8670 || timer: 0.0830 sec.
iter 3030 || Loss: 2.6565 || timer: 0.0740 sec.
iter 3040 || Loss: 2.6894 || timer: 0.0735 sec.
iter 3050 || Loss: 3.5615 || timer: 0.0835 sec.
iter 3060 || Loss: 2.3096 || timer: 0.0762 sec.
iter 3070 || Loss: 4.3983 || timer: 0.0897 sec.
iter 3080 || Loss: 4.2194 || timer: 0.0826 sec.
iter 3090 || Loss: 3.1924 || timer: 0.0736 sec.
iter 3100 || Loss: 3.5870 || timer: 0.0776 sec.
iter 3110 || Loss: 2.6453 || timer: 0.0863 sec.
iter 3120 || Loss: 3.3242 || timer: 0.0729 sec.
iter 3130 || Loss: 3.0989 || timer: 0.0940 sec.
iter 3140 || Loss: 4.4276 || timer: 0.0820 sec.
iter 3150 || Loss: 4.0181 || timer: 0.0810 sec.
iter 3160 || Loss: 4.0643 || timer: 0.0751 sec.
iter 3170 || Loss: 4.3064 || timer: 0.0806 sec.
iter 3180 || Loss: 4.0124 || timer: 0.0794 sec.
iter 3190 || Loss: 2.8059 || timer: 0.0257 sec.
iter 3200 || Loss: 2.4171 || timer: 0.0904 sec.
iter 3210 || Loss: 2.9124 || timer: 0.0839 sec.
iter 3220 || Loss: 3.0783 || timer: 0.0805 sec.
iter 3230 || Loss: 2.7433 || timer: 0.0811 sec.
iter 3240 || Loss: 3.0941 || timer: 0.0751 sec.
iter 3250 || Loss: 2.7774 || timer: 0.0994 sec.
iter 3260 || Loss: 2.6259 || timer: 0.0831 sec.
iter 3270 || Loss: 3.3691 || timer: 0.0809 sec.
iter 3280 || Loss: 2.7196 || timer: 0.0874 sec.
iter 3290 || Loss: 2.4949 || timer: 0.0993 sec.
iter 3300 || Loss: 2.0554 || timer: 0.0810 sec.
iter 3310 || Loss: 3.1087 || timer: 0.0765 sec.
iter 3320 || Loss: 2.5486 || timer: 0.0812 sec.
iter 3330 || Loss: 2.3713 || timer: 0.0809 sec.
iter 3340 || Loss: 2.2687 || timer: 0.0828 sec.
iter 3350 || Loss: 2.6716 || timer: 0.0896 sec.
iter 3360 || Loss: 2.5538 || timer: 0.0737 sec.
iter 3370 || Loss: 3.5444 || timer: 0.0918 sec.
iter 3380 || Loss: 2.7526 || timer: 0.0947 sec.
iter 3390 || Loss: 2.7258 || timer: 0.0742 sec.
iter 3400 || Loss: 2.6643 || timer: 0.0820 sec.
iter 3410 || Loss: 2.5212 || timer: 0.0729 sec.
iter 3420 || Loss: 1.9646 || timer: 0.0843 sec.
iter 3430 || Loss: 2.6968 || timer: 0.0788 sec.
iter 3440 || Loss: 3.8091 || timer: 0.0948 sec.
iter 3450 || Loss: 3.1247 || timer: 0.0807 sec.
iter 3460 || Loss: 2.7881 || timer: 0.0823 sec.
iter 3470 || Loss: 2.7036 || timer: 0.0909 sec.
iter 3480 || Loss: 2.4699 || timer: 0.0855 sec.
iter 3490 || Loss: 2.5700 || timer: 0.0984 sec.
iter 3500 || Loss: 3.1467 || timer: 0.0899 sec.
iter 3510 || Loss: 2.2647 || timer: 0.0732 sec.
iter 3520 || Loss: 2.7024 || timer: 0.0248 sec.
iter 3530 || Loss: 1.8623 || timer: 0.0833 sec.
iter 3540 || Loss: 2.5192 || timer: 0.0820 sec.
iter 3550 || Loss: 2.3916 || timer: 0.0814 sec.
iter 3560 || Loss: 3.0699 || timer: 0.0857 sec.
iter 3570 || Loss: 2.6982 || timer: 0.0778 sec.
iter 3580 || Loss: 2.4974 || timer: 0.0818 sec.
iter 3590 || Loss: 2.6876 || timer: 0.0822 sec.
iter 3600 || Loss: 2.0825 || timer: 0.0823 sec.
iter 3610 || Loss: 2.3564 || timer: 0.0955 sec.
iter 3620 || Loss: 2.0381 || timer: 0.1074 sec.
iter 3630 || Loss: 3.1255 || timer: 0.0740 sec.
iter 3640 || Loss: 2.3781 || timer: 0.0740 sec.
iter 3650 || Loss: 2.3425 || timer: 0.0749 sec.
iter 3660 || Loss: 2.5378 || timer: 0.0739 sec.
iter 3670 || Loss: 2.9887 || timer: 0.0797 sec.
iter 3680 || Loss: 2.4220 || timer: 0.0732 sec.
iter 3690 || Loss: 2.3934 || timer: 0.0974 sec.
iter 3700 || Loss: 2.9710 || timer: 0.0819 sec.
iter 3710 || Loss: 3.2187 || timer: 0.0789 sec.
iter 3720 || Loss: 2.8460 || timer: 0.0820 sec.
iter 3730 || Loss: 2.6923 || timer: 0.0742 sec.
iter 3740 || Loss: 3.1941 || timer: 0.0742 sec.
iter 3750 || Loss: 2.5896 || timer: 0.0729 sec.
iter 3760 || Loss: 2.5648 || timer: 0.1032 sec.
iter 3770 || Loss: 3.0934 || timer: 0.0769 sec.
iter 3780 || Loss: 2.5052 || timer: 0.1071 sec.
iter 3790 || Loss: 2.2678 || timer: 0.0733 sec.
iter 3800 || Loss: 2.5773 || timer: 0.0804 sec.
iter 3810 || Loss: 2.5863 || timer: 0.0768 sec.
iter 3820 || Loss: 2.2115 || timer: 0.0724 sec.
iter 3830 || Loss: 2.3834 || timer: 0.0776 sec.
iter 3840 || Loss: 2.8139 || timer: 0.0835 sec.
iter 3850 || Loss: 2.7773 || timer: 0.0223 sec.
iter 3860 || Loss: 2.3551 || timer: 0.0754 sec.
iter 3870 || Loss: 2.8489 || timer: 0.0820 sec.
iter 3880 || Loss: 2.2926 || timer: 0.0811 sec.
iter 3890 || Loss: 2.2648 || timer: 0.0804 sec.
iter 3900 || Loss: 2.5048 || timer: 0.0866 sec.
iter 3910 || Loss: 2.6839 || timer: 0.0752 sec.
iter 3920 || Loss: 2.2847 || timer: 0.0771 sec.
iter 3930 || Loss: 2.7614 || timer: 0.0722 sec.
iter 3940 || Loss: 2.4291 || timer: 0.0732 sec.
iter 3950 || Loss: 2.0792 || timer: 0.1109 sec.
iter 3960 || Loss: 2.5089 || timer: 0.0845 sec.
iter 3970 || Loss: 2.2183 || timer: 0.0821 sec.
iter 3980 || Loss: 2.5409 || timer: 0.0749 sec.
iter 3990 || Loss: 2.2078 || timer: 0.0800 sec.
iter 4000 || Loss: 2.0793 || timer: 0.0745 sec.
iter 4010 || Loss: 2.3768 || timer: 0.0792 sec.
iter 4020 || Loss: 2.1876 || timer: 0.0894 sec.
iter 4030 || Loss: 2.0812 || timer: 0.0798 sec.
iter 4040 || Loss: 2.6283 || timer: 0.0782 sec.
iter 4050 || Loss: 2.1560 || timer: 0.0837 sec.
iter 4060 || Loss: 2.4166 || timer: 0.0829 sec.
iter 4070 || Loss: 2.2643 || timer: 0.0666 sec.
iter 4080 || Loss: 2.4504 || timer: 0.0793 sec.
iter 4090 || Loss: 2.5519 || timer: 0.0984 sec.
iter 4100 || Loss: 2.6685 || timer: 0.0802 sec.
iter 4110 || Loss: 2.7526 || timer: 0.0885 sec.
iter 4120 || Loss: 2.1479 || timer: 0.0806 sec.
iter 4130 || Loss: 2.5572 || timer: 0.0815 sec.
iter 4140 || Loss: 2.6445 || timer: 0.0827 sec.
iter 4150 || Loss: 2.4807 || timer: 0.0816 sec.
iter 4160 || Loss: 3.1760 || timer: 0.0811 sec.
iter 4170 || Loss: 2.2223 || timer: 0.0811 sec.
iter 4180 || Loss: 2.2741 || timer: 0.0237 sec.
iter 4190 || Loss: 1.6309 || timer: 0.0852 sec.
iter 4200 || Loss: 2.2202 || timer: 0.0724 sec.
iter 4210 || Loss: 2.3936 || timer: 0.0833 sec.
iter 4220 || Loss: 2.1792 || timer: 0.0735 sec.
iter 4230 || Loss: 2.3788 || timer: 0.0815 sec.
iter 4240 || Loss: 2.8259 || timer: 0.0810 sec.
iter 4250 || Loss: 2.3707 || timer: 0.0744 sec.
iter 4260 || Loss: 2.2568 || timer: 0.0852 sec.
iter 4270 || Loss: 1.9413 || timer: 0.0772 sec.
iter 4280 || Loss: 3.0591 || timer: 0.0796 sec.
iter 4290 || Loss: 2.0810 || timer: 0.0744 sec.
iter 4300 || Loss: 2.3127 || timer: 0.0809 sec.
iter 4310 || Loss: 2.3056 || timer: 0.1032 sec.
iter 4320 || Loss: 2.4961 || timer: 0.0832 sec.
iter 4330 || Loss: 2.3823 || timer: 0.0796 sec.
iter 4340 || Loss: 1.9878 || timer: 0.0919 sec.
iter 4350 || Loss: 2.7981 || timer: 0.0803 sec.
iter 4360 || Loss: 2.8060 || timer: 0.1028 sec.
iter 4370 || Loss: 2.5759 || timer: 0.0914 sec.
iter 4380 || Loss: 2.6732 || timer: 0.0851 sec.
iter 4390 || Loss: 2.5661 || timer: 0.0732 sec.
iter 4400 || Loss: 2.0237 || timer: 0.0736 sec.
iter 4410 || Loss: 2.7279 || timer: 0.0799 sec.
iter 4420 || Loss: 2.3874 || timer: 0.0856 sec.
iter 4430 || Loss: 2.4781 || timer: 0.0818 sec.
iter 4440 || Loss: 2.7428 || timer: 0.0743 sec.
iter 4450 || Loss: 2.0350 || timer: 0.0933 sec.
iter 4460 || Loss: 2.6946 || timer: 0.0820 sec.
iter 4470 || Loss: 2.6531 || timer: 0.0862 sec.
iter 4480 || Loss: 3.2754 || timer: 0.0839 sec.
iter 4490 || Loss: 2.7087 || timer: 0.0743 sec.
iter 4500 || Loss: 3.1318 || timer: 0.0756 sec.
iter 4510 || Loss: 2.9891 || timer: 0.0246 sec.
iter 4520 || Loss: 1.3050 || timer: 0.0832 sec.
iter 4530 || Loss: 2.6896 || timer: 0.0821 sec.
iter 4540 || Loss: 2.0963 || timer: 0.0649 sec.
iter 4550 || Loss: 2.3046 || timer: 0.0809 sec.
iter 4560 || Loss: 2.3838 || timer: 0.0717 sec.
iter 4570 || Loss: 3.1490 || timer: 0.0669 sec.
iter 4580 || Loss: 2.6010 || timer: 0.0845 sec.
iter 4590 || Loss: 2.1908 || timer: 0.0750 sec.
iter 4600 || Loss: 2.3395 || timer: 0.0965 sec.
iter 4610 || Loss: 2.7786 || timer: 0.0899 sec.
iter 4620 || Loss: 3.1465 || timer: 0.0801 sec.
iter 4630 || Loss: 2.9085 || timer: 0.0764 sec.
iter 4640 || Loss: 2.9059 || timer: 0.0805 sec.
iter 4650 || Loss: 1.8090 || timer: 0.0755 sec.
iter 4660 || Loss: 2.7795 || timer: 0.0835 sec.
iter 4670 || Loss: 2.3912 || timer: 0.0849 sec.
iter 4680 || Loss: 2.9889 || timer: 0.0799 sec.
iter 4690 || Loss: 3.3603 || timer: 0.0889 sec.
iter 4700 || Loss: 2.3503 || timer: 0.0849 sec.
iter 4710 || Loss: 2.4372 || timer: 0.0738 sec.
iter 4720 || Loss: 2.4395 || timer: 0.1013 sec.
iter 4730 || Loss: 3.0162 || timer: 0.0907 sec.
iter 4740 || Loss: 2.7728 || timer: 0.0807 sec.
iter 4750 || Loss: 2.6171 || timer: 0.0843 sec.
iter 4760 || Loss: 2.7233 || timer: 0.0875 sec.
iter 4770 || Loss: 2.5208 || timer: 0.0734 sec.
iter 4780 || Loss: 2.7271 || timer: 0.0806 sec.
iter 4790 || Loss: 2.6531 || timer: 0.0990 sec.
iter 4800 || Loss: 2.3875 || timer: 0.0785 sec.
iter 4810 || Loss: 2.5975 || timer: 0.0742 sec.
iter 4820 || Loss: 2.2640 || timer: 0.0836 sec.
iter 4830 || Loss: 2.7511 || timer: 0.0737 sec.
iter 4840 || Loss: 2.3266 || timer: 0.0289 sec.
iter 4850 || Loss: 0.9414 || timer: 0.0960 sec.
iter 4860 || Loss: 2.9205 || timer: 0.0741 sec.
iter 4870 || Loss: 2.3001 || timer: 0.0830 sec.
iter 4880 || Loss: 2.8184 || timer: 0.0807 sec.
iter 4890 || Loss: 2.3533 || timer: 0.0813 sec.
iter 4900 || Loss: 2.5015 || timer: 0.0920 sec.
iter 4910 || Loss: 2.1977 || timer: 0.0843 sec.
iter 4920 || Loss: 2.3452 || timer: 0.0801 sec.
iter 4930 || Loss: 2.4621 || timer: 0.0810 sec.
iter 4940 || Loss: 2.0432 || timer: 0.1066 sec.
iter 4950 || Loss: 2.2172 || timer: 0.1016 sec.
iter 4960 || Loss: 2.3029 || timer: 0.0810 sec.
iter 4970 || Loss: 2.8231 || timer: 0.0842 sec.
iter 4980 || Loss: 1.9223 || timer: 0.0741 sec.
iter 4990 || Loss: 3.0418 || timer: 0.0800 sec.
iter 5000 || Loss: 1.8417 || Saving state, iter: 5000
timer: 0.0794 sec.
iter 5010 || Loss: 2.3142 || timer: 0.0814 sec.
iter 5020 || Loss: 2.6131 || timer: 0.0796 sec.
iter 5030 || Loss: 2.3829 || timer: 0.0819 sec.
iter 5040 || Loss: 3.1812 || timer: 0.1017 sec.
iter 5050 || Loss: 1.8699 || timer: 0.0789 sec.
iter 5060 || Loss: 2.0483 || timer: 0.0929 sec.
iter 5070 || Loss: 2.1464 || timer: 0.0796 sec.
iter 5080 || Loss: 2.2505 || timer: 0.0982 sec.
iter 5090 || Loss: 2.0439 || timer: 0.0817 sec.
iter 5100 || Loss: 2.2772 || timer: 0.0726 sec.
iter 5110 || Loss: 2.0786 || timer: 0.0762 sec.
iter 5120 || Loss: 3.3126 || timer: 0.1180 sec.
iter 5130 || Loss: 2.7099 || timer: 0.0770 sec.
iter 5140 || Loss: 2.3119 || timer: 0.0787 sec.
iter 5150 || Loss: 3.5163 || timer: 0.0794 sec.
iter 5160 || Loss: 2.7792 || timer: 0.0815 sec.
iter 5170 || Loss: 2.4322 || timer: 0.0309 sec.
iter 5180 || Loss: 2.6109 || timer: 0.0825 sec.
iter 5190 || Loss: 2.3816 || timer: 0.0800 sec.
iter 5200 || Loss: 2.7292 || timer: 0.1157 sec.
iter 5210 || Loss: 2.4733 || timer: 0.0740 sec.
iter 5220 || Loss: 2.9593 || timer: 0.0872 sec.
iter 5230 || Loss: 3.1724 || timer: 0.0768 sec.
iter 5240 || Loss: 2.6016 || timer: 0.0726 sec.
iter 5250 || Loss: 2.5513 || timer: 0.0978 sec.
iter 5260 || Loss: 2.2891 || timer: 0.0816 sec.
iter 5270 || Loss: 2.5297 || timer: 0.0826 sec.
iter 5280 || Loss: 2.1307 || timer: 0.0734 sec.
iter 5290 || Loss: 2.1199 || timer: 0.0755 sec.
iter 5300 || Loss: 2.3362 || timer: 0.0746 sec.
iter 5310 || Loss: 2.1199 || timer: 0.0953 sec.
iter 5320 || Loss: 2.2072 || timer: 0.0791 sec.
iter 5330 || Loss: 1.9843 || timer: 0.0752 sec.
iter 5340 || Loss: 2.4276 || timer: 0.0785 sec.
iter 5350 || Loss: 2.7509 || timer: 0.0828 sec.
iter 5360 || Loss: 2.3875 || timer: 0.0796 sec.
iter 5370 || Loss: 2.5749 || timer: 0.0836 sec.
iter 5380 || Loss: 2.5980 || timer: 0.0768 sec.
iter 5390 || Loss: 2.5281 || timer: 0.0762 sec.
iter 5400 || Loss: 2.0547 || timer: 0.0803 sec.
iter 5410 || Loss: 2.4700 || timer: 0.0802 sec.
iter 5420 || Loss: 2.1345 || timer: 0.0832 sec.
iter 5430 || Loss: 2.2354 || timer: 0.0828 sec.
iter 5440 || Loss: 2.3594 || timer: 0.0830 sec.
iter 5450 || Loss: 2.3915 || timer: 0.0728 sec.
iter 5460 || Loss: 2.5631 || timer: 0.0836 sec.
iter 5470 || Loss: 2.9070 || timer: 0.0816 sec.
iter 5480 || Loss: 2.8544 || timer: 0.0947 sec.
iter 5490 || Loss: 2.3328 || timer: 0.0840 sec.
iter 5500 || Loss: 2.0934 || timer: 0.0187 sec.
iter 5510 || Loss: 2.5990 || timer: 0.0776 sec.
iter 5520 || Loss: 2.7049 || timer: 0.0800 sec.
iter 5530 || Loss: 1.9711 || timer: 0.0817 sec.
iter 5540 || Loss: 2.3958 || timer: 0.0848 sec.
iter 5550 || Loss: 2.2030 || timer: 0.0736 sec.
iter 5560 || Loss: 1.8449 || timer: 0.0879 sec.
iter 5570 || Loss: 2.0490 || timer: 0.0752 sec.
iter 5580 || Loss: 2.7809 || timer: 0.0732 sec.
iter 5590 || Loss: 2.7588 || timer: 0.0997 sec.
iter 5600 || Loss: 1.9686 || timer: 0.0791 sec.
iter 5610 || Loss: 2.0643 || timer: 0.0864 sec.
iter 5620 || Loss: 1.9592 || timer: 0.0820 sec.
iter 5630 || Loss: 2.4692 || timer: 0.0822 sec.
iter 5640 || Loss: 2.3231 || timer: 0.0955 sec.
iter 5650 || Loss: 2.7918 || timer: 0.0960 sec.
iter 5660 || Loss: 1.8344 || timer: 0.0723 sec.
iter 5670 || Loss: 1.7183 || timer: 0.0778 sec.
iter 5680 || Loss: 2.2946 || timer: 0.0815 sec.
iter 5690 || Loss: 2.3784 || timer: 0.0982 sec.
iter 5700 || Loss: 2.4300 || timer: 0.0797 sec.
iter 5710 || Loss: 1.9968 || timer: 0.0871 sec.
iter 5720 || Loss: 2.6928 || timer: 0.0736 sec.
iter 5730 || Loss: 1.9311 || timer: 0.0832 sec.
iter 5740 || Loss: 2.6795 || timer: 0.1101 sec.
iter 5750 || Loss: 2.4547 || timer: 0.1150 sec.
iter 5760 || Loss: 2.3253 || timer: 0.0861 sec.
iter 5770 || Loss: 2.2331 || timer: 0.1005 sec.
iter 5780 || Loss: 2.6612 || timer: 0.0821 sec.
iter 5790 || Loss: 2.7381 || timer: 0.0801 sec.
iter 5800 || Loss: 2.9661 || timer: 0.0836 sec.
iter 5810 || Loss: 2.9231 || timer: 0.0862 sec.
iter 5820 || Loss: 2.2216 || timer: 0.0912 sec.
iter 5830 || Loss: 1.9226 || timer: 0.0287 sec.
iter 5840 || Loss: 1.6094 || timer: 0.0913 sec.
iter 5850 || Loss: 2.9699 || timer: 0.0847 sec.
iter 5860 || Loss: 2.2759 || timer: 0.0739 sec.
iter 5870 || Loss: 2.5556 || timer: 0.0863 sec.
iter 5880 || Loss: 2.6627 || timer: 0.0800 sec.
iter 5890 || Loss: 2.5693 || timer: 0.0806 sec.
iter 5900 || Loss: 2.4895 || timer: 0.0787 sec.
iter 5910 || Loss: 2.4237 || timer: 0.0833 sec.
iter 5920 || Loss: 2.3366 || timer: 0.0838 sec.
iter 5930 || Loss: 2.5169 || timer: 0.0862 sec.
iter 5940 || Loss: 2.1481 || timer: 0.0804 sec.
iter 5950 || Loss: 2.4789 || timer: 0.0802 sec.
iter 5960 || Loss: 2.4724 || timer: 0.0785 sec.
iter 5970 || Loss: 2.5843 || timer: 0.0765 sec.
iter 5980 || Loss: 2.6104 || timer: 0.0867 sec.
iter 5990 || Loss: 2.3353 || timer: 0.0765 sec.
iter 6000 || Loss: 2.3968 || timer: 0.0805 sec.
iter 6010 || Loss: 2.6765 || timer: 0.0732 sec.
iter 6020 || Loss: 2.2275 || timer: 0.0884 sec.
iter 6030 || Loss: 2.3113 || timer: 0.0908 sec.
iter 6040 || Loss: 2.2308 || timer: 0.0796 sec.
iter 6050 || Loss: 2.5602 || timer: 0.0739 sec.
iter 6060 || Loss: 1.9914 || timer: 0.0796 sec.
iter 6070 || Loss: 2.4689 || timer: 0.0780 sec.
iter 6080 || Loss: 1.9447 || timer: 0.0831 sec.
iter 6090 || Loss: 2.5075 || timer: 0.0838 sec.
iter 6100 || Loss: 1.7233 || timer: 0.0948 sec.
iter 6110 || Loss: 2.1355 || timer: 0.0731 sec.
iter 6120 || Loss: 1.8734 || timer: 0.0783 sec.
iter 6130 || Loss: 2.2468 || timer: 0.0728 sec.
iter 6140 || Loss: 2.6477 || timer: 0.0870 sec.
iter 6150 || Loss: 2.9818 || timer: 0.0786 sec.
iter 6160 || Loss: 3.0710 || timer: 0.0274 sec.
iter 6170 || Loss: 1.9090 || timer: 0.0790 sec.
iter 6180 || Loss: 2.6454 || timer: 0.0931 sec.
iter 6190 || Loss: 2.8480 || timer: 0.0702 sec.
iter 6200 || Loss: 1.9075 || timer: 0.0769 sec.
iter 6210 || Loss: 2.1236 || timer: 0.0747 sec.
iter 6220 || Loss: 1.9820 || timer: 0.0805 sec.
iter 6230 || Loss: 2.4486 || timer: 0.0830 sec.
iter 6240 || Loss: 2.1790 || timer: 0.0814 sec.
iter 6250 || Loss: 1.8760 || timer: 0.0781 sec.
iter 6260 || Loss: 2.3192 || timer: 0.0854 sec.
iter 6270 || Loss: 2.1239 || timer: 0.0871 sec.
iter 6280 || Loss: 1.9318 || timer: 0.0757 sec.
iter 6290 || Loss: 2.1008 || timer: 0.0726 sec.
iter 6300 || Loss: 2.3805 || timer: 0.0829 sec.
iter 6310 || Loss: 2.1902 || timer: 0.0768 sec.
iter 6320 || Loss: 2.1086 || timer: 0.0974 sec.
iter 6330 || Loss: 2.2956 || timer: 0.0974 sec.
iter 6340 || Loss: 2.3185 || timer: 0.0722 sec.
iter 6350 || Loss: 2.1798 || timer: 0.0715 sec.
iter 6360 || Loss: 1.9355 || timer: 0.0725 sec.
iter 6370 || Loss: 2.8439 || timer: 0.0744 sec.
iter 6380 || Loss: 2.8746 || timer: 0.0800 sec.
iter 6390 || Loss: 3.0958 || timer: 0.0710 sec.
iter 6400 || Loss: 2.0518 || timer: 0.1005 sec.
iter 6410 || Loss: 2.2534 || timer: 0.0796 sec.
iter 6420 || Loss: 2.1599 || timer: 0.0792 sec.
iter 6430 || Loss: 2.5074 || timer: 0.0938 sec.
iter 6440 || Loss: 2.4248 || timer: 0.0804 sec.
iter 6450 || Loss: 2.2011 || timer: 0.0825 sec.
iter 6460 || Loss: 2.8994 || timer: 0.0734 sec.
iter 6470 || Loss: 2.8507 || timer: 0.0840 sec.
iter 6480 || Loss: 2.7677 || timer: 0.0808 sec.
iter 6490 || Loss: 2.0033 || timer: 0.0261 sec.
iter 6500 || Loss: 3.7351 || timer: 0.0954 sec.
iter 6510 || Loss: 2.7763 || timer: 0.0723 sec.
iter 6520 || Loss: 2.5480 || timer: 0.1195 sec.
iter 6530 || Loss: 2.0299 || timer: 0.0789 sec.
iter 6540 || Loss: 2.0218 || timer: 0.0865 sec.
iter 6550 || Loss: 2.4941 || timer: 0.0730 sec.
iter 6560 || Loss: 1.9825 || timer: 0.0730 sec.
iter 6570 || Loss: 1.9710 || timer: 0.0976 sec.
iter 6580 || Loss: 3.6272 || timer: 0.1059 sec.
iter 6590 || Loss: 3.0472 || timer: 0.0910 sec.
iter 6600 || Loss: 2.2888 || timer: 0.1011 sec.
iter 6610 || Loss: 1.8513 || timer: 0.1041 sec.
iter 6620 || Loss: 2.5896 || timer: 0.0705 sec.
iter 6630 || Loss: 1.9188 || timer: 0.0728 sec.
iter 6640 || Loss: 2.4336 || timer: 0.0786 sec.
iter 6650 || Loss: 2.4018 || timer: 0.0764 sec.
iter 6660 || Loss: 2.2216 || timer: 0.0892 sec.
iter 6670 || Loss: 3.0222 || timer: 0.0721 sec.
iter 6680 || Loss: 2.3866 || timer: 0.0730 sec.
iter 6690 || Loss: 2.1575 || timer: 0.0805 sec.
iter 6700 || Loss: 2.4754 || timer: 0.0819 sec.
iter 6710 || Loss: 3.1172 || timer: 0.0828 sec.
iter 6720 || Loss: 3.5743 || timer: 0.0652 sec.
iter 6730 || Loss: 2.1933 || timer: 0.0820 sec.
iter 6740 || Loss: 2.2653 || timer: 0.0827 sec.
iter 6750 || Loss: 2.1618 || timer: 0.0795 sec.
iter 6760 || Loss: 1.8296 || timer: 0.0840 sec.
iter 6770 || Loss: 2.2841 || timer: 0.0739 sec.
iter 6780 || Loss: 1.8112 || timer: 0.0747 sec.
iter 6790 || Loss: 2.5940 || timer: 0.0781 sec.
iter 6800 || Loss: 1.8281 || timer: 0.0782 sec.
iter 6810 || Loss: 2.0411 || timer: 0.0736 sec.
iter 6820 || Loss: 2.3269 || timer: 0.0211 sec.
iter 6830 || Loss: 1.4650 || timer: 0.0774 sec.
iter 6840 || Loss: 2.2480 || timer: 0.0777 sec.
iter 6850 || Loss: 2.1108 || timer: 0.0782 sec.
iter 6860 || Loss: 1.9962 || timer: 0.0938 sec.
iter 6870 || Loss: 3.1656 || timer: 0.0807 sec.
iter 6880 || Loss: 2.2405 || timer: 0.0816 sec.
iter 6890 || Loss: 2.2344 || timer: 0.0810 sec.
iter 6900 || Loss: 2.6228 || timer: 0.0817 sec.
iter 6910 || Loss: 2.6566 || timer: 0.1028 sec.
iter 6920 || Loss: 2.1314 || timer: 0.0856 sec.
iter 6930 || Loss: 1.7896 || timer: 0.0752 sec.
iter 6940 || Loss: 2.0165 || timer: 0.0998 sec.
iter 6950 || Loss: 2.2722 || timer: 0.0829 sec.
iter 6960 || Loss: 2.2111 || timer: 0.0773 sec.
iter 6970 || Loss: 2.4042 || timer: 0.0721 sec.
iter 6980 || Loss: 2.2260 || timer: 0.0746 sec.
iter 6990 || Loss: 2.4506 || timer: 0.0817 sec.
iter 7000 || Loss: 2.8559 || timer: 0.0794 sec.
iter 7010 || Loss: 2.8558 || timer: 0.0764 sec.
iter 7020 || Loss: 2.6107 || timer: 0.0803 sec.
iter 7030 || Loss: 2.5256 || timer: 0.0805 sec.
iter 7040 || Loss: 1.8785 || timer: 0.0786 sec.
iter 7050 || Loss: 2.2750 || timer: 0.0778 sec.
iter 7060 || Loss: 2.0638 || timer: 0.0797 sec.
iter 7070 || Loss: 2.3432 || timer: 0.0818 sec.
iter 7080 || Loss: 1.9693 || timer: 0.0721 sec.
iter 7090 || Loss: 1.7818 || timer: 0.0724 sec.
iter 7100 || Loss: 1.9047 || timer: 0.0719 sec.
iter 7110 || Loss: 2.5526 || timer: 0.0716 sec.
iter 7120 || Loss: 1.7656 || timer: 0.0806 sec.
iter 7130 || Loss: 4.2171 || timer: 0.0760 sec.
iter 7140 || Loss: 3.1290 || timer: 0.0731 sec.
iter 7150 || Loss: 2.7704 || timer: 0.0258 sec.
iter 7160 || Loss: 1.6627 || timer: 0.0726 sec.
iter 7170 || Loss: 2.7514 || timer: 0.0872 sec.
iter 7180 || Loss: 2.2455 || timer: 0.0718 sec.
iter 7190 || Loss: 2.0451 || timer: 0.0780 sec.
iter 7200 || Loss: 2.0173 || timer: 0.1038 sec.
iter 7210 || Loss: 1.9261 || timer: 0.0808 sec.
iter 7220 || Loss: 2.0140 || timer: 0.0725 sec.
iter 7230 || Loss: 1.9129 || timer: 0.0793 sec.
iter 7240 || Loss: 1.9427 || timer: 0.0815 sec.
iter 7250 || Loss: 2.0416 || timer: 0.1031 sec.
iter 7260 || Loss: 2.1796 || timer: 0.0967 sec.
iter 7270 || Loss: 2.1260 || timer: 0.0844 sec.
iter 7280 || Loss: 2.2947 || timer: 0.0733 sec.
iter 7290 || Loss: 2.3549 || timer: 0.0796 sec.
iter 7300 || Loss: 2.0199 || timer: 0.0709 sec.
iter 7310 || Loss: 2.1620 || timer: 0.0934 sec.
iter 7320 || Loss: 2.1347 || timer: 0.0985 sec.
iter 7330 || Loss: 2.0749 || timer: 0.0778 sec.
iter 7340 || Loss: 1.9013 || timer: 0.0811 sec.
iter 7350 || Loss: 2.0461 || timer: 0.0901 sec.
iter 7360 || Loss: 1.9778 || timer: 0.0944 sec.
iter 7370 || Loss: 2.1409 || timer: 0.0817 sec.
iter 7380 || Loss: 1.9293 || timer: 0.0829 sec.
iter 7390 || Loss: 2.0288 || timer: 0.0803 sec.
iter 7400 || Loss: 1.9860 || timer: 0.0943 sec.
iter 7410 || Loss: 2.0355 || timer: 0.0872 sec.
iter 7420 || Loss: 2.3273 || timer: 0.0805 sec.
iter 7430 || Loss: 1.9462 || timer: 0.0984 sec.
iter 7440 || Loss: 2.2431 || timer: 0.0755 sec.
iter 7450 || Loss: 2.6339 || timer: 0.0724 sec.
iter 7460 || Loss: 1.9665 || timer: 0.0718 sec.
iter 7470 || Loss: 2.0327 || timer: 0.0881 sec.
iter 7480 || Loss: 2.1816 || timer: 0.0187 sec.
iter 7490 || Loss: 1.7787 || timer: 0.0818 sec.
iter 7500 || Loss: 2.4479 || timer: 0.0845 sec.
iter 7510 || Loss: 1.9682 || timer: 0.0726 sec.
iter 7520 || Loss: 1.8885 || timer: 0.1031 sec.
iter 7530 || Loss: 2.3626 || timer: 0.0698 sec.
iter 7540 || Loss: 2.3457 || timer: 0.0756 sec.
iter 7550 || Loss: 2.5094 || timer: 0.0807 sec.
iter 7560 || Loss: 2.2039 || timer: 0.0651 sec.
iter 7570 || Loss: 2.0765 || timer: 0.0717 sec.
iter 7580 || Loss: 2.1102 || timer: 0.1417 sec.
iter 7590 || Loss: 2.8666 || timer: 0.0740 sec.
iter 7600 || Loss: 5.1400 || timer: 0.0847 sec.
iter 7610 || Loss: 4.5062 || timer: 0.0732 sec.
iter 7620 || Loss: 3.6120 || timer: 0.0726 sec.
iter 7630 || Loss: 3.1428 || timer: 0.0976 sec.
iter 7640 || Loss: 2.4252 || timer: 0.0822 sec.
iter 7650 || Loss: 2.0668 || timer: 0.1023 sec.
iter 7660 || Loss: 2.5730 || timer: 0.0752 sec.
iter 7670 || Loss: 2.8613 || timer: 0.0724 sec.
iter 7680 || Loss: 2.0869 || timer: 0.0717 sec.
iter 7690 || Loss: 2.5423 || timer: 0.0793 sec.
iter 7700 || Loss: 2.6315 || timer: 0.0657 sec.
iter 7710 || Loss: 2.3745 || timer: 0.0706 sec.
iter 7720 || Loss: 2.2754 || timer: 0.0854 sec.
iter 7730 || Loss: 2.3319 || timer: 0.0811 sec.
iter 7740 || Loss: 1.9724 || timer: 0.0729 sec.
iter 7750 || Loss: 2.5292 || timer: 0.0885 sec.
iter 7760 || Loss: 2.3304 || timer: 0.0720 sec.
iter 7770 || Loss: 2.4267 || timer: 0.0947 sec.
iter 7780 || Loss: 2.0601 || timer: 0.0752 sec.
iter 7790 || Loss: 2.8188 || timer: 0.0862 sec.
iter 7800 || Loss: 2.0846 || timer: 0.0817 sec.
iter 7810 || Loss: 2.0453 || timer: 0.0136 sec.
iter 7820 || Loss: 1.9481 || timer: 0.0724 sec.
iter 7830 || Loss: 2.1107 || timer: 0.0732 sec.
iter 7840 || Loss: 2.4307 || timer: 0.0744 sec.
iter 7850 || Loss: 2.1780 || timer: 0.0827 sec.
iter 7860 || Loss: 2.5737 || timer: 0.1078 sec.
iter 7870 || Loss: 2.5702 || timer: 0.0804 sec.
iter 7880 || Loss: 2.0114 || timer: 0.0725 sec.
iter 7890 || Loss: 2.4896 || timer: 0.1024 sec.
iter 7900 || Loss: 2.2767 || timer: 0.0846 sec.
iter 7910 || Loss: 2.3628 || timer: 0.1091 sec.
iter 7920 || Loss: 2.1084 || timer: 0.0816 sec.
iter 7930 || Loss: 1.7373 || timer: 0.0919 sec.
iter 7940 || Loss: 1.9336 || timer: 0.0710 sec.
iter 7950 || Loss: 1.9836 || timer: 0.0806 sec.
iter 7960 || Loss: 1.5874 || timer: 0.0791 sec.
iter 7970 || Loss: 1.8576 || timer: 0.0752 sec.
iter 7980 || Loss: 2.5704 || timer: 0.0830 sec.
iter 7990 || Loss: 2.5936 || timer: 0.0808 sec.
iter 8000 || Loss: 2.5213 || timer: 0.0791 sec.
iter 8010 || Loss: 2.4353 || timer: 0.0683 sec.
iter 8020 || Loss: 2.1758 || timer: 0.0727 sec.
iter 8030 || Loss: 2.1633 || timer: 0.0789 sec.
iter 8040 || Loss: 2.0953 || timer: 0.0838 sec.
iter 8050 || Loss: 2.0846 || timer: 0.0844 sec.
iter 8060 || Loss: 2.1817 || timer: 0.0647 sec.
iter 8070 || Loss: 1.6084 || timer: 0.0816 sec.
iter 8080 || Loss: 1.9342 || timer: 0.0727 sec.
iter 8090 || Loss: 1.7067 || timer: 0.0716 sec.
iter 8100 || Loss: 1.7561 || timer: 0.0944 sec.
iter 8110 || Loss: 1.7064 || timer: 0.1143 sec.
iter 8120 || Loss: 1.5677 || timer: 0.0811 sec.
iter 8130 || Loss: 1.6706 || timer: 0.0797 sec.
iter 8140 || Loss: 2.0867 || timer: 0.0207 sec.
iter 8150 || Loss: 1.4866 || timer: 0.0812 sec.
iter 8160 || Loss: 1.8963 || timer: 0.0727 sec.
iter 8170 || Loss: 1.4246 || timer: 0.0715 sec.
iter 8180 || Loss: 1.7368 || timer: 0.0703 sec.
iter 8190 || Loss: 2.6840 || timer: 0.0687 sec.
iter 8200 || Loss: 1.7203 || timer: 0.0788 sec.
iter 8210 || Loss: 1.8677 || timer: 0.0816 sec.
iter 8220 || Loss: 2.3162 || timer: 0.0722 sec.
iter 8230 || Loss: 1.9957 || timer: 0.0769 sec.
iter 8240 || Loss: 1.7354 || timer: 0.0869 sec.
iter 8250 || Loss: 1.8143 || timer: 0.0725 sec.
iter 8260 || Loss: 1.7772 || timer: 0.0810 sec.
iter 8270 || Loss: 2.3392 || timer: 0.0786 sec.
iter 8280 || Loss: 1.6080 || timer: 0.0805 sec.
iter 8290 || Loss: 1.6102 || timer: 0.0812 sec.
iter 8300 || Loss: 1.7178 || timer: 0.0720 sec.
iter 8310 || Loss: 1.8805 || timer: 0.0829 sec.
iter 8320 || Loss: 1.5416 || timer: 0.0949 sec.
iter 8330 || Loss: 1.7181 || timer: 0.0754 sec.
iter 8340 || Loss: 1.8224 || timer: 0.0867 sec.
iter 8350 || Loss: 1.7046 || timer: 0.0812 sec.
iter 8360 || Loss: 1.9851 || timer: 0.0814 sec.
iter 8370 || Loss: 1.7040 || timer: 0.0779 sec.
iter 8380 || Loss: 1.6959 || timer: 0.0799 sec.
iter 8390 || Loss: 2.0743 || timer: 0.0673 sec.
iter 8400 || Loss: 1.9150 || timer: 0.0918 sec.
iter 8410 || Loss: 1.6516 || timer: 0.0725 sec.
iter 8420 || Loss: 1.7653 || timer: 0.0647 sec.
iter 8430 || Loss: 2.0793 || timer: 0.0683 sec.
iter 8440 || Loss: 1.3859 || timer: 0.0884 sec.
iter 8450 || Loss: 1.4178 || timer: 0.0803 sec.
iter 8460 || Loss: 1.3225 || timer: 0.1004 sec.
iter 8470 || Loss: 1.7174 || timer: 0.0251 sec.
iter 8480 || Loss: 0.8987 || timer: 0.0720 sec.
iter 8490 || Loss: 1.7511 || timer: 0.0764 sec.
iter 8500 || Loss: 1.7146 || timer: 0.0744 sec.
iter 8510 || Loss: 1.7102 || timer: 0.0729 sec.
iter 8520 || Loss: 1.7536 || timer: 0.0859 sec.
iter 8530 || Loss: 1.8591 || timer: 0.0815 sec.
iter 8540 || Loss: 1.7211 || timer: 0.0804 sec.
iter 8550 || Loss: 1.7435 || timer: 0.0734 sec.
iter 8560 || Loss: 2.2958 || timer: 0.0726 sec.
iter 8570 || Loss: 1.8795 || timer: 0.1038 sec.
iter 8580 || Loss: 1.9590 || timer: 0.0801 sec.
iter 8590 || Loss: 1.6831 || timer: 0.0773 sec.
iter 8600 || Loss: 1.5292 || timer: 0.0794 sec.
iter 8610 || Loss: 1.4639 || timer: 0.0827 sec.
iter 8620 || Loss: 1.8696 || timer: 0.0811 sec.
iter 8630 || Loss: 2.2120 || timer: 0.0707 sec.
iter 8640 || Loss: 1.4486 || timer: 0.0741 sec.
iter 8650 || Loss: 1.9989 || timer: 0.0843 sec.
iter 8660 || Loss: 1.8810 || timer: 0.0800 sec.
iter 8670 || Loss: 1.5701 || timer: 0.0918 sec.
iter 8680 || Loss: 1.9449 || timer: 0.0795 sec.
iter 8690 || Loss: 2.4145 || timer: 0.0840 sec.
iter 8700 || Loss: 1.6335 || timer: 0.0705 sec.
iter 8710 || Loss: 1.6719 || timer: 0.0931 sec.
iter 8720 || Loss: 1.9098 || timer: 0.0798 sec.
iter 8730 || Loss: 2.0127 || timer: 0.0852 sec.
iter 8740 || Loss: 2.1620 || timer: 0.1199 sec.
iter 8750 || Loss: 2.2627 || timer: 0.0807 sec.
iter 8760 || Loss: 1.5635 || timer: 0.0809 sec.
iter 8770 || Loss: 1.9496 || timer: 0.0799 sec.
iter 8780 || Loss: 1.6986 || timer: 0.1086 sec.
iter 8790 || Loss: 1.7256 || timer: 0.0797 sec.
iter 8800 || Loss: 1.6386 || timer: 0.0227 sec.
iter 8810 || Loss: 1.9940 || timer: 0.0731 sec.
iter 8820 || Loss: 1.8886 || timer: 0.0729 sec.
iter 8830 || Loss: 2.4179 || timer: 0.0803 sec.
iter 8840 || Loss: 2.0635 || timer: 0.0853 sec.
iter 8850 || Loss: 2.1518 || timer: 0.0940 sec.
iter 8860 || Loss: 2.3034 || timer: 0.0766 sec.
iter 8870 || Loss: 1.7537 || timer: 0.0963 sec.
iter 8880 || Loss: 1.4559 || timer: 0.0793 sec.
iter 8890 || Loss: 1.9556 || timer: 0.0928 sec.
iter 8900 || Loss: 1.7559 || timer: 0.1081 sec.
iter 8910 || Loss: 1.8865 || timer: 0.0766 sec.
iter 8920 || Loss: 1.6721 || timer: 0.0791 sec.
iter 8930 || Loss: 1.7451 || timer: 0.0798 sec.
iter 8940 || Loss: 1.6407 || timer: 0.0788 sec.
iter 8950 || Loss: 1.8453 || timer: 0.1282 sec.
iter 8960 || Loss: 1.6827 || timer: 0.0733 sec.
iter 8970 || Loss: 1.8285 || timer: 0.1117 sec.
iter 8980 || Loss: 1.4045 || timer: 0.1245 sec.
iter 8990 || Loss: 1.3889 || timer: 0.0868 sec.
iter 9000 || Loss: 1.9891 || timer: 0.0807 sec.
iter 9010 || Loss: 1.9048 || timer: 0.0727 sec.
iter 9020 || Loss: 1.8064 || timer: 0.0931 sec.
iter 9030 || Loss: 1.7237 || timer: 0.0725 sec.
iter 9040 || Loss: 2.0234 || timer: 0.0798 sec.
iter 9050 || Loss: 1.7626 || timer: 0.0914 sec.
iter 9060 || Loss: 1.5258 || timer: 0.1122 sec.
iter 9070 || Loss: 1.7436 || timer: 0.0725 sec.
iter 9080 || Loss: 1.3266 || timer: 0.0807 sec.
iter 9090 || Loss: 1.6495 || timer: 0.0837 sec.
iter 9100 || Loss: 2.7051 || timer: 0.0988 sec.
iter 9110 || Loss: 1.5300 || timer: 0.0796 sec.
iter 9120 || Loss: 1.8278 || timer: 0.0989 sec.
iter 9130 || Loss: 1.8598 || timer: 0.0142 sec.
iter 9140 || Loss: 6.1537 || timer: 0.0753 sec.
iter 9150 || Loss: 1.8823 || timer: 0.0657 sec.
iter 9160 || Loss: 1.3442 || timer: 0.0791 sec.
iter 9170 || Loss: 1.8088 || timer: 0.0850 sec.
iter 9180 || Loss: 1.4414 || timer: 0.0798 sec.
iter 9190 || Loss: 1.6162 || timer: 0.0764 sec.
iter 9200 || Loss: 1.4337 || timer: 0.0837 sec.
iter 9210 || Loss: 1.9533 || timer: 0.0953 sec.
iter 9220 || Loss: 2.2216 || timer: 0.0783 sec.
iter 9230 || Loss: 2.2690 || timer: 0.0894 sec.
iter 9240 || Loss: 2.1384 || timer: 0.0903 sec.
iter 9250 || Loss: 1.9958 || timer: 0.1198 sec.
iter 9260 || Loss: 1.9992 || timer: 0.0801 sec.
iter 9270 || Loss: 2.0628 || timer: 0.0793 sec.
iter 9280 || Loss: 1.9131 || timer: 0.0802 sec.
iter 9290 || Loss: 1.5698 || timer: 0.0726 sec.
iter 9300 || Loss: 2.2246 || timer: 0.0772 sec.
iter 9310 || Loss: 1.9271 || timer: 0.0818 sec.
iter 9320 || Loss: 1.8718 || timer: 0.0716 sec.
iter 9330 || Loss: 1.4796 || timer: 0.0719 sec.
iter 9340 || Loss: 1.5775 || timer: 0.0795 sec.
iter 9350 || Loss: 1.7048 || timer: 0.0875 sec.
iter 9360 || Loss: 1.9666 || timer: 0.0783 sec.
iter 9370 || Loss: 1.8312 || timer: 0.0739 sec.
iter 9380 || Loss: 1.9859 || timer: 0.0831 sec.
iter 9390 || Loss: 2.0924 || timer: 0.0911 sec.
iter 9400 || Loss: 1.2702 || timer: 0.0740 sec.
iter 9410 || Loss: 2.2027 || timer: 0.0836 sec.
iter 9420 || Loss: 2.2293 || timer: 0.0825 sec.
iter 9430 || Loss: 1.6385 || timer: 0.0726 sec.
iter 9440 || Loss: 1.6082 || timer: 0.0815 sec.
iter 9450 || Loss: 1.6470 || timer: 0.0835 sec.
iter 9460 || Loss: 1.4837 || timer: 0.0245 sec.
iter 9470 || Loss: 1.3378 || timer: 0.0795 sec.
iter 9480 || Loss: 2.0503 || timer: 0.0841 sec.
iter 9490 || Loss: 2.3440 || timer: 0.0909 sec.
iter 9500 || Loss: 1.8544 || timer: 0.0809 sec.
iter 9510 || Loss: 1.4457 || timer: 0.0736 sec.
iter 9520 || Loss: 1.7792 || timer: 0.0849 sec.
iter 9530 || Loss: 1.7053 || timer: 0.0776 sec.
iter 9540 || Loss: 2.0065 || timer: 0.0853 sec.
iter 9550 || Loss: 1.4116 || timer: 0.0728 sec.
iter 9560 || Loss: 1.8262 || timer: 0.0883 sec.
iter 9570 || Loss: 1.8119 || timer: 0.0814 sec.
iter 9580 || Loss: 1.3582 || timer: 0.0722 sec.
iter 9590 || Loss: 1.6063 || timer: 0.0713 sec.
iter 9600 || Loss: 1.7961 || timer: 0.0808 sec.
iter 9610 || Loss: 1.9226 || timer: 0.0930 sec.
iter 9620 || Loss: 1.6348 || timer: 0.0790 sec.
iter 9630 || Loss: 1.8368 || timer: 0.0754 sec.
iter 9640 || Loss: 2.5683 || timer: 0.0832 sec.
iter 9650 || Loss: 1.8919 || timer: 0.0859 sec.
iter 9660 || Loss: 1.7975 || timer: 0.0819 sec.
iter 9670 || Loss: 1.8924 || timer: 0.0835 sec.
iter 9680 || Loss: 1.5386 || timer: 0.0734 sec.
iter 9690 || Loss: 1.8634 || timer: 0.0812 sec.
iter 9700 || Loss: 1.9685 || timer: 0.0832 sec.
iter 9710 || Loss: 1.5012 || timer: 0.0783 sec.
iter 9720 || Loss: 1.6395 || timer: 0.0730 sec.
iter 9730 || Loss: 1.9644 || timer: 0.0937 sec.
iter 9740 || Loss: 1.7320 || timer: 0.0732 sec.
iter 9750 || Loss: 1.7321 || timer: 0.0755 sec.
iter 9760 || Loss: 2.0647 || timer: 0.0726 sec.
iter 9770 || Loss: 1.4075 || timer: 0.0715 sec.
iter 9780 || Loss: 1.3631 || timer: 0.0782 sec.
iter 9790 || Loss: 1.9243 || timer: 0.0183 sec.
iter 9800 || Loss: 1.2034 || timer: 0.0756 sec.
iter 9810 || Loss: 1.6637 || timer: 0.0790 sec.
iter 9820 || Loss: 1.5906 || timer: 0.0794 sec.
iter 9830 || Loss: 1.8488 || timer: 0.0807 sec.
iter 9840 || Loss: 1.5995 || timer: 0.0814 sec.
iter 9850 || Loss: 1.8974 || timer: 0.0784 sec.
iter 9860 || Loss: 2.5336 || timer: 0.0901 sec.
iter 9870 || Loss: 1.8252 || timer: 0.0785 sec.
iter 9880 || Loss: 1.4640 || timer: 0.0661 sec.
iter 9890 || Loss: 1.6621 || timer: 0.0961 sec.
iter 9900 || Loss: 1.7291 || timer: 0.0803 sec.
iter 9910 || Loss: 1.6592 || timer: 0.0907 sec.
iter 9920 || Loss: 1.6690 || timer: 0.0895 sec.
iter 9930 || Loss: 1.7768 || timer: 0.1050 sec.
iter 9940 || Loss: 1.6566 || timer: 0.0807 sec.
iter 9950 || Loss: 1.5434 || timer: 0.0777 sec.
iter 9960 || Loss: 1.9680 || timer: 0.0858 sec.
iter 9970 || Loss: 2.1698 || timer: 0.0729 sec.
iter 9980 || Loss: 1.8562 || timer: 0.0743 sec.
iter 9990 || Loss: 1.7019 || timer: 0.0752 sec.
iter 10000 || Loss: 1.3864 || Saving state, iter: 10000
timer: 0.0766 sec.
iter 10010 || Loss: 2.1807 || timer: 0.0811 sec.
iter 10020 || Loss: 1.9785 || timer: 0.0822 sec.
iter 10030 || Loss: 1.6832 || timer: 0.0804 sec.
iter 10040 || Loss: 1.4873 || timer: 0.0807 sec.
iter 10050 || Loss: 1.5814 || timer: 0.0724 sec.
iter 10060 || Loss: 1.4828 || timer: 0.0926 sec.
iter 10070 || Loss: 1.5117 || timer: 0.0819 sec.
iter 10080 || Loss: 1.8518 || timer: 0.0925 sec.
iter 10090 || Loss: 1.2298 || timer: 0.0788 sec.
iter 10100 || Loss: 1.8357 || timer: 0.0809 sec.
iter 10110 || Loss: 1.5098 || timer: 0.0763 sec.
iter 10120 || Loss: 1.5773 || timer: 0.0294 sec.
iter 10130 || Loss: 0.4689 || timer: 0.0727 sec.
iter 10140 || Loss: 2.5126 || timer: 0.0941 sec.
iter 10150 || Loss: 1.8063 || timer: 0.0801 sec.
iter 10160 || Loss: 1.5026 || timer: 0.0747 sec.
iter 10170 || Loss: 1.7002 || timer: 0.0811 sec.
iter 10180 || Loss: 1.6383 || timer: 0.0738 sec.
iter 10190 || Loss: 1.3407 || timer: 0.0727 sec.
iter 10200 || Loss: 1.7011 || timer: 0.0958 sec.
iter 10210 || Loss: 1.6923 || timer: 0.0843 sec.
iter 10220 || Loss: 2.3476 || timer: 0.0876 sec.
iter 10230 || Loss: 1.7184 || timer: 0.0803 sec.
iter 10240 || Loss: 1.6250 || timer: 0.0876 sec.
iter 10250 || Loss: 1.8177 || timer: 0.0812 sec.
iter 10260 || Loss: 1.5551 || timer: 0.0662 sec.
iter 10270 || Loss: 2.2688 || timer: 0.0822 sec.
iter 10280 || Loss: 1.9090 || timer: 0.0785 sec.
iter 10290 || Loss: 2.0694 || timer: 0.1173 sec.
iter 10300 || Loss: 1.4722 || timer: 0.0923 sec.
iter 10310 || Loss: 1.8220 || timer: 0.0844 sec.
iter 10320 || Loss: 2.3069 || timer: 0.0780 sec.
iter 10330 || Loss: 1.8056 || timer: 0.0738 sec.
iter 10340 || Loss: 2.4978 || timer: 0.0785 sec.
iter 10350 || Loss: 1.6544 || timer: 0.0775 sec.
iter 10360 || Loss: 2.0508 || timer: 0.0825 sec.
iter 10370 || Loss: 1.6753 || timer: 0.0956 sec.
iter 10380 || Loss: 1.5708 || timer: 0.0744 sec.
iter 10390 || Loss: 1.9302 || timer: 0.0733 sec.
iter 10400 || Loss: 1.6229 || timer: 0.0984 sec.
iter 10410 || Loss: 1.6123 || timer: 0.0964 sec.
iter 10420 || Loss: 1.3372 || timer: 0.0784 sec.
iter 10430 || Loss: 2.0048 || timer: 0.0721 sec.
iter 10440 || Loss: 1.2545 || timer: 0.0737 sec.
iter 10450 || Loss: 1.8116 || timer: 0.0239 sec.
iter 10460 || Loss: 2.3460 || timer: 0.0838 sec.
iter 10470 || Loss: 2.1377 || timer: 0.0796 sec.
iter 10480 || Loss: 1.6855 || timer: 0.0776 sec.
iter 10490 || Loss: 1.9122 || timer: 0.0881 sec.
iter 10500 || Loss: 1.8769 || timer: 0.0907 sec.
iter 10510 || Loss: 1.9638 || timer: 0.0734 sec.
iter 10520 || Loss: 1.7024 || timer: 0.0737 sec.
iter 10530 || Loss: 1.1895 || timer: 0.0857 sec.
iter 10540 || Loss: 1.7762 || timer: 0.0736 sec.
iter 10550 || Loss: 2.0687 || timer: 0.0909 sec.
iter 10560 || Loss: 1.6098 || timer: 0.0984 sec.
iter 10570 || Loss: 1.6156 || timer: 0.0726 sec.
iter 10580 || Loss: 1.7402 || timer: 0.0790 sec.
iter 10590 || Loss: 1.6057 || timer: 0.0834 sec.
iter 10600 || Loss: 1.9691 || timer: 0.0750 sec.
iter 10610 || Loss: 1.8717 || timer: 0.0740 sec.
iter 10620 || Loss: 2.2171 || timer: 0.0773 sec.
iter 10630 || Loss: 2.1305 || timer: 0.0737 sec.
iter 10640 || Loss: 1.2297 || timer: 0.0829 sec.
iter 10650 || Loss: 1.3909 || timer: 0.0805 sec.
iter 10660 || Loss: 1.6084 || timer: 0.0954 sec.
iter 10670 || Loss: 1.6757 || timer: 0.0811 sec.
iter 10680 || Loss: 1.8000 || timer: 0.0828 sec.
iter 10690 || Loss: 1.8794 || timer: 0.0820 sec.
iter 10700 || Loss: 1.6961 || timer: 0.0830 sec.
iter 10710 || Loss: 1.8433 || timer: 0.0828 sec.
iter 10720 || Loss: 1.8147 || timer: 0.0735 sec.
iter 10730 || Loss: 1.8019 || timer: 0.0883 sec.
iter 10740 || Loss: 2.1332 || timer: 0.0771 sec.
iter 10750 || Loss: 1.6904 || timer: 0.0828 sec.
iter 10760 || Loss: 1.4910 || timer: 0.0788 sec.
iter 10770 || Loss: 1.9983 || timer: 0.0735 sec.
iter 10780 || Loss: 1.9934 || timer: 0.0201 sec.
iter 10790 || Loss: 1.3403 || timer: 0.0743 sec.
iter 10800 || Loss: 1.6828 || timer: 0.1071 sec.
iter 10810 || Loss: 2.0123 || timer: 0.0714 sec.
iter 10820 || Loss: 1.6180 || timer: 0.0756 sec.
iter 10830 || Loss: 1.6775 || timer: 0.0819 sec.
iter 10840 || Loss: 1.7265 || timer: 0.0740 sec.
iter 10850 || Loss: 1.6069 || timer: 0.0751 sec.
iter 10860 || Loss: 1.6909 || timer: 0.0658 sec.
iter 10870 || Loss: 1.9038 || timer: 0.0732 sec.
iter 10880 || Loss: 1.5688 || timer: 0.1019 sec.
iter 10890 || Loss: 1.5724 || timer: 0.0735 sec.
iter 10900 || Loss: 1.8402 || timer: 0.0728 sec.
iter 10910 || Loss: 1.7243 || timer: 0.0758 sec.
iter 10920 || Loss: 2.8315 || timer: 0.0739 sec.
iter 10930 || Loss: 1.9024 || timer: 0.0825 sec.
iter 10940 || Loss: 1.6368 || timer: 0.0758 sec.
iter 10950 || Loss: 1.5413 || timer: 0.0826 sec.
iter 10960 || Loss: 1.6710 || timer: 0.0712 sec.
iter 10970 || Loss: 1.7866 || timer: 0.0815 sec.
iter 10980 || Loss: 2.1391 || timer: 0.0732 sec.
iter 10990 || Loss: 1.8421 || timer: 0.0812 sec.
iter 11000 || Loss: 1.8258 || timer: 0.0818 sec.
iter 11010 || Loss: 1.7165 || timer: 0.0778 sec.
iter 11020 || Loss: 2.0204 || timer: 0.0973 sec.
iter 11030 || Loss: 2.0015 || timer: 0.0735 sec.
iter 11040 || Loss: 1.7254 || timer: 0.0810 sec.
iter 11050 || Loss: 2.1502 || timer: 0.0748 sec.
iter 11060 || Loss: 1.9010 || timer: 0.0828 sec.
iter 11070 || Loss: 1.6671 || timer: 0.0914 sec.
iter 11080 || Loss: 1.8770 || timer: 0.0692 sec.
iter 11090 || Loss: 1.6406 || timer: 0.0809 sec.
iter 11100 || Loss: 2.0239 || timer: 0.0746 sec.
iter 11110 || Loss: 1.6411 || timer: 0.0247 sec.
iter 11120 || Loss: 1.9376 || timer: 0.0742 sec.
iter 11130 || Loss: 1.8764 || timer: 0.0819 sec.
iter 11140 || Loss: 1.9156 || timer: 0.0754 sec.
iter 11150 || Loss: 1.4772 || timer: 0.0734 sec.
iter 11160 || Loss: 1.8099 || timer: 0.0826 sec.
iter 11170 || Loss: 1.9675 || timer: 0.0816 sec.
iter 11180 || Loss: 1.5645 || timer: 0.0817 sec.
iter 11190 || Loss: 2.0121 || timer: 0.0739 sec.
iter 11200 || Loss: 1.8419 || timer: 0.0745 sec.
iter 11210 || Loss: 2.1768 || timer: 0.0883 sec.
iter 11220 || Loss: 1.4571 || timer: 0.0822 sec.
iter 11230 || Loss: 1.8118 || timer: 0.0833 sec.
iter 11240 || Loss: 1.6964 || timer: 0.1112 sec.
iter 11250 || Loss: 1.8144 || timer: 0.0827 sec.
iter 11260 || Loss: 1.9629 || timer: 0.0821 sec.
iter 11270 || Loss: 2.0650 || timer: 0.0803 sec.
iter 11280 || Loss: 1.9937 || timer: 0.0738 sec.
iter 11290 || Loss: 1.8634 || timer: 0.0983 sec.
iter 11300 || Loss: 1.3891 || timer: 0.0970 sec.
iter 11310 || Loss: 1.7478 || timer: 0.0860 sec.
iter 11320 || Loss: 1.2134 || timer: 0.1020 sec.
iter 11330 || Loss: 1.4029 || timer: 0.0934 sec.
iter 11340 || Loss: 1.7013 || timer: 0.0741 sec.
iter 11350 || Loss: 1.4893 || timer: 0.0729 sec.
iter 11360 || Loss: 1.6422 || timer: 0.0814 sec.
iter 11370 || Loss: 1.8278 || timer: 0.1008 sec.
iter 11380 || Loss: 1.5179 || timer: 0.0751 sec.
iter 11390 || Loss: 1.3591 || timer: 0.0739 sec.
iter 11400 || Loss: 2.0472 || timer: 0.0788 sec.
iter 11410 || Loss: 1.8204 || timer: 0.0758 sec.
iter 11420 || Loss: 1.7978 || timer: 0.0857 sec.
iter 11430 || Loss: 2.1976 || timer: 0.0834 sec.
iter 11440 || Loss: 1.5699 || timer: 0.0246 sec.
iter 11450 || Loss: 3.6501 || timer: 0.0817 sec.
iter 11460 || Loss: 1.8612 || timer: 0.0828 sec.
iter 11470 || Loss: 1.9383 || timer: 0.0747 sec.
iter 11480 || Loss: 1.6193 || timer: 0.0808 sec.
iter 11490 || Loss: 1.8551 || timer: 0.0805 sec.
iter 11500 || Loss: 1.9188 || timer: 0.0934 sec.
iter 11510 || Loss: 1.5238 || timer: 0.0816 sec.
iter 11520 || Loss: 1.7223 || timer: 0.0945 sec.
iter 11530 || Loss: 1.5610 || timer: 0.0732 sec.
iter 11540 || Loss: 1.8387 || timer: 0.1016 sec.
iter 11550 || Loss: 1.7838 || timer: 0.0916 sec.
iter 11560 || Loss: 1.3663 || timer: 0.0921 sec.
iter 11570 || Loss: 1.3409 || timer: 0.0766 sec.
iter 11580 || Loss: 1.6405 || timer: 0.0739 sec.
iter 11590 || Loss: 1.9355 || timer: 0.0798 sec.
iter 11600 || Loss: 1.4303 || timer: 0.0733 sec.
iter 11610 || Loss: 1.7083 || timer: 0.0827 sec.
iter 11620 || Loss: 1.7114 || timer: 0.0850 sec.
iter 11630 || Loss: 1.3926 || timer: 0.0813 sec.
iter 11640 || Loss: 1.6366 || timer: 0.0851 sec.
iter 11650 || Loss: 1.9711 || timer: 0.0791 sec.
iter 11660 || Loss: 1.4991 || timer: 0.0748 sec.
iter 11670 || Loss: 1.8372 || timer: 0.0775 sec.
iter 11680 || Loss: 1.6978 || timer: 0.0800 sec.
iter 11690 || Loss: 1.7412 || timer: 0.0771 sec.
iter 11700 || Loss: 1.9464 || timer: 0.0880 sec.
iter 11710 || Loss: 1.4641 || timer: 0.0834 sec.
iter 11720 || Loss: 2.3832 || timer: 0.0780 sec.
iter 11730 || Loss: 1.7327 || timer: 0.0811 sec.
iter 11740 || Loss: 1.4436 || timer: 0.0814 sec.
iter 11750 || Loss: 1.7403 || timer: 0.0829 sec.
iter 11760 || Loss: 1.6345 || timer: 0.0953 sec.
iter 11770 || Loss: 1.5638 || timer: 0.0276 sec.
iter 11780 || Loss: 1.4782 || timer: 0.0713 sec.
iter 11790 || Loss: 1.7482 || timer: 0.0766 sec.
iter 11800 || Loss: 1.6550 || timer: 0.0753 sec.
iter 11810 || Loss: 2.2341 || timer: 0.0804 sec.
iter 11820 || Loss: 1.5650 || timer: 0.0765 sec.
iter 11830 || Loss: 1.8790 || timer: 0.0724 sec.
iter 11840 || Loss: 1.4058 || timer: 0.0751 sec.
iter 11850 || Loss: 1.5444 || timer: 0.0731 sec.
iter 11860 || Loss: 1.5362 || timer: 0.0976 sec.
iter 11870 || Loss: 1.7005 || timer: 0.1103 sec.
iter 11880 || Loss: 1.4890 || timer: 0.0750 sec.
iter 11890 || Loss: 1.9612 || timer: 0.0752 sec.
iter 11900 || Loss: 1.6362 || timer: 0.0645 sec.
iter 11910 || Loss: 1.6494 || timer: 0.1089 sec.
iter 11920 || Loss: 1.6018 || timer: 0.0963 sec.
iter 11930 || Loss: 1.4598 || timer: 0.0794 sec.
iter 11940 || Loss: 1.7435 || timer: 0.0961 sec.
iter 11950 || Loss: 1.5441 || timer: 0.1109 sec.
iter 11960 || Loss: 1.5156 || timer: 0.0909 sec.
iter 11970 || Loss: 1.9271 || timer: 0.0829 sec.
iter 11980 || Loss: 1.3239 || timer: 0.0817 sec.
iter 11990 || Loss: 1.9742 || /home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
Traceback (most recent call last):
  File "/home/xj/xu/ssd.pytorch/train.py", line 266, in <module>
    train()
  File "/home/xj/xu/ssd.pytorch/train.py", line 90, in train
    dataset = VOCDetection(root=args.dataset_root,
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/xu/ssd.pytorch/data/voc0712.py", line 111, in __init__
    for line in open(osp.join(rootpath, 'ImageSets', 'Main', name + '.txt')):
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/xj/xu/data/VOC_250423/VOC2007/VOC2007/ImageSets/Main/trainval.txt'
/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
Traceback (most recent call last):
  File "/home/xj/xu/ssd.pytorch/train.py", line 266, in <module>
    train()
  File "/home/xj/xu/ssd.pytorch/train.py", line 90, in train
    dataset = VOCDetection(root=args.dataset_root,
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xj/xu/ssd.pytorch/data/voc0712.py", line 111, in __init__
    for line in open(osp.join(rootpath, 'ImageSets', 'Main', name + '.txt')):
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/xj/xu/data/VOC_250423/VOC2012/ImageSets/Main/trainval.txt'
/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
/home/xj/xu/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  self.priors = Variable(self.priorbox.forward(), volatile=True)
/home/xj/xu/ssd.pytorch/train.py:227: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  init.xavier_uniform(param)
Loading base network...
Initializing weights...
Loading the dataset...
Training SSD on: VOC0712
Using the specified args:
Namespace(dataset='VOC', dataset_root='/home/xj/xu/data/VOC_250423', basenet='vgg16_reducedfc.pth', batch_size=32, resume=None, start_iter=0, num_workers=4, cuda=True, lr=0.001, momentum=0.9, weight_decay=0.0005, gamma=0.1, visdom=False, save_folder='weights/')
/home/xj/xu/ssd.pytorch/train.py:182: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  targets = [Variable(ann.cuda(), volatile=True) for ann in targets]
/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
timer: 11.5671 sec.
iter 0 || Loss: 23.9175 || timer: 0.0819 sec.
iter 10 || Loss: 11.9930 || timer: 0.0714 sec.
iter 20 || Loss: 10.8293 || timer: 0.0741 sec.
iter 30 || Loss: 9.6226 || timer: 0.0730 sec.
iter 40 || Loss: 11.3362 || timer: 0.0840 sec.
iter 50 || Loss: 9.6927 || timer: 0.0750 sec.
iter 60 || Loss: 8.0266 || timer: 0.0874 sec.
iter 70 || Loss: 7.3713 || timer: 0.0810 sec.
iter 80 || Loss: 7.5511 || timer: 0.0727 sec.
iter 90 || Loss: 6.5726 || timer: 0.0728 sec.
iter 100 || Loss: 6.1383 || timer: 0.0778 sec.
iter 110 || Loss: 5.9125 || timer: 0.0814 sec.
iter 120 || Loss: 6.0211 || timer: 0.0759 sec.
iter 130 || Loss: 5.6575 || timer: 0.0746 sec.
iter 140 || Loss: 5.5084 || timer: 0.1047 sec.
iter 150 || Loss: 5.8801 || timer: 0.0782 sec.
iter 160 || Loss: 5.6910 || timer: 0.0829 sec.
iter 170 || Loss: 5.7531 || timer: 0.0723 sec.
iter 180 || Loss: 5.5283 || timer: 0.0781 sec.
iter 190 || Loss: 5.3640 || timer: 0.0768 sec.
iter 200 || Loss: 6.0208 || timer: 0.0759 sec.
iter 210 || Loss: 5.9071 || timer: 0.0870 sec.
iter 220 || Loss: 5.5594 || timer: 0.0292 sec.
iter 230 || Loss: 6.8480 || timer: 0.0816 sec.
iter 240 || Loss: 5.2113 || timer: 0.0833 sec.
iter 250 || Loss: 5.1786 || timer: 0.0824 sec.
iter 260 || Loss: 5.0987 || timer: 0.0941 sec.
iter 270 || Loss: 5.3465 || timer: 0.0794 sec.
iter 280 || Loss: 5.2836 || timer: 0.0761 sec.
iter 290 || Loss: 5.2645 || timer: 0.0718 sec.
iter 300 || Loss: 5.4980 || timer: 0.0807 sec.
iter 310 || Loss: 4.7922 || timer: 0.0798 sec.
iter 320 || Loss: 4.8834 || timer: 0.1272 sec.
iter 330 || Loss: 4.3694 || timer: 0.0804 sec.
iter 340 || Loss: 4.7716 || timer: 0.0828 sec.
iter 350 || Loss: 4.7575 || timer: 0.0722 sec.
iter 360 || Loss: 5.0757 || timer: 0.0914 sec.
iter 370 || Loss: 5.0824 || timer: 0.0933 sec.
iter 380 || Loss: 4.9158 || timer: 0.0816 sec.
iter 390 || Loss: 4.8822 || timer: 0.1174 sec.
iter 400 || Loss: 5.0914 || timer: 0.0809 sec.
iter 410 || Loss: 4.6168 || timer: 0.0728 sec.
iter 420 || Loss: 4.2936 || timer: 0.0807 sec.
iter 430 || Loss: 4.5518 || timer: 0.0788 sec.
iter 440 || Loss: 4.6159 || timer: 0.0716 sec.
iter 450 || Loss: 4.7875 || timer: 0.1138 sec.
iter 460 || Loss: 4.9843 || timer: 0.1050 sec.
iter 470 || Loss: 4.7184 || timer: 0.0802 sec.
iter 480 || Loss: 4.0877 || timer: 0.0812 sec.
iter 490 || Loss: 4.5205 || timer: 0.0761 sec.
iter 500 || Loss: 5.2794 || timer: 0.0973 sec.
iter 510 || Loss: 4.3079 || timer: 0.0942 sec.
iter 520 || Loss: 4.2769 || timer: 0.1374 sec.
iter 530 || Loss: 4.6912 || timer: 0.0855 sec.
iter 540 || Loss: 4.2137 || timer: 0.0732 sec.
iter 550 || Loss: 4.3591 || timer: 0.0384 sec.
iter 560 || Loss: 5.3808 || timer: 0.0720 sec.
iter 570 || Loss: 5.4246 || timer: 0.0858 sec.
iter 580 || Loss: 4.4871 || timer: 0.0712 sec.
iter 590 || Loss: 4.4012 || timer: 0.0820 sec.
iter 600 || Loss: 4.5284 || timer: 0.0770 sec.
iter 610 || Loss: 3.9230 || timer: 0.0765 sec.
iter 620 || Loss: 4.3014 || timer: 0.0732 sec.
iter 630 || Loss: 4.1856 || timer: 0.1150 sec.
iter 640 || Loss: 4.3144 || timer: 0.0769 sec.
iter 650 || Loss: 4.6722 || timer: 0.1182 sec.
iter 660 || Loss: 3.6302 || timer: 0.0794 sec.
iter 670 || Loss: 4.5124 || timer: 0.0818 sec.
iter 680 || Loss: 4.4876 || timer: 0.0814 sec.
iter 690 || Loss: 4.3253 || timer: 0.0799 sec.
iter 700 || Loss: 3.9769 || timer: 0.1181 sec.
iter 710 || Loss: 3.9427 || timer: 0.0807 sec.
iter 720 || Loss: 3.5389 || timer: 0.0843 sec.
iter 730 || Loss: 5.2691 || timer: 0.0824 sec.
iter 740 || Loss: 4.2588 || timer: 0.0838 sec.
iter 750 || Loss: 4.5563 || timer: 0.0841 sec.
iter 760 || Loss: 4.5124 || timer: 0.0894 sec.
iter 770 || Loss: 4.5090 || timer: 0.0823 sec.
iter 780 || Loss: 3.7339 || timer: 0.0731 sec.
iter 790 || Loss: 3.9673 || timer: 0.0966 sec.
iter 800 || Loss: 4.1577 || timer: 0.0808 sec.
iter 810 || Loss: 3.4819 || timer: 0.2338 sec.
iter 820 || Loss: 3.8106 || timer: 0.0743 sec.
iter 830 || Loss: 3.8352 || timer: 0.0898 sec.
iter 840 || Loss: 3.8196 || timer: 0.0827 sec.
iter 850 || Loss: 3.0036 || timer: 0.0849 sec.
iter 860 || Loss: 3.8335 || timer: 0.0886 sec.
iter 870 || Loss: 4.1557 || timer: 0.0800 sec.
iter 880 || Loss: 3.5708 || timer: 0.0265 sec.
iter 890 || Loss: 3.9931 || timer: 0.0840 sec.
iter 900 || Loss: 3.9751 || timer: 0.0824 sec.
iter 910 || Loss: 3.4404 || timer: 0.0841 sec.
iter 920 || Loss: 3.5534 || timer: 0.0749 sec.
iter 930 || Loss: 5.1003 || timer: 0.0849 sec.
iter 940 || Loss: 4.1412 || timer: 0.0768 sec.
iter 950 || Loss: 3.6385 || timer: 0.0733 sec.
iter 960 || Loss: 3.5539 || timer: 0.0757 sec.
iter 970 || Loss: 3.4827 || timer: 0.0752 sec.
iter 980 || Loss: 3.8846 || timer: 0.0941 sec.
iter 990 || Loss: 4.0371 || timer: 0.0893 sec.
iter 1000 || Loss: 3.4839 || timer: 0.0816 sec.
iter 1010 || Loss: 2.9207 || timer: 0.0758 sec.
iter 1020 || Loss: 3.3214 || timer: 0.0845 sec.
iter 1030 || Loss: 3.3135 || timer: 0.0806 sec.
iter 1040 || Loss: 3.4097 || timer: 0.0822 sec.
iter 1050 || Loss: 3.8033 || timer: 0.0971 sec.
iter 1060 || Loss: 3.7953 || timer: 0.0916 sec.
iter 1070 || Loss: 3.6390 || timer: 0.0937 sec.
iter 1080 || Loss: 3.7667 || timer: 0.0874 sec.
iter 1090 || Loss: 3.4049 || timer: 0.0835 sec.
iter 1100 || Loss: 3.1959 || timer: 0.0975 sec.
iter 1110 || Loss: 3.6254 || timer: 0.0739 sec.
iter 1120 || Loss: 3.5131 || timer: 0.0836 sec.
iter 1130 || Loss: 4.4932 || timer: 0.0794 sec.
iter 1140 || Loss: 3.8252 || timer: 0.0971 sec.
iter 1150 || Loss: 3.3011 || timer: 0.0736 sec.
iter 1160 || Loss: 3.9253 || timer: 0.0742 sec.
iter 1170 || Loss: 3.4295 || timer: 0.0842 sec.
iter 1180 || Loss: 3.8149 || timer: 0.0764 sec.
iter 1190 || Loss: 3.5991 || timer: 0.0810 sec.
iter 1200 || Loss: 3.3642 || timer: 0.0736 sec.
iter 1210 || Loss: 3.1184 || timer: 0.0194 sec.
iter 1220 || Loss: 3.1476 || timer: 0.0733 sec.
iter 1230 || Loss: 3.7832 || timer: 0.0802 sec.
iter 1240 || Loss: 3.6818 || timer: 0.0763 sec.
iter 1250 || Loss: 2.9831 || timer: 0.0747 sec.
iter 1260 || Loss: 5.4927 || timer: 0.0959 sec.
iter 1270 || Loss: 4.3338 || timer: 0.0789 sec.
iter 1280 || Loss: 3.2199 || timer: 0.0759 sec.
iter 1290 || Loss: 4.4153 || timer: 0.1017 sec.
iter 1300 || Loss: 3.7372 || timer: 0.0863 sec.
iter 1310 || Loss: 4.0340 || timer: 0.1166 sec.
iter 1320 || Loss: 3.1463 || timer: 0.0829 sec.
iter 1330 || Loss: 3.2620 || timer: 0.0818 sec.
iter 1340 || Loss: 3.7303 || timer: 0.0800 sec.
iter 1350 || Loss: 2.9923 || timer: 0.0960 sec.
iter 1360 || Loss: 3.9603 || timer: 0.0746 sec.
iter 1370 || Loss: 3.2801 || timer: 0.0743 sec.
iter 1380 || Loss: 3.3098 || timer: 0.0824 sec.
iter 1390 || Loss: 3.2281 || timer: 0.0810 sec.
iter 1400 || Loss: 3.1430 || timer: 0.0838 sec.
iter 1410 || Loss: 3.2302 || timer: 0.0776 sec.
iter 1420 || Loss: 3.5331 || timer: 0.0806 sec.
iter 1430 || Loss: 3.9707 || timer: 0.0772 sec.
iter 1440 || Loss: 3.0939 || timer: 0.0819 sec.
iter 1450 || Loss: 3.4924 || timer: 0.1050 sec.
iter 1460 || Loss: 4.3222 || timer: 0.0952 sec.
iter 1470 || Loss: 3.6869 || timer: 0.1072 sec.
iter 1480 || Loss: 3.5527 || timer: 0.0714 sec.
iter 1490 || Loss: 3.7506 || timer: 0.0753 sec.
iter 1500 || Loss: 3.2609 || timer: 0.0842 sec.
iter 1510 || Loss: 3.0811 || timer: 0.0796 sec.
iter 1520 || Loss: 5.1182 || timer: 0.0756 sec.
iter 1530 || Loss: 4.7052 || timer: 0.0808 sec.
iter 1540 || Loss: 3.7043 || timer: 0.0323 sec.
iter 1550 || Loss: 7.8037 || timer: 0.0804 sec.
iter 1560 || Loss: 3.3193 || timer: 0.0832 sec.
iter 1570 || Loss: 2.6306 || timer: 0.0780 sec.
iter 1580 || Loss: 3.5742 || timer: 0.0842 sec.
iter 1590 || Loss: 4.6872 || timer: 0.0813 sec.
iter 1600 || Loss: 3.6049 || timer: 0.0786 sec.
iter 1610 || Loss: 3.5893 || timer: 0.0819 sec.
iter 1620 || Loss: 4.1207 || timer: 0.0737 sec.
iter 1630 || Loss: 3.0132 || timer: 0.0989 sec.
iter 1640 || Loss: 3.2307 || timer: 0.1108 sec.
iter 1650 || Loss: 4.5426 || timer: 0.0799 sec.
iter 1660 || Loss: 3.3413 || timer: 0.0822 sec.
iter 1670 || Loss: 3.2966 || timer: 0.0758 sec.
iter 1680 || Loss: 3.6430 || timer: 0.1002 sec.
iter 1690 || Loss: 3.1411 || timer: 0.0823 sec.
iter 1700 || Loss: 3.3868 || timer: 0.0854 sec.
iter 1710 || Loss: 3.1920 || timer: 0.1081 sec.
iter 1720 || Loss: 2.8409 || timer: 0.0817 sec.
iter 1730 || Loss: 2.9869 || timer: 0.0896 sec.
iter 1740 || Loss: 2.9560 || timer: 0.0939 sec.
iter 1750 || Loss: 3.5807 || timer: 0.0873 sec.
iter 1760 || Loss: 2.9585 || timer: 0.0800 sec.
iter 1770 || Loss: 2.6591 || timer: 0.0854 sec.
iter 1780 || Loss: 3.3258 || timer: 0.0835 sec.
iter 1790 || Loss: 3.2436 || timer: 0.0921 sec.
iter 1800 || Loss: 3.2415 || timer: 0.0731 sec.
iter 1810 || Loss: 2.5717 || timer: 0.0993 sec.
iter 1820 || Loss: 3.3616 || timer: 0.0746 sec.
iter 1830 || Loss: 3.2890 || timer: 0.0835 sec.
iter 1840 || Loss: 2.6554 || timer: 0.0839 sec.
iter 1850 || Loss: 3.5847 || timer: 0.0781 sec.
iter 1860 || Loss: 4.1600 || timer: 0.0797 sec.
iter 1870 || Loss: 3.9926 || timer: 0.0282 sec.
iter 1880 || Loss: 3.2699 || timer: 0.0812 sec.
iter 1890 || Loss: 3.2722 || timer: 0.0822 sec.
iter 1900 || Loss: 3.3734 || timer: 0.0806 sec.
iter 1910 || Loss: 2.6895 || timer: 0.0785 sec.
iter 1920 || Loss: 3.4642 || timer: 0.0865 sec.
iter 1930 || Loss: 2.6410 || timer: 0.0834 sec.
iter 1940 || Loss: 2.8145 || timer: 0.0735 sec.
iter 1950 || Loss: 3.4475 || timer: 0.0802 sec.
iter 1960 || Loss: 2.8310 || timer: 0.0983 sec.
iter 1970 || Loss: 2.6668 || timer: 0.0981 sec.
iter 1980 || Loss: 3.0784 || timer: 0.0810 sec.
iter 1990 || Loss: 2.9679 || timer: 0.0790 sec.
iter 2000 || Loss: 2.6572 || timer: 0.0743 sec.
iter 2010 || Loss: 2.8164 || timer: 0.0769 sec.
iter 2020 || Loss: 2.7165 || timer: 0.0827 sec.
iter 2030 || Loss: 2.8275 || timer: 0.0816 sec.
iter 2040 || Loss: 3.3030 || timer: 0.0773 sec.
iter 2050 || Loss: 4.1257 || timer: 0.0805 sec.
iter 2060 || Loss: 2.8716 || timer: 0.0826 sec.
iter 2070 || Loss: 3.7346 || timer: 0.0743 sec.
iter 2080 || Loss: 3.9056 || timer: 0.0799 sec.
iter 2090 || Loss: 3.6235 || timer: 0.0668 sec.
iter 2100 || Loss: 3.4418 || timer: 0.0709 sec.
iter 2110 || Loss: 3.1178 || timer: 0.0779 sec.
iter 2120 || Loss: 2.9120 || timer: 0.0728 sec.
iter 2130 || Loss: 3.1954 || timer: 0.0904 sec.
iter 2140 || Loss: 2.6224 || timer: 0.0805 sec.
iter 2150 || Loss: 2.8821 || timer: 0.0977 sec.
iter 2160 || Loss: 2.4610 || timer: 0.0784 sec.
iter 2170 || Loss: 2.7588 || timer: 0.0825 sec.
iter 2180 || Loss: 3.3023 || timer: 0.0830 sec.
iter 2190 || Loss: 3.7511 || timer: 0.0803 sec.
iter 2200 || Loss: 2.9480 || timer: 0.0285 sec.
iter 2210 || Loss: 4.1838 || timer: 0.0924 sec.
iter 2220 || Loss: 3.3570 || timer: 0.0673 sec.
iter 2230 || Loss: 3.4208 || timer: 0.0744 sec.
iter 2240 || Loss: 3.1430 || timer: 0.0735 sec.
iter 2250 || Loss: 2.4836 || timer: 0.1115 sec.
iter 2260 || Loss: 3.6000 || timer: 0.0778 sec.
iter 2270 || Loss: 2.3920 || timer: 0.0804 sec.
iter 2280 || Loss: 2.7249 || timer: 0.1006 sec.
iter 2290 || Loss: 3.1129 || timer: 0.0796 sec.
iter 2300 || Loss: 2.7928 || timer: 0.1043 sec.
iter 2310 || Loss: 3.0344 || timer: 0.1256 sec.
iter 2320 || Loss: 3.3138 || timer: 0.1085 sec.
iter 2330 || Loss: 2.7677 || timer: 0.0824 sec.
iter 2340 || Loss: 3.0734 || timer: 0.0786 sec.
iter 2350 || Loss: 4.8893 || timer: 0.0821 sec.
iter 2360 || Loss: 3.2475 || timer: 0.0819 sec.
iter 2370 || Loss: 3.0083 || timer: 0.0810 sec.
iter 2380 || Loss: 2.8151 || timer: 0.0850 sec.
iter 2390 || Loss: 3.0700 || timer: 0.0846 sec.
iter 2400 || Loss: 2.9876 || timer: 0.0737 sec.
iter 2410 || Loss: 3.1022 || timer: 0.0837 sec.
iter 2420 || Loss: 2.8298 || timer: 0.0791 sec.
iter 2430 || Loss: 2.7413 || timer: 0.0728 sec.
iter 2440 || Loss: 2.8619 || timer: 0.0847 sec.
iter 2450 || Loss: 3.4312 || timer: 0.0925 sec.
iter 2460 || Loss: 3.0490 || timer: 0.0843 sec.
iter 2470 || Loss: 3.1262 || timer: 0.0813 sec.
iter 2480 || Loss: 2.6333 || timer: 0.0821 sec.
iter 2490 || Loss: 2.7675 || timer: 0.0937 sec.
iter 2500 || Loss: 2.6696 || timer: 0.0820 sec.
iter 2510 || Loss: 2.8092 || timer: 0.0824 sec.
iter 2520 || Loss: 2.7491 || timer: 0.0757 sec.
iter 2530 || Loss: 3.1522 || timer: 0.0166 sec.
iter 2540 || Loss: 2.1607 || timer: 0.0854 sec.
iter 2550 || Loss: 2.7749 || timer: 0.0791 sec.
iter 2560 || Loss: 2.9969 || timer: 0.0789 sec.
iter 2570 || Loss: 2.6310 || timer: 0.0759 sec.
iter 2580 || Loss: 2.7323 || timer: 0.0844 sec.
iter 2590 || Loss: 3.3867 || timer: 0.0821 sec.
iter 2600 || Loss: 2.4605 || timer: 0.0795 sec.
iter 2610 || Loss: 2.6501 || timer: 0.0826 sec.
iter 2620 || Loss: 3.0963 || timer: 0.0805 sec.
iter 2630 || Loss: 2.4905 || timer: 0.1338 sec.
iter 2640 || Loss: 2.5943 || timer: 0.1055 sec.
iter 2650 || Loss: 3.2589 || timer: 0.0795 sec.
iter 2660 || Loss: 2.3001 || timer: 0.0893 sec.
iter 2670 || Loss: 4.2866 || timer: 0.1001 sec.
iter 2680 || Loss: 2.4477 || timer: 0.0801 sec.
iter 2690 || Loss: 2.7209 || timer: 0.0774 sec.
iter 2700 || Loss: 2.8421 || timer: 0.0835 sec.
iter 2710 || Loss: 3.0571 || timer: 0.0992 sec.
iter 2720 || Loss: 2.4072 || timer: 0.0946 sec.
iter 2730 || Loss: 3.0999 || timer: 0.0741 sec.
iter 2740 || Loss: 2.3503 || timer: 0.0775 sec.
iter 2750 || Loss: 2.1722 || timer: 0.0942 sec.
iter 2760 || Loss: 2.5421 || timer: 0.0824 sec.
iter 2770 || Loss: 3.2243 || timer: 0.0766 sec.
iter 2780 || Loss: 3.2497 || timer: 0.0753 sec.
iter 2790 || Loss: 2.7198 || timer: 0.0792 sec.
iter 2800 || Loss: 2.9425 || timer: 0.0811 sec.
iter 2810 || Loss: 2.5340 || timer: 0.0832 sec.
iter 2820 || Loss: 2.6655 || timer: 0.0761 sec.
iter 2830 || Loss: 3.0869 || timer: 0.0742 sec.
iter 2840 || Loss: 3.0562 || timer: 0.0812 sec.
iter 2850 || Loss: 3.2745 || timer: 0.0826 sec.
iter 2860 || Loss: 2.9456 || timer: 0.0397 sec.
iter 2870 || Loss: 2.7417 || timer: 0.0762 sec.
iter 2880 || Loss: 2.7645 || timer: 0.0761 sec.
iter 2890 || Loss: 2.5929 || timer: 0.0815 sec.
iter 2900 || Loss: 2.5247 || timer: 0.0825 sec.
iter 2910 || Loss: 2.6155 || timer: 0.0739 sec.
iter 2920 || Loss: 2.4893 || timer: 0.0787 sec.
iter 2930 || Loss: 2.3312 || timer: 0.0731 sec.
iter 2940 || Loss: 2.6717 || timer: 0.0823 sec.
iter 2950 || Loss: 2.9245 || timer: 0.0858 sec.
iter 2960 || Loss: 2.2870 || timer: 0.0973 sec.
iter 2970 || Loss: 2.9806 || timer: 0.0741 sec.
iter 2980 || Loss: 2.7579 || timer: 0.0873 sec.
iter 2990 || Loss: 2.1764 || timer: 0.0801 sec.
iter 3000 || Loss: 2.3725 || timer: 0.0735 sec.
iter 3010 || Loss: 2.5706 || timer: 0.0789 sec.
iter 3020 || Loss: 2.7528 || timer: 0.0801 sec.
iter 3030 || Loss: 2.6802 || timer: 0.0961 sec.
iter 3040 || Loss: 2.3992 || timer: 0.1153 sec.
iter 3050 || Loss: 3.0338 || timer: 0.0799 sec.
iter 3060 || Loss: 1.9855 || timer: 0.0720 sec.
iter 3070 || Loss: 2.6211 || timer: 0.0726 sec.
iter 3080 || Loss: 2.4524 || timer: 0.0847 sec.
iter 3090 || Loss: 3.0048 || timer: 0.0817 sec.
iter 3100 || Loss: 2.9328 || timer: 0.0823 sec.
iter 3110 || Loss: 2.2661 || timer: 0.0739 sec.
iter 3120 || Loss: 2.2190 || timer: 0.1116 sec.
iter 3130 || Loss: 2.0152 || timer: 0.0898 sec.
iter 3140 || Loss: 2.6247 || timer: 0.0795 sec.
iter 3150 || Loss: 2.2054 || timer: 0.0830 sec.
iter 3160 || Loss: 2.2437 || timer: 0.0835 sec.
iter 3170 || Loss: 2.7551 || timer: 0.0829 sec.
iter 3180 || Loss: 1.9606 || timer: 0.0813 sec.
iter 3190 || Loss: 2.3845 || timer: 0.0254 sec.
iter 3200 || Loss: 1.9086 || timer: 0.0806 sec.
iter 3210 || Loss: 2.3543 || timer: 0.0929 sec.
iter 3220 || Loss: 2.5350 || timer: 0.0807 sec.
iter 3230 || Loss: 2.4026 || timer: 0.0847 sec.
iter 3240 || Loss: 2.9606 || timer: 0.0837 sec.
iter 3250 || Loss: 3.1432 || timer: 0.0789 sec.
iter 3260 || Loss: 2.2923 || timer: 0.0754 sec.
iter 3270 || Loss: 3.0611 || timer: 0.0780 sec.
iter 3280 || Loss: 2.7632 || timer: 0.0913 sec.
iter 3290 || Loss: 2.2821 || timer: 0.0810 sec.
iter 3300 || Loss: 2.7165 || timer: 0.0818 sec.
iter 3310 || Loss: 2.1927 || timer: 0.0979 sec.
iter 3320 || Loss: 2.2742 || timer: 0.0804 sec.
iter 3330 || Loss: 2.6919 || timer: 0.0813 sec.
iter 3340 || Loss: 3.2657 || timer: 0.0829 sec.
iter 3350 || Loss: 2.7683 || timer: 0.0849 sec.
iter 3360 || Loss: 2.3947 || timer: 0.1097 sec.
iter 3370 || Loss: 2.5612 || timer: 0.0815 sec.
iter 3380 || Loss: 2.5897 || timer: 0.0823 sec.
iter 3390 || Loss: 2.8083 || timer: 0.0771 sec.
iter 3400 || Loss: 3.5282 || timer: 0.0740 sec.
iter 3410 || Loss: 2.6122 || timer: 0.0778 sec.
iter 3420 || Loss: 2.2506 || timer: 0.0774 sec.
iter 3430 || Loss: 2.3578 || timer: 0.0845 sec.
iter 3440 || Loss: 2.3943 || timer: 0.0799 sec.
iter 3450 || Loss: 2.6795 || timer: 0.0834 sec.
iter 3460 || Loss: 2.6065 || timer: 0.0800 sec.
iter 3470 || Loss: 2.6172 || timer: 0.0745 sec.
iter 3480 || Loss: 3.2205 || timer: 0.0816 sec.
iter 3490 || Loss: 2.7463 || timer: 0.0735 sec.
iter 3500 || Loss: 2.5362 || timer: 0.0879 sec.
iter 3510 || Loss: 3.0360 || timer: 0.0801 sec.
iter 3520 || Loss: 2.6872 || timer: 0.0228 sec.
iter 3530 || Loss: 2.8213 || timer: 0.0931 sec.
iter 3540 || Loss: 2.3446 || timer: 0.0828 sec.
iter 3550 || Loss: 2.5725 || timer: 0.0652 sec.
iter 3560 || Loss: 2.5621 || timer: 0.0840 sec.
iter 3570 || Loss: 2.4474 || timer: 0.0823 sec.
iter 3580 || Loss: 2.9493 || timer: 0.0800 sec.
iter 3590 || Loss: 2.4816 || timer: 0.0824 sec.
iter 3600 || Loss: 2.7470 || timer: 0.0739 sec.
iter 3610 || Loss: 2.8990 || timer: 0.0795 sec.
iter 3620 || Loss: 2.3920 || timer: 0.1041 sec.
iter 3630 || Loss: 2.8236 || timer: 0.0806 sec.
iter 3640 || Loss: 3.2405 || timer: 0.1081 sec.
iter 3650 || Loss: 2.7569 || timer: 0.0840 sec.
iter 3660 || Loss: 2.2812 || timer: 0.0751 sec.
iter 3670 || Loss: 2.7203 || timer: 0.0805 sec.
iter 3680 || Loss: 2.3379 || timer: 0.0741 sec.
iter 3690 || Loss: 1.9536 || timer: 0.0742 sec.
iter 3700 || Loss: 3.9751 || timer: 0.0861 sec.
iter 3710 || Loss: 3.5440 || timer: 0.0799 sec.
iter 3720 || Loss: 2.0313 || timer: 0.0842 sec.
iter 3730 || Loss: 2.6795 || timer: 0.0796 sec.
iter 3740 || Loss: 2.8621 || timer: 0.0971 sec.
iter 3750 || Loss: 2.4252 || timer: 0.0761 sec.
iter 3760 || Loss: 2.2248 || timer: 0.0750 sec.
iter 3770 || Loss: 3.6589 || timer: 0.0816 sec.
iter 3780 || Loss: 4.2406 || timer: 0.0835 sec.
iter 3790 || Loss: 2.8400 || timer: 0.0806 sec.
iter 3800 || Loss: 2.6108 || timer: 0.0725 sec.
iter 3810 || Loss: 2.5814 || timer: 0.0975 sec.
iter 3820 || Loss: 2.7313 || timer: 0.0827 sec.
iter 3830 || Loss: 2.6530 || timer: 0.0834 sec.
iter 3840 || Loss: 2.6944 || timer: 0.0733 sec.
iter 3850 || Loss: 2.3314 || timer: 0.0254 sec.
iter 3860 || Loss: 2.9613 || timer: 0.0811 sec.
iter 3870 || Loss: 2.9694 || timer: 0.0964 sec.
iter 3880 || Loss: 2.9185 || timer: 0.0741 sec.
iter 3890 || Loss: 2.7139 || timer: 0.0817 sec.
iter 3900 || Loss: 2.4760 || timer: 0.0820 sec.
iter 3910 || Loss: 2.1616 || timer: 0.0840 sec.
iter 3920 || Loss: 3.4412 || timer: 0.0841 sec.
iter 3930 || Loss: 2.6212 || timer: 0.0851 sec.
iter 3940 || Loss: 2.6740 || timer: 0.0801 sec.
iter 3950 || Loss: 2.6789 || timer: 0.1387 sec.
iter 3960 || Loss: 2.1637 || timer: 0.0842 sec.
iter 3970 || Loss: 2.7304 || timer: 0.0788 sec.
iter 3980 || Loss: 2.4469 || timer: 0.0835 sec.
iter 3990 || Loss: 2.3385 || timer: 0.0758 sec.
iter 4000 || Loss: 3.1865 || timer: 0.0766 sec.
iter 4010 || Loss: 2.4448 || timer: 0.0810 sec.
iter 4020 || Loss: 2.1452 || timer: 0.0771 sec.
iter 4030 || Loss: 2.4972 || timer: 0.0774 sec.
iter 4040 || Loss: 2.0298 || timer: 0.0834 sec.
iter 4050 || Loss: 2.4591 || timer: 0.0918 sec.
iter 4060 || Loss: 2.3610 || timer: 0.0832 sec.
iter 4070 || Loss: 2.4427 || timer: 0.0803 sec.
iter 4080 || Loss: 2.4938 || timer: 0.0854 sec.
iter 4090 || Loss: 3.3091 || timer: 0.0820 sec.
iter 4100 || Loss: 2.9926 || timer: 0.0737 sec.
iter 4110 || Loss: 2.2449 || timer: 0.0749 sec.
iter 4120 || Loss: 2.2014 || timer: 0.0942 sec.
iter 4130 || Loss: 3.0002 || timer: 0.1008 sec.
iter 4140 || Loss: 2.4721 || timer: 0.0801 sec.
iter 4150 || Loss: 2.1811 || timer: 0.0751 sec.
iter 4160 || Loss: 2.6293 || timer: 0.1011 sec.
iter 4170 || Loss: 2.2281 || timer: 0.0826 sec.
iter 4180 || Loss: 2.3378 || timer: 0.0306 sec.
iter 4190 || Loss: 3.7838 || timer: 0.0850 sec.
iter 4200 || Loss: 2.4112 || timer: 0.0838 sec.
iter 4210 || Loss: 2.3376 || timer: 0.0744 sec.
iter 4220 || Loss: 2.1294 || timer: 0.0774 sec.
iter 4230 || Loss: 3.4464 || timer: 0.0818 sec.
iter 4240 || Loss: 3.6835 || timer: 0.0743 sec.
iter 4250 || Loss: 2.4956 || timer: 0.0755 sec.
iter 4260 || Loss: 2.9520 || timer: 0.0744 sec.
iter 4270 || Loss: 2.5952 || timer: 0.0773 sec.
iter 4280 || Loss: 2.4363 || timer: 0.1066 sec.
iter 4290 || Loss: 2.2491 || timer: 0.0740 sec.
iter 4300 || Loss: 2.0764 || timer: 0.0754 sec.
iter 4310 || Loss: 2.1482 || timer: 0.0794 sec.
iter 4320 || Loss: 2.5606 || timer: 0.0735 sec.
iter 4330 || Loss: 2.4788 || timer: 0.0774 sec.
iter 4340 || Loss: 2.3649 || timer: 0.0737 sec.
iter 4350 || Loss: 2.6831 || timer: 0.0732 sec.
iter 4360 || Loss: 2.0932 || timer: 0.0806 sec.
iter 4370 || Loss: 2.4502 || timer: 0.1029 sec.
iter 4380 || Loss: 2.5041 || timer: 0.0767 sec.
iter 4390 || Loss: 2.5027 || timer: 0.0748 sec.
iter 4400 || Loss: 2.4223 || timer: 0.1023 sec.
iter 4410 || Loss: 2.4145 || timer: 0.0805 sec.
iter 4420 || Loss: 2.2475 || timer: 0.0871 sec.
iter 4430 || Loss: 2.5878 || timer: 0.0739 sec.
iter 4440 || Loss: 2.3642 || timer: 0.0840 sec.
iter 4450 || Loss: 2.5775 || timer: 0.0749 sec.
iter 4460 || Loss: 3.0168 || timer: 0.0742 sec.
iter 4470 || Loss: 2.3952 || timer: 0.0738 sec.
iter 4480 || Loss: 2.1157 || timer: 0.0956 sec.
iter 4490 || Loss: 2.0869 || timer: 0.0744 sec.
iter 4500 || Loss: 2.3416 || timer: 0.0757 sec.
iter 4510 || Loss: 2.6117 || timer: 0.0244 sec.
iter 4520 || Loss: 2.1148 || timer: 0.1045 sec.
iter 4530 || Loss: 3.0998 || timer: 0.0799 sec.
iter 4540 || Loss: 2.0019 || timer: 0.0835 sec.
iter 4550 || Loss: 2.4774 || timer: 0.0845 sec.
iter 4560 || Loss: 2.3218 || timer: 0.0813 sec.
iter 4570 || Loss: 2.6605 || timer: 0.0781 sec.
iter 4580 || Loss: 2.3011 || timer: 0.0668 sec.
iter 4590 || Loss: 2.4848 || timer: 0.0922 sec.
iter 4600 || Loss: 2.7589 || timer: 0.0735 sec.
iter 4610 || Loss: 2.6780 || timer: 0.1078 sec.
iter 4620 || Loss: 2.3585 || timer: 0.0813 sec.
iter 4630 || Loss: 2.4268 || timer: 0.0973 sec.
iter 4640 || Loss: 1.8703 || timer: 0.0795 sec.
iter 4650 || Loss: 2.6443 || timer: 0.0793 sec.
iter 4660 || Loss: 2.0998 || timer: 0.0808 sec.
iter 4670 || Loss: 2.1298 || timer: 0.0799 sec.
iter 4680 || Loss: 2.2681 || timer: 0.0987 sec.
iter 4690 || Loss: 2.6913 || timer: 0.0832 sec.
iter 4700 || Loss: 2.0798 || timer: 0.1168 sec.
iter 4710 || Loss: 1.9009 || timer: 0.0800 sec.
iter 4720 || Loss: 2.6619 || timer: 0.0816 sec.
iter 4730 || Loss: 2.1559 || timer: 0.0819 sec.
iter 4740 || Loss: 1.7357 || timer: 0.0801 sec.
iter 4750 || Loss: 2.4534 || timer: 0.0753 sec.
iter 4760 || Loss: 2.5795 || timer: 0.0696 sec.
iter 4770 || Loss: 2.2385 || timer: 0.0840 sec.
iter 4780 || Loss: 2.1024 || timer: 0.0830 sec.
iter 4790 || Loss: 2.2703 || timer: 0.0799 sec.
iter 4800 || Loss: 2.2842 || timer: 0.0828 sec.
iter 4810 || Loss: 2.0803 || timer: 0.0816 sec.
iter 4820 || Loss: 2.0903 || timer: 0.0743 sec.
iter 4830 || Loss: 2.0102 || timer: 0.0823 sec.
iter 4840 || Loss: 2.4888 || timer: 0.0205 sec.
iter 4850 || Loss: 12.1859 || timer: 0.0874 sec.
iter 4860 || Loss: 2.7707 || timer: 0.0816 sec.
iter 4870 || Loss: 2.0520 || timer: 0.0737 sec.
iter 4880 || Loss: 2.3866 || timer: 0.0816 sec.
iter 4890 || Loss: 1.7696 || timer: 0.0736 sec.
iter 4900 || Loss: 2.4936 || timer: 0.0740 sec.
iter 4910 || Loss: 2.2720 || timer: 0.0983 sec.
iter 4920 || Loss: 2.5055 || timer: 0.1005 sec.
iter 4930 || Loss: 2.6768 || timer: 0.0742 sec.
iter 4940 || Loss: 2.4957 || timer: 0.0882 sec.
iter 4950 || Loss: 2.4307 || timer: 0.0819 sec.
iter 4960 || Loss: 2.3040 || timer: 0.0809 sec.
iter 4970 || Loss: 2.5944 || timer: 0.0692 sec.
iter 4980 || Loss: 2.2710 || timer: 0.0778 sec.
iter 4990 || Loss: 2.7540 || timer: 0.0806 sec.
iter 5000 || Loss: 2.1889 || Saving state, iter: 5000
timer: 0.0737 sec.
iter 5010 || Loss: 2.2746 || timer: 0.0984 sec.
iter 5020 || Loss: 1.8714 || timer: 0.0807 sec.
iter 5030 || Loss: 2.5216 || timer: 0.0829 sec.
iter 5040 || Loss: 2.9956 || timer: 0.0870 sec.
iter 5050 || Loss: 2.1745 || timer: 0.0815 sec.
iter 5060 || Loss: 2.1529 || timer: 0.0906 sec.
iter 5070 || Loss: 2.5615 || timer: 0.0802 sec.
iter 5080 || Loss: 1.9259 || timer: 0.0816 sec.
iter 5090 || Loss: 2.0035 || timer: 0.0741 sec.
iter 5100 || Loss: 2.4077 || timer: 0.0815 sec.
iter 5110 || Loss: 2.2149 || timer: 0.0844 sec.
iter 5120 || Loss: 2.2715 || timer: 0.0675 sec.
iter 5130 || Loss: 2.1125 || timer: 0.0753 sec.
iter 5140 || Loss: 1.6108 || timer: 0.0844 sec.
iter 5150 || Loss: 1.9770 || timer: 0.0935 sec.
iter 5160 || Loss: 2.3082 || timer: 0.0819 sec.
iter 5170 || Loss: 2.3471 || timer: 0.0177 sec.
iter 5180 || Loss: 1.4457 || timer: 0.0815 sec.
iter 5190 || Loss: 2.7138 || timer: 0.1015 sec.
iter 5200 || Loss: 2.3282 || timer: 0.0666 sec.
iter 5210 || Loss: 2.3756 || timer: 0.0835 sec.
iter 5220 || Loss: 2.3654 || timer: 0.0681 sec.
iter 5230 || Loss: 2.5737 || timer: 0.0711 sec.
iter 5240 || Loss: 1.8342 || timer: 0.0823 sec.
iter 5250 || Loss: 2.1092 || timer: 0.0847 sec.
iter 5260 || Loss: 2.3697 || timer: 0.0809 sec.
iter 5270 || Loss: 2.1752 || timer: 0.1166 sec.
iter 5280 || Loss: 2.4755 || timer: 0.0773 sec.
iter 5290 || Loss: 1.9491 || timer: 0.0794 sec.
iter 5300 || Loss: 2.1903 || timer: 0.0814 sec.
iter 5310 || Loss: 2.0682 || timer: 0.0822 sec.
iter 5320 || Loss: 2.1170 || timer: 0.0814 sec.
iter 5330 || Loss: 2.0883 || timer: 0.0821 sec.
iter 5340 || Loss: 2.4804 || timer: 0.0797 sec.
iter 5350 || Loss: 1.7757 || timer: 0.0805 sec.
iter 5360 || Loss: 2.3726 || timer: 0.0814 sec.
iter 5370 || Loss: 1.9773 || timer: 0.0788 sec.
iter 5380 || Loss: 2.9097 || timer: 0.0820 sec.
iter 5390 || Loss: 2.3371 || timer: 0.0829 sec.
iter 5400 || Loss: 2.3112 || timer: 0.0817 sec.
iter 5410 || Loss: 2.5736 || timer: 0.0843 sec.
iter 5420 || Loss: 2.5168 || timer: 0.0765 sec.
iter 5430 || Loss: 2.1929 || timer: 0.0754 sec.
iter 5440 || Loss: 1.9168 || timer: 0.0741 sec.
iter 5450 || Loss: 2.8970 || timer: 0.1053 sec.
iter 5460 || Loss: 2.3757 || timer: 0.0822 sec.
iter 5470 || Loss: 2.2493 || timer: 0.0784 sec.
iter 5480 || Loss: 2.7745 || timer: 0.0901 sec.
iter 5490 || Loss: 2.6680 || timer: 0.0809 sec.
iter 5500 || Loss: 2.4262 || timer: 0.0204 sec.
iter 5510 || Loss: 1.3430 || timer: 0.0820 sec.
iter 5520 || Loss: 2.0131 || timer: 0.0812 sec.
iter 5530 || Loss: 1.8884 || timer: 0.0875 sec.
iter 5540 || Loss: 2.7357 || timer: 0.0958 sec.
iter 5550 || Loss: 2.2357 || timer: 0.0675 sec.
iter 5560 || Loss: 2.1774 || timer: 0.1010 sec.
iter 5570 || Loss: 2.3358 || timer: 0.0736 sec.
iter 5580 || Loss: 2.2114 || timer: 0.0726 sec.
iter 5590 || Loss: 2.8808 || timer: 0.0823 sec.
iter 5600 || Loss: 1.9666 || timer: 0.1387 sec.
iter 5610 || Loss: 2.3064 || timer: 0.0870 sec.
iter 5620 || Loss: 2.0571 || timer: 0.0981 sec.
iter 5630 || Loss: 2.3432 || timer: 0.0806 sec.
iter 5640 || Loss: 2.2932 || timer: 0.0910 sec.
iter 5650 || Loss: 2.0064 || timer: 0.0852 sec.
iter 5660 || Loss: 2.2768 || timer: 0.1018 sec.
iter 5670 || Loss: 2.5509 || timer: 0.0811 sec.
iter 5680 || Loss: 1.8078 || timer: 0.0797 sec.
iter 5690 || Loss: 2.2445 || timer: 0.0910 sec.
iter 5700 || Loss: 2.4216 || timer: 0.0831 sec.
iter 5710 || Loss: 2.4591 || timer: 0.1063 sec.
iter 5720 || Loss: 2.3167 || timer: 0.0832 sec.
iter 5730 || Loss: 2.0270 || timer: 0.0790 sec.
iter 5740 || Loss: 2.5652 || timer: 0.0817 sec.
iter 5750 || Loss: 2.0983 || timer: 0.0792 sec.
iter 5760 || Loss: 2.1899 || timer: 0.3504 sec.
iter 5770 || Loss: 2.6138 || timer: 0.0741 sec.
iter 5780 || Loss: 2.4673 || timer: 0.0819 sec.
iter 5790 || Loss: 1.7499 || timer: 0.0738 sec.
iter 5800 || Loss: 2.0446 || timer: 0.0751 sec.
iter 5810 || Loss: 1.9965 || timer: 0.0869 sec.
iter 5820 || Loss: 2.4097 || timer: 0.0750 sec.
iter 5830 || Loss: 1.9447 || timer: 0.0256 sec.
iter 5840 || Loss: 1.6955 || timer: 0.0741 sec.
iter 5850 || Loss: 2.2304 || timer: 0.0956 sec.
iter 5860 || Loss: 2.5276 || timer: 0.0772 sec.
iter 5870 || Loss: 2.5912 || timer: 0.0810 sec.
iter 5880 || Loss: 2.1444 || timer: 0.0927 sec.
iter 5890 || Loss: 2.4193 || timer: 0.0811 sec.
iter 5900 || Loss: 2.3128 || timer: 0.0823 sec.
iter 5910 || Loss: 2.6393 || timer: 0.1079 sec.
iter 5920 || Loss: 1.8724 || timer: 0.0741 sec.
iter 5930 || Loss: 2.2048 || timer: 0.0844 sec.
iter 5940 || Loss: 2.1645 || timer: 0.0753 sec.
iter 5950 || Loss: 2.3645 || timer: 0.0850 sec.
iter 5960 || Loss: 2.1131 || timer: 0.0899 sec.
iter 5970 || Loss: 1.9492 || timer: 0.0785 sec.
iter 5980 || Loss: 2.4581 || timer: 0.0816 sec.
iter 5990 || Loss: 2.0883 || timer: 0.0769 sec.
iter 6000 || Loss: 2.1122 || timer: 0.0950 sec.
iter 6010 || Loss: 2.9948 || timer: 0.0750 sec.
iter 6020 || Loss: 3.4125 || timer: 0.0665 sec.
iter 6030 || Loss: 2.6643 || timer: 0.0766 sec.
iter 6040 || Loss: 2.7779 || timer: 0.0820 sec.
iter 6050 || Loss: 2.0679 || timer: 0.0822 sec.
iter 6060 || Loss: 2.2344 || timer: 0.0808 sec.
iter 6070 || Loss: 2.3401 || timer: 0.0743 sec.
iter 6080 || Loss: 2.3009 || timer: 0.0809 sec.
iter 6090 || Loss: 2.9342 || timer: 0.0740 sec.
iter 6100 || Loss: 2.3464 || timer: 0.0814 sec.
iter 6110 || Loss: 2.6874 || timer: 0.0847 sec.
iter 6120 || Loss: 2.9607 || timer: 0.1009 sec.
iter 6130 || Loss: 2.2036 || timer: 0.0817 sec.
iter 6140 || Loss: 2.8899 || timer: 0.0951 sec.
iter 6150 || Loss: 2.6757 || timer: 0.0733 sec.
iter 6160 || Loss: 2.7981 || timer: 0.0261 sec.
iter 6170 || Loss: 2.3286 || timer: 0.0918 sec.
iter 6180 || Loss: 2.2378 || timer: 0.0798 sec.
iter 6190 || Loss: 2.3459 || timer: 0.0809 sec.
iter 6200 || Loss: 2.1223 || timer: 0.0844 sec.
iter 6210 || Loss: 3.3384 || timer: 0.0742 sec.
iter 6220 || Loss: 2.4229 || timer: 0.0740 sec.
iter 6230 || Loss: 2.2135 || timer: 0.0667 sec.
iter 6240 || Loss: 2.7017 || timer: 0.0672 sec.
iter 6250 || Loss: 1.9676 || timer: 0.0826 sec.
iter 6260 || Loss: 2.3493 || timer: 0.1005 sec.
iter 6270 || Loss: 1.9119 || timer: 0.0832 sec.
iter 6280 || Loss: 2.6521 || timer: 0.0897 sec.
iter 6290 || Loss: 1.9870 || timer: 0.0734 sec.
iter 6300 || Loss: 2.5454 || timer: 0.0764 sec.
iter 6310 || Loss: 2.6195 || timer: 0.0924 sec.
iter 6320 || Loss: 2.3664 || timer: 0.0744 sec.
iter 6330 || Loss: 2.1193 || timer: 0.1027 sec.
iter 6340 || Loss: 2.0260 || timer: 0.0841 sec.
iter 6350 || Loss: 1.4540 || timer: 0.0953 sec.
iter 6360 || Loss: 1.8526 || timer: 0.0915 sec.
iter 6370 || Loss: 1.9404 || timer: 0.0834 sec.
iter 6380 || Loss: 1.7035 || timer: 0.0943 sec.
iter 6390 || Loss: 2.1992 || timer: 0.0818 sec.
iter 6400 || Loss: 1.8110 || timer: 0.0907 sec.
iter 6410 || Loss: 2.1767 || timer: 0.0985 sec.
iter 6420 || Loss: 2.7222 || timer: 0.1031 sec.
iter 6430 || Loss: 2.0881 || timer: 0.0788 sec.
iter 6440 || Loss: 2.2411 || timer: 0.0738 sec.
iter 6450 || Loss: 2.0268 || timer: 0.0823 sec.
iter 6460 || Loss: 1.7228 || timer: 0.0882 sec.
iter 6470 || Loss: 2.1057 || timer: 0.0813 sec.
iter 6480 || Loss: 2.2158 || timer: 0.0785 sec.
iter 6490 || Loss: 2.0848 || timer: 0.0195 sec.
iter 6500 || Loss: 1.5072 || timer: 0.0861 sec.
iter 6510 || Loss: 1.9879 || timer: 0.0742 sec.
iter 6520 || Loss: 2.3210 || timer: 0.0824 sec.
iter 6530 || Loss: 2.2520 || timer: 0.0777 sec.
iter 6540 || Loss: 2.2866 || timer: 0.0802 sec.
iter 6550 || Loss: 1.9114 || timer: 0.0784 sec.
iter 6560 || Loss: 2.5038 || timer: 0.0820 sec.
iter 6570 || Loss: 2.4086 || timer: 0.0774 sec.
iter 6580 || Loss: 2.3322 || timer: 0.0719 sec.
iter 6590 || Loss: 2.0733 || timer: 0.1165 sec.
iter 6600 || Loss: 2.1272 || timer: 0.0786 sec.
iter 6610 || Loss: 2.7961 || timer: 0.0730 sec.
iter 6620 || Loss: 2.3656 || timer: 0.0715 sec.
iter 6630 || Loss: 2.2923 || timer: 0.0824 sec.
iter 6640 || Loss: 2.3375 || timer: 0.0818 sec.
iter 6650 || Loss: 2.1032 || timer: 0.0811 sec.
iter 6660 || Loss: 2.1309 || timer: 0.0789 sec.
iter 6670 || Loss: 2.1722 || timer: 0.0730 sec.
iter 6680 || Loss: 2.3326 || timer: 0.0792 sec.
iter 6690 || Loss: 2.0390 || timer: 0.0822 sec.
iter 6700 || Loss: 1.8674 || timer: 0.0747 sec.
iter 6710 || Loss: 2.2133 || timer: 0.0903 sec.
iter 6720 || Loss: 1.9808 || timer: 0.0812 sec.
iter 6730 || Loss: 2.2675 || timer: 0.0726 sec.
iter 6740 || Loss: 2.1338 || timer: 0.1002 sec.
iter 6750 || Loss: 2.1169 || timer: 0.0784 sec.
iter 6760 || Loss: 2.5613 || timer: 0.0880 sec.
iter 6770 || Loss: 2.2672 || timer: 0.0811 sec.
iter 6780 || Loss: 2.0537 || timer: 0.0737 sec.
iter 6790 || Loss: 1.8981 || timer: 0.0733 sec.
iter 6800 || Loss: 2.7323 || timer: 0.0746 sec.
iter 6810 || Loss: 1.7584 || timer: 0.0770 sec.
iter 6820 || Loss: 2.0584 || timer: 0.0215 sec.
iter 6830 || Loss: 2.4455 || timer: 0.0951 sec.
iter 6840 || Loss: 1.6591 || timer: 0.0989 sec.
iter 6850 || Loss: 2.0612 || timer: 0.0905 sec.
iter 6860 || Loss: 2.3895 || timer: 0.0811 sec.
iter 6870 || Loss: 3.0623 || timer: 0.0787 sec.
iter 6880 || Loss: 2.1415 || timer: 0.0811 sec.
iter 6890 || Loss: 2.4152 || timer: 0.0660 sec.
iter 6900 || Loss: 1.9896 || timer: 0.1033 sec.
iter 6910 || Loss: 2.2977 || timer: 0.0794 sec.
iter 6920 || Loss: 2.2360 || timer: 0.1110 sec.
iter 6930 || Loss: 2.2830 || timer: 0.0747 sec.
iter 6940 || Loss: 2.6855 || timer: 0.0797 sec.
iter 6950 || Loss: 2.1924 || timer: 0.0838 sec.
iter 6960 || Loss: 1.7047 || timer: 0.0830 sec.
iter 6970 || Loss: 2.6714 || timer: 0.0876 sec.
iter 6980 || Loss: 2.5116 || timer: 0.0847 sec.
iter 6990 || Loss: 2.0655 || timer: 0.0799 sec.
iter 7000 || Loss: 2.0666 || timer: 0.0815 sec.
iter 7010 || Loss: 2.1661 || timer: 0.0814 sec.
iter 7020 || Loss: 2.0051 || timer: 0.0850 sec.
iter 7030 || Loss: 2.1081 || timer: 0.0800 sec.
iter 7040 || Loss: 2.4417 || timer: 0.0740 sec.
iter 7050 || Loss: 1.7188 || timer: 0.0814 sec.
iter 7060 || Loss: 2.2385 || timer: 0.1015 sec.
iter 7070 || Loss: 6.0136 || timer: 0.0838 sec.
iter 7080 || Loss: 3.2594 || timer: 0.0821 sec.
iter 7090 || Loss: 3.5628 || timer: 0.0812 sec.
iter 7100 || Loss: 2.0207 || timer: 0.0822 sec.
iter 7110 || Loss: 2.1771 || timer: 0.1176 sec.
iter 7120 || Loss: 1.9167 || timer: 0.0814 sec.
iter 7130 || Loss: 2.3775 || timer: 0.0832 sec.
iter 7140 || Loss: 2.1346 || timer: 0.0730 sec.
iter 7150 || Loss: 2.2041 || timer: 0.0148 sec.
iter 7160 || Loss: 0.9734 || timer: 0.0807 sec.
iter 7170 || Loss: 1.8338 || timer: 0.0735 sec.
iter 7180 || Loss: 2.0232 || timer: 0.0690 sec.
iter 7190 || Loss: 2.4606 || timer: 0.0810 sec.
iter 7200 || Loss: 2.1546 || timer: 0.0925 sec.
iter 7210 || Loss: 1.9959 || timer: 0.0839 sec.
iter 7220 || Loss: 2.1870 || timer: 0.0841 sec.
iter 7230 || Loss: 1.9714 || timer: 0.0786 sec.
iter 7240 || Loss: 2.5185 || timer: 0.0990 sec.
iter 7250 || Loss: 1.9799 || timer: 0.1107 sec.
iter 7260 || Loss: 2.1434 || timer: 0.0740 sec.
iter 7270 || Loss: 2.1889 || timer: 0.0781 sec.
iter 7280 || Loss: 1.9754 || timer: 0.0794 sec.
iter 7290 || Loss: 1.7603 || timer: 0.0898 sec.
iter 7300 || Loss: 1.8181 || timer: 0.0843 sec.
iter 7310 || Loss: 2.5203 || timer: 0.0950 sec.
iter 7320 || Loss: 2.0563 || timer: 0.0802 sec.
iter 7330 || Loss: 2.0376 || timer: 0.0735 sec.
iter 7340 || Loss: 2.3350 || timer: 0.1144 sec.
iter 7350 || Loss: 1.8596 || timer: 0.0811 sec.
iter 7360 || Loss: 2.0618 || timer: 0.0732 sec.
iter 7370 || Loss: 2.1693 || timer: 0.1139 sec.
iter 7380 || Loss: 2.2659 || timer: 0.0736 sec.
iter 7390 || Loss: 2.1405 || timer: 0.0801 sec.
iter 7400 || Loss: 2.1237 || timer: 0.0840 sec.
iter 7410 || Loss: 2.1545 || timer: 0.0849 sec.
iter 7420 || Loss: 2.1733 || timer: 0.0850 sec.
iter 7430 || Loss: 2.8516 || timer: 0.0944 sec.
iter 7440 || Loss: 2.3364 || timer: 0.0857 sec.
iter 7450 || Loss: 2.4430 || timer: 0.1148 sec.
iter 7460 || Loss: 4.3609 || timer: 0.0819 sec.
iter 7470 || Loss: 3.4160 || timer: 0.0732 sec.
iter 7480 || Loss: 2.0401 || timer: 0.0255 sec.
iter 7490 || Loss: 9.4983 || timer: 0.0954 sec.
iter 7500 || Loss: 3.1904 || timer: 0.0721 sec.
iter 7510 || Loss: 2.3647 || timer: 0.0808 sec.
iter 7520 || Loss: 2.7673 || timer: 0.0763 sec.
iter 7530 || Loss: 3.9014 || timer: 0.0806 sec.
iter 7540 || Loss: 3.5033 || timer: 0.0813 sec.
iter 7550 || Loss: 2.0953 || timer: 0.0823 sec.
iter 7560 || Loss: 2.5820 || timer: 0.0812 sec.
iter 7570 || Loss: 2.0490 || timer: 0.0734 sec.
iter 7580 || Loss: 1.7853 || timer: 0.1180 sec.
iter 7590 || Loss: 2.1463 || timer: 0.0806 sec.
iter 7600 || Loss: 2.2309 || timer: 0.0807 sec.
iter 7610 || Loss: 2.2055 || timer: 0.0854 sec.
iter 7620 || Loss: 1.6346 || timer: 0.0973 sec.
iter 7630 || Loss: 2.0010 || timer: 0.0796 sec.
iter 7640 || Loss: 1.8297 || timer: 0.0729 sec.
iter 7650 || Loss: 2.7418 || timer: 0.0783 sec.
iter 7660 || Loss: 1.8882 || timer: 0.0769 sec.
iter 7670 || Loss: 2.4300 || timer: 0.0832 sec.
iter 7680 || Loss: 1.7395 || timer: 0.0744 sec.
iter 7690 || Loss: 2.1803 || timer: 0.0744 sec.
iter 7700 || Loss: 1.8498 || timer: 0.0763 sec.
iter 7710 || Loss: 1.9852 || timer: 0.0737 sec.
iter 7720 || Loss: 2.5124 || timer: 0.0884 sec.
iter 7730 || Loss: 1.9965 || timer: 0.0808 sec.
iter 7740 || Loss: 1.7065 || timer: 0.0794 sec.
iter 7750 || Loss: 2.0436 || timer: 0.0757 sec.
iter 7760 || Loss: 1.5046 || timer: 0.0853 sec.
iter 7770 || Loss: 1.6228 || timer: 0.0829 sec.
iter 7780 || Loss: 2.0873 || timer: 0.0753 sec.
iter 7790 || Loss: 1.9956 || timer: 0.0991 sec.
iter 7800 || Loss: 2.1230 || timer: 0.0794 sec.
iter 7810 || Loss: 1.9376 || timer: 0.0258 sec.
iter 7820 || Loss: 1.1765 || timer: 0.0814 sec.
iter 7830 || Loss: 1.6403 || timer: 0.0759 sec.
iter 7840 || Loss: 1.7137 || timer: 0.0792 sec.
iter 7850 || Loss: 2.0456 || timer: 0.0839 sec.
iter 7860 || Loss: 2.1789 || timer: 0.0848 sec.
iter 7870 || Loss: 2.1934 || timer: 0.0750 sec.
iter 7880 || Loss: 2.4536 || timer: 0.0749 sec.
iter 7890 || Loss: 1.8527 || timer: 0.0819 sec.
iter 7900 || Loss: 2.2452 || timer: 0.0799 sec.
iter 7910 || Loss: 1.9894 || timer: 0.0918 sec.
iter 7920 || Loss: 1.8998 || timer: 0.1002 sec.
iter 7930 || Loss: 2.0866 || timer: 0.0834 sec.
iter 7940 || Loss: 1.8021 || timer: 0.0756 sec.
iter 7950 || Loss: 1.8662 || timer: 0.0748 sec.
iter 7960 || Loss: 2.1328 || timer: 0.1129 sec.
iter 7970 || Loss: 2.2195 || timer: 0.0948 sec.
iter 7980 || Loss: 2.0490 || timer: 0.0957 sec.
iter 7990 || Loss: 2.1481 || timer: 0.0730 sec.
iter 8000 || Loss: 1.9043 || timer: 0.0823 sec.
iter 8010 || Loss: 1.6706 || timer: 0.0839 sec.
iter 8020 || Loss: 1.7261 || timer: 0.0853 sec.
iter 8030 || Loss: 1.5589 || timer: 0.0832 sec.
iter 8040 || Loss: 1.6666 || timer: 0.0835 sec.
iter 8050 || Loss: 1.7268 || timer: 0.0946 sec.
iter 8060 || Loss: 1.6237 || timer: 0.0810 sec.
iter 8070 || Loss: 2.1756 || timer: 0.0760 sec.
iter 8080 || Loss: 1.8061 || timer: 0.0665 sec.
iter 8090 || Loss: 1.6884 || timer: 0.0731 sec.
iter 8100 || Loss: 1.3480 || timer: 0.0866 sec.
iter 8110 || Loss: 1.9170 || timer: 0.0830 sec.
iter 8120 || Loss: 2.0449 || timer: 0.0803 sec.
iter 8130 || Loss: 1.7133 || timer: 0.0793 sec.
iter 8140 || Loss: 1.9958 || timer: 0.0178 sec.
iter 8150 || Loss: 2.6895 || timer: 0.0823 sec.
iter 8160 || Loss: 2.1384 || timer: 0.0807 sec.
iter 8170 || Loss: 1.8475 || timer: 0.0798 sec.
iter 8180 || Loss: 1.8231 || timer: 0.0804 sec.
iter 8190 || Loss: 2.3100 || timer: 0.0817 sec.
iter 8200 || Loss: 1.3515 || timer: 0.0829 sec.
iter 8210 || Loss: 1.8023 || timer: 0.0737 sec.
iter 8220 || Loss: 1.9099 || timer: 0.0812 sec.
iter 8230 || Loss: 1.9197 || timer: 0.0820 sec.
iter 8240 || Loss: 2.0397 || timer: 0.0984 sec.
iter 8250 || Loss: 2.0102 || timer: 0.0827 sec.
iter 8260 || Loss: 1.7309 || timer: 0.0788 sec.
iter 8270 || Loss: 1.5551 || timer: 0.0784 sec.
iter 8280 || Loss: 1.8544 || timer: 0.0827 sec.
iter 8290 || Loss: 1.4291 || timer: 0.0825 sec.
iter 8300 || Loss: 1.8419 || timer: 0.1039 sec.
iter 8310 || Loss: 1.7537 || timer: 0.0839 sec.
iter 8320 || Loss: 1.7266 || timer: 0.0744 sec.
iter 8330 || Loss: 1.6336 || timer: 0.0912 sec.
iter 8340 || Loss: 1.3300 || timer: 0.0735 sec.
iter 8350 || Loss: 1.6599 || timer: 0.0870 sec.
iter 8360 || Loss: 1.6678 || timer: 0.0789 sec.
iter 8370 || Loss: 1.3727 || timer: 0.0722 sec.
iter 8380 || Loss: 1.5494 || timer: 0.0745 sec.
iter 8390 || Loss: 1.8808 || timer: 0.0952 sec.
iter 8400 || Loss: 1.7964 || timer: 0.0841 sec.
iter 8410 || Loss: 1.3694 || timer: 0.0844 sec.
iter 8420 || Loss: 1.7753 || timer: 0.0811 sec.
iter 8430 || Loss: 1.5332 || timer: 0.0728 sec.
iter 8440 || Loss: 2.2627 || timer: 0.0702 sec.
iter 8450 || Loss: 1.4300 || timer: 0.0801 sec.
iter 8460 || Loss: 1.8655 || timer: 0.0949 sec.
iter 8470 || Loss: 1.9888 || timer: 0.0145 sec.
iter 8480 || Loss: 2.1276 || timer: 0.0933 sec.
iter 8490 || Loss: 1.7559 || timer: 0.0821 sec.
iter 8500 || Loss: 1.5297 || timer: 0.0794 sec.
iter 8510 || Loss: 1.9987 || timer: 0.0817 sec.
iter 8520 || Loss: 1.9982 || timer: 0.0826 sec.
iter 8530 || Loss: 1.8291 || timer: 0.1008 sec.
iter 8540 || Loss: 1.8528 || timer: 0.0670 sec.
iter 8550 || Loss: 1.9282 || timer: 0.0838 sec.
iter 8560 || Loss: 1.9371 || timer: 0.0734 sec.
iter 8570 || Loss: 1.4491 || timer: 0.1020 sec.
iter 8580 || Loss: 1.9225 || timer: 0.0833 sec.
iter 8590 || Loss: 1.5691 || timer: 0.0801 sec.
iter 8600 || Loss: 1.8803 || timer: 0.0817 sec.
iter 8610 || Loss: 1.5679 || timer: 0.0752 sec.
iter 8620 || Loss: 1.7661 || timer: 0.0871 sec.
iter 8630 || Loss: 1.5130 || timer: 0.0899 sec.
iter 8640 || Loss: 2.1771 || timer: 0.0934 sec.
iter 8650 || Loss: 1.9621 || timer: 0.0808 sec.
iter 8660 || Loss: 1.4267 || timer: 0.0807 sec.
iter 8670 || Loss: 1.8280 || timer: 0.0839 sec.
iter 8680 || Loss: 2.2823 || timer: 0.0930 sec.
iter 8690 || Loss: 1.9112 || timer: 0.0769 sec.
iter 8700 || Loss: 1.4195 || timer: 0.0708 sec.
iter 8710 || Loss: 2.0096 || timer: 0.0991 sec.
iter 8720 || Loss: 1.8804 || timer: 0.0839 sec.
iter 8730 || Loss: 1.2839 || timer: 0.0740 sec.
iter 8740 || Loss: 2.0750 || timer: 0.0846 sec.
iter 8750 || Loss: 1.3679 || timer: 0.0806 sec.
iter 8760 || Loss: 1.3951 || timer: 0.1002 sec.
iter 8770 || Loss: 1.6484 || timer: 0.0745 sec.
iter 8780 || Loss: 1.6397 || timer: 0.0799 sec.
iter 8790 || Loss: 1.4898 || timer: 0.0729 sec.
iter 8800 || Loss: 1.7087 || timer: 0.0281 sec.
iter 8810 || Loss: 1.6693 || timer: 0.0805 sec.
iter 8820 || Loss: 2.0425 || timer: 0.0764 sec.
iter 8830 || Loss: 1.6183 || timer: 0.0805 sec.
iter 8840 || Loss: 1.8270 || timer: 0.0747 sec.
iter 8850 || Loss: 1.7128 || timer: 0.0804 sec.
iter 8860 || Loss: 1.8111 || timer: 0.0824 sec.
iter 8870 || Loss: 1.7859 || timer: 0.0818 sec.
iter 8880 || Loss: 1.4389 || timer: 0.0731 sec.
iter 8890 || Loss: 1.7183 || timer: 0.0837 sec.
iter 8900 || Loss: 1.4342 || timer: 0.1133 sec.
iter 8910 || Loss: 1.5734 || timer: 0.0833 sec.
iter 8920 || Loss: 1.6104 || timer: 0.0821 sec.
iter 8930 || Loss: 1.4902 || timer: 0.1237 sec.
iter 8940 || Loss: 1.6286 || timer: 0.0783 sec.
iter 8950 || Loss: 1.7151 || timer: 0.0731 sec.
iter 8960 || Loss: 1.8556 || timer: 0.0754 sec.
iter 8970 || Loss: 1.4035 || timer: 0.1064 sec.
iter 8980 || Loss: 1.7761 || timer: 0.0981 sec.
iter 8990 || Loss: 1.1924 || timer: 0.0952 sec.
iter 9000 || Loss: 1.7258 || timer: 0.0742 sec.
iter 9010 || Loss: 2.0922 || timer: 0.0826 sec.
iter 9020 || Loss: 1.7313 || timer: 0.0727 sec.
iter 9030 || Loss: 1.9245 || timer: 0.0837 sec.
iter 9040 || Loss: 1.6202 || timer: 0.0795 sec.
iter 9050 || Loss: 1.9040 || timer: 0.0816 sec.
iter 9060 || Loss: 1.5671 || timer: 0.0852 sec.
iter 9070 || Loss: 1.9749 || timer: 0.0795 sec.
iter 9080 || Loss: 1.8832 || timer: 0.0834 sec.
iter 9090 || Loss: 1.5987 || timer: 0.0833 sec.
iter 9100 || Loss: 1.5427 || timer: 0.0762 sec.
iter 9110 || Loss: 1.5397 || timer: 0.0814 sec.
iter 9120 || Loss: 1.5244 || timer: 0.0837 sec.
iter 9130 || Loss: 2.4834 || timer: 0.0206 sec.
iter 9140 || Loss: 1.4709 || timer: 0.0822 sec.
iter 9150 || Loss: 1.5459 || timer: 0.0812 sec.
iter 9160 || Loss: 1.9563 || timer: 0.0786 sec.
iter 9170 || Loss: 1.7070 || timer: 0.0726 sec.
iter 9180 || Loss: 1.6781 || timer: 0.0742 sec.
iter 9190 || Loss: 1.6715 || timer: 0.0724 sec.
iter 9200 || Loss: 1.5893 || timer: 0.1186 sec.
iter 9210 || Loss: 1.6146 || timer: 0.1063 sec.
iter 9220 || Loss: 1.9253 || timer: 0.0796 sec.
iter 9230 || Loss: 1.4555 || timer: 0.0955 sec.
iter 9240 || Loss: 1.8114 || timer: 0.0902 sec.
iter 9250 || Loss: 1.5039 || timer: 0.0838 sec.
iter 9260 || Loss: 1.9267 || timer: 0.0903 sec.
iter 9270 || Loss: 1.6166 || timer: 0.1097 sec.
iter 9280 || Loss: 1.8658 || timer: 0.0773 sec.
iter 9290 || Loss: 1.7660 || timer: 0.0802 sec.
iter 9300 || Loss: 1.3961 || timer: 0.1001 sec.
iter 9310 || Loss: 2.1810 || timer: 0.0826 sec.
iter 9320 || Loss: 1.8860 || timer: 0.0653 sec.
iter 9330 || Loss: 1.6534 || timer: 0.0846 sec.
iter 9340 || Loss: 1.8057 || timer: 0.0676 sec.
iter 9350 || Loss: 1.9027 || timer: 0.0736 sec.
iter 9360 || Loss: 1.9687 || timer: 0.0792 sec.
iter 9370 || Loss: 1.6109 || timer: 0.0761 sec.
iter 9380 || Loss: 1.7950 || timer: 0.0808 sec.
iter 9390 || Loss: 2.1148 || timer: 0.0813 sec.
iter 9400 || Loss: 1.7910 || timer: 0.0740 sec.
iter 9410 || Loss: 1.3712 || timer: 0.0803 sec.
iter 9420 || Loss: 1.9260 || timer: 0.0873 sec.
iter 9430 || Loss: 1.7184 || timer: 0.0833 sec.
iter 9440 || Loss: 1.7825 || timer: 0.0805 sec.
iter 9450 || Loss: 1.7893 || timer: 0.0800 sec.
iter 9460 || Loss: 1.7541 || timer: 0.0170 sec.
iter 9470 || Loss: 0.8285 || timer: 0.0664 sec.
iter 9480 || Loss: 1.6597 || timer: 0.0736 sec.
iter 9490 || Loss: 1.6190 || timer: 0.0796 sec.
iter 9500 || Loss: 1.7346 || timer: 0.0786 sec.
iter 9510 || Loss: 1.9159 || timer: 0.0944 sec.
iter 9520 || Loss: 1.7214 || timer: 0.0818 sec.
iter 9530 || Loss: 1.6363 || timer: 0.0807 sec.
iter 9540 || Loss: 1.3368 || timer: 0.0733 sec.
iter 9550 || Loss: 1.4521 || timer: 0.0794 sec.
iter 9560 || Loss: 1.9139 || timer: 0.1239 sec.
iter 9570 || Loss: 1.5699 || timer: 0.0806 sec.
iter 9580 || Loss: 1.3956 || timer: 0.0852 sec.
iter 9590 || Loss: 1.8218 || timer: 0.1193 sec.
iter 9600 || Loss: 1.5662 || timer: 0.0859 sec.
iter 9610 || Loss: 1.7484 || timer: 0.0735 sec.
iter 9620 || Loss: 1.8199 || timer: 0.0989 sec.
iter 9630 || Loss: 1.6464 || timer: 0.0997 sec.
iter 9640 || Loss: 1.8138 || timer: 0.0742 sec.
iter 9650 || Loss: 1.1961 || timer: 0.0894 sec.
iter 9660 || Loss: 1.8925 || timer: 0.0872 sec.
iter 9670 || Loss: 1.9769 || timer: 0.0966 sec.
iter 9680 || Loss: 1.4607 || timer: 0.0807 sec.
iter 9690 || Loss: 1.4685 || timer: 0.0734 sec.
iter 9700 || Loss: 1.6459 || timer: 0.0798 sec.
iter 9710 || Loss: 1.5622 || timer: 0.0798 sec.
iter 9720 || Loss: 1.7702 || timer: 0.0837 sec.
iter 9730 || Loss: 2.0493 || timer: 0.1019 sec.
iter 9740 || Loss: 1.9967 || timer: 0.0745 sec.
iter 9750 || Loss: 2.0985 || timer: 0.0770 sec.
iter 9760 || Loss: 1.4466 || timer: 0.0742 sec.
iter 9770 || Loss: 1.8739 || timer: 0.0804 sec.
iter 9780 || Loss: 1.5256 || timer: 0.0727 sec.
iter 9790 || Loss: 1.4800 || timer: 0.0245 sec.
iter 9800 || Loss: 1.6814 || timer: 0.1002 sec.
iter 9810 || Loss: 1.8873 || timer: 0.0888 sec.
iter 9820 || Loss: 1.8951 || timer: 0.0791 sec.
iter 9830 || Loss: 1.5170 || timer: 0.0842 sec.
iter 9840 || Loss: 1.7339 || timer: 0.0816 sec.
iter 9850 || Loss: 1.4763 || timer: 0.0787 sec.
iter 9860 || Loss: 1.5735 || timer: 0.0826 sec.
iter 9870 || Loss: 1.5299 || timer: 0.0874 sec.
iter 9880 || Loss: 2.0443 || timer: 0.0922 sec.
iter 9890 || Loss: 1.4817 || timer: 0.0959 sec.
iter 9900 || Loss: 1.5508 || timer: 0.0796 sec.
iter 9910 || Loss: 1.5263 || timer: 0.0831 sec.
iter 9920 || Loss: 2.0252 || timer: 0.1038 sec.
iter 9930 || Loss: 1.7147 || timer: 0.0724 sec.
iter 9940 || Loss: 1.9184 || timer: 0.0822 sec.
iter 9950 || Loss: 1.4227 || timer: 0.0805 sec.
iter 9960 || Loss: 1.5681 || timer: 0.0791 sec.
iter 9970 || Loss: 1.5070 || timer: 0.1015 sec.
iter 9980 || Loss: 1.8854 || timer: 0.0801 sec.
iter 9990 || Loss: 1.9687 || timer: 0.0861 sec.
iter 10000 || Loss: 1.5463 || Saving state, iter: 10000
timer: 0.0816 sec.
iter 10010 || Loss: 1.6685 || timer: 0.0845 sec.
iter 10020 || Loss: 1.7919 || timer: 0.0785 sec.
iter 10030 || Loss: 1.7756 || timer: 0.0835 sec.
iter 10040 || Loss: 1.6051 || timer: 0.1086 sec.
iter 10050 || Loss: 1.9338 || timer: 0.0840 sec.
iter 10060 || Loss: 1.8155 || timer: 0.0812 sec.
iter 10070 || Loss: 1.5598 || timer: 0.1099 sec.
iter 10080 || Loss: 1.9542 || timer: 0.0806 sec.
iter 10090 || Loss: 1.4872 || timer: 0.0784 sec.
iter 10100 || Loss: 1.6598 || timer: 0.0938 sec.
iter 10110 || Loss: 1.2578 || timer: 0.0727 sec.
iter 10120 || Loss: 1.9045 || timer: 0.0174 sec.
iter 10130 || Loss: 1.4310 || timer: 0.0731 sec.
iter 10140 || Loss: 1.7144 || timer: 0.0801 sec.
iter 10150 || Loss: 1.4155 || timer: 0.0814 sec.
iter 10160 || Loss: 1.7139 || timer: 0.0766 sec.
iter 10170 || Loss: 1.6911 || timer: 0.0735 sec.
iter 10180 || Loss: 1.5889 || timer: 0.0995 sec.
iter 10190 || Loss: 1.5354 || timer: 0.0774 sec.
iter 10200 || Loss: 2.0169 || timer: 0.0734 sec.
iter 10210 || Loss: 1.8854 || timer: 0.0731 sec.
iter 10220 || Loss: 1.7999 || timer: 0.1107 sec.
iter 10230 || Loss: 1.8993 || timer: 0.0792 sec.
iter 10240 || Loss: 1.8981 || timer: 0.0897 sec.
iter 10250 || Loss: 1.4959 || timer: 0.0775 sec.
iter 10260 || Loss: 1.9307 || timer: 0.0792 sec.
iter 10270 || Loss: 1.1968 || timer: 0.0835 sec.
iter 10280 || Loss: 2.0318 || timer: 0.0738 sec.
iter 10290 || Loss: 1.9539 || timer: 0.0746 sec.
iter 10300 || Loss: 1.6496 || timer: 0.0829 sec.
iter 10310 || Loss: 1.6175 || timer: 0.0862 sec.
iter 10320 || Loss: 1.5002 || timer: 0.0849 sec.
iter 10330 || Loss: 1.2481 || timer: 0.0822 sec.
iter 10340 || Loss: 1.7692 || timer: 0.0738 sec.
iter 10350 || Loss: 1.8447 || timer: 0.0710 sec.
iter 10360 || Loss: 1.6121 || timer: 0.0875 sec.
iter 10370 || Loss: 1.5974 || timer: 0.0978 sec.
iter 10380 || Loss: 1.8823 || timer: 0.0743 sec.
iter 10390 || Loss: 1.8672 || timer: 0.0894 sec.
iter 10400 || Loss: 1.7705 || timer: 0.0835 sec.
iter 10410 || Loss: 1.6874 || timer: 0.0739 sec.
iter 10420 || Loss: 1.5857 || timer: 0.0737 sec.
iter 10430 || Loss: 1.7546 || timer: 0.0745 sec.
iter 10440 || Loss: 1.9537 || timer: 0.0946 sec.
iter 10450 || Loss: 1.7020 || timer: 0.0283 sec.
iter 10460 || Loss: 2.7074 || timer: 0.0840 sec.
iter 10470 || Loss: 1.6806 || timer: 0.0782 sec.
iter 10480 || Loss: 1.7623 || timer: 0.0726 sec.
iter 10490 || Loss: 1.6566 || timer: 0.0905 sec.
iter 10500 || Loss: 1.8697 || timer: 0.0865 sec.
iter 10510 || Loss: 1.5468 || timer: 0.0730 sec.
iter 10520 || Loss: 1.1650 || timer: 0.0846 sec.
iter 10530 || Loss: 1.5440 || timer: 0.0904 sec.
iter 10540 || Loss: 1.5629 || timer: 0.0844 sec.
iter 10550 || Loss: 1.7531 || timer: 0.1101 sec.
iter 10560 || Loss: 1.9750 || timer: 0.0787 sec.
iter 10570 || Loss: 1.8226 || timer: 0.0794 sec.
iter 10580 || Loss: 1.7462 || timer: 0.0818 sec.
iter 10590 || Loss: 1.8632 || timer: 0.0822 sec.
iter 10600 || Loss: 1.8231 || timer: 0.0919 sec.
iter 10610 || Loss: 1.8798 || timer: 0.0807 sec.
iter 10620 || Loss: 2.0001 || timer: 0.0724 sec.
iter 10630 || Loss: 1.6637 || timer: 0.0781 sec.
iter 10640 || Loss: 1.7767 || timer: 0.0978 sec.
iter 10650 || Loss: 1.7598 || timer: 0.0838 sec.
iter 10660 || Loss: 1.5879 || timer: 0.0850 sec.
iter 10670 || Loss: 2.2408 || timer: 0.0832 sec.
iter 10680 || Loss: 1.2955 || timer: 0.0804 sec.
iter 10690 || Loss: 1.6893 || timer: 0.0712 sec.
iter 10700 || Loss: 2.2609 || timer: 0.0936 sec.
iter 10710 || Loss: 1.5368 || timer: 0.0728 sec.
iter 10720 || Loss: 1.9826 || timer: 0.0762 sec.
iter 10730 || Loss: 1.9544 || timer: 0.0730 sec.
iter 10740 || Loss: 1.6125 || timer: 0.0976 sec.
iter 10750 || Loss: 1.9884 || timer: 0.0822 sec.
iter 10760 || Loss: 1.5947 || timer: 0.0836 sec.
iter 10770 || Loss: 1.5616 || timer: 0.0786 sec.
iter 10780 || Loss: 2.0854 || timer: 0.0277 sec.
iter 10790 || Loss: 2.4393 || timer: 0.0819 sec.
iter 10800 || Loss: 1.5050 || timer: 0.0803 sec.
iter 10810 || Loss: 1.9823 || timer: 0.0798 sec.
iter 10820 || Loss: 2.0882 || timer: 0.0810 sec.
iter 10830 || Loss: 1.8530 || timer: 0.1228 sec.
iter 10840 || Loss: 1.6339 || timer: 0.0987 sec.
iter 10850 || Loss: 1.6968 || timer: 0.0806 sec.
iter 10860 || Loss: 1.4197 || timer: 0.0847 sec.
iter 10870 || Loss: 2.0261 || timer: 0.0753 sec.
iter 10880 || Loss: 1.4608 || timer: 0.0922 sec.
iter 10890 || Loss: 1.7300 || timer: 0.0789 sec.
iter 10900 || Loss: 1.9632 || timer: 0.0734 sec.
iter 10910 || Loss: 1.4337 || timer: 0.0830 sec.
iter 10920 || Loss: 1.7289 || timer: 0.0826 sec.
iter 10930 || Loss: 1.4206 || timer: 0.1007 sec.
iter 10940 || Loss: 1.8289 || timer: 0.1072 sec.
iter 10950 || Loss: 1.6340 || timer: 0.0799 sec.
iter 10960 || Loss: 2.2839 || timer: 0.0828 sec.
iter 10970 || Loss: 1.8043 || timer: 0.0795 sec.
iter 10980 || Loss: 1.3735 || timer: 0.0739 sec.
iter 10990 || Loss: 1.4061 || timer: 0.0892 sec.
iter 11000 || Loss: 1.7896 || timer: 0.0802 sec.
iter 11010 || Loss: 1.9513 || timer: 0.0799 sec.
iter 11020 || Loss: 1.9797 || timer: 0.0851 sec.
iter 11030 || Loss: 1.4572 || timer: 0.0809 sec.
iter 11040 || Loss: 1.9745 || timer: 0.0806 sec.
iter 11050 || Loss: 1.5758 || timer: 0.0799 sec.
iter 11060 || Loss: 1.8106 || timer: 0.0802 sec.
iter 11070 || Loss: 1.6722 || timer: 0.0955 sec.
iter 11080 || Loss: 1.4853 || timer: 0.0825 sec.
iter 11090 || Loss: 1.6636 || timer: 0.0785 sec.
iter 11100 || Loss: 1.6726 || timer: 0.1050 sec.
iter 11110 || Loss: 1.6498 || timer: 0.0252 sec.
iter 11120 || Loss: 0.9844 || timer: 0.0816 sec.
iter 11130 || Loss: 2.2676 || timer: 0.0805 sec.
iter 11140 || Loss: 1.6565 || timer: 0.0819 sec.
iter 11150 || Loss: 1.8722 || timer: 0.0840 sec.
iter 11160 || Loss: 1.3961 || timer: 0.0755 sec.
iter 11170 || Loss: 1.5787 || timer: 0.0769 sec.
iter 11180 || Loss: 1.6822 || timer: 0.0735 sec.
iter 11190 || Loss: 1.8402 || timer: 0.0817 sec.
iter 11200 || Loss: 2.0789 || timer: 0.0734 sec.
iter 11210 || Loss: 1.8992 || timer: 0.1024 sec.
iter 11220 || Loss: 1.4931 || timer: 0.0850 sec.
iter 11230 || Loss: 1.2647 || timer: 0.0831 sec.
iter 11240 || Loss: 1.9717 || timer: 0.0738 sec.
iter 11250 || Loss: 1.8772 || timer: 0.0813 sec.
iter 11260 || Loss: 1.6725 || timer: 0.0837 sec.
iter 11270 || Loss: 1.7645 || timer: 0.0830 sec.
iter 11280 || Loss: 1.9315 || timer: 0.0963 sec.
iter 11290 || Loss: 1.8913 || timer: 0.0814 sec.
iter 11300 || Loss: 1.6674 || timer: 0.0862 sec.
iter 11310 || Loss: 1.6575 || timer: 0.0752 sec.
iter 11320 || Loss: 1.6772 || timer: 0.0795 sec.
iter 11330 || Loss: 1.8116 || timer: 0.0795 sec.
iter 11340 || Loss: 1.7355 || timer: 0.0793 sec.
iter 11350 || Loss: 1.8249 || timer: 0.1055 sec.
iter 11360 || Loss: 1.6782 || timer: 0.0793 sec.
iter 11370 || Loss: 1.5112 || timer: 0.1075 sec.
iter 11380 || Loss: 1.9572 || timer: 0.0831 sec.
iter 11390 || Loss: 1.7564 || timer: 0.0813 sec.
iter 11400 || Loss: 1.8184 || timer: 0.0884 sec.
iter 11410 || Loss: 1.8770 || timer: 0.0816 sec.
iter 11420 || Loss: 1.7678 || timer: 0.0822 sec.
iter 11430 || Loss: 1.4645 || timer: 0.0950 sec.
iter 11440 || Loss: 1.9004 || timer: 0.0176 sec.
iter 11450 || Loss: 2.7762 || timer: 0.0818 sec.
iter 11460 || Loss: 1.8852 || timer: 0.0819 sec.
iter 11470 || Loss: 2.2783 || timer: 0.0794 sec.
iter 11480 || Loss: 2.2155 || timer: 0.0735 sec.
iter 11490 || Loss: 1.4569 || timer: 0.0894 sec.
iter 11500 || Loss: 1.8191 || timer: 0.0727 sec.
iter 11510 || Loss: 1.8438 || timer: 0.0854 sec.
iter 11520 || Loss: 1.6832 || timer: 0.0816 sec.
iter 11530 || Loss: 1.9195 || timer: 0.0813 sec.
iter 11540 || Loss: 1.5027 || timer: 0.1450 sec.
iter 11550 || Loss: 1.7644 || timer: 0.0814 sec.
iter 11560 || Loss: 1.6401 || timer: 0.0800 sec.
iter 11570 || Loss: 1.8355 || timer: 0.0942 sec.
iter 11580 || Loss: 1.4065 || timer: 0.0818 sec.
iter 11590 || Loss: 1.4076 || timer: 0.0825 sec.
iter 11600 || Loss: 1.8031 || timer: 0.0726 sec.
iter 11610 || Loss: 1.5434 || timer: 0.0810 sec.
iter 11620 || Loss: 1.2386 || timer: 0.0733 sec.
iter 11630 || Loss: 1.6371 || timer: 0.0853 sec.
iter 11640 || Loss: 1.3040 || timer: 0.0779 sec.
iter 11650 || Loss: 1.5807 || timer: 0.0816 sec.
iter 11660 || Loss: 1.6062 || timer: 0.0737 sec.
iter 11670 || Loss: 1.7484 || timer: 0.0817 sec.
iter 11680 || Loss: 1.6729 || timer: 0.0822 sec.
iter 11690 || Loss: 1.6712 || timer: 0.0916 sec.
iter 11700 || Loss: 2.1336 || timer: 0.0804 sec.
iter 11710 || Loss: 1.6410 || timer: 0.0743 sec.
iter 11720 || Loss: 2.2685 || timer: 0.0820 sec.
iter 11730 || Loss: 2.0053 || timer: 0.0737 sec.
iter 11740 || Loss: 1.9260 || timer: 0.0813 sec.
iter 11750 || Loss: 1.5663 || timer: 0.0805 sec.
iter 11760 || Loss: 1.8151 || timer: 0.0740 sec.
iter 11770 || Loss: 1.4090 || timer: 0.0270 sec.
iter 11780 || Loss: 0.7353 || timer: 0.0799 sec.
iter 11790 || Loss: 1.7651 || timer: 0.0744 sec.
iter 11800 || Loss: 1.7665 || timer: 0.0643 sec.
iter 11810 || Loss: 1.7052 || timer: 0.0761 sec.
iter 11820 || Loss: 1.6982 || timer: 0.0796 sec.
iter 11830 || Loss: 1.6739 || timer: 0.0776 sec.
iter 11840 || Loss: 1.1653 || timer: 0.0809 sec.
iter 11850 || Loss: 1.7185 || timer: 0.0739 sec.
iter 11860 || Loss: 1.8966 || timer: 0.0739 sec.
iter 11870 || Loss: 1.7753 || timer: 0.0944 sec.
iter 11880 || Loss: 1.4413 || timer: 0.0827 sec.
iter 11890 || Loss: 1.5842 || timer: 0.0892 sec.
iter 11900 || Loss: 1.6821 || timer: 0.0790 sec.
iter 11910 || Loss: 1.4195 || timer: 0.0789 sec.
iter 11920 || Loss: 1.2720 || timer: 0.0948 sec.
iter 11930 || Loss: 1.1830 || timer: 0.0945 sec.
iter 11940 || Loss: 1.3831 || timer: 0.0989 sec.
iter 11950 || Loss: 1.9722 || timer: 0.1041 sec.
iter 11960 || Loss: 1.7532 || timer: 0.0792 sec.
iter 11970 || Loss: 1.5820 || timer: 0.0860 sec.
iter 11980 || Loss: 1.9533 || timer: 0.0787 sec.
iter 11990 || Loss: 1.4199 || python: can't open file '/home/xj/xu/ssd.pytorch/eval/': [Errno 2] No such file or directory
python: can't open file '/home/xj/xu/ssd.pytorch/eval': [Errno 2] No such file or directory
Traceback (most recent call last):
  File "/home/xj/xu/ssd.pytorch/eval.py", line 8, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
/home/xj/miniconda3/envs/ssd/lib/python3.11/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
/home/xj/xu/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  self.priors = Variable(self.priorbox.forward(), volatile=True)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [182], which does not match the required output shape [178]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [182], which does not match the required output shape [178]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [182], which does not match the required output shape [178]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [182], which does not match the required output shape [178]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [178], which does not match the required output shape [177]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [178], which does not match the required output shape [177]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [178], which does not match the required output shape [177]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [178], which does not match the required output shape [177]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [177], which does not match the required output shape [176]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [177], which does not match the required output shape [176]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [177], which does not match the required output shape [176]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [177], which does not match the required output shape [176]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [176], which does not match the required output shape [175]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [176], which does not match the required output shape [175]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [176], which does not match the required output shape [175]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [176], which does not match the required output shape [175]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [175], which does not match the required output shape [173]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [175], which does not match the required output shape [173]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [175], which does not match the required output shape [173]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [175], which does not match the required output shape [173]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [173], which does not match the required output shape [172]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [173], which does not match the required output shape [172]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [173], which does not match the required output shape [172]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [173], which does not match the required output shape [172]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [172], which does not match the required output shape [171]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [172], which does not match the required output shape [171]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [172], which does not match the required output shape [171]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [172], which does not match the required output shape [171]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [171], which does not match the required output shape [169]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [171], which does not match the required output shape [169]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [171], which does not match the required output shape [169]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [171], which does not match the required output shape [169]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [169], which does not match the required output shape [168]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [169], which does not match the required output shape [168]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [169], which does not match the required output shape [168]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [169], which does not match the required output shape [168]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [168], which does not match the required output shape [167]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [168], which does not match the required output shape [167]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [168], which does not match the required output shape [167]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [168], which does not match the required output shape [167]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [167], which does not match the required output shape [165]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [167], which does not match the required output shape [165]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [167], which does not match the required output shape [165]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [167], which does not match the required output shape [165]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [165], which does not match the required output shape [164]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [165], which does not match the required output shape [164]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [165], which does not match the required output shape [164]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [165], which does not match the required output shape [164]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [164], which does not match the required output shape [161]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [164], which does not match the required output shape [161]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [164], which does not match the required output shape [161]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [164], which does not match the required output shape [161]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [161], which does not match the required output shape [160]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [161], which does not match the required output shape [160]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [161], which does not match the required output shape [160]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [161], which does not match the required output shape [160]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [160], which does not match the required output shape [158]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [160], which does not match the required output shape [158]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [160], which does not match the required output shape [158]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [160], which does not match the required output shape [158]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [158], which does not match the required output shape [157]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [158], which does not match the required output shape [157]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [158], which does not match the required output shape [157]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [158], which does not match the required output shape [157]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [157], which does not match the required output shape [156]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [157], which does not match the required output shape [156]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [157], which does not match the required output shape [156]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [157], which does not match the required output shape [156]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [156], which does not match the required output shape [152]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [156], which does not match the required output shape [152]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [156], which does not match the required output shape [152]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [156], which does not match the required output shape [152]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [152], which does not match the required output shape [151]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [152], which does not match the required output shape [151]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [152], which does not match the required output shape [151]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [152], which does not match the required output shape [151]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [151], which does not match the required output shape [150]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [151], which does not match the required output shape [150]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [151], which does not match the required output shape [150]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [151], which does not match the required output shape [150]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [149]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [149]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [149]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [149]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [149], which does not match the required output shape [148]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [149], which does not match the required output shape [148]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [149], which does not match the required output shape [148]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [149], which does not match the required output shape [148]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [148], which does not match the required output shape [147]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [148], which does not match the required output shape [147]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [148], which does not match the required output shape [147]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [148], which does not match the required output shape [147]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [147], which does not match the required output shape [146]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [147], which does not match the required output shape [146]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [147], which does not match the required output shape [146]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [147], which does not match the required output shape [146]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [146], which does not match the required output shape [144]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [146], which does not match the required output shape [144]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [146], which does not match the required output shape [144]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [146], which does not match the required output shape [144]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [144], which does not match the required output shape [143]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [144], which does not match the required output shape [143]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [144], which does not match the required output shape [143]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [144], which does not match the required output shape [143]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [143], which does not match the required output shape [142]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [143], which does not match the required output shape [142]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [143], which does not match the required output shape [142]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [143], which does not match the required output shape [142]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [142], which does not match the required output shape [141]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [142], which does not match the required output shape [141]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [142], which does not match the required output shape [141]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [142], which does not match the required output shape [141]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [141], which does not match the required output shape [140]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [141], which does not match the required output shape [140]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [141], which does not match the required output shape [140]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [141], which does not match the required output shape [140]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [140], which does not match the required output shape [139]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [140], which does not match the required output shape [139]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [140], which does not match the required output shape [139]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [140], which does not match the required output shape [139]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [130]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [130]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [130]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [130]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [130], which does not match the required output shape [129]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [130], which does not match the required output shape [129]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [130], which does not match the required output shape [129]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [130], which does not match the required output shape [129]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [129], which does not match the required output shape [127]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [129], which does not match the required output shape [127]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [129], which does not match the required output shape [127]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [129], which does not match the required output shape [127]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [127], which does not match the required output shape [126]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [127], which does not match the required output shape [126]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [127], which does not match the required output shape [126]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [127], which does not match the required output shape [126]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [126], which does not match the required output shape [124]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [126], which does not match the required output shape [124]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [126], which does not match the required output shape [124]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [126], which does not match the required output shape [124]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [123]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [123]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [123]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [123]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [121]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [121]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [121]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [121]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [121], which does not match the required output shape [120]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [121], which does not match the required output shape [120]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [121], which does not match the required output shape [120]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [121], which does not match the required output shape [120]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [119]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [119]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [119]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [119]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [117]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [117]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [117]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [117]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [117], which does not match the required output shape [116]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [117], which does not match the required output shape [116]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [117], which does not match the required output shape [116]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [117], which does not match the required output shape [116]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [116], which does not match the required output shape [115]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [116], which does not match the required output shape [115]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [116], which does not match the required output shape [115]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [116], which does not match the required output shape [115]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [115], which does not match the required output shape [114]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [115], which does not match the required output shape [114]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [115], which does not match the required output shape [114]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [115], which does not match the required output shape [114]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [113]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [113]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [113]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [113]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [113], which does not match the required output shape [111]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [113], which does not match the required output shape [111]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [113], which does not match the required output shape [111]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [113], which does not match the required output shape [111]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [111], which does not match the required output shape [110]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [111], which does not match the required output shape [110]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [111], which does not match the required output shape [110]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [111], which does not match the required output shape [110]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [110], which does not match the required output shape [109]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [110], which does not match the required output shape [109]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [110], which does not match the required output shape [109]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [110], which does not match the required output shape [109]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [109], which does not match the required output shape [106]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [109], which does not match the required output shape [106]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [109], which does not match the required output shape [106]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [109], which does not match the required output shape [106]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [106], which does not match the required output shape [105]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [106], which does not match the required output shape [105]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [106], which does not match the required output shape [105]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [106], which does not match the required output shape [105]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [105], which does not match the required output shape [104]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [105], which does not match the required output shape [104]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [105], which does not match the required output shape [104]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [105], which does not match the required output shape [104]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [104], which does not match the required output shape [103]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [104], which does not match the required output shape [103]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [104], which does not match the required output shape [103]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [104], which does not match the required output shape [103]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [99]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [99]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [99]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [99]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [99], which does not match the required output shape [96]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [99], which does not match the required output shape [96]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [99], which does not match the required output shape [96]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [99], which does not match the required output shape [96]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [93]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [93]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [93]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [93]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [93], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [93], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [93], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [93], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [91]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [91]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [91]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [91]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [91], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [91], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [91], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [91], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [88]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [88]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [88]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [88]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [85], which does not match the required output shape [83]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [85], which does not match the required output shape [83]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [85], which does not match the required output shape [83]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [85], which does not match the required output shape [83]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [81]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [81]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [81]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [81]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [81], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [81], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [81], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [81], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [79]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [79]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [79]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [79]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [78]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [78]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [78]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [78]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [76]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [76]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [76]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [76]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [76], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [76], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [76], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [76], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [70]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [70]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [70]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [70]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [70], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [70], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [70], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [70], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [68]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [68]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [68]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [68]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [67]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [67]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [67]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [67]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [66]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [66]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [66]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [66]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [63]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [63]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [63]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [63]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [62]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [62]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [62]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [62]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [60]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [60]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [60]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [60]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [59]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [59]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [59]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [59]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [57], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [57], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [57], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [57], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [53]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [53]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [53]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [53]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [49]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [49]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [49]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [49]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [47]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [47]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [47]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [47]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [43]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [43]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [43]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [43]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [33]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [33]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [33]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [33]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [178], which does not match the required output shape [154]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [178], which does not match the required output shape [154]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [178], which does not match the required output shape [154]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [178], which does not match the required output shape [154]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [154], which does not match the required output shape [142]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [154], which does not match the required output shape [142]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [154], which does not match the required output shape [142]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [154], which does not match the required output shape [142]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [141], which does not match the required output shape [131]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [141], which does not match the required output shape [131]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [141], which does not match the required output shape [131]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [141], which does not match the required output shape [131]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [131], which does not match the required output shape [127]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [131], which does not match the required output shape [127]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [131], which does not match the required output shape [127]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [131], which does not match the required output shape [127]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [127], which does not match the required output shape [125]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [127], which does not match the required output shape [125]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [127], which does not match the required output shape [125]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [127], which does not match the required output shape [125]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [125], which does not match the required output shape [124]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [125], which does not match the required output shape [124]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [125], which does not match the required output shape [124]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [125], which does not match the required output shape [124]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [122]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [122]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [122]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [122]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [122], which does not match the required output shape [121]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [122], which does not match the required output shape [121]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [122], which does not match the required output shape [121]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [122], which does not match the required output shape [121]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [118]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [118]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [118]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [118]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [116]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [116]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [116]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [116]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [113], which does not match the required output shape [112]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [113], which does not match the required output shape [112]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [113], which does not match the required output shape [112]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [113], which does not match the required output shape [112]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [112], which does not match the required output shape [110]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [112], which does not match the required output shape [110]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [112], which does not match the required output shape [110]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [112], which does not match the required output shape [110]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [110], which does not match the required output shape [106]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [110], which does not match the required output shape [106]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [110], which does not match the required output shape [106]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [110], which does not match the required output shape [106]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [106], which does not match the required output shape [104]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [106], which does not match the required output shape [104]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [106], which does not match the required output shape [104]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [106], which does not match the required output shape [104]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [99], which does not match the required output shape [98]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [99], which does not match the required output shape [98]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [99], which does not match the required output shape [98]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [99], which does not match the required output shape [98]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [98], which does not match the required output shape [97]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [98], which does not match the required output shape [97]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [98], which does not match the required output shape [97]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [98], which does not match the required output shape [97]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [97], which does not match the required output shape [96]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [97], which does not match the required output shape [96]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [97], which does not match the required output shape [96]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [97], which does not match the required output shape [96]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [89]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [89]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [89]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [89]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [89], which does not match the required output shape [88]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [89], which does not match the required output shape [88]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [89], which does not match the required output shape [88]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [89], which does not match the required output shape [88]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [87]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [87]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [87]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [87]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [86]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [86]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [86]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [86]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [86], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [86], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [86], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [86], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [85], which does not match the required output shape [84]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [85], which does not match the required output shape [84]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [85], which does not match the required output shape [84]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [85], which does not match the required output shape [84]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [84], which does not match the required output shape [83]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [84], which does not match the required output shape [83]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [84], which does not match the required output shape [83]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [84], which does not match the required output shape [83]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [77]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [77]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [77]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [77]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [77], which does not match the required output shape [76]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [77], which does not match the required output shape [76]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [77], which does not match the required output shape [76]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [77], which does not match the required output shape [76]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [76], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [76], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [76], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [76], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [74]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [74]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [74]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [74]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [74], which does not match the required output shape [73]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [74], which does not match the required output shape [73]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [74], which does not match the required output shape [73]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [74], which does not match the required output shape [73]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [73], which does not match the required output shape [72]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [73], which does not match the required output shape [72]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [73], which does not match the required output shape [72]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [73], which does not match the required output shape [72]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [72], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [72], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [72], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [72], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [70], which does not match the required output shape [68]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [70], which does not match the required output shape [68]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [70], which does not match the required output shape [68]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [70], which does not match the required output shape [68]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [65]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [65]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [65]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [65]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [65], which does not match the required output shape [64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [65], which does not match the required output shape [64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [65], which does not match the required output shape [64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [65], which does not match the required output shape [64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [53]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [53]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [53]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [53]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [38]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [38]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [38]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [38]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [99], which does not match the required output shape [70]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [99], which does not match the required output shape [70]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [99], which does not match the required output shape [70]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [99], which does not match the required output shape [70]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [70], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [70], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [70], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [70], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [89], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [89], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [89], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [89], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [78]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [78]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [78]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [78]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [73]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [73]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [73]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [73]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [73], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [73], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [73], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [73], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [66]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [66]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [66]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [66]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [177]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [177]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [177]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [177]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [177], which does not match the required output shape [125]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [177], which does not match the required output shape [125]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [177], which does not match the required output shape [125]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [177], which does not match the required output shape [125]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [125], which does not match the required output shape [77]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [125], which does not match the required output shape [77]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [125], which does not match the required output shape [77]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [125], which does not match the required output shape [77]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [77], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [77], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [77], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [77], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [84], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [84], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [84], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [84], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [26]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [142], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [142], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [142], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [142], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [149]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [149]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [149]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [149]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [149], which does not match the required output shape [109]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [149], which does not match the required output shape [109]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [149], which does not match the required output shape [109]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [149], which does not match the required output shape [109]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [109], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [109], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [109], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [109], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [106], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [106], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [106], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [106], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [63]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [63]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [63]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [63]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [58]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [53]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [53]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [53]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [53]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [38]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [38]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [38]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [38]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [168]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [168]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [168]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [168]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [168], which does not match the required output shape [136]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [168], which does not match the required output shape [136]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [168], which does not match the required output shape [136]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [168], which does not match the required output shape [136]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [136], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [136], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [136], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [136], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [67]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [67]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [67]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [67]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [143], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [143], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [143], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [143], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [103]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [103]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [103]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [103]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [65], which does not match the required output shape [63]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [65], which does not match the required output shape [63]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [65], which does not match the required output shape [63]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [65], which does not match the required output shape [63]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [59]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [59]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [59]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [59]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [157], which does not match the required output shape [131]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [157], which does not match the required output shape [131]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [157], which does not match the required output shape [131]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [157], which does not match the required output shape [131]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [131], which does not match the required output shape [128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [131], which does not match the required output shape [128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [131], which does not match the required output shape [128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [131], which does not match the required output shape [128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [128], which does not match the required output shape [119]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [128], which does not match the required output shape [119]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [128], which does not match the required output shape [119]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [128], which does not match the required output shape [119]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [114]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [114]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [114]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [114]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [107]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [107]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [107]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [107]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [107], which does not match the required output shape [105]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [107], which does not match the required output shape [105]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [107], which does not match the required output shape [105]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [107], which does not match the required output shape [105]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [105], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [105], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [105], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [105], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [77]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [77]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [77]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [77]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [76], which does not match the required output shape [74]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [76], which does not match the required output shape [74]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [76], which does not match the required output shape [74]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [76], which does not match the required output shape [74]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [74], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [74], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [74], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [74], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [67]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [67]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [67]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [67]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [92]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [72]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [72]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [72]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [72]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [72], which does not match the required output shape [65]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [72], which does not match the required output shape [65]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [72], which does not match the required output shape [65]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [72], which does not match the required output shape [65]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [59]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [59]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [59]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [59]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [49]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [49]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [49]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [49]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [47]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [47]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [47]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [47]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [44]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [155]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [155]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [155]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [155]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [155], which does not match the required output shape [87]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [155], which does not match the required output shape [87]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [155], which does not match the required output shape [87]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [155], which does not match the required output shape [87]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [121]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [121]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [121]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [121]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [121], which does not match the required output shape [110]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [121], which does not match the required output shape [110]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [121], which does not match the required output shape [110]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [121], which does not match the required output shape [110]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [110], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [110], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [110], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [110], which does not match the required output shape [102]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [91]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [91]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [91]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [91]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [85]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [85], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [85], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [85], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [85], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [79]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [79]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [79]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [79]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [74]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [74]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [74]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [74]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [65]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [65]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [65]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [65]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [153], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [153], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [153], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [153], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [36]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [97]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [97]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [97]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [97]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [97], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [97], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [97], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [97], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [127]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [127]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [127]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [127]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [127], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [127], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [127], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [127], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [116], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [116], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [116], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [116], which does not match the required output shape [82]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [43]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [43]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [43]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [43]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [37]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [43]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [43]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [43]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [43]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [129], which does not match the required output shape [78]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [129], which does not match the required output shape [78]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [129], which does not match the required output shape [78]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [129], which does not match the required output shape [78]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [78], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [9]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [113], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [113], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [113], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [113], which does not match the required output shape [42]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [76]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [76]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [76]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [76]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [60]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [60]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [60]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [75], which does not match the required output shape [60]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [54]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [52]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [173]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [173]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [173]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [173]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [173], which does not match the required output shape [138]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [173], which does not match the required output shape [138]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [173], which does not match the required output shape [138]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [173], which does not match the required output shape [138]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [138], which does not match the required output shape [108]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [138], which does not match the required output shape [108]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [138], which does not match the required output shape [108]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [138], which does not match the required output shape [108]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [87]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [87]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [87]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [87]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [23]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [39]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [68]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [68]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [68]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [68]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [55]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [48]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [132]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [132]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [132]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [132]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [132], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [132], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [132], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [132], which does not match the required output shape [80]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [71]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [46]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [77], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [77], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [77], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [77], which does not match the required output shape [35]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [27]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [151], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [151], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [151], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [151], which does not match the required output shape [95]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [186]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [186]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [186]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [199], which does not match the required output shape [186]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [186], which does not match the required output shape [160]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [186], which does not match the required output shape [160]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [186], which does not match the required output shape [160]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [186], which does not match the required output shape [160]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [160], which does not match the required output shape [136]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [160], which does not match the required output shape [136]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [160], which does not match the required output shape [136]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [160], which does not match the required output shape [136]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [136], which does not match the required output shape [123]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [136], which does not match the required output shape [123]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [136], which does not match the required output shape [123]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [136], which does not match the required output shape [123]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [120]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [120]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [120]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [120]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [113]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [113]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [113]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [113]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [112], which does not match the required output shape [107]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [112], which does not match the required output shape [107]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [112], which does not match the required output shape [107]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [112], which does not match the required output shape [107]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [107], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [107], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [107], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [107], which does not match the required output shape [101]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [96]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [96]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [96]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [96]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [96], which does not match the required output shape [94]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [94], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [81], which does not match the required output shape [79]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [81], which does not match the required output shape [79]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [81], which does not match the required output shape [79]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [81], which does not match the required output shape [79]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [41]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [82], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [24]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [129], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [129], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [129], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [129], which does not match the required output shape [90]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [56]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [31]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [18]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [15]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [13]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [22]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [29]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [17]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [10]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [25]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [11]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [130], which does not match the required output shape [119]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [130], which does not match the required output shape [119]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [130], which does not match the required output shape [119]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [130], which does not match the required output shape [119]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [111]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [111]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [111]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [111]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [111], which does not match the required output shape [105]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [111], which does not match the required output shape [105]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [111], which does not match the required output shape [105]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [111], which does not match the required output shape [105]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [104], which does not match the required output shape [99]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [104], which does not match the required output shape [99]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [104], which does not match the required output shape [99]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [104], which does not match the required output shape [99]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [98], which does not match the required output shape [91]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [98], which does not match the required output shape [91]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [98], which does not match the required output shape [91]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [98], which does not match the required output shape [91]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [91], which does not match the required output shape [88]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [91], which does not match the required output shape [88]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [91], which does not match the required output shape [88]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [91], which does not match the required output shape [88]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [86]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [86]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [86]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [86]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [86], which does not match the required output shape [81]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [86], which does not match the required output shape [81]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [86], which does not match the required output shape [81]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [86], which does not match the required output shape [81]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [75]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [72], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [72], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [72], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [72], which does not match the required output shape [69]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [61]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [57]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [45]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [33]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [33]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [33]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [33]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [185], which does not match the required output shape [108]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [185], which does not match the required output shape [108]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [185], which does not match the required output shape [108]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [185], which does not match the required output shape [108]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [34]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [28]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [21]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:216: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x1, 0, idx, out=xx1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:217: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y1, 0, idx, out=yy1)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:218: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(x2, 0, idx, out=xx2)
/home/xj/xu/ssd.pytorch/layers/box_utils.py:219: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)
  torch.index_select(y2, 0, idx, out=yy2)
Finished loading model!
im_detect: 1/114 2.270s
im_detect: 2/114 0.056s
im_detect: 3/114 0.010s
im_detect: 4/114 0.014s
im_detect: 5/114 0.021s
im_detect: 6/114 0.032s
im_detect: 7/114 0.007s
im_detect: 8/114 0.008s
im_detect: 9/114 0.010s
im_detect: 10/114 0.012s
im_detect: 11/114 0.015s
im_detect: 12/114 0.013s
im_detect: 13/114 0.005s
im_detect: 14/114 0.021s
im_detect: 15/114 0.009s
im_detect: 16/114 0.017s
im_detect: 17/114 0.007s
im_detect: 18/114 0.007s
im_detect: 19/114 0.008s
im_detect: 20/114 0.010s
im_detect: 21/114 0.021s
im_detect: 22/114 0.026s
im_detect: 23/114 0.006s
im_detect: 24/114 0.006s
im_detect: 25/114 0.013s
im_detect: 26/114 0.017s
im_detect: 27/114 0.012s
im_detect: 28/114 0.009s
im_detect: 29/114 0.011s
im_detect: 30/114 0.030s
im_detect: 31/114 0.039s
im_detect: 32/114 0.008s
im_detect: 33/114 0.011s
im_detect: 34/114 0.010s
im_detect: 35/114 0.010s
im_detect: 36/114 0.024s
im_detect: 37/114 0.005s
im_detect: 38/114 0.024s
im_detect: 39/114 0.006s
im_detect: 40/114 0.008s
im_detect: 41/114 0.008s
im_detect: 42/114 0.006s
im_detect: 43/114 0.012s
im_detect: 44/114 0.038s
im_detect: 45/114 0.013s
im_detect: 46/114 0.030s
im_detect: 47/114 0.012s
im_detect: 48/114 0.018s
im_detect: 49/114 0.007s
im_detect: 50/114 0.009s
im_detect: 51/114 0.012s
im_detect: 52/114 0.006s
im_detect: 53/114 0.015s
im_detect: 54/114 0.011s
im_detect: 55/114 0.009s
im_detect: 56/114 0.005s
im_detect: 57/114 0.010s
im_detect: 58/114 0.020s
im_detect: 59/114 0.012s
im_detect: 60/114 0.021s
im_detect: 61/114 0.010s
im_detect: 62/114 0.006s
im_detect: 63/114 0.006s
im_detect: 64/114 0.007s
im_detect: 65/114 0.006s
im_detect: 66/114 0.011s
im_detect: 67/114 0.008s
im_detect: 68/114 0.025s
im_detect: 69/114 0.021s
im_detect: 70/114 0.007s
im_detect: 71/114 0.005s
im_detect: 72/114 0.009s
im_detect: 73/114 0.010s
im_detect: 74/114 0.007s
im_detect: 75/114 0.009s
im_detect: 76/114 0.010s
im_detect: 77/114 0.025s
im_detect: 78/114 0.015s
im_detect: 79/114 0.016s
im_detect: 80/114 0.007s
im_detect: 81/114 0.006s
im_detect: 82/114 0.007s
im_detect: 83/114 0.005s
im_detect: 84/114 0.012s
im_detect: 85/114 0.012s
im_detect: 86/114 0.009s
im_detect: 87/114 0.014s
im_detect: 88/114 0.008s
im_detect: 89/114 0.006s
im_detect: 90/114 0.009s
im_detect: 91/114 0.047s
im_detect: 92/114 0.008s
im_detect: 93/114 0.007s
im_detect: 94/114 0.010s
im_detect: 95/114 0.005s
im_detect: 96/114 0.009s
im_detect: 97/114 0.009s
im_detect: 98/114 0.005s
im_detect: 99/114 0.007s
im_detect: 100/114 0.006s
im_detect: 101/114 0.005s
im_detect: 102/114 0.016s
im_detect: 103/114 0.004s
im_detect: 104/114 0.004s
im_detect: 105/114 0.005s
im_detect: 106/114 0.010s
im_detect: 107/114 0.007s
im_detect: 108/114 0.006s
im_detect: 109/114 0.033s
im_detect: 110/114 0.006s
im_detect: 111/114 0.012s
im_detect: 112/114 0.006s
im_detect: 113/114 0.006s
im_detect: 114/114 0.006s
Evaluating detections
Writing cellosilk VOC results file
Writing black_spot VOC results file
Writing fragment VOC results file
Writing crack VOC results file
Writing scratching VOC results file
VOC07 metric? Yes
AP for cellosilk = 0.8390
AP for black_spot = 0.7311
AP for fragment = 0.8385
AP for crack = 0.9091
AP for scratching = 0.9005
Mean AP = 0.8436
~~~~~~~~
Results:
0.839
0.731
0.839
0.909
0.900
0.844
~~~~~~~~

--------------------------------------------------------------
Results computed with the **unofficial** Python eval code.
Results should be very close to the official MATLAB eval code.
--------------------------------------------------------------
